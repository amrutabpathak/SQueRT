{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ParallelDots_ContextSimilarityTools",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkT2T7pqIeyO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "8a612d83-9069-401a-8c71-c747854f4264"
      },
      "source": [
        "pip install paralleldots\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: paralleldots in /usr/local/lib/python3.6/dist-packages (3.2.13)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from paralleldots) (2.21.0)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->paralleldots) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->paralleldots) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->paralleldots) (2019.11.28)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->paralleldots) (1.24.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9Bk0iLDI4Lx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import paralleldots\n",
        "paralleldots.set_api_key(\"YITjlO9WQSOys9HI9iyEA4grgsnphwrC5s6d7Pv8Ads\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hq0yfiwcJCRF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "034030e8-d1b4-45d3-9590-7da66e7ede01"
      },
      "source": [
        "paralleldots.get_api_key()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'YITjlO9WQSOys9HI9iyEA4grgsnphwrC5s6d7Pv8Ads'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0eKO1iKmJPuf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "outputId": "37bd3cb6-a8bb-4e87-e52b-4cbc772024e2"
      },
      "source": [
        "greets = [\"We introduce BERT and its detailed implementation in this section. There are two steps in our framework: pre-training and fine-tuning. During pre-training, the model is trained on unlabeled data over different pre-training tasks. For finetuning, the BERT model is first initialized with the pre-trained parameters, and all of the parameters are fine-tuned using labeled data from the downstream tasks. Each downstream task has separate\"+\n",
        "                  \"fine-tuned models, even though they are initialize with the same pre-trained parameters. The question-answering example in Figure 1 will serve as a running example for this section.\",\n",
        "                  \"We find that BERTLARGE significantly outperforms BERTBASE across all tasks, especially those with very little training data. The effect of model size is explored more thoroughly in Section\",\n",
        "                  \"we regret for your inconvenience\",\n",
        "                  \"he numbers in the left part of the table represent the probabilities of the specific strategies used during MLM pre-training (BERT uses 80%, 10%,10%). The right part of the paper represents the Dev set results. For the feature-based approach, we concatenate the last 4 layers of BERT as thefeatures, which was shown to be the best approach in Section\",\n",
        "                  'Top of the morning to you!',\n",
        "                  'Hi',\n",
        "                  'How are you doing?',\n",
        "                  'Hello','Boring lecture',\n",
        "                  'Greetings!',\n",
        "                  'Hi, How is it going?',\n",
        "                  'Hi, nice to meet you.',\n",
        "                  'Nice to meet you.']\n",
        "\n",
        "for greet in greets:\n",
        "    print(paralleldots.similarity( greet, \"What model or approach is best?\"))\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'similarity_score': 0.1346322149}\n",
            "{'similarity_score': 0.3041133285}\n",
            "{'similarity_score': 0.4131705165}\n",
            "{'similarity_score': 0.2037199289}\n",
            "{'similarity_score': 0.3930695057}\n",
            "{'Error': 'Parameter `text_1` cannot have fewer than 2 words.', 'code': 400}\n",
            "{'similarity_score': 0.5352706909}\n",
            "{'Error': 'Parameter `text_1` cannot have fewer than 2 words.', 'code': 400}\n",
            "{'similarity_score': 0.3055790365}\n",
            "{'Error': 'Parameter `text_1` cannot have fewer than 2 words.', 'code': 400}\n",
            "{'similarity_score': 0.5574709773}\n",
            "{'similarity_score': 0.4339035749}\n",
            "{'similarity_score': 0.4284186959}\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}