{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLProj_ExploringGoogleUSE",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtHwfK6EAHmk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_w5I0SnRBF8w",
        "colab_type": "code",
        "outputId": "b8437486-0221-454f-c2b4-55e8c0271a90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        }
      },
      "source": [
        "#download the model to local so it can be used again and again\n",
        "!mkdir /content/module_useT\n",
        "# Download the module, and uncompress it to the destination folder. \n",
        "!curl -L \"https://tfhub.dev/google/universal-sentence-encoder-large/3?tf-hub-format=compressed\" | tar -zxvC /content/module_useT"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘/content/module_useT’: File exists\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "./\n",
            "./tfhub_module.pb\n",
            "./variables/\n",
            "./variables/variables.data-00000-of-00001\n",
            " 95  745M   95  714M    0     0  42.3M      0  0:00:17  0:00:16  0:00:01 37.4M./variables/variables.index\n",
            "./assets/\n",
            "./saved_model.pb\n",
            "100  745M  100  745M    0     0  42.8M      0  0:00:17  0:00:17 --:--:-- 47.5M\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bX5bpWLBCMFE",
        "colab_type": "code",
        "outputId": "edbbae76-aa3b-4acd-91c6-6b36656e0110",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "#Function so that one session can be called multiple times. \n",
        "#Useful while multiple calls need to be done for embedding. \n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "def embed_useT(module):\n",
        "    with tf.Graph().as_default():\n",
        "        sentences = tf.placeholder(tf.string)\n",
        "        embed = hub.Module(module)\n",
        "        embeddings = embed(sentences)\n",
        "        session = tf.train.MonitoredSession()\n",
        "    return lambda x: session.run(embeddings, {sentences: x})\n",
        "embed_fn = embed_useT('/content/module_useT')\n",
        "messages = [\n",
        "    \"We introduce BERT and its detailed implementation in this section. There are two steps in our framework: pre-training and fine-tuning. During pre-training, the model is trained on unlabeled data over different pre-training tasks. For finetuning, the BERT model is first initialized with the pre-trained parameters, and all of the parameters are fine-tuned using labeled data from the downstream tasks. Each downstream task has separate\"+\n",
        "\"fine-tuned models, even though they are initialize with the same pre-trained parameters. The question-answering example in Figure 1 will serve as a running example for this section.\",\n",
        "    \"We find that BERTLARGE significantly outperforms BERTBASE across all tasks, especially those with very little training data. The effect of model size is explored more thoroughly in Section\",\n",
        "    \"we regret for your inconvenience\",\n",
        "    \"we don't deliver to baner region in pune\",\n",
        "    \"we will get you the best possible rate\"\n",
        "]\n",
        "embed_fn(messages)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.00021285, -0.04443965,  0.03926146, ...,  0.04830163,\n",
              "        -0.11913302,  0.03370818],\n",
              "       [ 0.01778567, -0.05192031, -0.0079232 , ..., -0.01799904,\n",
              "        -0.09819185,  0.0602073 ],\n",
              "       [-0.00748425, -0.0240158 ,  0.01762747, ...,  0.09334017,\n",
              "        -0.11837558,  0.00603596],\n",
              "       [-0.03505318, -0.01932573, -0.0324861 , ...,  0.0035643 ,\n",
              "        -0.082398  ,  0.03887842],\n",
              "       [-0.05111799, -0.03090659,  0.0354201 , ..., -0.01343898,\n",
              "        -0.10434883, -0.03150062]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tce4CDTiG7G9",
        "colab_type": "code",
        "outputId": "6fb172ce-5c2c-4b7e-ab67-92efbee260b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        }
      },
      "source": [
        "greets = [\"We introduce BERT and its detailed implementation in this section. There are two steps in our framework: pre-training and fine-tuning. During pre-training, the model is trained on unlabeled data over different pre-training tasks. For finetuning, the BERT model is first initialized with the pre-trained parameters, and all of the parameters are fine-tuned using labeled data from the downstream tasks. Each downstream task has separate\"+\n",
        "\"fine-tuned models, even though they are initialize with the same pre-trained parameters. The question-answering example in Figure 1 will serve as a running example for this section.\",\n",
        "    \"We find that BERTLARGE significantly outperforms BERTBASE across all tasks, especially those with very little training data. The effect of model size is explored more thoroughly in Section\",\n",
        "    \"we regret for your inconvenience\",\n",
        " \"he numbers in the left part of the table represent the probabilities of the specific strategies used during MLM pre-training (BERT uses 80%, 10%,10%). The right part of the paper represents the Dev set results. For the feature-based approach, we concatenate the last 4 layers of BERT as thefeatures, which was shown to be the best approach in Section\",\n",
        " 'Top of the morning to you!',\n",
        " 'Hi',\n",
        " 'How are you doing?',\n",
        " 'Hello','Boring lecture',\n",
        " 'Greetings!',\n",
        " 'Hi, How is it going?',\n",
        " 'Hi, nice to meet you.',\n",
        " 'Nice to meet you.']\n",
        "greet_matrix = embed_fn(greets)\n",
        "#print(greet_matrix)\n",
        "test_text = \"What model or approach is best?\"\n",
        "test_embed = embed_fn([test_text])\n",
        "np.inner(test_embed, greet_matrix)\n",
        "sim_matrix  = np.inner( greet_matrix,test_embed)\n",
        "print(sim_matrix)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0.38421667]\n",
            " [ 0.45192227]\n",
            " [-0.00742567]\n",
            " [ 0.3050881 ]\n",
            " [-0.02701793]\n",
            " [ 0.1511176 ]\n",
            " [ 0.12064075]\n",
            " [ 0.15520406]\n",
            " [ 0.1858898 ]\n",
            " [ 0.09915753]\n",
            " [ 0.16401812]\n",
            " [ 0.11464099]\n",
            " [ 0.08677776]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}