{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SwoogleUMBC_ContextSimilarityTool",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHWggLNJxGYA",
        "colab_type": "code",
        "outputId": "41ffd1df-f27c-492d-a67e-a8f38be128a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        }
      },
      "source": [
        "from requests import get\n",
        "sss_url = \"http://swoogle.umbc.edu/SimService/GetSimilarity\"\n",
        "def sss( s2, type='relation', corpus='webbase'):\n",
        "    try:\n",
        "      greets = [\"We introduce BERT and its detailed implementation in this section. There are two steps in our framework: pre-training and fine-tuning. During pre-training, the model is trained on unlabeled data over different pre-training tasks. For finetuning, the BERT model is first initialized with the pre-trained parameters, and all of the parameters are fine-tuned using labeled data from the downstream tasks. Each downstream task has separate\"+\n",
        "                  \"fine-tuned models, even though they are initialize with the same pre-trained parameters. The question-answering example in Figure 1 will serve as a running example for this section.\",\n",
        "                  \"We find that BERTLARGE significantly outperforms BERTBASE across all tasks, especially those with very little training data. The effect of model size is explored more thoroughly in Section\",\n",
        "                  \"we regret for your inconvenience\",\n",
        "                  \"he numbers in the left part of the table represent the probabilities of the specific strategies used during MLM pre-training (BERT uses 80%, 10%,10%). The right part of the paper represents the Dev set results. For the feature-based approach, we concatenate the last 4 layers of BERT as thefeatures, which was shown to be the best approach in Section\",\n",
        "                  'Top of the morning to you!',\n",
        "                  'Hi',\n",
        "                  'How are you doing?',\n",
        "                  'Hello','Boring lecture',\n",
        "                  'Greetings!',\n",
        "                  'Hi, How is it going?',\n",
        "                  'Hi, nice to meet you.',\n",
        "                  'Nice to meet you.']\n",
        "\n",
        "      for greet in greets:\n",
        "         s1 = greet                   \n",
        "         response = get(sss_url, params={'operation':'api','phrase1':s1,'phrase2':s2,'type':type,'corpus':corpus})\n",
        "         print(float(response.text.strip()))\n",
        "    except:\n",
        "        print(\"Error in getting similarity for\")\n",
        "        return 0.0\n",
        "\n",
        "sss('What model or approach is best?')        "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.3516132\n",
            "0.33909008\n",
            "0.04657261\n",
            "0.5039076\n",
            "0.16541593\n",
            "-inf\n",
            "0.014515708\n",
            "-inf\n",
            "0.017908268\n",
            "0.07628353\n",
            "0.10656741\n",
            "0.21128519\n",
            "0.26031926\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}