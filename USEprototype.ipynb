{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS7800.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "yD5wmoFVoL98",
        "colab_type": "code",
        "outputId": "5a6e1a93-d7fb-46ad-a3e8-4395882ce414",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        }
      },
      "source": [
        "pip install tika"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tika\n",
            "  Downloading https://files.pythonhosted.org/packages/9a/c3/088827903bc1862f67b185e1df428071b8da6118155c1b46bcb0c61992ea/tika-1.23.1.tar.gz\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from tika) (45.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from tika) (2.21.0)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->tika) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->tika) (2019.11.28)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->tika) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->tika) (1.24.3)\n",
            "Building wheels for collected packages: tika\n",
            "  Building wheel for tika (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tika: filename=tika-1.23.1-cp36-none-any.whl size=32561 sha256=763574a8d7c5e5e3a592eec6c8af54bb40034fcf8ae363207f32986799aba08e\n",
            "  Stored in directory: /root/.cache/pip/wheels/fd/6b/6d/c850c2a934057edce9779d41400d910c6a9b1f22027566b10f\n",
            "Successfully built tika\n",
            "Installing collected packages: tika\n",
            "Successfully installed tika-1.23.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFoUogMjzMGu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mkdir USE/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDJs0KyXzurd",
        "colab_type": "code",
        "outputId": "9c9e8d2f-e7a6-40eb-d11f-8252cf3f5434",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        }
      },
      "source": [
        "!curl -L 'https://tfhub.dev/google/universal-sentence-encoder-large/3?tf-hub-format=compressed' | tar -zxvC USE/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "\r  0  745M    0  130k    0     0   688k      0  0:18:29 --:--:--  0:18:29  688k./\n",
            "./tfhub_module.pb\n",
            "./variables/\n",
            "./variables/variables.data-00000-of-00001\n",
            " 95  745M   95  712M    0     0  43.9M      0  0:00:16  0:00:16 --:--:-- 40.1M./variables/variables.index\n",
            "./assets/\n",
            "./saved_model.pb\n",
            "100  745M  100  745M    0     0  44.1M      0  0:00:16  0:00:16 --:--:-- 42.1M\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X36Qf2F71hdw",
        "colab_type": "code",
        "outputId": "ffa1ff0e-9da1-4004-c705-67d98bc08a6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "def embed_useT(module):\n",
        "    with tf.Graph().as_default():\n",
        "        sentences = tf.placeholder(tf.string)\n",
        "        embed = hub.Module(module)\n",
        "        embeddings = embed(sentences)\n",
        "        session = tf.train.MonitoredSession()\n",
        "    return lambda x: session.run(embeddings, {sentences: x})\n",
        "embed_fn = embed_useT('USE/')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8OJ8G0Ta1CyA",
        "colab_type": "code",
        "outputId": "3acb3a76-cef0-4208-f706-40f1d9971a20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import numpy as np\n",
        "\n",
        "greets = [\"What's up?\",\n",
        " 'It is a pleasure to meet you.',\n",
        " 'How do you do?',\n",
        " 'Top of the morning to you!',\n",
        " 'Hi',\n",
        " 'How are you doing?',\n",
        " 'Hello',\n",
        " 'Greetings!',\n",
        " 'Hi, How is it going?',\n",
        " 'Hi, nice to meet you.',\n",
        " 'Nice to meet you.']\n",
        "greet_matrix = embed_fn(greets)\n",
        "test_text = \"Hey, how are you?\"\n",
        "test_embed = embed_fn([test_text])\n",
        "np.inner(test_embed, greet_matrix)\n",
        "sim_matrix  = np.inner(test_embed, greet_matrix)\n",
        "if sim_matrix.max() > 0.8:\n",
        "    print(\"it is a greetings\")\n",
        "else:\n",
        "    print(\"it is not a greetings\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "it is a greetings\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gVbba0-Nx_t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mkdir data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6CIOYZXNoxIG",
        "colab_type": "code",
        "outputId": "28ce8543-30e1-486a-bdc9-3bb5431f5ff4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 135
        }
      },
      "source": [
        "from tika import parser\n",
        "import glob\n",
        "import re\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# question = [\"What model was used in this paper and what was the model's performance\"]\n",
        "question = ['What is the architecture of the models used?']\n",
        "question_matrix = embed_fn(question)\n",
        "dir = 'data/*.pdf'\n",
        "# print(glob.glob('*/'))\n",
        "# print(glob.glob(dir))\n",
        "for research_paper_name in glob.glob(dir):\n",
        "  print(research_paper_name)\n",
        "  research_paper_data = parser.from_file(research_paper_name)\n",
        "  research_paper_content = research_paper_data['content']\n",
        "  paragraphs = research_paper_content.split(\". \")\n",
        "  # print(paragraphs)\n",
        "  research_paper_txt_name = research_paper_name.replace(\"pdf\",\"txt\")\n",
        "  rp_file= open(research_paper_txt_name,\"w+\")\n",
        "  data_file= open('data/data.txt',\"a+\")\n",
        "  edited_paragraphs = []\n",
        "  for i in range(len(paragraphs)):\n",
        "    rp_file.write(str(paragraphs[i]))\n",
        "    if paragraphs[i] != ' ':\n",
        "      edited_paragraphs.append(paragraphs[i])\n",
        "    if len(edited_paragraphs) != 0:\n",
        "      edited_paragraphs[len(edited_paragraphs)-1] = re.sub(r\"\\n\", \"\", edited_paragraphs[len(edited_paragraphs)-1] )\n",
        "  data_file.write(research_paper_txt_name + '\\n\\n')\n",
        "  data_file.write('------------------------------- \\n')\n",
        "  for paragraph in edited_paragraphs:\n",
        "    test_text = paragraph\n",
        "    test_embed = embed_fn([test_text])\n",
        "    np.inner(test_embed, question_matrix)\n",
        "    sim_matrix  = np.inner(test_embed, question_matrix)\n",
        "    if sim_matrix.max() > 0.3:\n",
        "        # print(paragraph)\n",
        "        data_file.write(paragraph + '\\n\\n')\n",
        "    else:\n",
        "      pass\n",
        "  data_file.write(''' \n",
        "  \n",
        "\n",
        "  \n",
        "  '''\n",
        "  )\n",
        "  rp_file.close()\n",
        "data_file.close()\n",
        "print(\"Done!\")\n",
        "        \n",
        "  \n",
        "  # print(edited_paragraphs)\n",
        "  \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data/sharing_knowledge_in_multi_task_deep_reinforcement_learning.pdf\n",
            "data/sqil_imitation_learning_via_reinforcement_learning_with_sparse_rewards.pdf\n",
            "data/structpool_structured_graph_pooling_via_conditional_random_fields.pdf\n",
            "data/reinforcement_learning_based_graph_to_sequence_model_for_natural_question_generation.pdf\n",
            "data/self_learning_to_filter_noisy_labels_with_self_ensembling.pdf\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}