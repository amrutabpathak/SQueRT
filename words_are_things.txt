published as a conference paper at iclr 2020\n\nlarge batch optimization for deep learning : \ntraining bert in 76 minutes\n\nyang you2 , jing li1 , sashank reddi1 , jonathan hseu1 , sanjiv kumar1 , srinadh bhojanapalli1\nxiaodan song1 , james demmel2 , kurt keutzer2 , cho-jui hsieh1,3\nyang you was a student researcher at google brain . this project was done when he was at google brain.\ngoogle1 , uc berkeley2 , ucla3\n { youyang , demmel , keutzer } @ cs.berkeley.edu , { jingli , sashank , jhseu , sanjivk , bsrinadh , xiaodansong , chojui } @ google.com\n\nabstract\n\ntraining large deep neural networks on massive datasets is computationally very\nchallenging . there has been recent surge in interest in using large batch stochastic\noptimization methods to tackle this issue . the most prominent algorithm in this\nline of research is lars , which by employing layerwise adaptive learning rates\ntrains resnet on imagenet in a few minutes . however , lars performs poorly for\nattention models like bert , indicating that its performance gains are not consistent\nacross tasks . in this paper , we ﬁrst study a principled layerwise adaptation strategy\nto accelerate training of deep neural networks using large mini-batches . using this\nstrategy , we develop a new layerwise adaptive large batch optimization technique\ncalled lamb ; we then provide convergence analysis of lamb as well as lars , \nshowing convergence to a stationary point in general nonconvex settings . our\nempirical results demonstrate the superior performance of lamb across various\ntasks such as bert and resnet-50 training with very little hyperparameter tuning.\nin particular , for bert training , our optimizer enables use of very large batch sizes\nof 32868 without any degradation of performance . by increasing the batch size to\nthe memory limit of a tpuv3 pod , bert training time can be reduced from 3 days\nto just 76 minutes ( table 1 ) . the lamb implementation is available online1.\n\n1\n\nintroduction\n\nwith the advent of large scale datasets , training large deep neural networks , even using computation-\nally efﬁcient optimization methods like stochastic gradient descent ( sgd ) , has become particularly\nchallenging . for instance , training state-of-the-art deep learning models like bert and resnet-50\ntakes 3 days on 16 tpuv3 chips and 29 hours on 8 tesla p100 gpus respectively ( devlin et al. , 2018 ; \nhe et al. , 2016 ) . thus , there is a growing interest to develop optimization solutions to tackle this\ncritical issue . the goal of this paper is to investigate and develop optimization techniques to accelerate\ntraining large deep neural networks , mostly focusing on approaches based on variants of sgd.\n\nmethods based on sgd iteratively update the parameters of the model by moving them in a scaled\n ( negative ) direction of the gradient calculated on a minibatch . however , sgd ’ s scalability is limited\nby its inherent sequential nature . owing to this limitation , traditional approaches to improve sgd\ntraining time in the context of deep learning largely resort to distributed asynchronous setup ( dean\net al. , 2012 ; recht et al. , 2011 ) . however , the implicit staleness introduced due to the asynchrony\nlimits the parallelization of the approach , often leading to degraded performance . the feasibility of\ncomputing gradient on large minibatches in parallel due to recent hardware advances has seen the\nresurgence of simply using synchronous sgd with large minibatches as an alternative to asynchronous\nsgd . however , naïvely increasing the batch size typically results in degradation of generalization\nperformance and reduces computational beneﬁts ( goyal et al. , 2017 ) .\n\nsynchronous sgd on large minibatches beneﬁts from reduced variance of the stochastic gradients\nused in sgd . this allows one to use much larger learning rates in sgd , typically of the order square\nroot of the minibatch size . surprisingly , recent works have demonstrated that up to certain minibatch\nsizes , linear scaling of the learning rate with minibatch size can be used to further speed up the\n\n1https : //github.com/tensorflow/addons/blob/master/tensorflow_addons/\n\noptimizers/lamb.py\n\n1\n\n published as a conference paper at iclr 2020\n\ntraining goyal et al . ( 2017 ) . these works also elucidate two interesting aspects to enable the use of\nlinear scaling in large batch synchronous sgd : ( i ) linear scaling of learning rate is harmful during the\ninitial phase ; thus , a hand-tuned warmup strategy of slowly increasing the learning rate needs to be\nused initially , and ( ii ) linear scaling of learning rate can be detrimental beyond a certain batch size.\nusing these tricks , goyal et al . ( 2017 ) was able to drastically reduce the training time of resnet-50\nmodel from 29 hours to 1 hour using a batch size of 8192 . while these works demonstrate the\nfeasibility of this strategy for reducing the wall time for training large deep neural networks , they\nalso highlight the need for an adaptive learning rate mechanism for large batch learning.\n\nvariants of sgd using layerwise adaptive learning rates have been recently proposed to address this\nproblem . the most successful in this line of research is the lars algorithm ( you et al. , 2017 ) , which\nwas initially proposed for training resnet . using lars , resnet-50 can be trained on imagenet in\njust a few minutes ! however , it has been observed that its performance gains are not consistent across\ntasks . for instance , lars performs poorly for attention models like bert . furthermore , theoretical\nunderstanding of the adaptation employed in lars is largely missing . to this end , we study and\ndevelop new approaches specially catered to the large batch setting of our interest.\n\ncontributions . more speciﬁcally , we make the following main contributions in this paper.\n\n• inspired by lars , we investigate a general adaptation strategy specially catered to large\n\nbatch learning and provide intuition for the strategy.\n\n• based on the adaptation strategy , we develop a new optimization algorithm ( lamb ) for\nachieving adaptivity of learning rate in sgd . furthermore , we provide convergence analysis\nfor both lars and lamb to achieve a stationary point in nonconvex settings . we highlight\nthe beneﬁts of using these methods for large batch settings.\n\n• we demonstrate the strong empirical performance of lamb across several challenging tasks.\nusing lamb we scale the batch size in training bert to more than 32k without degrading\nthe performance ; thereby , cutting the time down from 3 days to 76 minutes . ours is the ﬁrst\nwork to reduce bert training wall time to less than couple of hours.\n\n• we also demonstrate the efﬁciency of lamb for training state-of-the-art image classiﬁcation\nmodels like resnet . to the best of our knowledge , ours is ﬁrst adaptive solver that can\nachieve state-of-the-art accuracy for resnet-50 as adaptive solvers like adam fail to obtain\nthe accuracy of sgd with momentum for these tasks.\n\n1.1 related work\n\nthe literature on optimization for machine learning is vast and hence , we restrict our attention to the\nmost relevant works here . earlier works on large batch optimization for machine learning mostly\nfocused on convex models , beneﬁting by a factor of square root of batch size using appropriately large\nlearning rate . similar results can be shown for nonconvex settings wherein using larger minibatches\nimproves the convergence to stationary points ; albeit at the cost of extra computation . however , \nseveral important concerns were raised with respect to generalization and computational performance\nin large batch nonconvex settings . it was observed that training with extremely large batch was\ndifﬁcult ( keskar et al. , 2016 ; hoffer et al. , 2017 ) . thus , several prior works carefully hand-tune\ntraining hyper-parameters , like learning rate and momentum , to avoid degradation of generalization\nperformance ( goyal et al. , 2017 ; li , 2017 ; you et al. , 2018 ; shallue et al. , 2018 ) .\n\n ( krizhevsky , 2014 ) empirically found that simply scaling the learning rate linearly with respect to\nbatch size works better up to certain batch sizes . to avoid optimization instability due to linear scaling\nof learning rate , goyal et al . ( 2017 ) proposed a highly hand-tuned learning rate which involves a\nwarm-up strategy that gradually increases the lr to a larger value and then switching to the regular\nlr policy ( e.g . exponential or polynomial decay ) . using lr warm-up and linear scaling , goyal et al.\n ( 2017 ) managed to train resnet-50 with batch size 8192 without loss in generalization performance.\nhowever , empirical study ( shallue et al. , 2018 ) shows that learning rate scaling heuristics with the\nbatch size do not hold across all problems or across all batch sizes.\n\nmore recently , to reduce hand-tuning of hyperparameters , adaptive learning rates for large batch\ntraining garnered signiﬁcant interests . several recent works successfully scaled the batch size to large\nvalues using adaptive learning rates without degrading the performance , thereby , ﬁnishing resnet-\n50 training on imagenet in a few minutes ( you et al. , 2018 ; iandola et al. , 2016 ; codreanu et al. , \n2017 ; akiba et al. , 2017 ; jia et al. , 2018 ; smith et al. , 2017 ; martens & grosse , 2015 ; devarakonda\n\n2\n\n published as a conference paper at iclr 2020\n\net al. , 2017 ; mikami et al. , 2018 ; osawa et al. , 2018 ; you et al. , 2019 ; yamazaki et al. , 2019 ) . to the\nbest of our knowledge , the fastest training result for resnet-50 on imagenet is due to ying et al.\n ( 2018 ) , who achieve 76+ % top-1 accuracy . by using the lars optimizer and scaling the batch size to\n32k on a tpuv3 pod , ying et al . ( 2018 ) was able to train resnet-50 on imagenet in 2.2 minutes.\nhowever , it was empirically observed that none of these performance gains hold in other tasks such\nas bert training ( see section 4 ) .\n\n2 preliminaries\n\nnotation . for any vector xt ∈ rd , either xt , j or [ xt ] j are used to denote its jth coordinate where\nj ∈ [ d ] . let i be the d × d identity matrix , and let i  [ i1 , i2 , ... , ih ] be its decomposition into column\nsubmatrices ii  d × dh . for x ∈ rd , let x ( i ) be the block of variables corresponding to the columns\nof ii i.e. , x ( i )  i ( cid:62 ) \ni x ∈ rdi for i  { 1 , 2 , · · · , h } . for any function f : rd → r , we use ∇if ( x ) to\ndenote the gradient with respect to x ( i ) . for any vectors u , v ∈ rd , we use u2 and u/v to denote\nelementwise square and division operators respectively . we use ( cid:107 ) . ( cid:107 ) and ( cid:107 ) . ( cid:107 ) 1 to denote l2-norm and\nl1-norm of a vector respectively.\n\nwe start our discussion by formally stating the problem setup . in this paper , we study nonconvex\nstochastic optimization problems of the form\n\nf ( x ) :  es∼p [ ( cid:96 ) ( x , s ) ] +\n\n ( cid:107 ) x ( cid:107 ) 2 , \n\nmin\nx∈rd\n\nλ\n2\n\n ( 1 ) \n\nwhere ( cid:96 ) is a smooth ( possibly nonconvex ) function and p is a probability distribution on the domain\ns ⊂ rk . here , x corresponds to model parameters , ( cid:96 ) is the loss function and p is an unknown data\ndistribution.\nwe assume function ( cid:96 ) ( x ) is li-smooth with respect to x ( i ) , i.e. , there exists a constant li such that\n\n ( cid:107 ) ∇i ( cid:96 ) ( x , s ) − ∇i ( cid:96 ) ( y , s ) ( cid:107 ) ≤ li ( cid:107 ) x ( i ) − y ( i ) ( cid:107 ) , \n\n∀ x , y ∈ rd , and s ∈ s , \n\n ( 2 ) \n\nfor all i ∈ [ h ] . we use l  ( l1 , · · · , lh ) ( cid:62 ) to denote the h-dimensional vector of lipschitz constants.\nwe use l∞ and lavg to denote maxi li and ( cid:80 ) \nli\nh respectively . we assume the following bound\ni for all x ∈ rd and i ∈ [ h ] .\non the variance in stochastic gradients : e ( cid:107 ) ∇i ( cid:96 ) ( x , s ) − ∇if ( x ) ( cid:107 ) 2 ≤ σ2\nfurthermore , we also assume e ( cid:107 ) [ ∇ ( cid:96 ) ( x , s ) ] i − [ ∇f ( x ) ] i ( cid:107 ) 2 ≤ ˜σ2\ni for all x ∈ rd and i ∈ [ d ] . we use\nσ  ( σ1 , · · · , σh ) ( cid:62 ) and ˜σ  ( ˜σ1 , · · · , ˜σd ) ( cid:62 ) to denote the vectors of standard deviations of stochastic\ngradient per layer and per dimension respectively . finally , we assume that the gradients are bounded\ni.e. , [ ∇l ( x , s ) ] j ≤ g for all i ∈ [ d ] , x ∈ rd and s ∈ s. note that such assumptions are typical in the\nanalysis of stochastic ﬁrst-order methods ( cf . ( ghadimi & lan , 2013a ; ghadimi et al. , 2014 ) ) .\n\ni\n\nstochastic gradient descent ( sgd ) is one of the simplest ﬁrst-order algorithms for solving problem in\nequation 1 . the update at the tth iteration of sgd is of the following form : \n\nxt+1  xt − ηt\n\n∇ ( cid:96 ) ( xt , st ) + λxt , \n\n ( sgd ) \n\n1\n|st|\n\n ( cid:88 ) \n\nst∈st\n\nwhere st is set of b random samples drawn from the distribution p. for very large batch settings , the\nfollowing is a well-known result for sgd.\n\ntheorem 1 ( ( ghadimi & lan , 2013b ) ) . with large batch b  t and using appropriate learning rate , \nwe have the following for the iterates of sgd : \n\ne ( cid:2 ) ( cid:107 ) ∇f ( xa ) ( cid:107 ) 2 ( cid:3 ) ≤ o\n\n ( cid:18 ) ( f ( x1 ) − f ( x∗ ) ) l∞\n\n ( cid:107 ) σ ( cid:107 ) 2\n\n ( cid:19 ) \n\nt\n\n+\n\nt\n\n.\n\nwhere x∗ is an optimal solution to the problem in equation 1 and xa is an iterate uniformly randomly\nchosen from { x1 , · · · , xt } .\n\nhowever , tuning the learning rate ηt in sgd , especially in large batch settings , is difﬁcult in practice.\nfurthermore , the dependence on l∞ ( the maximum of smoothness across dimension ) can lead to\nsigniﬁcantly slow convergence . in the next section , we discuss algorithms to circumvent this issue.\n\n3\n\n published as a conference paper at iclr 2020\n\n3 algorithms\n\nin this section , we ﬁrst discuss a general strategy to adapt the learning rate in large batch settings.\nusing this strategy , we discuss two speciﬁc algorithms in the later part of the section . since our\nprimary focus is on deep learning , our discussion is centered around training a h-layer neural network.\n\ngeneral strategy . suppose we use an iterative base algorithm a ( e.g . sgd or adam ) in the small\nbatch setting with the following layerwise update rule : \n\nwhere ut is the update made by a at time step t. we propose the following two changes to the update\nfor large batch settings : \n\nxt+1  xt + ηtut , \n\n1 . the update is normalized to unit l2-norm . this is ensured by modifying the update to the\nform ut/ ( cid:107 ) ut ( cid:107 ) . throughout this paper , such a normalization is done layerwise i.e. , the update\nfor each layer is ensured to be unit l2-norm.\n\n2 . the learning rate is scaled by φ ( ( cid:107 ) xt ( cid:107 ) ) for some function φ : r+ → r+ . similar to the\n\nnormalization , such a scaling is done layerwise.\n\nsuppose the base algorithm a is sgd , then the modiﬁcation results in the following update rule : \n\nx ( i ) \nt+1  x ( i ) \n\nt − ηt\n\nφ ( ( cid:107 ) x ( i ) \nt ( cid:107 ) ) \n ( cid:107 ) g ( i ) \nt ( cid:107 ) \n\ng ( i ) \nt\n\n , \n\n ( 3 ) \n\nand g ( i ) \nt\n\nfor all layers i ∈ [ h ] and where x ( i ) \nare the parameters and the gradients of the ith layer at\nt\ntime step t. the normalization modiﬁcation is similar to one typically used in normalized gradient\ndescent except that it is done layerwise . note that the modiﬁcation leads to a biased gradient update ; \nhowever , in large-batch settings , it can be shown that this bias is small . it is intuitive that such a\nnormalization provides robustness to exploding gradients ( where the gradient can be arbitrarily large ) \nand plateaus ( where the gradient can be arbitrarily small ) . normalization of this form essentially\nignores the size of the gradient and is particularly useful in large batch settings where the direction of\nthe gradient is largely preserved.\n\nthe scaling term involving φ ensures that the norm of the update is of the same order as that of\nthe parameter . we found that this typically ensures faster convergence in deep neural networks.\nin practice , we observed that a simple function of φ ( z )  min { max { z , γl } , γu } works well . it is\ninstructive to consider the case where φ ( z )  z . in this scenario , the overall change in the learning\nrate is ( cid:107 ) x ( i ) \nt ( cid:107 ) \n ( cid:107 ) g ( i ) \nt ( cid:107 ) \ngradient ( see equation 2 ) . we now discuss different instantiations of the strategy discussed above . in\nparticular , we focus on two algorithms : lars ( 3.1 ) and the proposed method , lamb ( 3.2 ) .\n\n , which can also be interpreted as an estimate on the inverse of lipschitz constant of the\n\n3.1 lars algorithm\n\nthe ﬁrst instantiation of the general strategy is lars algorithm ( you et al. , 2017 ) , which is obtained\nby using momentum optimizer as the base algorithm a in the framework . lars was earlier proposed\nfor large batch learning for resnet on imagenet . in general , it is observed that the using ( heavy-ball ) \nmomentum , one can reduce the variance in the stochastic gradients at the cost of little bias . the\npseudocode for lars is provide in algorithm 1.\n\nwe now provide convergence analysis for lars in general nonconvex setting stated in this paper . for\nthe sake of simplicity , we analyze the case where β1  0 and λ  0 in algorithm 1 . however , our\nanalysis should extend to the general case as well . we will defer all discussions about the convergence\nrate to the end of the section.\n\ntheorem 2 . let ηt  η  αl , αu > 0 . then for xt generated using lars ( algorithm 1 ) , we have the following bound\n\nfor all t ∈ [ t ] , b  t , αl ≤ φ ( v ) ≤ αu for all v > 0\n\nu ( cid:107 ) l ( cid:107 ) 1t\n\nα2\n\n ( cid:113 ) 2 ( f ( x1 ) −f ( x∗ ) ) \n\n ( cid:32 ) \n\n ( cid:34 ) \n\ne\n\n1\n√\nh\n\nh\n ( cid:88 ) \n\ni ( cid:35 ) ( cid:33 ) 2\n\n ( cid:18 ) ( f ( x1 ) − f ( x∗ ) ) lavg\n\n ( cid:107 ) ∇if ( xa ) ( cid:107 ) \n\n≤ o\n\nt\n\n+\n\n ( cid:19 ) \n\n , \n\n ( cid:107 ) σ ( cid:107 ) 2\n1\nt h\n\nwhere x∗ is an optimal solution to the problem in equation 1 and xa is an iterate uniformly randomly\nchosen from { x1 , · · · , xt } .\n\n4\n\n published as a conference paper at iclr 2020\n\nalgorithm 2 lamb\n\ninput : x1 ∈ rd , learning rate { ηt } t\n0 < β1 , β2 < 1 , scaling function φ , ( cid:15 ) > 0\nset m0  0 , v0  0\nfor t  1 to t do\n\nt , parameters\n\n∇ ( cid:96 ) ( xt , st ) .\n\nalgorithm 1 lars\n\ninput : x1 ∈ rd , learning rate { ηt } t\n0 < β1 < 1 , scaling function φ , ( cid:15 ) > 0\nset m0  0\nfor t  1 to t do\n\nt , parameter\n\n ( cid:80 ) \n\ndraw b samples st from p\ncompute gt  1\n∇ ( cid:96 ) ( xt , st ) \nst∈st\n|st|\nmt  β1mt−1 + ( 1 − β1 ) ( gt + λxt ) \nx ( i ) \nt+1  x ( i ) \n\nt − ηt\n\nm ( i ) \nt\n\nφ ( ( cid:107 ) x\n\n ( i ) \nt ( cid:107 ) ) \n ( i ) \nt ( cid:107 ) \n\n ( cid:107 ) m\n\nfor all i ∈ [ h ] \n\nend for\n\n ( cid:80 ) \n\ndraw b samples st from p.\ncompute gt  1\nst∈st\n|st|\nmt  β1mt−1 + ( 1 − β1 ) gt\nvt  β2vt−1 + ( 1 − β2 ) g2\nt\nmt  mt/ ( 1 − βt\n1 ) \nvt  vt/ ( 1 − βt\n2 ) \ncompute ratio rt  mt√\nx ( i ) \nt+1  x ( i ) \n\nt − ηt\n\nvt+ ( cid:15 ) \n ( i ) \nt ( cid:107 ) ) \n ( i ) \nt ( cid:107 ) \n\nφ ( ( cid:107 ) x\n ( i ) \nt +λx\n\n ( cid:107 ) r\n\nend for\n\n ( r ( i ) \n\nt + λx ( i ) \nt ) \n\n3.2 lamb algorithm\n\nthe second instantiation of the general strategy is obtained by using adam as the base algorithm a.\nadam optimizer is popular in deep learning community and has shown to have good performance\nfor training state-of-the-art language models like bert . unlike lars , the adaptivity of lamb is\ntwo-fold : ( i ) per dimension normalization with respect to the square root of the second moment used\nin adam and ( ii ) layerwise normalization obtained due to layerwise adaptivity . the pseudocode for\nlamb is provided in algorithm 2 . when β1  0 and β2  0 , the algorithm reduces to be sign sgd\nwhere the learning rate is scaled by square root of the layer dimension ( bernstein et al. , 2018 ) .\n\nthe following result provides convergence rate for lamb in general nonconvex settings . similar to\nthe previous case , we focus on the setting where β1  0 and λ  0 . as before , our analysis extends\nto the general case ; however , the calculations become messy.\n\n ( cid:113 ) 2 ( f ( x1 ) −f ( x∗ ) ) \n\nfor all t ∈ [ t ] , b  t , di  d/h for all i ∈ [ h ] , and\ntheorem 3 . let ηt  η  ≤ φ ( v ) ≤ αu for all v > 0 where αl , αu > 0 . then for xt generated using lamb ( algorithm 2 ) , \nwe have the following bounds : \n\nu ( cid:107 ) l ( cid:107 ) 1t\n\nα2\n\n1 . when β2  0 , we have\n\n2 . when β2 > 0 , we have\n\n ( cid:18 ) \n\ne\n\n ( cid:20 ) 1\n√\nd\n\n ( cid:21 ) ( cid:19 ) 2\n\n ( cid:18 ) ( f ( x1 ) − f ( x∗ ) ) lavg\n\n ( cid:107 ) ∇f ( xa ) ( cid:107 ) 1\n\n≤ o\n\nt\n\n+\n\n ( cid:19 ) \n\n , \n\n ( cid:107 ) ˜σ ( cid:107 ) 2\n1\nt h\n\ne [ ( cid:107 ) ∇f ( xa ) ( cid:107 ) 2 ] ≤ o\n\n ( cid:32 ) ( cid:115 ) \n\ng2d\n\nh ( 1 − β2 ) \n\n ( cid:34 ) ( cid:114 ) \n\n×\n\n2 ( f ( x1 ) − f ( x∗ ) ) ( cid:107 ) l ( cid:107 ) 1\n\nt\n\n ( cid:35 ) ( cid:33 ) \n\n , \n\n+\n\n ( cid:107 ) ˜σ ( cid:107 ) 1√\nt\n\nwhere x∗ is an optimal solution to the problem in equation 1 and xa is an iterate uniformly randomly\nchosen from { x1 , · · · , xt } .\n\ndiscussion on convergence rates . we ﬁrst start our discussion with the comparison of convergence\nrate of lars with that of sgd ( theorem 1 ) . the convergence rates of lars and sgd differ in\ntwo ways : ( 1 ) the convergence criterion is ( e [ ( cid:80 ) h\ni ( cid:107 ) ∇if ( cid:107 ) ] ) 2 as opposed to e [ ( cid:107 ) ∇f ( cid:107 ) 2 ] in sgd and\n ( 2 ) the dependence on l and σ in the convergence rate . brieﬂy , the convergence rate of lars is\nbetter than sgd when the gradient is denser than curvature and stochasticity . this convergence rate\ncomparison is similar in spirit to the one obtained in ( bernstein et al. , 2018 ) . assuming that the\nconvergence criterion in theorem 1 and theorem 2 is of similar order ( which happens when gradients\nare fairly dense ) , convergence rate of lars and lamb depend on lavg instead of l∞ and are thus , \nsigniﬁcantly better than that of sgd . a more quantitative comparison is provided in section c of\nthe appendix . the comparison of lamb ( with β2  0 ) with sgd is along similar lines . we obtain\nslightly worse rates for the case where β2 > 0 ; although , we believe that its behavior should be better\nthan the case β2  0 . we leave this investigation to future work.\n\n5\n\n published as a conference paper at iclr 2020\n\n4 experiments\n\nwe now present empirical results comparing lamb with existing optimizers on two important\nlarge batch training tasks : bert and resnet-50 training . we also compare lamb with existing\noptimizers for small batch size ( < 1k ) and small dataset ( e.g . cifar , mnist ) ( see appendix ) .\n\nexperimental setup . to demonstrate its robustness , we use very minimal hyperparameter tuning for\nthe lamb optimizer . thus , it is possible to achieve better results by further tuning the hyperparameters.\nthe parameters β1 and β2 in algorithm 2 are set to 0.9 and 0.999 respectively in all our experiments ; \nwe only tune the learning rate . we use a polynomially decaying learning rate of ηt  η0 × ( 1−t/t ) in\nalgorithm 2 ) , which is the same as in bert baseline . this setting also works for all other applications\nin this paper . furthermore , for bert and resnet-50 training , we did not tune the hyperparameters\nof lamb while increasing the batch size . we use the square root of lr scaling rule to automatically\nadjust learning rate and linear-epoch warmup scheduling . we use tpuv3 in all the experiments . a\ntpuv3 pod has 1024 chips and can provide more than 100 petaﬂops performance for mixed precision\ncomputing . to make sure we are comparing with solid baselines , we use grid search to tune the\nhyper-parameters for adam , adagrad , adamw ( adam with weight decay ) , and lars . we also\ntune weight decay for adamw . all the hyperparameter tuning settings are reported in the appendix.\ndue to space constraints , several experimental details are relegated to the appendix.\n\n4.1 bert training\n\nwe ﬁrst discuss empirical results for speeding up bert training . for this experiment , we use the same\ndataset as devlin et al . ( 2018 ) , which is a concatenation of wikipedia and bookscorpus with 2.5b\nand 800m words respectively . we speciﬁcally focus on the squad task2 in this paper . the f1 score\non squad-v1 is used as the accuracy metric in our experiments . all our comparisons are with respect\nto the baseline bert model by devlin et al . ( 2018 ) . to train bert , devlin et al . ( 2018 ) ﬁrst train the\nmodel for 900k iterations using a sequence length of 128 and then switch to a sequence length of\n512 for the last 100k iterations . this results in a training time of around 3 days on 16 tpuv3 chips.\nthe baseline bert model3 achieves a f1 score of 90.395 . to ensure a fair comparison , we follow\nthe same squad ﬁne-tune procedure of devlin et al . ( 2018 ) without modifying any conﬁguration\n ( including number of epochs and hyperparameters ) . as noted earlier , we could get even better results\nby changing the ﬁne-tune conﬁguration . for instance , by just slightly changing the learning rate in\nthe ﬁne-tune stage , we can obtain a higher f1 score of 91.688 for the batch size of 16k using lamb.\nwe report a f1 score of 91.345 in table 1 , which is the score obtained for the untuned version . below\nwe describe two different training choices for training bert and discuss the corresponding speedups.\n\nfor the ﬁrst choice , we maintain the same training procedure as the baseline except for changing the\ntraining optimizer to lamb . we run with the same number of epochs as the baseline but with batch\nsize scaled from 512 to 32k . the choice of 32k batch size ( with sequence length 512 ) is mainly\ndue to memory limits of tpu pod . our results are shown in table 1 . by using the lamb optimizer , \nwe are able to achieve a f1 score of 91.460 in 15625 iterations for a batch size of 32768 ( 14063\niterations for sequence length 128 and 1562 iterations for sequence length 512 ) . with 32k batch size , \nwe reduce bert training time from 3 days to around 100 minutes . we achieved 49.1 times speedup\nby 64 times computational resources ( 76.7 % efﬁciency ) . we consider the speedup is great because we\nuse the synchronous data-parallelism . there is a communication overhead coming from transferring\nof the gradients over the interconnect . for resnet-50 , researchers are able to achieve 90 % scaling\nefﬁciency because resnet-50 has much fewer parameters ( # parameters is equal to # gradients ) than\nbert ( 25 million versus 300 million ) .\n\nto obtain further improvements , we use the mixed-batch training procedure with lamb . recall\nthat bert training involves two stages : the ﬁrst 9/10 of the total epochs use a sequence length of 128 , \nwhile the last 1/10 of the total epochs use a sequence length of 512 . for the second stage training , \nwhich involves a longer sequence length , due to memory limits , a maximum batch size of only\n32768 can be used on a tpuv3 pod . however , we can potentially use a larger batch size for the\nﬁrst stage because of a shorter sequence length . in particular , the batch size can be increased to\n131072 for the ﬁrst stage . however , we did not observe any speedup by increasing the batch size from\n65536 to 131072 for the ﬁrst stage , thus , we restrict the batch size to 65536 for this stage . by using\nthis strategy , we are able to make full utilization of the hardware resources throughout the training\n\n2https : //rajpurkar.github.io/squad-explorer/\n3pre-trained bert model can be downloaded from https : //github.com/google-research/bert\n\n6\n\n published as a conference paper at iclr 2020\n\ntable 1 : we use the f1 score on squad-v1 as the accuracy metric . the baseline f1 score is the\nscore obtained by the pre-trained model ( bert-large ) provided on bert ’ s public repository ( as of\nfebruary 1st , 2019 ) . we use tpuv3s in our experiments . we use the same setting as the baseline : the\nﬁrst 9/10 of the total epochs used a sequence length of 128 and the last 1/10 of the total epochs used\na sequence length of 512 . all the experiments run the same number of epochs . dev set means the test\ndata . it is worth noting that we can achieve better results by manually tuning the hyperparameters.\nthe data in this table is collected from the untuned version.\n\nsolver\n\nbatch size\n\nsteps\n\nf1 score on dev set tpus\n\ntime\n\nbaseline\nlamb\nlamb\nlamb\nlamb\nlamb\nlamb\nlamb\n\n512\n512\n1k\n2k\n4k\n8k\n16k\n32k\n\n1000k\n1000k\n500k\n250k\n125k\n62500\n31250\n15625\n\nlamb\n\n64k/32k\n\n8599\n\n90.395\n91.752\n91.761\n91.946\n91.137\n91.263\n91.345\n91.475\n\n90.584\n\n16\n16\n32\n64\n128\n256\n512\n1024\n\n1024\n\n81.4h\n82.8h\n43.2h\n21.4h\n693.6m\n390.5m\n200.0m\n101.2m\n\n76.19m\n\nprocedure . increasing the batch size is able to warm-up and stabilize the optimization process ( smith\net al. , 2017 ) , but decreasing the batch size brings chaos to the optimization process and can cause\ndivergence . in our experiments , we found a technique that is useful to stabilize the second stage\noptimization . because we switched to a different optimization problem , it is necessary to re-warm-up\nthe optimization . instead of decaying the learning rate at the second stage , we ramp up the learning\nrate from zero again in the second stage ( re-warm-up ) . as with the ﬁrst stage , we decay the learning\nrate after the re-warm-up phase . with this method , we only need 8599 iterations and ﬁnish bert\ntraining in 76 minutes ( 100.2 % efﬁciency ) .\n\ncomparison with adamw and lars . to ensure that our approach is compared to a solid\nbaseline for the bert training , we tried three different strategies for tuning adamw : ( 1 ) adamw\nwith default hyperparameters ( see devlin et al . ( 2018 ) ) ( 2 ) adamw with the same hyperparameters\nas lamb , and ( 3 ) adamw with tuned hyperparameters . adamw stops scaling at the batch size of\n16k because it is not able to achieve the target f1 score ( 88.1 vs 90.4 ) . the tuning information of\nadamw is shown in the appendix . for 64k/32k mixed-batch training , even after extensive tuning\nof the hyperparameters , we fail to get any reasonable result with adamw optimizer . we conclude\nthat adamw does not work well in large-batch bert training or is at least hard to tune . we also\nobserve that lamb performs better than lars for all batch sizes ( see table 2 ) .\n\ntable 2 : lamb achieves a higher performance ( f1 score ) than lars for all the batch sizes . the\nbaseline achieves a f1 score of 90.390 . thus , lars stops scaling at the batch size of 16k.\n\nbatch size\n\n512\n\n1k\n\n2k\n\n4k\n\n8k\n\n16k\n\n32k\n\nlars\nlamb\n\n90.717\n91.752\n\n90.369\n91.761\n\n90.748\n91.946\n\n90.537\n91.137\n\n90.548\n91.263\n\n89.589\n91.345\n\ndiverge\n91.475\n\n4.2\n\nimagenet training with resnet-50.\n\nimagenet training with resnet-50 is an industry standard metric that is being used in mlperf4.\nthe baseline can get 76.3 % top-1 accuracy in 90 epochs ( goyal et al. , 2017 ) . all the successful\nimplementations are based on momentum sgd ( he et al. , 2016 ; goyal et al. , 2017 ) or lars optimizer\n ( ying et al. , 2018 ; jia et al. , 2018 ; mikami et al. , 2018 ; you et al. , 2018 ; yamazaki et al. , 2019 ) .\nbefore our study , we did not ﬁnd any paper reporting a state-of-the-art accuracy achieved by adam , \n\n4https : //mlperf.org/\n\n7\n\n published as a conference paper at iclr 2020\n\nadagrad , or adamw optimizer . in our experiments , even with comprehensive hyper-parameter\ntuning , adagrad/adam/adamw ( with batch size 16k ) only achieves 55.38 % /66.04 % /67.27 % \ntop-1 accuracy . after adding learning rate scheme of goyal et al . ( 2017 ) , the top-1 accuracy of\nadagrad/adam/adamw was improved to 72.0 % /73.48 % /73.07 % . however , they are still much\nlower than 76.3 % . the details of the tuning information are in the appendix . table 3 shows that\nlamb can achieve the target accuracy . beyond a batch size of 8k , lamb ’ s accuracy is higher than\nthe momentum . lamb ’ s accuracy is also slightly better than lars . at a batch size of 32k , lamb\nachieves 76.4 % top-1 accuracy while lars achieves 76.3 % . at a batch size of 2k , lamb is able to\nachieve 77.11 % top-1 accuracy while lars achieves 76.6 % .\n\ntable 3 : top-1 validation accuracy of imagenet/resnet-50 training at the batch size of 16k ( 90\nepochs ) . the performance of momentum was reported by ( goyal et al. , 2017 ) . + means adding the\nlearning rate scheme of goyal et al . ( 2017 ) to the optimizer : ( 1 ) 5-epoch warmup to stablize the initial\nstage ; and ( 2 ) multiply the learning rate by 0.1 at 30th , 60th , and 80th epoch . the target accuracy is\naround 0.763 ( goyal et al. , 2017 ) . all the adaptive solvers were comprehensively tuned . the tuning\ninformation was in the appendix.\nadagrad/adagrad+\n\nadamw/adamw+ momentum\n\nadam/adam+\n\noptimizer\n\nlamb\n\naccuracy\n\n0.5538/0.7201\n\n0.6604/0.7348\n\n0.6727/0.7307\n\n0.7520\n\n0.7666\n\n4.3 hyperparameters for scaling the batch size\n\nfor bert and imagenet training , we did not tune the hyperparameters of lamb optimizer when\nincreasing the batch size . we use the square root lr scaling rule and linear-epoch warmup scheduling\nto automatically adjust learning rate . the details can be found in tables 4 and 5\n\ntable 4 : untuned lamb for bert training across different batch sizes ( ﬁxed # epochs ) . we use\nsquare root lr scaling and linear-epoch warmup . for example , batch size 32k needs to ﬁnish 15625\niterations . it uses 0.2×15625  3125 iterations for learning rate warmup . bert ’ s baseline achieved a\nf1 score of 90.395 . we can achieve an even higher f1 score if we manually tune the hyperparameters.\n\nbatch size\n\n512\n\nlearning rate\nwarmup ratio\n\nf1 score\n\nexact match\n\n5\n\n1\n\n320\n\n91.752\n85.090\n\n23.0×103\n\n22.5×103\n\n22.0×103\n\n21.5×103\n\n21.0×103\n\n20.5×103\n\n20.0×103\n\n1k\n5\n\n1\n\n160\n\n2k\n5\n\n1\n80\n\n4k\n5\n\n1\n40\n\n8k\n5\n\n1\n20\n\n16k\n\n32k\n\n5\n\n1\n10\n\n5\n\n1\n5\n\n91.761\n85.260\n\n91.946\n85.355\n\n91.137\n84.172\n\n91.263\n84.901\n\n91.345\n84.816\n\n91.475\n84.939\n\ntable 5 : untuned lamb for imagenet training with resnet-50 for different batch sizes ( 90 epochs ) .\nwe use square root lr scaling and linear-epoch warmup . the baseline goyal et al . ( 2017 ) gets 76.3 % \ntop-1 accuracy in 90 epochs . stanford dawn bench ( coleman et al. , 2017 ) baseline achieves 93 % \ntop-5 accuracy . lamb achieves both of them . lamb can achieve an even higher accuracy if we\nmanually tune the hyperparameters.\n\nbatch size\n\nlearning rate\nwarmup epochs\ntop-5 accuracy\ntop-1 accuracy\n\n512\n\n4\n\n23.0×100\n0.3125\n0.9335\n0.7696\n\n1k\n4\n\n22.5×100\n0.625\n0.9349\n0.7706\n\n2k\n4\n\n1.25\n0.9353\n0.7711\n\n22.0×100\n\n21.5×100\n\n21.0×100\n\n20.5×100\n\n20.0×100\n\n4k\n4\n\n2.5\n\n8k\n4\n\n5\n\n16k\n\n4\n\n10\n\n32k\n\n4\n\n20\n\n0.9332\n0.7692\n\n0.9331\n0.7689\n\n0.9322\n0.7666\n\n0.9308\n0.7642\n\n5 conclusion\n\nlarge batch techniques are critical to speeding up deep neural network training . in this paper , we\npropose the lamb optimizer , which supports adaptive elementwise updating and layerwise learning\n\n8\n\n published as a conference paper at iclr 2020\n\nrates . furthermore , lamb is a general purpose optimizer that works for both small and large batches.\nwe also provided theoretical analysis for the lamb optimizer , highlighting the cases where it\nperforms better than standard sgd . lamb achieves a better performance than existing optimizers for\na wide range of applications . by using lamb , we are able to scale the batch size of bert pre-training\nto 64k without losing accuracy , thereby , reducing the bert training time from 3 days to around 76\nminutes . lamb is also the ﬁrst large batch adaptive solver that can achieve state-of-the-art accuracy\non imagenet training with resnet-50.\n\n6 acknowledgement\nwe want to thank the comments from george dahl and jeff dean . we want to thank michael banﬁeld , \ndehao chen , youlong cheng , sameer kumar , and zak stone for tpu pod support.\n\nreferences\n\ntakuya akiba , shuji suzuki , and keisuke fukuda . extremely large minibatch sgd : training resnet-50\n\non imagenet in 15 minutes . arxiv preprint arxiv:1711.04325 , 2017.\n\nyoshua bengio . practical recommendations for gradient-based training of deep architectures . in\n\nneural networks : tricks of the trade , pp . 437–478 . springer , 2012.\n\njeremy bernstein , yu-xiang wang , kamyar azizzadenesheli , and anima anandkumar . signsgd : \n\ncompressed optimisation for non-convex problems . corr , abs/1802.04434 , 2018.\n\nvaleriu codreanu , damian podareanu , and vikram saletore . scale out for large minibatch sgd : \nresidual network training on imagenet-1k with improved accuracy and reduced time to train . arxiv\npreprint arxiv:1711.04291 , 2017.\n\ncody coleman , deepak narayanan , daniel kang , tian zhao , jian zhang , luigi nardi , peter bailis , \nkunle olukotun , chris ré , and matei zaharia . dawnbench : an end-to-end deep learning bench-\nmark and competition . training , 100 ( 101 ) :102 , 2017.\n\njeffrey dean , greg corrado , rajat monga , kai chen , matthieu devin , mark mao , andrew senior , \npaul tucker , ke yang , quoc v le , et al . large scale distributed deep networks . in advances in\nneural information processing systems , pp . 1223–1231 , 2012.\n\naditya devarakonda , maxim naumov , and michael garland . adabatch : adaptive batch sizes for\n\ntraining deep neural networks . arxiv preprint arxiv:1712.02029 , 2017.\n\njacob devlin , ming-wei chang , kenton lee , and kristina toutanova . bert : pre-training of deep\n\nbidirectional transformers for language understanding . arxiv preprint arxiv:1810.04805 , 2018.\n\ntimothy dozat . incorporating nesterov momentum into adam . 2016.\n\nsaeed ghadimi and guanghui lan . stochastic ﬁrst- and zeroth-order methods for nonconvex\nstochastic programming . siam journal on optimization , 23 ( 4 ) :2341–2368 , 2013a . doi : 10.1137/\n120880811.\n\nsaeed ghadimi and guanghui lan . stochastic ﬁrst-and zeroth-order methods for nonconvex stochastic\n\nprogramming . siam journal on optimization , 23 ( 4 ) :2341–2368 , 2013b.\n\nsaeed ghadimi , guanghui lan , and hongchao zhang . mini-batch stochastic approximation methods\nfor nonconvex stochastic composite optimization . mathematical programming , 155 ( 1-2 ) :267–305 , \n2014.\n\npriya goyal , piotr dollár , ross girshick , pieter noordhuis , lukasz wesolowski , aapo kyrola , \nandrew tulloch , yangqing jia , and kaiming he . accurate , large minibatch sgd : training imagenet\nin 1 hour . arxiv preprint arxiv:1706.02677 , 2017.\n\nkaiming he , xiangyu zhang , shaoqing ren , and jian sun . deep residual learning for image\nrecognition . in proceedings of the ieee conference on computer vision and pattern recognition , \npp . 770–778 , 2016.\n\nelad hoffer , itay hubara , and daniel soudry . train longer , generalize better : closing the generalization\n\ngap in large batch training of neural networks . arxiv preprint arxiv:1705.08741 , 2017.\n\n9\n\n published as a conference paper at iclr 2020\n\nforrest n iandola , matthew w moskewicz , khalid ashraf , and kurt keutzer . firecaffe : near-linear\nacceleration of deep neural network training on compute clusters . in proceedings of the ieee\nconference on computer vision and pattern recognition , pp . 2592–2600 , 2016.\n\nxianyan jia , shutao song , wei he , yangzihao wang , haidong rong , feihu zhou , liqiang xie , \nzhenyu guo , yuanzhou yang , liwei yu , et al . highly scalable deep learning training system with\nmixed-precision : training imagenet in four minutes . arxiv preprint arxiv:1807.11205 , 2018.\n\nnitish shirish keskar , dheevatsa mudigere , jorge nocedal , mikhail smelyanskiy , and ping tak peter\ntang . on large-batch training for deep learning : generalization gap and sharp minima . arxiv\npreprint arxiv:1609.04836 , 2016.\n\nalex krizhevsky . one weird trick for parallelizing convolutional neural networks . arxiv preprint\n\nmu li . scaling distributed machine learning with system and algorithm co-design . phd thesis , \n\narxiv:1404.5997 , 2014.\n\nintel , 2017.\n\njames martens and roger grosse . optimizing neural networks with kronecker-factored approximate\n\ncurvature . in international conference on machine learning , pp . 2408–2417 , 2015.\n\nhiroaki mikami , hisahiro suganuma , yoshiki tanaka , yuichi kageyama , et al . imagenet/resnet-50\n\ntraining in 224 seconds . arxiv preprint arxiv:1811.05233 , 2018.\n\nyurii e nesterov . a method for solving the convex programming problem with convergence rate o\n\n ( 1/kˆ 2 ) . in dokl . akad . nauk sssr , volume 269 , pp . 543–547 , 1983.\n\nkazuki osawa , yohei tsuji , yuichiro ueno , akira naruse , rio yokota , and satoshi matsuoka.\nsecond-order optimization method for large mini-batch : training resnet-50 on imagenet in 35\nepochs . arxiv preprint arxiv:1811.12019 , 2018.\n\nbenjamin recht , christopher re , stephen wright , and feng niu . hogwild : a lock-free approach to\nparallelizing stochastic gradient descent . in advances in neural information processing systems , \npp . 693–701 , 2011.\n\nchristopher j shallue , jaehoon lee , joe antognini , jascha sohl-dickstein , roy frostig , and george e\ndahl . measuring the effects of data parallelism on neural network training . arxiv preprint\narxiv:1811.03600 , 2018.\n\nsamuel l smith , pieter-jan kindermans , and quoc v le . don ’ t decay the learning rate , increase the\n\nbatch size . arxiv preprint arxiv:1711.00489 , 2017.\n\nilya sutskever , james martens , george dahl , and geoffrey hinton . on the importance of initialization\nand momentum in deep learning . in international conference on machine learning , pp . 1139–1147 , \n2013.\n\nmasafumi yamazaki , akihiko kasagi , akihiro tabuchi , takumi honda , masahiro miwa , naoto\nfukumoto , tsuguchika tabaru , atsushi ike , and kohta nakashima . yet another accelerated sgd : \nresnet-50 training on imagenet in 74.7 seconds . arxiv preprint arxiv:1903.12650 , 2019.\n\nchris ying , sameer kumar , dehao chen , tao wang , and youlong cheng . image classiﬁcation at\n\nsupercomputer scale . arxiv preprint arxiv:1811.06992 , 2018.\n\nyang you , igor gitman , and boris ginsburg . scaling sgd batch size to 32k for imagenet training.\n\narxiv preprint arxiv:1708.03888 , 2017.\n\nyang you , zhao zhang , cho-jui hsieh , james demmel , and kurt keutzer . imagenet training in\nminutes . in proceedings of the 47th international conference on parallel processing , pp . 1 . acm , \n2018.\n\nyang you , jonathan hseu , chris ying , james demmel , kurt keutzer , and cho-jui hsieh . large-batch\n\ntraining for lstm and beyond . arxiv preprint arxiv:1901.08256 , 2019.\n\n10\n\n published as a conference paper at iclr 2020\n\nappendix\n\na proof of theorem 2\n\nproof . we analyze the convergence of lars for general minibatch size here . recall that the update\nof lars is the following\n\nx ( i ) \nt+1  x ( i ) \n\nt − ηtφ ( ( cid:107 ) x ( i ) \n\nt ( cid:107 ) ) \n\ng ( i ) \nt\n ( cid:107 ) g ( i ) \nt ( cid:107 ) \n\n , \n\nfor all i ∈ [ h ] . for simplicity of notation , we reason the\n\nsince the function f is l-smooth , we have the following : \n\nf ( xt+1 ) ≤ f ( xt ) + ( cid:104 ) ∇if ( xt ) , x ( i ) \n\nt+1 − x ( i ) \n\nt ( cid:105 ) +\n\n ( cid:107 ) x ( i ) \n\nt+1 − x ( i ) \n\nt ( cid:107 ) 2\n\n f ( xt ) − ηt\n\nφ ( ( cid:107 ) x ( i ) \n\nt ( cid:107 ) ) ×\n\n [ ∇if ( xt ) ] j ×\n\n≤ f ( xt ) − ηt\n\nφ ( ( cid:107 ) x ( i ) \n\nt ( cid:107 ) ) ×\n\n [ ∇if ( xt ) ] j ×\n\n ( cid:32 ) \n\n ( cid:32 ) \n\nh\n ( cid:88 ) \n\nliη2\n\nt φ2 ( ( cid:107 ) x ( i ) \n\nt ( cid:107 ) ) \n\n+\n\ni [ ∇if ( xt ) ] j\n ( cid:107 ) ∇if ( xt ) ( cid:107 ) \n\n+\n\n [ ∇if ( xt ) ] j\n ( cid:107 ) ∇if ( xt ) ( cid:107 ) \n\n ( cid:33 ) ( cid:33 ) \n\n+\n\nt α2\nη2\nu\n2\n\n ( cid:107 ) l ( cid:107 ) 1\n\n f ( xt ) − ηt\n\nφ ( ( cid:107 ) x ( i ) \n\nt ( cid:107 ) ) × ( cid:107 ) ∇if ( xt ) ( cid:107 ) − ηt\n\n [ ∇if ( xt ) ] j ×\n\n ( cid:32 ) g ( i ) \nt , j\n ( cid:107 ) g ( i ) \nt ( cid:107 ) \n\n−\n\n [ ∇if ( xt ) ] j\n ( cid:107 ) ∇if ( xt ) ( cid:107 ) \n\n+\n\nt α2\nη2\nu\n2\n\n ( cid:107 ) l ( cid:107 ) 1\n\n ( cid:33 ) ( cid:33 ) \n\n ( 4 ) \n\nh\n ( cid:88 ) \n\ni ( cid:33 ) \n\ng ( i ) \nt , j\n ( cid:107 ) g ( i ) \nt ( cid:107 ) \n ( cid:32 ) g ( i ) \nt , j\n ( cid:107 ) g ( i ) \nt ( cid:107 ) \n ( cid:32 ) \ndi ( cid:88 ) \n\nh\n ( cid:88 ) \n\ni ﬁrst inequality follows from the lipschitz continuous nature of the gradient . let ∆ ( i ) \n∇if ( xt ) . then the above inequality can be rewritten in the following manner : \n\nt  g ( i ) \n\nt −\n\nf ( xt+1 ) ≤ f ( xt ) − ηt\n\nφ ( ( cid:107 ) x ( i ) \n\nt ( cid:107 ) ) ( cid:107 ) ∇if ( xt ) ( cid:107 ) \n\nh\n ( cid:88 ) \n\ni ηt\n\nh\n ( cid:88 ) \n\ndi ( cid:88 ) \n\ni ( cid:32 ) \n\nφ ( ( cid:107 ) x ( i ) \n\nt ( cid:107 ) ) ×\n\n [ ∇if ( xt ) ] j ×\n\n ( cid:32 ) ( ∆ ( i ) \n ( cid:107 ) ∆ ( i ) \n\nt , j + [ ∇if ( xt ) ] j ) \nt + ∇if ( xt ) ( cid:107 ) \n\n ( cid:33 ) ( cid:33 ) \n\n−\n\n [ ∇if ( xt ) ] j\n ( cid:107 ) ∇if ( xt ) ( cid:107 ) \n\n+\n\nt α2\nη2\nu\n2\n\n ( cid:107 ) l ( cid:107 ) 1\n\n f ( xt ) − ηt\n\nφ ( ( cid:107 ) x ( i ) \n\nt ( cid:107 ) ) ( cid:107 ) ∇if ( xt ) ( cid:107 ) \n\n− ηt\n\nφ ( ( cid:107 ) x ( i ) \n\nt ( cid:107 ) ) ×\n\n ( cid:32 ) \n\n ( cid:104 ) ∆ ( i ) \n\nt + ∇if ( xt ) , ∇if ( xt ) ( cid:105 ) \n ( cid:107 ) ∆ ( i ) \n\nt + ∇if ( xt ) ( cid:107 ) \n\n− ( cid:107 ) ∇if ( xt ) ( cid:107 ) \n\n+\n\n ( cid:107 ) l ( cid:107 ) 1\n\n ( cid:33 ) \n\nη2\nt α2\nu\n2\n\n f ( xt ) − ηt\n\nφ ( ( cid:107 ) x ( i ) \n\nt ( cid:107 ) ) ( cid:107 ) ∇if ( xt ) ( cid:107 ) \n\n+ ηt\n\nφ ( ( cid:107 ) x ( i ) \n\nt ( cid:107 ) ) ×\n\n ( cid:32 ) \n\n ( cid:107 ) ∇if ( xt ) ( cid:107 ) ( cid:107 ) ∆ ( i ) \n\nt + ∇if ( xt ) ( cid:107 ) − ( cid:104 ) ∆ ( i ) \nt + ∇if ( xt ) ( cid:107 ) \n\n ( cid:107 ) ∆ ( i ) \n\nt + ∇if ( xt ) , ∇if ( xt ) ( cid:105 ) \n\n ( cid:33 ) \n\n+\n\nη2\nt α2\nu\n2\n\n ( cid:107 ) l ( cid:107 ) 1\n\n f ( xt ) − ηt\n\nφ ( ( cid:107 ) x ( i ) \n\nt ( cid:107 ) ) ( cid:107 ) ∇if ( xt ) ( cid:107 ) +\n\nη2\nt α2\nu\n2\n\n ( cid:107 ) l ( cid:107 ) 1\n\n+ ηt\n\nφ ( ( cid:107 ) x ( i ) \n\nt ( cid:107 ) ) ×\n\nh\n ( cid:88 ) \n\ni ( cid:32 ) \n\n ( cid:107 ) ∇if ( xt ) ( cid:107 ) ( cid:107 ) ∆ ( i ) \n\nt + ∇if ( xt ) ( cid:107 ) − ( cid:107 ) ∆ ( i ) \n\nt + ∇if ( xt ) ( cid:107 ) 2 + ( cid:104 ) ∆ ( i ) \n\nt\n\n , ∆ ( i ) \n\nt + ∇if ( xt ) ( cid:105 ) \n\n ( cid:33 ) \n\n.\n\n ( cid:107 ) ∆ ( i ) \n\nt + ∇if ( xt ) ( cid:107 ) \n\n ( 5 ) \n\nh\n ( cid:88 ) \n\ndi ( cid:88 ) \n\ni ( cid:88 ) \n\ndi ( cid:88 ) \n\ni ( cid:88 ) \n\ni ( cid:88 ) \n\ni ( cid:88 ) \n\ni ( cid:88 ) \n\ni ( cid:88 ) \n\ni ( cid:88 ) \n\ni published as a conference paper at iclr 2020\n\nusing cauchy-schwarz inequality in the above inequality , we have : \n\nf ( xt+1 ) ≤ f ( xt ) − ηt\n\nφ ( ( cid:107 ) x ( i ) \n\nt ( cid:107 ) ) ( cid:107 ) ∇if ( xt ) ( cid:107 ) \n\nh\n ( cid:88 ) \n\ni ( cid:88 ) \n\ni ( cid:88 ) \n\ni ( cid:88 ) \n\ni ( cid:88 ) \n\ni ( cid:88 ) \n\ni ( cid:88 ) \n\ni ηt\n\nφ ( ( cid:107 ) x ( i ) \n\nt ( cid:107 ) ) ×\n\n ( cid:107 ) ∇if ( xt ) ( cid:107 ) − ( cid:107 ) ∆ ( i ) \n\nt + ∇if ( xt ) ( cid:107 ) + ( cid:107 ) ∆ ( i ) \nt ( cid:107 ) \n\n ( cid:16 ) \n\n ( cid:17 ) \n\n+\n\nη2\nt α2\nu\n2\n\n ( cid:107 ) l ( cid:107 ) 1\n\n≤ f ( xt ) − ηt\n\nφ ( ( cid:107 ) x ( i ) \n\nt ( cid:107 ) ) ( cid:107 ) ∇if ( xt ) ( cid:107 ) + 2ηt\n\nφ ( ( cid:107 ) x ( i ) \n\nt ( cid:107 ) ) × ( cid:107 ) ∆ ( i ) \n\nt ( cid:107 ) +\n\nη2\nt α2\nu\n2\n\n ( cid:107 ) l ( cid:107 ) 1\n\ntaking expectation , we obtain the following : \n\ne [ f ( xt+1 ) ] ≤ f ( xt ) − ηt\n\nφ ( ( cid:107 ) x ( i ) \n\nt ( cid:107 ) ) ( cid:107 ) ∇if ( xt ) ( cid:107 ) + 2ηt\n\nφ ( ( cid:107 ) x ( i ) \n\nt ( cid:107 ) ) × e [ ( cid:107 ) ∆ ( i ) \n\nt ( cid:107 ) ] +\n\n≤ f ( xt ) − ηtαl\n\n ( cid:107 ) ∇if ( xt ) ( cid:107 ) + 2ηtαu\n\n ( cid:107 ) σ ( cid:107 ) 1√\nb\n\n+\n\nt α2\nη2\nu\n2\n\n ( cid:107 ) l ( cid:107 ) 1.\n\nη2\nt α2\nu\n2\n\n ( cid:107 ) l ( cid:107 ) 1\n\n ( 6 ) \n\nsumming the above inequality for t  1 to t and using telescoping sum , we have the following\ninequality : \n\ne [ f ( xt +1 ) ] ≤ f ( x1 ) − ηαl\n\ne [ ( cid:107 ) ∇if ( xt ) ( cid:107 ) ] + 2ηt\n\nαu ( cid:107 ) σ ( cid:107 ) 1√\n\n+\n\nb\n\nη2α2\nut\n2\n\n ( cid:107 ) l ( cid:107 ) 1.\n\nt\n ( cid:88 ) \n\nh\n ( cid:88 ) \n\nt the terms of the above inequality , and dividing by ηt αl , we have : \n\n1\nt\n\nt\n ( cid:88 ) \n\nh\n ( cid:88 ) \n\nt [ ( cid:107 ) ∇if ( xt ) ( cid:107 ) ] ≤\n\nf ( x1 ) − e [ f ( xt +1 ) ] \n\nt ηαl\n\n2αu ( cid:107 ) σ ( cid:107 ) 1\n\n+\n\n√\n\n+\n\nbαl\n\nηα2\nu\n2αl\n\n ( cid:107 ) l ( cid:107 ) 1\n\nf ( x1 ) − f ( x∗ ) \n\n≤\n\nt ηαl\n\n+\n\n2αu ( cid:107 ) σ ( cid:107 ) 1\n\n√\n\nαl\n\nb\n\n+\n\nηα2\nu\n2αl\n\n ( cid:107 ) l ( cid:107 ) 1.\n\nb proof of theorem 3\n\nproof . we analyze the convergence of lamb for general minibatch size here . recall that the update\nof lamb is the following\n\nt+1  x ( i ) \nx ( i ) \n\nt − ηtφ ( ( cid:107 ) x ( i ) \n\nt ( cid:107 ) ) \n\nr ( i ) \nt\n ( cid:107 ) r ( i ) \nt ( cid:107 ) \n\n , \n\nfor all i ∈ [ h ] . for simplicity of notation , we reason the\n\nsince the function f is l-smooth , we have the following : \n\nf ( xt+1 ) ≤ f ( xt ) + ( cid:104 ) ∇if ( xt ) , x ( i ) \n\nt+1 − x ( i ) \n\nt ( cid:105 ) +\n\n ( cid:107 ) x ( i ) \n\nt+1 − x ( i ) \n\nt ( cid:107 ) 2\n\n f ( xt ) −ηt\n\nφ ( ( cid:107 ) x ( i ) \n\nt ( cid:107 ) ) ×\n\n [ ∇if ( xt ) ] j ×\n\nh\n ( cid:88 ) \n\ndi ( cid:88 ) \n\ni ( cid:124 ) \n\nr ( i ) \nt , j\n ( cid:107 ) r ( i ) \nt ( cid:107 ) \n\n ( cid:33 ) \n\n ( cid:125 ) \n\n+\n\nh\n ( cid:88 ) \n\ni ( 7 ) \n\nh\n ( cid:88 ) \n\ni ( cid:32 ) \n\n ( cid:123 ) ( cid:122 ) \nt1\n\n12\n\n published as a conference paper at iclr 2020\n\nthe above inequality simply follows from the lipschitz continuous nature of the gradient . we bound\nterm t1 in the following manner : \n\nt1 ≤ −ηt\n\nφ ( ( cid:107 ) x ( i ) \n\nt ( cid:107 ) ) ×\n\n [ ∇if ( xt ) ] j ×\n\n ( cid:32 ) \n\n ( cid:33 ) \n\nr ( i ) \nt , j\n ( cid:107 ) r ( i ) \nt ( cid:107 ) \n\n≤ −ηt\n\n ( cid:114 ) 1 − β2\ng2di\n\n ( cid:16 ) \n\nφ ( ( cid:107 ) x ( i ) \n\nt ( cid:107 ) ) × [ ∇if ( xt ) ] j × g ( i ) \n\nt , j\n\n ( cid:17 ) \n\nh\n ( cid:88 ) \n\ndi ( cid:88 ) \n\ni ( cid:88 ) \n\ndi ( cid:88 ) \n\ni ( ( cid:107 ) x ( i ) \n\nt ( cid:107 ) ) × [ ∇if ( xt ) ] j ×\n\n1 ( sign ( ∇if ( xt ) ] j ) ( cid:54 )  sign ( r ( i ) \n\nt , j ) ) \n\n ( cid:33 ) \n\nr ( i ) \nt , j\n ( cid:107 ) r ( i ) \nt ( cid:107 ) \n\n ( cid:33 ) \n\nr ( i ) \nt , j\n ( cid:107 ) r ( i ) \nt ( cid:107 ) \n\n− ηt\n\nh\n ( cid:88 ) \n\ndi ( cid:88 ) \n\n ( cid:32 ) \n\ni ( cid:88 ) \n\ndi ( cid:88 ) \n\ni ( cid:16 ) \n\n ( cid:114 ) 1\ndi\n\n− ηt\n\nh\n ( cid:88 ) \n\ndi ( cid:88 ) \n\n ( cid:32 ) \n\ni follows from the fact that ( cid:107 ) r ( i ) \nas follows : \n\nt ( cid:107 ) ≤\n\n ( cid:113 ) di\n1−β2\n\n√\n\nand\n\nvt ≤ g. if β2  0 , then t1 can be bounded\n\nt1 ≤ −ηt\n\nφ ( ( cid:107 ) x ( i ) \n\nt ( cid:107 ) ) × | [ ∇if ( xt ) ] j|\n\n ( cid:17 ) \n\nφ ( ( cid:107 ) x ( i ) \n\nt ( cid:107 ) ) × [ ∇if ( xt ) ] j ×\n\n1 ( sign ( ∇if ( xt ) ] j ) ( cid:54 )  sign ( r ( i ) \n\nt , j ) ) \n\nthe rest of the proof for β2  0 is similar to argument for the case β2 > 0 , which is shown below.\ntaking expectation , we have the following : \n\ne [ t1 ] ≤ −ηt\n\nh\n ( cid:88 ) \n\ndi ( cid:88 ) \n\ni ( cid:114 ) 1 − β2\ng2di\n\n ( cid:104 ) \nφ ( ( cid:107 ) x ( i ) \n\ne\n\nt ( cid:107 ) ) ×\n\n ( cid:16 ) \n\n [ ∇if ( xt ) ] j × g ( i ) \nt , j\n\n ( cid:17 ) ( cid:105 ) \n\nh\n ( cid:88 ) \n\ndi ( cid:88 ) \n\n ( cid:34 ) \nφ ( ( cid:107 ) x ( i ) \n\ne\n\n− ηt\n\n ( cid:32 ) \n\nt ( cid:107 ) ) ×\n\n [ ∇if ( xt ) ] j ×\n\n1 ( sign ( ∇if ( xt ) ] j ) ( cid:54 )  sign ( g ( i ) \n\nt , j ) ) \n\n ( cid:35 ) \n\n≤ −ηt\n\nh\n ( cid:88 ) \n\ndi ( cid:88 ) \n\ni ( cid:114 ) 1 − β2\ng2di\n\n ( cid:104 ) ( cid:16 ) \n\ne\n\nφ ( ( cid:107 ) x ( i ) \n\nt ( cid:107 ) ) × [ ∇if ( xt ) ] j × g ( i ) \n\nt , j\n\n ( cid:33 ) \n\nr ( i ) \nt , j\n ( cid:107 ) r ( i ) \nt ( cid:107 ) \n\n ( cid:17 ) ( cid:105 ) \n\ne\n\n ( cid:104 ) \nαu| [ ∇if ( xt ) ] j|1 ( sign ( ∇if ( xt ) ] j ) ( cid:54 )  sign ( g ( i ) \n\n ( cid:105 ) \n\nt , j ) ) \n\nh\n ( cid:88 ) \n\ndi ( cid:88 ) \n\n+ ηt\n\ni ( cid:114 ) 1 − β2\ng2di\n\nh\n ( cid:88 ) \n\ndi ( cid:88 ) \n\ni ηt\n\nh\n ( cid:88 ) \n\ndi ( cid:88 ) \n\ni −ηt\n\n ( cid:104 ) \nφ ( ( cid:107 ) x ( i ) \n\ne\n\nt ( cid:107 ) ) ×\n\n ( cid:16 ) \n\n [ ∇if ( xt ) ] j × g ( i ) \nt , j\n\n ( cid:17 ) ( cid:105 ) \n\nαu| [ ∇if ( xt ) ] j|p ( sign ( ∇if ( xt ) ] j ) ( cid:54 )  sign ( g ( i ) \n\nt , j ) ) \n\n ( 8 ) \n\n ( 9 ) \n\nusing the bound on the probability that the signs differ , we get : \n\ne [ t1 ] ≤ −ηtαl\n\n ( cid:107 ) ∇f ( xt ) ( cid:107 ) 2 + ηtαu\n\n ( cid:114 ) \n\nh ( 1 − β2 ) \n\ng2d\n\nh\n ( cid:88 ) \n\ndi ( cid:88 ) \n\ni , j√\nb\n\n.\n\nsubstituting the above bound on t1 in equation 7 , we have the following bound : \n\ne [ f ( xt+1 ) ] ≤ f ( xt ) − ηtαl\n\n ( cid:107 ) ∇f ( xt ) ( cid:107 ) 2 + ηtαu\n\n ( cid:107 ) ˜σ ( cid:107 ) 1√\nb\n\n+\n\nt α2\nη2\nu ( cid:107 ) l ( cid:107 ) 1\n2\n\n ( 10 ) \n\n ( cid:114 ) \n\nh ( 1 − β2 ) \n\ng2d\n\n13\n\n published as a conference paper at iclr 2020\n\nalgorithm 3 n-lamb\n\ninput : x1 ∈ rd , learning rate { ηt } t\nt , parame-\nters 0 < β1 , β2 < 1 , scaling function φ , ( cid:15 ) > 0 , \nparameters 0 < { βt\nset m0  0 , v0  0\nfor t  1 to t do\n\nt < 1\n\n1 } t\n\nalgorithm 4 nn-lamb\n\ninput : x1 ∈ rd , learning rate { ηt } t\nt , parameters\n0 < β1 , β2 < 1 , scaling function φ , ( cid:15 ) > 0 , parame-\nters 0 < { βt\nset m0  0 , v0  0\nfor t  1 to t do\n\nt < 1\n\n1 } t\n\n∇ ( cid:96 ) ( xt , st ) .\n\n∇ ( cid:96 ) ( xt , st ) .\n\n ( cid:80 ) \n\n+ ( 1−βt\n1−πt\n\ndraw b samples st from p.\ncompute gt  1\nst∈st\n|st|\nmt  β1mt−1 + ( 1 − β1 ) gt\nˆm  βt+1\n1 mt\n1 ) gt\ni βi\n1\n1\nvt  β2vt−1 + ( 1 − β2 ) g2\nt\nˆv  β2vt\n1−βt\n2\ncompute ratio rt  ˆm√\nx ( i ) \nt+1  x ( i ) \n\nt − ηt\n\nˆv+ ( cid:15 ) \n ( i ) \nt ( cid:107 ) ) \n ( i ) \nt ( cid:107 ) \n\nφ ( ( cid:107 ) x\n ( i ) \nt +λx\n\n ( cid:107 ) r\n\n ( cid:80 ) \n\n+ ( 1−βt\n1−πt\n\ndraw b samples st from p.\ncompute gt  1\nst∈st\n|st|\nmt  β1mt−1 + ( 1 − β1 ) gt\nˆm  βt+1\n1 mt\n1 ) gt\ni βi\n1\n1\nvt  β2vt−1 + ( 1 − β2 ) g2\nt\nˆv  βt+1\n2 ) g2\n+ ( 1−βt\nvt\nt\n1−πt\ni ratio rt  ˆm√\nx ( i ) \nt+1  x ( i ) \n\nt − ηt\n\ni βi\n2\n\nˆv+ ( cid:15 ) \n ( i ) \nt ( cid:107 ) ) \n ( i ) \nt ( cid:107 ) \n\nφ ( ( cid:107 ) x\n ( i ) \nt +λx\n\n ( cid:107 ) r\n\n2\n\nend for\n\nend for\n\n ( r ( i ) \n\nt + λxt ) \n\n ( r ( i ) \n\nt + λxt ) \n\nsumming the above inequality for t  1 to t and using telescoping sum , we have the following\ninequality : \n\ne [ f ( xt +1 ) ] ≤ f ( x1 ) − ηtαl\n\ne [ ( cid:107 ) ∇f ( xt ) ( cid:107 ) 2 ] + ηt αu\n\n ( cid:114 ) \n\nh ( 1 − β2 ) \n\ng2d\n\nt\n ( cid:88 ) \n\nt ( cid:107 ) ˜σ ( cid:107 ) 1√\nb\n\n+\n\nη2α2\nut\n2\n\n ( cid:107 ) l ( cid:107 ) 1.\n\nrearranging the terms of the above inequality , and dividing by ηt αl , we have : \n\n ( cid:114 ) \n\nh ( 1 − β2 ) \n\ng2d\n\n1\nt\n\nt\n ( cid:88 ) \n\nt [ ( cid:107 ) ∇f ( xt ) ( cid:107 ) 2 ] ≤\n\nf ( x1 ) − e [ f ( xt +1 ) ] \n\nt ηαl\n\nf ( x1 ) − f ( x∗ ) \n\n≤\n\nt ηαl\n\n+\n\nαu ( cid:107 ) ˜σ ( cid:107 ) 1\nαl\nb\n\n√\n\n+\n\n√\n\nαu ( cid:107 ) ˜σ ( cid:107 ) 1\nb\nαl\nηα2\nu\n2αl\n\n+\n\n+\n\n ( cid:107 ) l ( cid:107 ) 1\n\nη\n2\n\n ( cid:107 ) l ( cid:107 ) 1.\n\nc comparison of convergence rates of lars and sgd\n\ninspired by the comparison used by ( bernstein et al. , 2018 ) for comparing sign sgd with sgd , we\ndeﬁne the following quantities : \n\n ( cid:33 ) 2\n\n ( cid:107 ) ∇if ( xt ) ( cid:107 ) \n\n ( cid:32 ) h\n ( cid:88 ) \n\ni ( ∇f ( xt ) ) d ( cid:107 ) ∇f ( xt ) ( cid:107 ) 2\n\nψgd ( cid:107 ) ∇f ( xt ) ( cid:107 ) 2\n\n≥\n\nh\n\nthen lars convergence rate can be written in the following manner : \n\n ( e [ ( cid:107 ) ∇f ( xa ) ( cid:107 ) ) 2 ≤ o\n\n ( cid:18 ) ( f ( x1 ) − f ( x∗ ) ) l∞\n\nψl\nψ2\ng\n\n+\n\n ( cid:107 ) σ ( cid:107 ) 2\n\nt\n\n ( cid:19 ) \n\n.\n\nψ2\nσ\nψ2\ng\n\nif ψl ( cid:28 ) ψ2\nwe gain over sgd . otherwise , sgd ’ s upper bound on convergence rate is better.\n\ng then lars ( i.e. , gradient is more denser than curvature or stochasticity ) , \n\ng and ψσ ( cid:28 ) ψ2\n\n ( cid:107 ) l ( cid:107 ) 2\n\n1 ≤\n\n ( cid:107 ) σ ( cid:107 ) 2\n\n1  ( cid:107 ) l ( cid:107 ) 2\n∞\n\nh2\n\nψσd ( cid:107 ) σ ( cid:107 ) 2\n\n.\n\nh\n\nh\n\nt\n\n14\n\n published as a conference paper at iclr 2020\n\nfigure 1 : this ﬁgure shows n-lamb and nn-lamb can achieve a comparable accuracy compared\nto lamb optimizer . their performances are much better than momentum solver . the result of\nmomentum optimizer was reported by goyal et al . ( 2017 ) . for nadam , we use the learning rate recipe\nof ( goyal et al. , 2017 ) : ( 1 ) 5-epoch warmup to stablize the initial stage ; and ( 2 ) multiply the learning\nrate by 0.1 at 30th , 60th , and 80th epoch . the target accuracy is around 0.763 ( goyal et al. , 2017 ) .\nwe also tuned the learning rate of nadam in { 1e-4 , 2e-4 , ... , 9e-4 , 1e-3 , 2e-3 , ... , 9e-3 , 1e-2 } .\n\nd n-lamb : nesterov momentum for lamb\n\nsutskever et al . ( 2013 ) report that nesterov ’ s accelerated gradient ( nag ) proposed by nesterov ( 1983 ) \nis conceptually and empirically better than the regular momentum method for convex , non-stochastic\nobjectives . dozat ( 2016 ) incorporated nesterov ’ s momentum into adam optimizer and proposed\nthe nadam optimizer . speciﬁcally , only the ﬁrst moment of adam was modiﬁed and the second\nmoment of adam was unchanged . the results on several applications ( word2vec , image recognition , \nand lstm language model ) showed that nadam optimizer improves the speed of convergence\nand the quality of the learned models . we also tried using nesterov ’ s momentum to replace the\nregular momentum of lamb optimizer ’ s ﬁrst moment . in this way , we got a new algorithm named\nas n-lamb ( nesterov lamb ) . the complete algorithm is in algorithm 3 . we can also nesterov ’ s\nmomentum to replace the regular momentum of lamb optimizer ’ s second moment . we refer to this\nalgorithm as nn-lamb ( nesterov ’ s momentum for both the ﬁrst moment and the second moment ) .\nthe details of nn-lamb were shown in algorithm 4.\n\ndozat ( 2016 ) suggested the best performance of nadam was achieved by β1  0.975 , β2  0.999 , and\n ( cid:15 )  1e-8 . we used the same settings for n-lamb and nn-lamb . we scaled the batch size to 32k\nfor imagenet training with resnet-50 . our experimental results show that n-lamb and nn-lamb\ncan achieve a comparable accuracy compared to lamb optimizer . their performances are much\nbetter than momentum solver ( figure 1 ) .\n\ne lamb with learning rate correction\n\nthere are two operations at each iteration in original adam optimizer ( let us call it adam-correction ) : \n\nmt  mt/ ( 1 − βt\n1 ) \n\nvt  vt/ ( 1 − βt\n2 ) \nit has an impact on the learning rate by ηt :  ηt∗ ( cid:112 ) ( 1 − βt\n1 ) . according to our experimental\nresults , adam-correction essentially has the same effect as learning rate warmup ( see figure 2 ) . the\nwarmup function often was implemented in the modern deep learning system . thus , we can remove\nadam-correction from the lamb optimizer . we did not observe any drop in the test or validation\naccuracy for bert and imagenet training.\n\n2 ) / ( 1 − βt\n\n15\n\n published as a conference paper at iclr 2020\n\nfigure 2 : the ﬁgure shows that adam-correction has the same effect as learning rate warmup . we\nremoved adam-correction from the lamb optimizer . we did not observe any drop in the test or\nvalidation accuracy for bert and imagenet training.\n\nfigure 3 : we tried different norms in lamb optimizer . however , we did not observe a signiﬁcant\ndifference in the validation accuracy of imagenet training with resnet-50 . we use l2 norm as the\ndefault.\n\nf lamb with different norms\n\nwe need to compute the matrix/tensor norm for each layer when we do the parameter updating in\nthe lamb optimizer . we tried different norms in lamb optimizer . however , we did not observe\na signiﬁcant difference in the validation accuracy of imagenet training with resnet-50 . in our\nexperiments , the difference in validation accuracy is less than 0.1 percent ( figure 3 ) . we use l2 norm\nas the default.\n\ng regular batch sizes for small datasets : mnist and cifar-10.\n\naccording to dawnbench , davidnet ( a custom 9-layer residual convnet ) is the fastest model\nfor cifar-10 dataset ( as of april 1st , 2019 ) 5 . the baseline uses the momentum sgd optimizer.\ntable 6 and figure 4 show the test accuracy of cifar-10 training with davidnet . the pytorch\nimplementation ( momentum sgd optimizer ) on gpus was reported on standford dawnbench ’ s\nwebsite , which achieves 94.06 % in 24 epochs . the tensorﬂow implementation ( momentum sgd\noptimizer ) on tpu achieves a 93.72 % accuracy in 24 epochs6 . we use the implementation of\ntensorflow on tpus . lamb optimizer is able to achieve 94.08 % test accuracy in 24 epochs , which\nis better than other adaptive optimizers and momentum sgd . even on the smaller tasks like mnist\ntraining with lenet , lamb is able to achieve a better accuracy than existing solvers ( table 7 ) .\n\n5https : //dawn.cs.stanford.edu/benchmark/cifar10/train.html\n6https : //github.com/fenwickslab/dl_tutorials/blob/master/tutorial3_cifar10_davidnet_ﬁx.ipynb\n\n16\n\n published as a conference paper at iclr 2020\n\nfigure 4 : lamb is better than the existing solvers ( batch size  512 ) . we make sure all the solvers are\ncarefully tuned . the learning rate tuning space of adam , adamw , adagrad and lamb is { 0.0001 , \n0.0002 , 0.0004 , 0.0006 , 0.0008 , 0.001 , 0.002 , 0.004 , 0.006 , 0.008 , 0.01 , 0.02 , 0.04 , 0.06 , 0.08 , 0.1 , \n0.2 , 0.4 , 0.6 , 0.8 , 1 , 2 , 4 , 6 , 8 , 10 , 15 , 20 , 25 , 30 , 35 , 40 , 45 , 50 } . the momentum optimizer was tuned\nby the baseline implementer . the weight decay term of adamw was tuned by { 0.0001 , 0.001 , 0.01 , \n0.1 , 1.0 } .\n\ntable 6 : cifar-10 training with davidnet ( batch size  512 ) . all of them run 24 epochs and ﬁnish\nthe training under one minute on one cloud tpu . we make sure all the solvers are carefully tuned.\nthe learning rate tuning space of adam , adamw , adagrad and lamb is { 0.0001 , 0.0002 , 0.0004 , \n0.0006 , 0.0008 , 0.001 , 0.002 , 0.004 , 0.006 , 0.008 , 0.01 , 0.02 , 0.04 , 0.06 , 0.08 , 0.1 , 0.2 , 0.4 , 0.6 , 0.8 , \n1 , 2 , 4 , 6 , 8 , 10 , 15 , 20 , 25 , 30 , 35 , 40 , 45 , 50 } . the momentum optimizer was tuned by the baseline\nimplementer . the weight decay term of adamw was tuned by { 0.0001 , 0.001 , 0.01 , 0.1 , 1.0 } .\n\noptimizer\n\nadagrad adam adamw momentum lamb\n\ntest accuracy\n\n0.9074\n\n0.9225\n\n0.9271\n\n0.9372\n\n0.9408\n\nh implementation details and additional results\n\nthere are several hyper-parameters in lamb optimizer . although users do not need to tune them , \nwe explain them to help users to have a better understanding . β1 is used for decaying the running\naverage of the gradient . β2 is used for decaying the running average of the square of gradient . the\ndefault setting for other parameters : weight decay rate λ , β1 , β2 , ( cid:15 )  . we did not\ntune β1 and β2 . however , our experiments show that tuning them may get a higher accuracy.\n\nbased on our experience , learning rate is the most important hyper-parameter that affects the learning\nefﬁciency and ﬁnal accuracy . bengio ( 2012 ) suggests that it is often the single most important\nhyper-parameter and that it always should be tuned . thus , to make sure we have a solid baseline , we\ncarefully tune the learning rate of adam , adamw , adagrad , and momentum sgd\n\nin our experiments , we found that the validation loss is not reliable for large-batch training . a lower\nvalidation loss does not necessarily lead to a higher validation accuracy ( figure 5 ) . thus , we use the\ntest/val accuracy or f1 score on dev set to evaluate the optimizers.\n\nh.0.1 bert\n\ntable 8 shows some of the tuning information from bert training with adamw optimizer . adamw\nstops scaling at the batch size of 16k . the target f1 score is 90.5 . lamb achieves a f1 score of\n91.345 . the table shows the tuning information of adamw . in table 8 , we report the best f1 score\nwe observed from our experiments.\n\nthe loss curves of bert training by lamb for different batch sizes are shown in figure 6 . we\nobserve that the loss curves are almost identical to each other , which means our optimizer scales well\nwith the batch size.\n\nthe training loss curve of bert mixed-batch pre-training with lamb is shown in figure 7 . this\nﬁgure shows that lamb can make the training converge smoothly at the batch size of 64k.\n\nfigure 8 shows that we can achieve 76.8 % scaling efﬁciency by scaling the batch size ( 49.1 times\nspeedup by 64 times computational resources ) and 101.8 % scaling efﬁciency with mixed-batch ( 65.2\ntimes speedup by 64 times computational resources ) \n\n17\n\n published as a conference paper at iclr 2020\n\ntable 7 : test accuracy by mnist training with lenet ( 30 epochs for batch size  1024 ) . the\ntuning space of learning rate for all the optimizers is { 0.0001 , 0.001 , 0.01 , 0.1 } . we use the same\nlearning rate warmup and decay schedule for all of them.\n\noptimizer\n\nmomentum addgrad adam adamw lamb\n\naverage accuracy over 5 runs\n\n0.9933\n\n0.9928\n\n0.9936\n\n0.9941\n\n0.9945\n\nfigure 5 : our experiments show that even the validation loss is not reliable in the large-scale training.\na lower validation loss may lead to a worse accuracy . thus , we use the test/val accuracy or f1 score\non dev set to evaluate the optimizers.\n\nh.0.2\n\nimagenet\n\nfigures 9 - 14 show the lamb trust ratio at different iterations for imagenet training with resnet-50.\nfrom these ﬁgures we can see that these ratios are very different from each other for different layers.\nlamb uses the trust ratio to help the slow learners to train faster.\n\nh.1 baseline tuning details for imagenet training with resnet-50\n\nif you are not interested in the baseline tuning details , please skip this section.\n\ngoyal et al . ( 2017 ) suggested a proper learning rate warmup and decay scheme may help improve\nthe imagenet classiﬁcation accuracy . we included these techniques in adam/adamw/adagrad\ntuning . speciﬁcally , we use the learning rate recipe of goyal et al . ( 2017 ) : ( 1 ) 5-epoch warmup\nto stablize the initial stage ; and ( 2 ) multiply the learning rate by 0.1 at 30th , 60th , and 80th\nepoch . the target accuracy is around 76.3 % ( goyal et al. , 2017 ) . there techniques help to im-\nprove the accuracy of adam/adamw/adagrad to around 73 % . however , even with these techniques , \nadam/adamw/adagrad stil can not achieve the target validation accuracy.\n\nto make sure our baseline is solid , we carefully tuned the hyper-parameters . table 9 shows the tuning\ninformation of standard adagrad . table 10 shows the tuning information of adding the learning rate\nscheme of goyal et al . ( 2017 ) to standard adagrad . table 11 shows the tuning information of standard\nadam . table shows the tuning information of adding the learning rate scheme of goyal et al . ( 2017 ) \nto standard adam . it is tricky to tune the adamw optimizer since both the l2 regularization and\nweight decay have the effect on the performance . thus we have four tuning sets.\n\nthe ﬁrst tuning set is based on adamw with default l2 regularization . we tune the learning rate and\nweight decay . the tuning information is in figures 13 , 14 , 15 , and 16.\n\nthe second tuning set is based on adamw with disabled l2 regularization . we tune the learning rate\nand weight decay . the tuning information is in figures 17 , 18 , 19 , and 20.\n\n18\n\n published as a conference paper at iclr 2020\n\ntable 8 : adamw stops scaling at the batch size of 16k . the target f1 score is 90.5 . lamb achieves\na f1 score of 91.345 . the table shows the tuning information of adamw . in this table , we report the\nbest f1 score we observed from our experiments.\nlr\n\nbatch size warmup steps\n\nlast step infomation\n\nf1 score on dev set\n\nsolver\n\nadamw\nadamw\nadamw\nadamw\nadamw\nadamw\nadamw\nadamw\nadamw\n\n16k\n16k\n16k\n16k\n16k\n16k\n16k\n16k\n16k\n\n0.05×31250\n0.05×31250\n0.05×31250\n0.10×31250\n0.10×31250\n0.10×31250\n0.20×31250\n0.20×31250\n0.20×31250\n\n0.0001\n0.0002\n0.0003\n0.0001\n0.0002\n0.0003\n0.0001\n0.0002\n0.0003\n\nloss , step , step , step , step , step , step , step , step , step 6 : this ﬁgure shows the training loss curve of lamb optimizer . we just want to use this ﬁgure\nto show that lamb can make the training converge smoothly . even if we scale the batch size to the\nextremely large cases , the loss curves are almost identical to each other.\n\nthen we add the learning rate scheme of goyal et al . ( 2017 ) to adamw and refer to it as adamw+.\n\nthe third tuning set is based on adamw+ with default l2 regularization . we tune the learning rate\nand weight decay . the tuning information is figure 21 and 22.\n\nthe fourth tuning set is based on adamw+ with disabled l2 regularization . we tune the learning rate\nand weight decay . the tuning information is in figures 23 , 24 , 25.\n\nbased on our comprehensive tuning results , we conclude the existing adaptive solvers do not perform\nwell on imagenet training or at least it is hard to tune them.\n\n19\n\n published as a conference paper at iclr 2020\n\nfigure 7 : this ﬁgure shows the training loss curve of lamb optimizer . this ﬁgure shows that lamb\ncan make the training converge smoothly at the extremely large batch size ( e.g . 64k ) .\n\nfigure 8 : we achieve 76.8 % scaling efﬁciency ( 49 times speedup by 64 times computational resources ) \nand 101.8 % scaling efﬁciency with a mixed , scaled batch size ( 65.2 times speedup by 64 times\ncomputational resources ) . 1024-mixed means the mixed-batch training on 1024 tpus.\n\nfigure 9 : the lamb trust ratio.\n\n20\n\n published as a conference paper at iclr 2020\n\nfigure 10 : the lamb trust ratio.\n\nfigure 11 : the lamb trust ratio.\n\nfigure 12 : the lamb trust ratio.\n\nfigure 13 : the lamb trust ratio.\n\n21\n\n published as a conference paper at iclr 2020\n\nfigure 14 : the lamb trust ratio.\n\ntable 9 : the accuracy information of tuning default adagrad optimizer for imagenet training with\nresnet-50 ( batch size  16384 , 90 epochs , 7038 iterations ) .\n\nlearning rate top-1 validation accuracy\n\n0.0001\n0.001\n0.002\n0.004\n0.008\n0.010\n0.020\n0.040\n0.080\n0.100\n0.200\n0.400\n0.800\n1.000\n2.000\n4.000\n6.000\n8.000\n10.00\n12.00\n14.00\n16.00\n18.00\n20.00\n30.00\n40.00\n50.00\n\n0.0026855469\n0.015563965\n0.022684732\n0.030924479\n0.04486084\n0.054158527\n0.0758667\n0.1262614\n0.24037679\n0.27357993\n0.458313\n0.553833\n0.54103595\n0.5489095\n0.47680664\n0.5295207\n0.36950684\n0.31081137\n0.30670166\n0.3091024\n0.3227946\n\n0.0063680015\n0.11287435\n0.21602376\n0.08315023\n0.0132039385\n0.0009969076\n\n22\n\n published as a conference paper at iclr 2020\n\ntable 10 : the accuracy information of tuning adagrad optimizer for imagenet training with resnet-\n50 ( batch size  16384 , 90 epochs , 7038 iterations ) . we use the learning rate recipe of ( goyal et al. , \n2017 ) : ( 1 ) 5-epoch warmup to stablize the initial stage ; and ( 2 ) multiply the learning rate by 0.1 at\n30th , 60th , and 80th epoch . the target accuracy is around 0.763 ( goyal et al. , 2017 ) .\n\nlearning rate top-1 validation accuracy\n\n0.0001\n0.001\n0.002\n0.004\n0.008\n0.010\n0.020\n0.040\n0.080\n0.100\n0.200\n0.400\n0.800\n1.000\n2.000\n4.000\n6.000\n8.000\n10.00\n12.00\n14.00\n16.00\n18.00\n20.00\n30.00\n40.00\n50.00\n\n0.0011189779\n0.00793457\n0.012573242\n0.019022623\n0.027079264\n0.029012045\n0.0421346\n0.06618246\n0.10970052\n0.13429768\n0.26550293\n0.41918945\n0.5519816\n0.58614093\n0.67252606\n0.70306396\n0.709493\n0.7137858\n0.71797687\n0.7187703\n0.72007245\n0.7194214\n0.7149251\n0.71293133\n0.70458984\n0.69085693\n0.67976886\n\n23\n\n published as a conference paper at iclr 2020\n\ntable 11 : the accuracy information of tuning default adam optimizer for imagenet training with\nresnet-50 ( batch size  16384 , 90 epochs , 7038 iterations ) . the target accuracy is around 0.763\n ( goyal et al. , 2017 ) .\n\nlearning rate top-1 validation accuracy\n\ntable 12 : the accuracy information of tuning adam optimizer for imagenet training with resnet-50\n ( batch size  16384 , 90 epochs , 7038 iterations ) . we use the learning rate recipe of ( goyal et al. , \n2017 ) : ( 1 ) 5-epoch warmup to stablize the initial stage ; and ( 2 ) multiply the learning rate by 0.1 at\n30th , 60th , and 80th epoch . the target accuracy is around 0.763 ( goyal et al. , 2017 ) .\n\nlearning rate top-1 validation accuracy\n\n0.0001\n0.0002\n0.0004\n0.0006\n0.0008\n0.001\n0.002\n0.004\n0.006\n0.008\n0.010\n\n0.0001\n0.0002\n0.0004\n0.0006\n0.0008\n0.001\n0.002\n0.004\n0.006\n0.008\n0.010\n0.012\n0.014\n0.016\n0.020\n0.040\n0.060\n0.080\n0.100\n\n0.5521\n0.6089\n0.6432\n0.6465\n0.6479\n0.6604\n0.6408\n0.5687\n0.5165\n0.4812\n0.3673\n\n0.410319\n0.55263263\n0.6455485\n0.6774495\n0.6996867\n0.71010333\n0.73476154\n0.73286945\n0.72648114\n0.72214764\n0.71466064\n0.7081502\n0.6993001\n0.69108075\n0.67997235\n0.58658856\n0.51090497\n0.45174155\n0.40297446\n\n24\n\n published as a conference paper at iclr 2020\n\ntable 13 : the accuracy information of tuning default adamw optimizer for imagenet training with\nresnet-50 ( batch size  16384 , 90 epochs , 7038 iterations ) . the target accuracy is around 0.763\n ( goyal et al. , 2017 ) .\n\nlearning rate weight decay l2 regularization top-1 validation accuracy\n\n0.0001\n0.0002\n0.0004\n0.0006\n0.0008\n0.001\n0.002\n0.004\n0.006\n0.008\n0.010\n0.012\n0.014\n0.016\n0.018\n0.020\n0.025\n0.030\n0.040\n0.050\n\n0.00001\n0.00001\n0.00001\n0.00001\n0.00001\n0.00001\n0.00001\n0.00001\n0.00001\n0.00001\n0.00001\n0.00001\n0.00001\n0.00001\n0.00001\n0.00001\n0.00001\n0.00001\n0.00001\n0.00001\n\ndefault ( 0.01 ) \ndefault ( 0.01 ) \ndefault ( 0.01 ) \ndefault ( 0.01 ) \ndefault ( 0.01 ) \ndefault ( 0.01 ) \ndefault ( 0.01 ) \ndefault ( 0.01 ) \ndefault ( 0.01 ) \ndefault ( 0.01 ) \ndefault ( 0.01 ) \ndefault ( 0.01 ) \ndefault ( 0.01 ) \ndefault ( 0.01 ) \ndefault ( 0.01 ) \ndefault ( 0.01 ) \ndefault ( 0.01 ) \ndefault ( 0.01 ) \ndefault ( 0.01 ) \ndefault ( 0.01 ) \n\n0.53312176\n0.5542806\n0.48769125\n0.46317545\n0.40903726\n0.42401123\n0.33870444\n0.12339274\n0.122924805\n0.08099365\n0.016764322\n0.032714844\n0.018147787\n0.0066731772\n0.010294597\n0.008260091\n0.008870442\n0.0064493814\n0.0018107096\n0.003540039\n\n25\n\n published as a conference paper at iclr 2020\n\ntable 14 : the accuracy information of tuning default adamw optimizer for imagenet training with\nresnet-50 ( batch size  16384 , 90 epochs , 7038 iterations ) . the target accuracy is around 0.763\n ( goyal et al. , 2017 ) .\n\nlearning rate weight decay l2 regularization top-1 validation accuracy\n\n0.0001\n0.0002\n0.0004\n0.0006\n0.0008\n0.001\n0.002\n0.004\n0.006\n0.008\n0.010\n0.012\n0.014\n0.016\n0.018\n0.020\n0.025\n0.030\n0.040\n0.050\n\n0.0001\n0.0001\n0.0001\n0.0001\n0.0001\n0.0001\n0.0001\n0.0001\n0.0001\n0.0001\n0.0001\n0.0001\n0.0001\n0.0001\n0.0001\n0.0001\n0.0001\n0.0001\n0.0001\n0.0001\n\ndefault ( 0.01 ) \ndefault ( 0.01 ) \ndefault ( 0.01 ) \ndefault ( 0.01 ) \ndefault ( 0.01 ) \ndefault ( 0.01 ) \ndefault ( 0.01 ) \ndefault ( 0.01 ) \ndefault ( 0.01 ) \ndefault ( 0.01 ) \ndefault ( 0.01 ) \ndefault ( 0.01 ) \ndefault ( 0.01 ) \ndefault ( 0.01 ) \ndefault ( 0.01 ) \ndefault ( 0.01 ) \ndefault ( 0.01 ) \ndefault ( 0.01 ) \ndefault ( 0.01 ) \ndefault ( 0.01 ) \n\n0.55489093\n0.56514484\n0.4986979\n0.47595215\n0.44685873\n0.41029868\n0.2808024\n0.08111572\n0.068115234\n0.057922363\n0.05222575\n0.017313639\n0.029785156\n0.016540527\n0.00575765\n0.0102335615\n0.0060831704\n0.0036417644\n0.0010782877\n0.0037638347\n\n26\n\n published as a conference paper at iclr 2020\n\ntable 15 : the accuracy information of tuning default adamw optimizer for imagenet training with\nresnet-50 ( batch size  16384 , 90 epochs , 7038 iterations ) . the target accuracy is around 0.763\n ( goyal et al. , 2017 ) .\n\nlearning rate weight decay l2 regularization top-1 validation accuracy\n\n0.0001\n0.0002\n0.0004\n0.0006\n0.0008\n0.001\n0.002\n0.004\n0.006\n0.008\n0.010\n0.012\n0.014\n0.016\n0.018\n0.020\n0.025\n0.030\n0.040\n0.050\n\n0.001\n0.001\n0.001\n0.001\n0.001\n0.001\n0.001\n0.001\n0.001\n0.001\n0.001\n0.001\n0.001\n0.001\n0.001\n0.001\n0.001\n0.001\n0.001\n0.001\n\ndefault ( 0.01 ) \ndefault ( 0.01 ) \ndefault ( 0.01 ) \ndefault ( 0.01 ) \ndefault ( 0.01 ) \ndefault ( 0.01 ) \ndefault ( 0.01 ) \ndefault ( 0.01 ) \ndefault ( 0.01 ) \ndefault ( 0.01 ) \ndefault ( 0.01 ) \ndefault ( 0.01 ) \ndefault ( 0.01 ) \ndefault ( 0.01 ) \ndefault ( 0.01 ) \ndefault ( 0.01 ) \ndefault ( 0.01 ) \ndefault ( 0.01 ) \ndefault ( 0.01 ) \ndefault ( 0.01 ) \n\n0.21142578\n0.4289144\n0.13537598\n0.33803305\n0.32611084\n0.22194417\n0.1833903\n0.08256022\n0.020507812\n0.018269857\n0.007507324\n0.020080566\n0.010762532\n0.0021362305\n0.007954915\n0.005859375\n0.009724935\n0.0019124349\n0.00390625\n0.0009969076\n\n27\n\n published as a conference paper at iclr 2020\n\ntable 16 : the accuracy information of tuning default adamw optimizer for imagenet training with\nresnet-50 ( batch size  16384 , 90 epochs , 7038 iterations ) . the target accuracy is around 0.763\n ( goyal et al. , 2017 ) .\n\nlearning rate weight decay l2 regularization top-1 validation accuracy\n\n0.0001\n0.0002\n0.0004\n0.0006\n0.0008\n0.001\n0.002\n0.004\n0.006\n0.008\n0.010\n0.012\n0.014\n0.016\n0.018\n0.020\n0.025\n0.030\n0.040\n0.050\n\n0.01\n0.01\n0.01\n0.01\n0.01\n0.01\n0.01\n0.01\n0.01\n0.01\n0.01\n0.01\n0.01\n0.01\n0.01\n0.01\n0.01\n0.01\n0.01\n0.01\n\n0.0009765625\n0.0009969076\n0.0010172526\n0.0009358724\n0.0022379558\n0.001566569\n0.009480794\n0.0033569336\n0.0029907227\n0.0018513998\n0.009134929\n0.0022176106\n0.0040690103\n0.0017293295\n0.00061035156\n0.0022379558\n0.0017089844\n0.0014241537\n0.0020345051\n0.0012817383\n\ndefault ( 0.01 ) \ndefault ( 0.01 ) \ndefault ( 0.01 ) \ndefault ( 0.01 ) \ndefault ( 0.01 ) \ndefault ( 0.01 ) \ndefault ( 0.01 ) \ndefault ( 0.01 ) \ndefault ( 0.01 ) \ndefault ( 0.01 ) \ndefault ( 0.01 ) \ndefault ( 0.01 ) \ndefault ( 0.01 ) \ndefault ( 0.01 ) \ndefault ( 0.01 ) \ndefault ( 0.01 ) \ndefault ( 0.01 ) \ndefault ( 0.01 ) \ndefault ( 0.01 ) \ndefault ( 0.01 ) \n\n28\n\n published as a conference paper at iclr 2020\n\ntable 17 : the accuracy information of tuning default adamw optimizer for imagenet training with\nresnet-50 ( batch size  16384 , 90 epochs , 7038 iterations ) . the target accuracy is around 0.763\n ( goyal et al. , 2017 ) .\n\nlearning rate weight decay l2 regularization top-1 validation accuracy\n\n0.0001\n0.0002\n0.0004\n0.0006\n0.0008\n0.001\n0.002\n0.004\n0.006\n0.008\n0.010\n0.012\n0.014\n0.016\n0.018\n0.020\n0.025\n0.030\n0.040\n0.050\n\n0.00001\n0.00001\n0.00001\n0.00001\n0.00001\n0.00001\n0.00001\n0.00001\n0.00001\n0.00001\n0.00001\n0.00001\n0.00001\n0.00001\n0.00001\n0.00001\n0.00001\n0.00001\n0.00001\n0.00001\n\n0.48917642\n0.58152264\n0.63460284\n0.64849854\n0.6598918\n0.6662801\n0.67266846\n0.6692708\n0.6573079\n0.6639404\n0.65230304\n0.6505534\n0.64990234\n0.65323895\n0.67026776\n0.66086835\n0.65425617\n0.6476237\n0.55478925\n0.61869305\n\ndisable\ndisable\ndisable\ndisable\ndisable\ndisable\ndisable\ndisable\ndisable\ndisable\ndisable\ndisable\ndisable\ndisable\ndisable\ndisable\ndisable\ndisable\ndisable\ndisable\n\n29\n\n published as a conference paper at iclr 2020\n\ntable 18 : the accuracy information of tuning default adamw optimizer for imagenet training with\nresnet-50 ( batch size  16384 , 90 epochs , 7038 iterations ) . the target accuracy is around 0.763\n ( goyal et al. , 2017 ) .\n\nlearning rate weight decay l2 regularization top-1 validation accuracy\n\n0.0001\n0.0002\n0.0004\n0.0006\n0.0008\n0.001\n0.002\n0.004\n0.006\n0.008\n0.010\n0.012\n0.014\n0.016\n0.018\n0.020\n0.025\n0.030\n0.040\n0.050\n\n0.0001\n0.0001\n0.0001\n0.0001\n0.0001\n0.0001\n0.0001\n0.0001\n0.0001\n0.0001\n0.0001\n0.0001\n0.0001\n0.0001\n0.0001\n0.0001\n0.0001\n0.0001\n0.0001\n0.0001\n\n0.5033366\n0.5949707\n0.62561035\n0.6545207\n0.66326904\n0.6677043\n0.67244464\n0.6702881\n0.66033936\n0.66426593\n0.66151935\n0.6545817\n0.65509033\n0.6529338\n0.65651447\n0.65334064\n0.655009\n0.64552814\n0.6425374\n0.5988159\n\ndisable\ndisable\ndisable\ndisable\ndisable\ndisable\ndisable\ndisable\ndisable\ndisable\ndisable\ndisable\ndisable\ndisable\ndisable\ndisable\ndisable\ndisable\ndisable\ndisable\n\n30\n\n published as a conference paper at iclr 2020\n\ntable 19 : the accuracy information of tuning default adamw optimizer for imagenet training with\nresnet-50 ( batch size  16384 , 90 epochs , 7038 iterations ) . the target accuracy is around 0.763\n ( goyal et al. , 2017 ) .\n\nlearning rate weight decay l2 regularization top-1 validation accuracy\n\n0.0001\n0.0002\n0.0004\n0.0006\n0.0008\n0.001\n0.002\n0.004\n0.006\n0.008\n0.010\n0.012\n0.014\n0.016\n0.018\n0.020\n0.025\n0.030\n0.040\n0.050\n\n0.001\n0.001\n0.001\n0.001\n0.001\n0.001\n0.001\n0.001\n0.001\n0.001\n0.001\n0.001\n0.001\n0.001\n0.001\n0.001\n0.001\n0.001\n0.001\n0.001\n\n0.4611206\n\n0.0076293945\n0.29233804\n0.57295734\n0.5574748\n0.5988566\n0.586263\n0.62076825\n0.61503094\n0.4697876\n0.619751\n0.54243976\n0.5429077\n0.55281574\n0.5819295\n0.5938924\n0.541097\n0.45890298\n0.56193036\n0.5279134\n\ndisable\ndisable\ndisable\ndisable\ndisable\ndisable\ndisable\ndisable\ndisable\ndisable\ndisable\ndisable\ndisable\ndisable\ndisable\ndisable\ndisable\ndisable\ndisable\ndisable\n\n31\n\n published as a conference paper at iclr 2020\n\ntable 20 : the accuracy information of tuning default adamw optimizer for imagenet training with\nresnet-50 ( batch size  16384 , 90 epochs , 7038 iterations ) . the target accuracy is around 0.763\n ( goyal et al. , 2017 ) .\n\nlearning rate weight decay l2 regularization top-1 validation accuracy\n\n0.0001\n0.0002\n0.0004\n0.0006\n0.0008\n0.001\n0.002\n0.004\n0.006\n0.008\n0.010\n0.012\n0.014\n0.016\n0.018\n0.020\n0.025\n0.030\n0.040\n0.050\n\n0.01\n0.01\n0.01\n0.01\n0.01\n0.01\n0.01\n0.01\n0.01\n0.01\n0.01\n0.01\n0.01\n0.01\n0.01\n0.01\n0.01\n0.01\n0.01\n0.01\n\n0.0009969076\n0.0008951823\n0.00095621747\n0.0012817383\n0.016886393\n0.038146973\n0.0015258789\n0.0014241537\n0.081441246\n0.028116861\n0.011820476\n0.08138021\n0.010111491\n0.0041910806\n0.0038248699\n0.002746582\n0.011555989\n0.0065104165\n0.016438803\n0.007710775\n\ndisable\ndisable\ndisable\ndisable\ndisable\ndisable\ndisable\ndisable\ndisable\ndisable\ndisable\ndisable\ndisable\ndisable\ndisable\ndisable\ndisable\ndisable\ndisable\ndisable\n\n32\n\n published as a conference paper at iclr 2020\n\ntable 21 : the accuracy information of tuning adamw optimizer for imagenet training with resnet-\n50 ( batch size  16384 , 90 epochs , 7038 iterations ) . we use the learning rate recipe of ( goyal et al. , \n2017 ) : ( 1 ) 5-epoch warmup to stablize the initial stage ; and ( 2 ) multiply the learning rate by 0.1 at\n30th , 60th , and 80th epoch . the target accuracy is around 0.763 ( goyal et al. , 2017 ) .\n\nlearning rate weight decay l2 regularization top-1 validation accuracy\n\n0.0001\n0.0002\n0.0004\n0.0006\n0.0008\n0.001\n0.002\n0.004\n0.006\n0.008\n0.0001\n0.0002\n0.0004\n0.0006\n0.0008\n0.001\n0.002\n0.004\n0.006\n0.008\n\n0.01\n0.01\n0.01\n0.01\n0.01\n0.01\n0.01\n0.01\n0.01\n0.01\n0.001\n0.001\n0.001\n0.001\n0.001\n0.001\n0.001\n0.001\n0.001\n0.001\n\ndefault ( 0.01 ) \ndefault ( 0.01 ) \ndefault ( 0.01 ) \ndefault ( 0.01 ) \ndefault ( 0.01 ) \ndefault ( 0.01 ) \ndefault ( 0.01 ) \ndefault ( 0.01 ) \ndefault ( 0.01 ) \ndefault ( 0.01 ) \ndefault ( 0.01 ) \ndefault ( 0.01 ) \ndefault ( 0.01 ) \ndefault ( 0.01 ) \ndefault ( 0.01 ) \ndefault ( 0.01 ) \ndefault ( 0.01 ) \ndefault ( 0.01 ) \ndefault ( 0.01 ) \ndefault ( 0.01 ) \n\n0.0009969076\n0.0009969076\n0.0009969076\n0.0009358724\n0.0009969076\n0.0009765625\n0.0010172526\n0.0010172526\n0.0010172526\n0.0010172526\n0.0010172526\n0.0010172526\n0.0010172526\n0.0009969076\n0.0010172526\n0.0010172526\n0.0010172526\n0.0038452148\n0.011881511\n0.0061442056\n\n33\n\n published as a conference paper at iclr 2020\n\ntable 22 : the accuracy information of tuning adamw optimizer for imagenet training with resnet-\n50 ( batch size  16384 , 90 epochs , 7038 iterations ) . we use the learning rate recipe of ( goyal et al. , \n2017 ) : ( 1 ) 5-epoch warmup to stablize the initial stage ; and ( 2 ) multiply the learning rate by 0.1 at\n30th , 60th , and 80th epoch . the target accuracy is around 0.763 ( goyal et al. , 2017 ) .\n\nlearning rate weight decay l2 regularization top-1 validation accuracy\n\n0.0001\n0.0002\n0.0004\n0.0006\n0.0008\n0.001\n0.002\n0.004\n0.006\n0.008\n0.0001\n0.0002\n0.0004\n0.0006\n0.0008\n0.001\n0.002\n0.004\n0.006\n0.008\n\n0.0001\n0.0001\n0.0001\n0.0001\n0.0001\n0.0001\n0.0001\n0.0001\n0.0001\n0.0001\n0.00001\n0.00001\n0.00001\n0.00001\n0.00001\n0.00001\n0.00001\n0.00001\n0.00001\n0.00001\n\ndefault ( 0.01 ) \ndefault ( 0.01 ) \ndefault ( 0.01 ) \ndefault ( 0.01 ) \ndefault ( 0.01 ) \ndefault ( 0.01 ) \ndefault ( 0.01 ) \ndefault ( 0.01 ) \ndefault ( 0.01 ) \ndefault ( 0.01 ) \ndefault ( 0.01 ) \ndefault ( 0.01 ) \ndefault ( 0.01 ) \ndefault ( 0.01 ) \ndefault ( 0.01 ) \ndefault ( 0.01 ) \ndefault ( 0.01 ) \ndefault ( 0.01 ) \ndefault ( 0.01 ) \ndefault ( 0.01 ) \n\n0.3665975\n0.5315755\n0.6369222\n0.6760457\n0.69557697\n0.7076009\n0.73065186\n0.72806805\n0.72161865\n\n0.71816\n\n0.49804688\n0.6287028\n0.6773885\n0.67348224\n0.6622111\n0.6468709\n0.5846761\n0.4868978\n0.34969077\n0.31193033\n\n34\n\n published as a conference paper at iclr 2020\n\ntable 23 : the accuracy information of tuning adamw optimizer for imagenet training with resnet-\n50 ( batch size  16384 , 90 epochs , 7038 iterations ) . we use the learning rate recipe of ( goyal et al. , \n2017 ) : ( 1 ) 5-epoch warmup to stablize the initial stage ; and ( 2 ) multiply the learning rate by 0.1 at\n30th , 60th , and 80th epoch . the target accuracy is around 0.763 ( goyal et al. , 2017 ) .\n\nlearning rate weight decay l2 regularization top-1 validation accuracy\n\n0.0001\n0.0002\n0.0004\n0.0006\n0.0008\n0.001\n0.002\n0.004\n0.006\n0.008\n0.0001\n0.0002\n0.0004\n0.0006\n0.0008\n0.001\n0.002\n0.004\n0.006\n0.008\n\n0.01\n0.01\n0.01\n0.01\n0.01\n0.01\n0.01\n0.01\n0.01\n0.01\n0.001\n0.001\n0.001\n0.001\n0.001\n0.001\n0.001\n0.001\n0.001\n0.001\n\n0.0010172526\n0.0009765625\n0.0010172526\n0.0009969076\n0.0010172526\n0.0009765625\n0.0009969076\n0.0009969076\n0.0009765625\n0.0010172526\n0.0009765625\n0.0010172526\n0.0010172526\n0.0010172526\n0.0010172526\n0.0009969076\n0.0010579427\n0.0016886393\n0.019714355\n0.1329956\n\ndisable\ndisable\ndisable\ndisable\ndisable\ndisable\ndisable\ndisable\ndisable\ndisable\ndisable\ndisable\ndisable\ndisable\ndisable\ndisable\ndisable\ndisable\ndisable\ndisable\n\n35\n\n published as a conference paper at iclr 2020\n\ntable 24 : the accuracy information of tuning adamw optimizer for imagenet training with resnet-\n50 ( batch size  16384 , 90 epochs , 7038 iterations ) . we use the learning rate recipe of ( goyal et al. , \n2017 ) : ( 1 ) 5-epoch warmup to stablize the initial stage ; and ( 2 ) multiply the learning rate by 0.1 at\n30th , 60th , and 80th epoch . the target accuracy is around 0.763 ( goyal et al. , 2017 ) .\n\nlearning rate weight decay l2 regularization top-1 validation accuracy\n\n0.28515625\n0.44055176\n0.56815594\n0.6234741\n0.6530762\n0.6695964\n0.70048016\n\n0.71698\n\n0.72021484\n0.7223918\n0.72017413\n0.72058105\n0.7188924\n0.71695966\n0.7154134\n0.71358234\n0.7145386\n0.7114258\n0.7066447\n0.70284015\n\n0.0001\n0.0002\n0.0004\n0.0006\n0.0008\n0.001\n0.002\n0.004\n0.006\n0.008\n0.010\n0.012\n0.014\n0.016\n0.018\n0.020\n0.025\n0.030\n0.040\n0.050\n\n0.0001\n0.0001\n0.0001\n0.0001\n0.0001\n0.0001\n0.0001\n0.0001\n0.0001\n0.0001\n0.0001\n0.0001\n0.0001\n0.0001\n0.0001\n0.0001\n0.0001\n0.0001\n0.0001\n0.0001\n\ndisable\ndisable\ndisable\ndisable\ndisable\ndisable\ndisable\ndisable\ndisable\ndisable\ndisable\ndisable\ndisable\ndisable\ndisable\ndisable\ndisable\ndisable\ndisable\ndisable\n\n36\n\n published as a conference paper at iclr 2020\n\ntable 25 : the accuracy information of tuning adamw optimizer for imagenet training with resnet-\n50 ( batch size  16384 , 90 epochs , 7038 iterations ) . we use the learning rate recipe of ( goyal et al. , \n2017 ) : ( 1 ) 5-epoch warmup to stablize the initial stage ; and ( 2 ) multiply the learning rate by 0.1 at\n30th , 60th , and 80th epoch . the target accuracy is around 0.763 ( goyal et al. , 2017 ) .\n\nlearning rate weight decay l2 regularization top-1 validation accuracy\n\n0.31247965\n0.4534912\n0.57765704\n0.6277669\n0.65321857\n0.6682129\n0.69938153\n0.7095947\n0.710612\n0.70857745\n0.7094116\n0.70717365\n0.7109375\n0.7058309\n0.7052409\n0.7064412\n0.7035319\n0.6994629\n0.6972656\n0.6971232\n\n0.0001\n0.0002\n0.0004\n0.0006\n0.0008\n0.001\n0.002\n0.004\n0.006\n0.008\n0.010\n0.012\n0.014\n0.016\n0.018\n0.020\n0.025\n0.030\n0.040\n0.050\n\n0.00001\n0.00001\n0.00001\n0.00001\n0.00001\n0.00001\n0.00001\n0.00001\n0.00001\n0.00001\n0.00001\n0.00001\n0.00001\n0.00001\n0.00001\n0.00001\n0.00001\n0.00001\n0.00001\n0.00001\n\ndisable\ndisable\ndisable\ndisable\ndisable\ndisable\ndisable\ndisable\ndisable\ndisable\ndisable\ndisable\ndisable\ndisable\ndisable\ndisable\ndisable\ndisable\ndisable\ndisable\n\n37\n\n published as a conference paper at iclr 2020\n\nlearning deep graph matching via channel-\nindependent embedding and hungarian atten-\ntion\n\ntianshu yu† , runzhong wang‡ , junchi yan‡ , baoxin li†\n†arizona state university\n‡shanghai jiao tong university\n { tianshuy , baoxin.li } @ asu.edu\n { runzhong.wang , yanjunchi } @ sjtu.edu.cn\n\nabstract\n\ngraph matching aims to establishing node-wise correspondence between two\ngraphs , which is a classic combinatorial problem and in general np-complete . un-\ntil very recently , deep graph matching methods start to resort to deep networks to\nachieve unprecedented matching accuracy . along this direction , this paper makes\ntwo complementary contributions which can also be reused as plugin in existing\nworks : i ) a novel node and edge embedding strategy which stimulates the multi-\nhead strategy in attention models and allows the information in each channel to\nbe merged independently . in contrast , only node embedding is accounted in pre-\nvious works ; ii ) a general masking mechanism over the loss function is devised to\nimprove the smoothness of objective learning for graph matching . using hungar-\nian algorithm , it dynamically constructs a structured and sparsely connected layer , \ntaking into account the most contributing matching pairs as hard attention . our\napproach performs competitively , and can also improve state-of-the-art methods\nas plugin , regarding with matching accuracy on three public benchmarks.\n\n1\n\nintroduction\n\nwithout loss of generality , we consider the bijection problem for graph matching : given graph g1\nand g2 of equal size n , graph matching seeks to ﬁnd the one-vs-one node correspondence1 : \n\nmax\n\nx ( cid:62 ) kx\n\nx\n\ns.t . px  1\n\n ( 1 ) \n\nwhere x  vec ( x ) ∈ { 0 , 1 } n2\nwhich is the column-wise vectorized form of the permutation ma-\ntrix x that encodes the node-to-node correspondence between two graphs , and k ∈ rn2×n2\nis\nthe so-called afﬁnity matrix2 , respectively . note p is a selection matrix encoding the one-to-one\ncorrespondence constraint . this problem is called lawler ’ s qap ( lawler , 1963 ) and has attracted\nenormous attention for its generally np-complete ( hartmanis , 1982 ) challenge , as well as a wide\nspectrum of applications in computer vision , graphics , machine learning and operational research\netc . in particular , koopmans-beckmann ’ s qap ( loiola et al. , 2007 ) with objective tr ( x ( cid:62 ) f1xf2 ) \nis a special case of eq . ( 1 ) , which can be converted to lawler ’ s qap by k  f2 ⊗ f1 and fi refers\nto the weighted adjacency matrix . a series of solvers haven been developed to solve graph match-\ning problem ( leordeanu & hebert , 2005 ; cho et al. , 2010 ; bernard et al. , 2018 ; yan et al. , 2015 ; \nyu et al. , 2018 ) . all these methods are based on deterministic optimization , which are conditioned\nwith pre-deﬁned afﬁnity matrix and no learning paradigm is involved . this fact greatly limits the\nperformance and broad application w.r.t . different problem settings considering its np-hard nature.\n\n+\n\nrecently , the seminal work namely deep graph matching ( dgm ) ( zanﬁr & sminchisescu , 2018 ) is\nproposed to exploit the high capacity of deep networks for graph matching , which achieves state-\nof-the-art performance . this is in contrast to some early works which incorporate learning strategy\n\n1we assume graphs are of equal size for narrative simplicity . one can easily handle unbalanced graph size\n\nby adding dummy nodes as a common protocol in graph matching literature ( cho et al. , 2010 ) .\n\n2aia : jb typically encodes the afﬁnity between pair ( i , j ) and ( a , b ) where node i , j ∈ g1 and a , b ∈ g2.\n\n1\n\n published as a conference paper at iclr 2020\n\nseparately in local stages ( caetano et al. , 2009 ; cho et al. , 2013 ) . on the other hand , graph convolu-\ntional networks ( gcn ) ( kipf & welling , 2017 ) brings about new capability on tasks over graph-like\ndata , as it naturally integrates the intrinsic graph structure in a general updating rule : \n\nh ( l+1 )  σ\n\n ( cid:16 ) ˆah ( l ) w ( l ) ( cid:17 ) \n\n ( 2 ) \n\nwhere ˆa is the normalized connectivity matrix . h ( l ) and w ( l ) are the features and weights at layer\nl , respectively . node embedding is updated by aggregation from 1-neighboring nodes , which is akin\nto the convolution operator in cnn . by taking advantages of both dgm and gcn , wang et al.\n ( 2019 ) and zhang & lee ( 2019 ) incorporate permutation loss instead of displacement loss in ( zanﬁr\n & sminchisescu , 2018 ) , with notable improvement across both synthetic and real data.\n\nnote that eq . ( 1 ) involves both node and edge information , which exactly correspond to the diag-\nonal and off-diagonal elements in k , respectively . edges can carry informative multi-dimensional\nattributes ( namely weights ) which are fundamental to graph matching . however existing embed-\nding based graph matching methods ( wang et al. , 2019 ; xu et al. , 2019 ) are focused on the explicit\nmodeling of node level features , whereby the edges are only used as topological node connection for\nmessage passing in gcn . besides , edge attributes are neither well modeled in the embedding-free\nmodel ( zanﬁr & sminchisescu , 2018 ) since the edge information is derived from the concatena-\ntion of node features . to our best knowledge , there is no deep graph matching method explicitly\nincorporating edge attributes . in contrast , edge attributes e.g.\nlength and orientation are widely\nused in traditional graph matching models ( cho et al. , 2010 ; yan et al. , 2015 ; yu et al. , 2018 ) for\nconstructing the afﬁnity matrix k. such a gap shall be ﬁlled in the deep graph matching pipeline.\n\nanother important consideration refers to the design of loss function . there are mainly two forms in\nexisting deep graph matching works : i ) displacement loss ( zanﬁr & sminchisescu , 2018 ) similar to\nthe use in optical ﬂow estimation ( ren et al. , 2017 ) ; ii ) the so-called permutation loss ( wang et al. , \n2019 ) involving iterative sinkhorn procedure followed by a cross-entropy loss . results in ( wang\net al. , 2019 ) show the latter is an effective improvement against the former regression based loss.\nhowever , we argue that the continuous sinkhorn procedure ( in training stage ) is yet an unnatural\napproximation to hungarian sampling ( in testing stage ) for discretization . if the network is equipped\nwith a continuous loss function ( e.g . cross-entropy ) , we argue that the training process will make a\ngreat “ meaningless effort ” to enforce some network output digits of the ﬁnal matching matrix into\nbinary and neglect the resting digits which might have notable impact on accuracy.\n\nthis paper strikes an endeavor on the above two gaps and makes the following main contributions : \n\ni ) we propose a new approach for edge embedding via channel-wise operation , namely channel-\nindependent embedding ( cie ) . the hope is to effectively explore the edge attribute and simulate\nthe multi-head strategy in attention models ( veliˇckovi´c et al. , 2018 ) by decoupling the calculations\nparallel and orthogonal to channel direction . in fact , edge attribute information has not been consid-\nered in existing embedding based graph matching methods ( wang et al. , 2019 ; xu et al. , 2019 ) .\n\nii ) we devise a new mechanism to adjust the loss function based on the hungarian method which is\nwidely used for linear assignment problem , as termed by hungarian attention . it resorts to dynami-\ncally generating sparse matching mask according to hungarian sampling during training , rather than\napproximating hungarian sampling with a differentiable function . as such , the hungarian attention\nintroduces higher smoothness against traditional loss functions to ease the training.\n\niii ) the empirical results on three public benchmarks shows that the two proposed techniques are\northogonal and beneﬁcial to existing techniques . speciﬁcally , on the one hand , our cie module\ncan effectively boost the accuracy by exploring the edge attributes which otherwise are not consid-\nered in state-of-the-art deep graph matching methods ; on the other hand , our hungarian attention\nmechanism also shows generality and it is complementary to existing graph matching loss.\n\n2 related works\n\ngraph embedding . to handle graph-like data , early works adopt recursive neural networks ( rnns ) \ntreating input as directed acyclic graphs ( sperduti & starita , 1997 ; frasconi et al. , 1998 ) . gori et al.\n ( 2005 ) ; scarselli et al . ( 2008 ) generalized early models to graph neural networks ( gnns ) so as to be\ndirectly applied on cyclic , directed or undirected graphs . li et al . ( 2016 ) further improved this line\n\n2\n\n published as a conference paper at iclr 2020\n\nof model by replacing standard rnns with gated recurrent units ( grus ) ( cho et al. , 2013 ) . inspired\nby the great success of convolutional neural networks ( cnns ) ( simonyan & zisserman , 2014 ; he\net al. , 2016 ) , researchers have made tremendous effort on applying convolution operator to graphs\n ( bruna et al. , 2014 ; kipf & welling , 2017 ; gong & cheng , 2019 ) . bruna et al . ( 2014 ) deﬁned\na convolution operator in fourier domain which is obtained by performing eigen-decomposition\non graph laplacian . however , such convolution will affect the whole spatial domain once taking\ninverse fourier transformation . this method was improved by chebyshev expansion to approximate\nﬁlters ( defferrard et al. , 2016 ) . kipf & welling ( 2017 ) propose a graph convolutional operator\nover 1-neighbor nodes derived from graph spectral theory , which is invariant to node permutation\nand achieved signiﬁcant performance on semi-supervised learning tasks . there are series of works\nfollowing gcn , such as graphsage ( hamilton et al. , 2017 ) , gat ( veliˇckovi´c et al. , 2018 ) and\nmpnn ( gilmer et al. , 2017 ) . refer to ( cai et al. , 2018 ) for a more comprehensive survey.\n\nwhile the aforementioned models are focused on learning node state/embedding , a parallel line of\nwork seek to learn edge embedding by taking into account the information carried on edges ( li et al. , \n2016 ; gilmer et al. , 2017 ; gong & cheng , 2019 ) . edges are intrinsic portion of graphs , and thus\nedge embedding can be essential to reveal the relation among nodes . gilmer et al . ( 2017 ) introduce\na general embedding network incorporating edge information and node-edge information merging , \nand a serious of works fall into this framework e.g . gated gnn ( li et al. , 2016 ) , tensor gnn\n ( sch¨utt et al. , 2017 ) and egnn ( gong & cheng , 2019 ) . an improved version is devised in chen\net al . ( 2019 ) by interpreting this framework as maximizing mutual information across layers.\n\nloss for combinatorial learning . for the relatively easy linear assignment problem , it has been\nknown that sinkhorn algorithm ( sinkhorn , 1964 ) is the approximate and differentiable version of\nhungarian algorithm ( mena et al. , 2017 ) . the sinkhorn network ( adams & zemel , 2011 ) is de-\nveloped given known assignment cost , whereby doubly-stochastic regulation is performed on input\nnon-negative square matrix . patrini et al . ( 2018 ) devise the sinkhorn autoencoder to minimize\nwasserstein distance , and emami & ranka ( 2018 ) propose to learning a linear assignment solver\nvia reinforcement learning . for permutation prediction , deeppermnet ( santa cruz et al. , 2018 ) \nadopts the sinkhorn layer on top of a deep convolutional network . however this method can not be\ndirectly applied for graph matching as it is not invariant to input permutations which is conditioned\non a predeﬁned node permutation as reference . in particular , existing supervised methods on combi-\nnatorial learning are generally cross-entropy-based . pointer net ( vinyals et al. , 2015 ) incorporates\ncross-entropy loss on learning heuristics for combinatorial problems . milan et al . ( 2017 ) propose an\nobjective-based loss , where the gradients are only updated if the objective improves after update.\n\nlearning for graph matching . the early effort ( caetano et al. , 2009 ) aims to incorporate learning\nto graph matching . the key is to learn a more effective afﬁnity function with given correspon-\ndence as supervision . while the ability by only learning afﬁnity is limited , cho et al . ( 2013 ) pro-\npose a matching function learning paradigm using histogram-based attributes with structured-svm\n ( tsochantaridis et al. , 2005 ) . a recent work ( zanﬁr & sminchisescu , 2018 ) is a breakthrough to\nintroduce deep learning paradigm into graph matching task , which utilizes a neural network to learn\nthe afﬁnity function . the learning procedure is explicitly derived from the factorization of afﬁnity\nmatrix ( zhou & de la torre , 2012 ) , which makes the interpretation of the network behavior possible.\nhowever , the displacement loss in ( zanﬁr & sminchisescu , 2018 ) measures the pixel-wise transla-\ntion which is similar to optical-ﬂow ( dosovitskiy et al. , 2015 ) , being essentially a regression task\ninstead of combinaotiral optimization . seeing this limitation , wang et al . ( 2019 ) employ element-\nwise binary cross-entropy , termed as permutation loss . this loss has proved capable of capturing\nthe combinatorial nature rather than pixel offset , and achieves improvement over displacement loss.\nnode embedding is also used in ( wang et al. , 2019 ) to explore the structure information.\n\n3 the proposed learning approach for graph matching\n\n3.1 approach overview\n\nan overall structure of our approach is illustrated in fig . 1 . in line with ( wang et al. , 2019 ) , we em-\nploy vgg16 ( simonyan & zisserman , 2014 ) to extract features from input images and bi-linearly\ninterpolate the features at key points ( provided by datasets ) . we concatenate lower-level ( relu4 2 ) \nand higher-level ( relu5 1 ) features to incorporate local and contextual information . for an image\nwith k key points , the feature is denoted as h ∈ rk×d , where d is the feature dimension . unless\n\n3\n\n published as a conference paper at iclr 2020\n\nfigure 1 : architecture overview of the proposed deep graph matching networks that consist of the\nproposed channel-independent embedding and hungarian attention layer over the loss function.\notherwise speciﬁed , the adjacency matrix a ∈ rk×k is consequentially constructed via delaunay\ntriangulation ( delaunay et al. , 1934 ) , which is a widely adopted strategy to produce sparsely con-\nnected graph . to introduce more rich edge information , we also generate k × k m-dimensional edge\nfeatures e ∈ rm×k×k . e can be initialized with some basic edge information ( e.g.\nlength and\nangle and other attributes ) or a commutative function eij  p ( hi , hj )  p ( hj , hi ) ∈ rm , where\nhi refers to the feature of node i . note for directed graph , the commutative property is not required.\n\nthe features h and e , together with the adjacency a , are then fed into gnn module . pairs of\nfeatures are processed in a siamese fashion ( bromley et al. , 1994 ) . standard gcn ’ s message passing\nrule simply updates node embedding as shown in eq . ( 2 ) . in contrast , each gnn layer in our model\ncomputes a new pair of node and edge embeddings simultaneously : \n\n0 and w l\n\nh ( l+1 )  fi ( h ( l ) , e ( l ) , a ; w l\n\n ( 3 ) \nwhere w l\n1 are the learnable parameters at layer l. the edge information is essential to\nprovide structural feature enhancing graph matching . we initialize h ( 0 )  h and e ( 0 )  e in\nour setting . we will discuss the details of functions f and g in sec . 3.2 . following state-of-the-art\nwork ( wang et al. , 2019 ) , we also compute the cross-graph afﬁnity followed by a column/row-wise\nsoftmax activation and a sinkhorn layer ( adams & zemel , 2011 ) : \n\n0 ) , e ( l+1 )  g ( h ( l ) , e ( l ) , a ; w l\n1 ) \n\nmij  exp\n\nτ h ( cid:62 ) \n\n ( 1 ) iλh ( 2 ) j\n\n , s  sinkhorn ( m ) \n\n ( cid:16 ) \n\n ( cid:17 ) \n\nnote here m ∈ rk×k is the node-level similarity matrix encoding similarity between two graphs , \ndiffering from the edege-level afﬁnity matrix k in eq . 1. τ is the weighting parameter of similarity , \nλ contains learnable parameters and h ( 1 ) i is the node i ’ s embedding from graph g1 . the output\ns ∈ [ 0 , 1 ] k×k , s1  1 , s ( cid:62 ) 1  1 is a so-called doubly-stochastic matrix . here sinkhorn ( · ) denotes\nthe following update iteratively to project m into doubly stochastic polygon : \n\nm ( t+1 )  m ( t ) −\n\nm ( t ) 11 ( cid:62 ) −\n\n11 ( cid:62 ) m ( t ) +\n\n1\nn\n\n1\nn2 11 ( cid:62 ) m ( t ) 11 ( cid:62 ) −\n\n1\nn\n\n11 ( cid:62 ) \n\n1\nn\n\nthe sinkhorn layer is shown to be an approximation of hungarian algorithm which produces discrete\nmatching output ( kuhn , 1955 ) . as there are only matrix multiplication and normalization operators\ninvolved in sinkhorn layer , it is differentiable . in practice , eq . ( 5 ) converges rapidly within 10\niterations for decades of nodes . less iterations involved , more precise back-propagated gradients\ncan be achieved . we employ a cross-graph node embedding strategy following ( wang et al. , 2019 ) : \n\nh ( l ) \n\n ( 1 )  fc\n\n ( cid:16 ) \ncat ( h ( l ) \n\n ( cid:17 ) \n ( 1 ) , sh ( l ) \n ( 2 ) ) \n\n , h ( l ) \n\n ( 2 )  fc\n\ncat ( h ( l ) \n\n ( 2 ) , s ( cid:62 ) h ( l ) \n ( 2 ) ) \n\n ( cid:16 ) \n\n ( cid:17 ) \n\nwhere fc is a network and cat ( · , · ) is the concatenation operator . h ( i ) is the node feature of graph i.\nthis procedure seeks to merge similar features from another graph into the node feature in current\ngraph . it is similar to the feature transfer strategy in ( aberman et al. , 2018 ) for sparse correspon-\ndence , which employs a feature merging method analogous to style transfer ( li et al. , 2017 ) .\n\nas sinkhorn layer does not necessarily output binary digits , we employ hungarian algorithm ( kuhn , \n1955 ) to discretize matching output s in testing . the testing differs from the training due to the\nhungarian discretization . we introduce a novel attention-like mechanism termed as hungarian\nattention , along with existing loss functions ( will be detailed in sec . 3.3 ) . the ﬁnal training loss is\nas follows , where sg and h correspond to binary true matching and hungarian attention loss.\n\n ( 4 ) \n\n ( 5 ) \n\n ( 6 ) \n\n ( 7 ) \n\nmin h ( s , sg ) \n\n4\n\ncnngaussian\tkernelimage\t1initial\tnode\tembeddinginitial\tedge\tembeddingcienode\tembeddingedge\tembeddingcross\tgraphnode\tembeddingedge\tembeddingnode\tembeddingedge\tembeddingciecnngaussian\tkernelinitial\tnode\tembeddinginitial\tedge\tembeddingcienode\tembeddingedge\tembeddingcross\tgraphnode\tembeddingedge\tembeddingcienode\tembeddingedge\tembeddingaffinityaffinityimage\t2similarity\tmatrixdoubly-stochastic\tmatrixsinkhornhungarian\tattentionground\ttruth\tmatchingloss published as a conference paper at iclr 2020\n\nfigure 2 : illustration of the proposed cie layer for embedding based deep graph matching . the\noperation “ linear ” refers to the linear mapping , e.g . h ( l ) \n\nw → w ( l ) \n\nw in eq ( 9 ) .\n\n2 h ( l ) \n\n3.2 channel-independent embedding\n\nwe detail the updating rule in eq . ( 3 ) . we propose a method to merge edge features into node\nfeatures and perform matching on nodes . edge information acts an important role in modeling\nrelational data , whereby such relation can be complex thus should be encoded with high-dimensional\nfeature . to this end , gilmer et al . ( 2017 ) introduce a general embedding layer , which takes node and\nedge features and outputs a message to node v , then fuses the message and the current embedding : \n\nm ( l ) \n\nv  σ\n\nft ( evw ) h ( l ) \n\nw + w ( l ) h ( l ) \n\n , h ( t+1 ) \n\nv\n\n ut\n\nh ( t ) \n\nv , m ( l ) \nv\n\n ( 8 ) \n\n ( cid:16 ) \n\n ( cid:17 ) \n\n ( cid:33 ) \n\n ( cid:32 ) \n\n ( cid:88 ) \n\nw∈nv\n\nv and h ( l ) \n\nwhere evw is the feature corresponding to edge ( v , w ) . in the realization of eq . ( 8 ) ( gilmer et al. , \n2017 ) , m ( l ) \nv are fed to gru ( cho et al. , 2014 ) as a sequential input . there are several\nvariants which take into account speciﬁc tasks ( li et al. , 2016 ; sch¨utt et al. , 2017 ; chen et al. , 2019 ) .\namong these , li et al . ( 2016 ) generates a transformation matrix for each edge and sch¨utt et al.\n ( 2017 ) resorts to merge embedding via fully connected neural networks . while edge-wise merging\nis straightforward , the representation ability is also limited . on the other hand , fully connected\nmerging strategy will result in high computational cost and instability for back-propagation . to\naddress these issues , we propose to merge embedding in a channel-wise fashion , which is termed as\nchannel-independent embedding ( cie ) . concretely , the updating rule is written as : \n\nh ( l+1 ) \n\nv\n\n σ\n\n\n\n\n\n\n\n ( cid:16 ) \n\n ( cid:88 ) \n\n ( cid:16 ) \n\nw∈nv\n\nγn\n ( cid:124 ) \n\nw ( l ) \n\nvw ◦ w ( l ) \n1 e ( l ) \n ( cid:123 ) ( cid:122 ) \n\n2 h ( l ) \nw\n\nchannel-wise operator/function\n ( cid:17 ) \n\n\n\n ( cid:17 ) \n\n\n\n\n ( cid:125 ) \n\ne ( l+1 ) \n\nvw  σ\n\nw ( l ) \n\n1 e ( l ) \nvw\n\n ( cid:16 ) \n\n+ σ\n\nw ( l ) \n\n0 h ( l ) \nv\n\n ( cid:17 ) \n\n ( 9 ) \n\n ( 10 ) \n\nwhere γn ( · ◦ · ) is a channel-wise operator/function ( above the underbrace ) , and it performs calcu-\nlation per-channel and the output channel dimension is the same as input . the second σ ( · ) term is\nthe message a node passes to itself , which is necessary in keeping the node information contextually\nconsistent through each cie layer . in this fashion , cie is thus a procedure to aggregate node and\n2 h ( l ) \nedge embedding in each channel independently , which requires the dimensions of node ( w ( l ) \nw ) \nand edge ( w ( l ) \nvw ) representations to be equal . similarly , we also propose an corresponding up-\ndating rule of edge embedding by substituting eq . ( 10 ) : \n\n1 e ( l ) \n\ne ( l+1 ) \n\nvw  σ\n\n ( cid:16 ) \n\n ( cid:16 ) \n\nγe\n\nw ( l ) \n\n1 e ( l ) \n\nvw ◦ h\n\nh ( l ) \n\nv , h ( l ) \nw\n\n ( cid:16 ) \n\n ( cid:17 ) ( cid:17 ) ( cid:17 ) \n\n ( cid:16 ) \n\n+ σ\n\n ( cid:17 ) \n\nw ( l ) \n\n1 e ( l ) \nvw\n\n ( 11 ) \n\nwhere h ( · , · ) is commutative h ( x , y )  h ( y , x ) . eq . ( 11 ) is supplementary to eq . ( 9 ) .\n\nfig . 2 shows a schematic diagram of cie layer , which is motivated from two perspectives . first , \ncie is motivated by counterparts in cnn ( qiu et al. , 2017 ; tran et al. , 2018 ) which decouple a 3d\nconvolution into two 2d ones ( e.g . a 3 × 3 × 3 convolution can be decomposed to a 1 × 3 × 3 and\na 3 × 1 × 1 convolutions ) . in this sense , the number of parameters can be signiﬁcantly reduced.\nas shown in fig . 2 , node and edge embedding is ﬁrst manipulated along the channel direction via\na linear layer , then operated via γn and γe orthogonal to the channel direction . instead of merging\nnode and edge as a whole , cie layer decouples it into two operations . second , cie is also motivated\nby the triumph of multi-head structure ( e.g . graph attention ( veliˇckovi´c et al. , 2018 ) ) , the key of\n\n5\n\n ( # ) ( # ) linearlinearγ ’ γ ( ( # ) * ) ( # ) * ) × × × channels published as a conference paper at iclr 2020\n\nfigure 3 : a working example illustrating of our proposed hungarian attention pipeline starting from\nsimilarity matrix . sinkhorn algorithm solves similarity matrix into a doubly-stochastic matrix in a\ndifferentiable way . a discrete permutation matrix is further obtained via hungarian algorithm . our\nproposed hungarian attention , taking the ground truth matching matrix into account , focuses on\nthe “ important ” digits either labeled true or being mis-classiﬁed . the output matrix is obtained by\nattention pooling from doubly-stochastic matrix , where we compute a loss on it.\nwhich is to conduct unit calculation multiple times and concatenate the results . multi-head proved\neffective to further improve the performance since it is capable of capturing information at different\nscales or aspects . traditional neural node-edge message passing algorithms ( gilmer et al. , 2017 ; li\net al. , 2016 ; sch¨utt et al. , 2017 ) typically produce a uniﬁed transformation matrix for all the channels.\non the other hand , in eq . ( 9 ) ( 10 ) and ( 11 ) , one can consider that the basic operator in each channel\nis repeated d times in a multi-head fashion . the cross-channel information exchange , as signiﬁed in\neq . ( 9 ) ( 10 ) and ( 11 ) , only happens before the channel-wise operator ( i.e . weights w ( l ) \ni as the cross-\nchannel matrices ) . the main difference between cie and traditional multi-head approaches e.g.\n ( veliˇckovi´c et al. , 2018 ) is that cie assumes the channel-independence of two embedded features\n ( node and edge ) , while traditional ones only take one input under head-independence assumption.\n\n3.3 hungarian attention mechanism\n\nfor most graph matching algorithms , the output is in a continuous domain . though there are some\nalternatives that deliver discrete solutions by adding more constraints or introducing numerical con-\ntinuation ( zhou & de la torre , 2012 ; yu et al. , 2018 ) , the main line of methods is to incorporate a\nsampling procedure ( e.g . winner-take-all and hungarian ) . among them , the hungarian algorithm\n ( kuhn , 1955 ) is a widely adopted , for its efﬁciency and theoretical optimality.\n\nhowever , the hungarian algorithm incurs a gap between training ( loss function ) and testing stages\n ( hungarian sampling ) . we compare the permutation loss ( wang et al. , 2019 ) for concrete analysis : \n\nlce  −\n\n ( cid:88 ) \n\n ( cid:0 ) sg\n\nij log sij + ( cid:0 ) 1 − sg\n\nij\n\n ( cid:1 ) log ( 1 − sij ) ( cid:1 ) \n\n ( 12 ) \n\ni∈g1 , j∈g2\n\nnote eq . ( 12 ) is an element-wise version of binary cross-entropy . during training , this loss tends\nto drag the digits in s into binary format and is likely trapped to local optima . this is because this\nloss will back-propagate the gradients of training samples that are easy to learn in the early training\nstage . in later iterations , this loss is then hard to give up the digits that have become binary . in fact , \nthe similar phenomenon is also investigated in the focal loss ( lin et al. , 2017 ) in comparison to the\ntraditional cross-entropy loss . during the testing stage , however , the hungarian algorithm has no\npreference on the case if digits in s are close to 0 − 1 or not . it binarizes s anyway . therefore , the\neffort of eq . ( 12 ) to drag s into binary might be meaningless.\n\nthis issue is likely to be solved by integrating hungarian algorithm during the training stage . un-\nfortunately , hungarian algorithm is undifferentiable and its behavior is difﬁcult to mimic with a\ndifferentiable counterpart . in this paper , instead of ﬁnding a continuous approximation of hungar-\nian algorithm , we treat it as a black box and dynamically generate network structure ( sparse link ) \n\n6\n\n012345abcdef1.41.73.83.80.80.81.33.60.90.82.16.72.50.44.31.20.71.00.41.81.91.12.73.81.11.50.50.40.63.60.80.72.72.15.51.0similarity matrix012345abcdef0.20.10.20.40.10.00.10.30.10.10.10.30.30.00.30.10.10.10.10.20.10.10.20.20.20.20.10.10.10.30.10.10.20.20.40.0doubly-stochastic matrix012345abcdef0.00.00.01.00.00.00.01.00.00.00.00.01.00.00.00.00.00.00.00.01.00.00.00.00.00.00.00.00.01.00.00.00.00.01.00.0permutation matrix012345abcdef0.00.00.01.00.00.01.00.00.00.00.00.00.01.00.00.00.00.00.00.01.00.00.00.00.00.00.00.00.01.00.00.00.00.01.00.0ground truth matrix012345abcdef0.00.00.01.00.00.01.01.00.00.00.00.01.01.00.00.00.00.00.00.01.00.00.00.00.00.00.00.00.01.00.00.00.00.01.00.0attention activation012345abcdef0.00.00.00.40.00.00.10.30.00.00.00.00.30.00.00.00.00.00.00.00.10.00.00.00.00.00.00.00.00.30.00.00.00.00.40.0output matrixsinkhornhungarianattention poolinghungarian attention published as a conference paper at iclr 2020\n\ntable 1 : accuracy on pascal voc ( best in bold ) . white and gray background refer to results on test-\ning and training , respectively . compared methods include gmn ( zanﬁr & sminchisescu , 2018 ) , \ngat ( veliˇckovi´c et al. , 2018 ) , epn ( gong & cheng , 2019 ) , pca/pia ( wang et al. , 2019 ) .\n\nmethod aero bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv ave\ngmn-d 31.9 47.2 51.9 40.8 68.7 72.2 53.6 52.8 34.6 48.6 72.3 47.7 54.8 51.0\n38.6 75.1 49.5 45.0 83.0 86.3 55.3\n34.1 77.5 57.1 53.6 83.2 88.6 57.9\ngmn-p 31.1 46.2 58.2 45.9 70.6 76.4 61.2 61.7 35.5 53.7 58.9 57.5 56.9 49.3\n39.5 82.0 66.9 50.1 78.5 90.3 63.6\ngat-p 46.4 60.5 60.9 51.8 79.0 70.9 62.7 70.1 39.7 63.9 66.2 63.8 65.8 62.8\ngat-h 47.2 61.6 63.2 53.3 79.7 70.1 65.3 70.5 38.4 64.7 62.9 65.1 66.2 62.5\n41.1 78.8 67.1 61.6 81.4 91.0 64.6\n39.9 80.5 66.7 45.5 77.6 90.6 63.2\nepn-p 47.6 65.2 62.2 52.7 77.8 69.5 63.4 69.6 37.8 62.8 63.6 63.9 64.6 61.9\npia-d 39.7 57.7 58.6 47.2 74.0 74.5 62.1 66.6 33.6 61.7 65.4 58.0 67.1 58.9\n41.9 77.7 64.7 50.5 81.8 89.9 61.6\npia-p 41.5 55.8 60.9 51.9 75.0 75.8 59.6 65.2 33.3 65.9 62.8 62.7 67.7 62.1\n42.9 80.2 64.3 59.5 82.7 90.1 63.0\n44.9 77.5 67.4 57.5 86.7 90.9 63.8\npca-p 40.9 55.0 65.8 47.9 76.9 77.9 63.5 67.4 33.7 65.5 63.6 61.3 68.9 62.8\n40.5 84.7 66.1 47.9 80.5 91.1 64.6\npca-h 49.8 60.7 63.9 52.6 79.8 72.5 63.8 71.2 38.4 62.5 71.7 65.4 66.6 62.5\npca+-p 46.6 61.0 62.3 53.9 78.2 72.5 64.4 70.5 39.0 63.5 74.8 65.2 65.0 61.6\n40.8 83.2 67.1 50.5 79.6 91.6 64.6\ncie2-p 50.9 65.5 68.0 57.0 81.0 75.9 70.3 73.4 41.1 66.7 53.2 68.3 68.4 63.5\n45.3 84.8 69.7 57.2 79.8 91.6 66.9\ncie2-h 51.2 68.4 69.5 57.3 82.5 73.5 69.5 74.0 40.3 67.8 60.0 69.7 70.3 65.1\n44.7 86.9 70.7 57.3 84.2 92.2 67.4\n46.1 85.1 70.4 61.6 80.7 91.7 68.1\ncie1-p 52.1 69.4 69.9 58.9 80.6 76.3 71.0 74.2 41.1 68.0 60.4 69.7 70.7 65.1\n44.8 85.2 69.9 65.4 85.2 92.4 68.9\ncie1-h 51.2 69.2 70.1 55.0 82.8 72.8 69.0 74.2 39.6 68.8 71.8 70.0 71.8 66.8\n66.5 99.1 80.7 99.7 98.2 97.0 88.2\npca-p 75.8 99.2 83.3 74.7 98.7 96.3 74.3 87.8 80.9 85.7 100.0 83.7 83.8 98.7\n46.1 94.8 72.7 93.6 93.7 91.6 76.2\ncie1-p 56.5 84.0 73.5 58.0 91.5 81.1 67.8 76.8 46.4 72.2 98.0 73.9 73.6 77.9\ncie1-h 59.4 88.1 75.9 58.0 94.3 81.9 69.4 78.9 49.5 78.2 99.7 78.1 78.0 82.1\n47.4 95.8 75.7 97.6 96.0 91.1 78.7\n\naccording to its output . concretely , the sparse link is calculated as : \n\nz  atten ( cid:0 ) hungarian ( s ) , sg ( cid:1 )  p ∪ q\n\n ( 13 ) \n\nwhere the attention mechanism atten is fulﬁlled by an element-wise “ logic or ” function . fig . 3\nshows an example of hungarian attention procedure , and eq . ( 13 ) highlights the most contributing\ndigit locations : positive digits p  s where hungarian agrees with the ground-truth ; negative digits\nq  hungarian ( s ) \\ sg where hungarian differs from ground-truth . while gt ( positive digits ) \nnaturally points out the digits that must be considered , negative ones indicate the digits that most\nhinder the matching ( most impeding ones among all mis-matchings ) . thus we need only minimize\nthe loss at z , without considering the rest of digits . as we note that this mechanism only focuses on\na small portion of the matching matrix which is analogous to producing hard attention , we term it\nhungarian attention . now that with the attention mask z , the hungarian attention loss becomes : \n\nhce  −\n\n ( cid:88 ) \n\nzij\n\n ( cid:0 ) sg\n\nij log sij + ( cid:0 ) 1 − sg\n\nij\n\n ( cid:1 ) log ( 1 − sij ) ( cid:1 ) \n\n ( 14 ) \n\ni∈g1 , j∈g2\n\nnote that hungarian attention mechanism can also be applied to other loss functions once the match-\ning score is calculated in an element-wise fashion . our experiment also studies hungarian attention\nloss when casted on focal loss ( lin et al. , 2017 ) and a speciﬁcally designed margin loss.\n\nfinally we give a brief qualitative analysis on why hungarian attention can improve matching loss.\nas discrete graph matching problem is actually built upon delta function over permutation vertices\n ( 1 at ground-truth matching and 0 otherwise ) ( yu et al. , 2018 ) , learning of graph matching with per-\nmutation loss is actually to approximate such functions with continuous counterparts . unfortunately , \nmore precise approximation to delta function will result in higher non-smoothness , as discussed in\nyu et al . ( 2018 ) . for highly non-smooth objective , the network is more likely trapped at local optima.\nhungarian attention , however , focuses on a small portion of the output locations , thus does not care\nabout if most of the output digits are in { 0 , 1 } . in this sense , hungarian attention allows moderate\nsmoothness of the objective , thus optimizer with momentum is likely to avoid local optima.\n\n4 experiments\n\nexperiments are conducted on three benchmarks widely used for learning-based graph matching : \ncub2011 dataset ( welinder et al. , 2010 ) following the protocol in ( choy et al. , 2016 ) , pascal voc\nkeypoint matching ( everingham et al. , 2010 ; bourdev & malik , 2009 ) which is challenging and\nwillow object class dataset ( cho et al. , 2013 ) . mean matching accuracy is adopted for evaluation : \n\nacc  , j∈g2\n\n ( cid:88 ) \n\nand ( cid:0 ) hungarian ( s ) ij , sg\n\n ( cid:1 ) \n\nij\n\n ( 15 ) \n\nthe algorithm abbreviation is in the form “ x-y ” , where “ x ” and “ y ” refer to the network structure\n ( e.g . cie ) and loss function ( e.g . hungarian attention loss ) , respectively . speciﬁcally , d , p and h\n\n7\n\n published as a conference paper at iclr 2020\n\ncorrespond to displacement used in ( zanﬁr & sminchisescu , 2018 ) , permutation as adopted in ( wang\net al. , 2019 ) and hungarian attention over permutation loss devised by this paper , respectively.\n\npeer methods . we compare our method with the following selected counterparts : 1 ) harg ( cho\net al. , 2013 ) . this shallow learning method is based on hand-crafted feature and structured svm ; \n2 ) gmn ( zanﬁr & sminchisescu , 2018 ) . this is a seminal work incorporating graph matching and\ndeep learning , and the solver is upon spectral matching ( leordeanu & hebert , 2005 ) . while the\nloss of this method is displacement loss , we also report the results of gmn by replacing its loss\nwith permutation loss ( gmn-p ) ; 3 ) pia/pca ( wang et al. , 2019 ) . pca and pia correspond to\nthe algorithms with and without cross-graph node embedding , respectively . readers are referred\nto wang et al . ( 2019 ) for more details ; we further replace the gnn layer in our framework with : \n4 ) gat ( veliˇckovi´c et al. , 2018 ) . graph attention network is an attention mechanism on graphs , \nwhich reweights the embedding according to attention score ; 5 ) epn ( gong & cheng , 2019 ) . this\nmethod exploits multi-dimensional edge embedding and can further be applied on directed graphs.\nthe edge dimension is set to 32 in our experiments . finally , we term our network structure cie for\nshort . to investigate the capacity of edge embedding update , we also devise a version without edge\nembedding , in which connectivity is initialized as reciprocal of the edge length then normalized , \nrather than a . this model is called pca+ since the node embedding strategy follows pca.\n\nimplementation details . as the node number of each graph might vary , we add dummy nodes for\neach graph pair such that the node number reaches the maximal graph size in a mini-batch in line\nwith the protocol in ( wang et al. , 2019 ) . in either training or testing stages , these dummy nodes will\nnot be updated or counted . the activation function in eq . ( 9 ) ( 10 ) and ( 11 ) is set as relu ( nair & \nhinton , 2010 ) in all experiments . speciﬁcally , the node and edge embedding is implemented by : \n\nh ( l+1 ) \n\n·q\n\n σ\n\n ( cid:18 ) ( cid:18 ) \n\na ( cid:12 ) \n\n ( cid:16 ) \n\nw ( l ) \n\n1 e ( l ) ( cid:17 ) \n\n ( cid:19 ) ( cid:16 ) \n\n·q\n\ne ( l+1 ) \n\n·q\n\n σ\n\n ( cid:18 ) ( cid:12 ) \n ( cid:16 ) \n ( cid:12 ) \n ( cid:12 ) \n ( cid:12 ) \n\nw ( l ) \n\n0 h ( l ) ( cid:17 ) \n\n ( cid:16 ) \n\nw ( l ) \n\n0 h ( l ) ( cid:17 ) ( cid:62 ) \n\n ( cid:9 ) \n\n·q\n\nw ( l ) \n\n2 h ( l ) ( cid:17 ) \n ( cid:12 ) \n ( cid:12 ) \n ( cid:12 ) \n ( cid:12 ) \n\n ( cid:12 ) e ( l ) \n·q\n\n·q\n\n·q\n ( cid:19 ) \n\n ( cid:19 ) \n\n ( cid:18 ) ( cid:16 ) \n\nw ( l ) \n\n0 h ( l ) ( cid:17 ) \n\n+ σ\n\n ( cid:19 ) \n\n ( cid:18 ) ( cid:16 ) \n\nw ( l ) \n\n1 e ( l ) ( cid:17 ) \n\n+ σ\n\n·q\n ( cid:19 ) \n\n·q\n\n ( 16a ) \n\n ( 16b ) \n\nwhere ( cid:12 ) and ( cid:9 ) refer to element-wise product and pairwise difference , respectively . h·q is the\nqth channel of h. in cie1 setting , only node-level merging eq . ( 16a ) is considered and the edge\nfeature is updated as eq . ( 10 ) . in cie2 setting , we also replace the edge update eq . ( 11 ) with eq.\n ( 16b ) . note edge embedding is used in both cie1 and cie2 and note pca-h can be regarded as\nthe pure node embedding version of our approach . the edge feature is initiated as reciprocal of the\nedge length . for training , batch size is set to 8 . we employ sgd optimizer ( bottou , 2010 ) with\nmomentum 0.9 . two cie layers are stacked after vgg16.\n\ncub2011 test cub2011 consists of 11,788 images from 200 kinds of birds with 15 annotated\nparts . we randomly sample image pairs from the dataset following the implementation released by\nchoy et al . ( 2016 ) . we do not use the pre-alignment of poses during testing , because their alignment\nresult is not publicly available . therefore , there exists signiﬁcant variation in pose , articulation and\nappearance across images , in both training and testing phase . images are cropped around bounding\nbox and resized to 256 × 256 before fed into the network . instead of evaluating the performance\nin a retrieval fashion ( zanﬁr & sminchisescu , 2018 ) , we directly evaluate the matching accuracy\nsince the semantic key-points are pre-given . we test two settings : 1 ) intra-class . during training , \nwe randomly sample images , with each pair sampled from the same category ( out of 200 bird cate-\ngories ) . in testing , 2,000 image pairs ( 100 pairs for each category ) are sampled ; 2 ) cross-class . we\nanalogously sample image pairs without considering the category information and 5,000 randomly\nsampled image pairs are employed for testing . while the ﬁrst setting is for a class-aware situation , \nthe second setting is considered for testing the class-agnostic case . results are shown in table 3.\n\nwe see our method surpasses all the competing methods in terms of matching accuracy . besides , \nalmost all the selected algorithms can reach over 90 % accuracy , indicating that this dataset contains\nmostly “ easy ” learning samples.\nin this case , the hungarian attention can slightly improve the\nperformance since easy gradients agree with descending trend of the loss on the whole dataset.\n\npascal voc test the pascal voc dataset with key-point annotation ( bourdev & malik , 2009 ) \ncontains 7,020 training images and 1,682 testing images with 20 classes in total . to the best of\nour knowledge , this is the largest and most challenging dataset for graph matching in computer vi-\nsion . each image is cropped around its object bounding box and is resized to 256 × 256 . the node\n\n8\n\n published as a conference paper at iclr 2020\n\n ( a ) accuracy/loss vs. training epoch.\n\n ( b ) ablation study by hungarian attention.\n\nfigure 4 : performance study on pascal voc . note in ( a ) the loss is calculated on all matching digits\nfor both cie1-p and cie1-h . note around 10th epoch , the accuracy of cie1-p almost reaches the\nhighest , but the loss keeps descending until 30th epoch . this indicates that in most of the latter\nepochs , p-loss performs “ meaningless ” back-propagation to drag the output to binary . h-loss , by\naccommodating smoothness , can emphasize most contributing digits and achieves higher accuracy.\nsize of this dataset varies from 6 to 23 and there are various scale , pose and illumination perturba-\ntions . experimental results are summarized in table 1 . we see in either setting , cie signiﬁcantly\noutperforms all peer algorithms . speciﬁcally , cie1-h achieves the best performance and has 0.8 % \nimprovement w.r.t . average accuracy over cie1-p. for each class , cie1-h and cie1-p carve up most\nof the top performance . we also note that cie1-h has a close performance on “ table ” compared\nwith gmn-d . since p-loss is naturally not as robust as d-loss on symmetric objects , p-loss showed\ngreat degradation over d-loss on “ table ” ( as discussed in ( wang et al. , 2019 ) ) . however , with the\nhelp of hungarian link , h-loss can maintain relatively high accuracy despite natural ﬂaw of p-loss.\nthis observation indicates that h-loss can focus on “ difﬁcult ” examples . we also note that cie1\nproduces better results against cie2 , which implies that updating edge embedding is less effective\ncompared to a singleton node updating strategy . we can also see from table 1 that pca-p has much\nhigher performance on training samples than cie1-h , which is to the contrary of the result on testing\nsamples . this might indicate that pca-p overﬁts the training samples.\n\naccuracy/loss vs. training epoch . we further show the typical training behavior of p-loss and h-\nloss on pascal voc dataset in fig . 4 . 30 epochs are involved in a whole training process . accuracy\nis evaluated on testing samples after each epoch while loss is the average loss value within each\nepoch . in the early training stage , the loss of cie1-p immediately drops . on the other hand , cie1-h\nhesitates for several epochs to ﬁnd the most effective descending direction . on the late stage , we\nobserve that even though p-loss ( eq . ( 12 ) ) calculates much more digits than h-loss ( eq . ( 14 ) ) , the\nloss values are opposite . this counter-intuitive fact strongly indicates that p-loss makes meaningless\neffort , which is not helpful to improve the performance , at late stage . the proposed h-loss , on the\nother hand , is capable of avoiding easy but meaningless gradients.\n\neffect of hungarian attention mechanism . we also conduct experiments to show the improve-\nment of hungarian attention over several loss functions ( with and without hungarian attention ) : \nhungarian attention is applied on focal loss ( focal ) ( lin et al. , 2017 ) as : \n\n ( cid:40 ) \n\nlfocal  ( 1 − sij ) γ log ( sij ) , \n− ( 1 − α ) zijsγ\n\nsg\nij log ( 1 − sij ) , sg\n\nij  1\nij  0\n\nwhere controlling parameters α  0.75 and γ  2 in our setting . we also design a margin loss\n ( margin ) with hungarian attention under a max-margin rule . note we insert the hungarian attention\nmask zij into eq . ( 17 ) and eq . ( 18 ) based on the vanilla forms.\n\nlmargin  ( cid:26 ) zij × max ( 1 − sij − β , 0 ) , sg\nsg\n\nzij × max ( sij − β , 0 ) , \n\nij  1\nij  0\n\n ( 17 ) \n\n ( 18 ) \n\nwhere we set the margin value β  0.2 . loss of eq . ( 18 ) is valid because after softmax and\nsinkhorn operations , sij ∈ [ 0 , 1 ] . we also show permutation loss ( perm ) ( wang et al. , 2019 ) .\nresult can be found in fig . 4 ( b ) whereby the average accuracy on pascal voc is reported . all the\nsettings are under cie1 . for either loss , the proposed hungarian attention can further enhance the\naccuracy , which is further visualized by a pair of matching results under p-loss and h-loss in fig . 5.\n\n9\n\n051015202530epoch0.20.250.30.350.40.450.50.550.60.650.7accuracy0.511.522.53lossacc : cie1-pacc : cie1-hloss : cie1-ploss : cie1-hfocalmarginperm0.60.620.640.660.680.7accuracyno hungwith hung published as a conference paper at iclr 2020\n\n ( a ) reference image\n\n ( b ) p-loss : 7/10\n\n ( c ) h-loss : 8/10\n\nfigure 5 : visualization of a matching result : 10 key points in each image with 7 and 8 correct\nmatchings dispalyed , respectively . different colors across images indicate node correspondence.\nthe larger size of dot , the larger is the predicted value sij . ( a ) the reference image . ( b ) result on\nthe target image from cie1-p. ( c ) result on the target image from cie1-h. we see though h-loss\ni.e . hungarian attention loss outputs smaller predicted values , it delivers a more accurate matching.\n\ntable 2 : accuracy ( % ) on willow object.\n\nface\nmethod\n91.2\nharg\ngmn-v\n98.1\ngmn-w 99.3\n100.0\npca-v\npca-w 100.0\ncie-v\n99.9\ncie-w 100.0\n\nmbike\n44.4\n65.0\n71.4\n69.8\n76.7\n71.5\n90.0\n\ncar\n58.4\n72.9\n74.3\n78.6\n84.0\n75.4\n82.2\n\nduck wbottle\n55.2\n74.3\n82.8\n82.4\n93.5\n73.2\n81.2\n\n66.6\n70.5\n76.7\n95.1\n96.9\n97.6\n97.6\n\nintra-class\n\ntable 3 : accuracy ( % ) on cub.\nmethod\ncross-class\ngmn-d\ngmn-p\ngat-p\npca-p\npca-h\ncie-p\ncie-h\n\n89.9\n90.8\n93.4\n93.5\n93.5\n93.8\n94.2\n\n89.6\n90.4\n93.2\n92.9\n93.7\n94.1\n94.4\n\nwillow object class test we test the transfer ability on willow object class ( cho et al. , 2013 ) .\nit contains 256 images3 of 5 categories in total , with three categories ( face , duck and winebottle ) \ncollected from caltech-256 and resting two ( car and motorbike ) from pascal voc 2007 . this dataset\nis considered to have bias compared with pascal voc since images in the same category are with\nrelatively ﬁxed pose and background is much cleaner . we crop the object inside its bounding box and\nresize it to 256 × 256 as cnn input . while harg is trained from scratch following the protocol\nin ( cho et al. , 2013 ) , all the resting counterparts are either directly pre-trained from the previous\nsection or ﬁne-tuned upon the pre-trained models . we term the method “ x-v ” or “ x-w ” to indicate\npre-trained model on pascal voc or ﬁne-tuned on willow , respectively . cie refers to cie1-h for\nshort . results in table 2 suggest that our method is competitive to state-of-the-art.\n\n5 conclusion\n\nwe have presented a novel and effective approach for learning based graph matching . on one hand , \nthe novelty of our method partially lies in the development of the hungarian attention , which in-\ntrinsically adapts the matching problem . it is further observed from the experiments that hungarian\nattention can improve several matching-oriented loss functions , which might bring about potential\nfor a series of combinatorial problems . on the other hand , we also devise the channel independent\nembedding ( cie ) technique for deep graph matching , which decouples the basic merging opera-\ntions and is shown robust in learning effective graph representation . extensive experimental results\non multiple matching benchmarks show the leading performance of our solver , and highlight the\northogonal contribution of the two proposed components on top of existing techniques.\n\nacknowledgments\n\ntianshu yu and baoxin li were supported in part by a grant from onr . any opinions expressed\nin this material are those of the authors and do not necessarily reﬂect the views of onr . runzhong\nwang and junchi yan were supported in part by nsfc 61972250 and u19b2035.\n\n3the data size is too small to train a deep model . hence we only evaluate the transfer ability on this dataset.\n\n10\n\n published as a conference paper at iclr 2020\n\n 