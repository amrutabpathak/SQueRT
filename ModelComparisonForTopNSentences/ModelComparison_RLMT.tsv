Q: RLMT: Is reinforcement learning an effective approach for neural machine translation
A: Not with most currently used RL methods

Scores:
Order of scoring: Evan, 
Bert: 6
Alberta: 5
Distilbert: 5
USE: 5

en_trf_bertbaseuncased_lg
en_trf_bertbaseuncased_lg1	Is reinforcement learning an effective approach for neural machine translation	Fourth  our controlled simulations show that asymptotic convergence is not reached in any but the easiest conditions (§5.1). Our analysis further suggests that gradient clipping  sometimes used in NMT (Zhang et al  2016; Wieting et al  2019)  is expected to hinder convergence further.It should be avoided when using REINFORCE as it violates REINFORCE’s assumptions.	0.7221140546933564
en_trf_bertbaseuncased_lg0	Is reinforcement learning an effective approach for neural machine translation	From here follows that if R is constant with respect to θ  then the expected ∆θ prescribed by REINFORCE is zero.We note that r may be shifted by a constant term (called a “baseline”)  without affecting the optimal value for θ. REINFORCE is used in MT  text generation  and image-to-text tasks (Liu et al  2016; Wu et al  2018; Rennie et al  2017; Shetty et al  2017; Hendricks et al  2016) – in isolation  or as a part of training .	0.7258945489121336
en_trf_bertbaseuncased_lg0	Is reinforcement learning an effective approach for neural machine translation	We return to this point in §7.Fur- thermore  given their findings  it is reasonable to assume that our results are relevant for RL use in other generation tasks  whose output space too is discrete  high-dimensional and concentrated. 4.1 CONTROLLED SIMULATIONS  We experiment with a 1-layer softmax model  that predicts a single token i ∈ V with probability eθi j eθjθ = {θj}j∈V are the model’s parameters.	0.7268375713893207
en_trf_bertbaseuncased_lg1	Is reinforcement learning an effective approach for neural machine translation	Another contribution of this paper is in showing that CMRT does not optimize the expected reward and is thus theoretically unmotivated. A number of reasons lead us to believe that in our NMT experiments  improvements are not due to the reward function  but to artefacts such as PKE.First  reducing a constant baseline from r  so as to make the expected reward zero  disallows learning.	0.7279934273799147
en_trf_bertbaseuncased_lg0	Is reinforcement learning an effective approach for neural machine translation	This tradeoff explains the importance of tuning α reported in the literature.In §6 we present simulations with CMRT  showing very similar trends as presented by REINFORCE. k  3 MOTIVATING DISCUSSION  Implementing a stochastic gradient ascent  REINFORCE is guaranteed to converge to a stationary point of R under broad conditions.	0.7294867163959248
en_trf_bertbaseuncased_lg0	Is reinforcement learning an effective approach for neural machine translation	 Caccia et al (2018) recently observed in the context of language modeling using GANs that per- formance gains similar to those GAN yield can be achieved by decreasing the temperature for the prediction softmax (i.e.  making it peakier).However  they proposed no causes for this effect.Our findings propose an underlying mechanism leading to this trend.	0.7304413143257851
en_trf_bertbaseuncased_lg1	Is reinforcement learning an effective approach for neural machine translation	 Second  using both naturalistic experiments and carefully constructed simulations  we show that performance gains observed in the literature likely stem not from making target tokens the most probable  but from unrelated effects  such as increasing the peakiness of the output distribution (i.e.  the probability mass of the most probable tokens).We do so by comparing a setting where the reward is informative  vs. one where it is constant.In §4 we discuss this peakiness effect (PKE).	0.7309738561457275
en_trf_bertbaseuncased_lg0	Is reinforcement learning an effective approach for neural machine translation	The model was pretrained on WMT2015 training data .Hyperparameters are reported in Appendix A.3. We define one of the tokens in V to be the target token and denote it with ybest.We assign deterministic token reward  this makes learning easier than when relying on approximations and our predictions optimistic.	0.7362608602843627
en_trf_bertbaseuncased_lg0	Is reinforcement learning an effective approach for neural machine translation	 We also assume there is exactly one valid target token  as de facto  training is done against a single reference .In practice  either a token-level reward is approximated using Monte- Carlo methods (e.g.  Yang et al  2018)  or a sentence-level (sparse) reward is given at the end of the episode (sentence).The latter is equivalent to a uniform token-level reward.	0.7444878802239189
en_trf_bertbaseuncased_lg2	Is reinforcement learning an effective approach for neural machine translation	In fact  our results suggest that RL practices in MT are likely to improve performance only where the pre-trained parameters are already close to yielding the correct translation.Our findings further suggest that observed gains may be due to effects unrelated to the training signal  concretely  changes in the shape of the distribution curve. 1  INTRODUCTION  Reinforcement learning (RL) is an appealing path for advancement in Machine Translation (MT)  as it allows training systems to optimize non-differentiable score functions  common in MT evaluation  as well as tackling the “exposure bias”  in standard training  namely that the model is not exposed during training to incorrectly generated tokens  and is thus unlikely to recover from generating such tokens at test time.	0.7481603598708125
en_trf_bertbaseuncased_lg0	Is reinforcement learning an effective approach for neural machine translation	More sophisticated exploration methods have been extensively studied  for example using measures for the exploratory usefulness of states or actions   or relying on parameter-space noise rather than action-space noise . For MT  an additional challenge is that even effective exploration (sampling diverse sets of obser- vations)  may not be enough  since the state-action space is too large to be effectively covered  with almost all sentences being not rewarding.Recently  diversity-based and multi-goal methods for RL were proposed to tackle similar challenges (Andrychowicz et al  2017; Ghosh et al  2018; Eysen- bach et al  2019).	0.7560541955913402
en_trf_bertbaseuncased_lg1	Is reinforcement learning an effective approach for neural machine translation	Published as a conference paper at ICLR 2020  ON THE WEAKNESSES OF REINFORCEMENT LEARN- ING FOR NEURAL MACHINE TRANSLATION  Leshem Choshen1  Lior Fox2  Zohar Aizenbud1  Omri Abend1 3 1 School of Computer Science and Engineering  2 The Edmond and Lily Safra Center for Brain Sciences 3 Department of Cognitive Sciences The Hebrew University of Jerusalem first.last@mail.huji.ac.il  oabend@cs.huji.ac.il  ABSTRACT  Reinforcement learning (RL) is frequently used to increase performance in text generation tasks  including machine translation (MT)  notably through the use of Minimum Risk Training (MRT) and Generative Adversarial Networks (GAN).However  little is known about what and how these methods learn in the context of MT.We prove that one of the most common RL methods for MT does not optimize the expected reward  as well as show that other methods take an infeasibly long time to converge.	0.7606045348071738
en_trf_bertbaseuncased_lg0	Is reinforcement learning an effective approach for neural machine translation	This contrasts with the more common sce- nario studied by contemporary RL methods  which focuses mostly on much smaller discrete action spaces (e.g.  video games (Mnih et al  2015; 2016))  or continuous action spaces of relatively low dimensions (e.g.  simulation of robotic control tasks ).Second  reward for MT is naturally very sparse – almost all possible sentences are “wrong” (hence  not rewarding) in a given context.Finally  it is common in MT to use RL for tuning a pretrained model.	0.7620436358728205
en_trf_bertbaseuncased_lg0	Is reinforcement learning an effective approach for neural machine translation	However  not much is known about its convergence rate under the prevailing conditions in NMT. We begin with a qualitative  motivating analysis of these questions.As work on language generation empirically showed  RNNs quickly learn to output very peaky distributions .	0.7830789261328325
en_trf_bertbaseuncased_lg0	Is reinforcement learning an effective approach for neural machine translation	In principle  such methods allow learning from a more “exploratory” policy.Moreover  a key mo- tivation for using α in CMRT is smoothing; off-policy sampling allows smoothing while keeping convergence guarantees. In its basic form  exploration in REINFORCE relies on stochasticity in the action-selection (in MT  this is due to sampling).	0.7883253640458888


en_trf_robertabase_lg
en_trf_robertabase_lg1	Is reinforcement learning an effective approach for neural machine translation	Another contribution of this paper is in showing that CMRT does not optimize the expected reward and is thus theoretically unmotivated. A number of reasons lead us to believe that in our NMT experiments  improvements are not due to the reward function  but to artefacts such as PKE.First  reducing a constant baseline from r  so as to make the expected reward zero  disallows learning.	0.9706891687297056
en_trf_robertabase_lg1	Is reinforcement learning an effective approach for neural machine translation	 Third  we show that promoting the target token to be the mode is likely to take a prohibitively long time.The only case we find  where improvements are likely  is where the target token is among the first 2-3 most probable tokens according to the pretrained model.These findings suggest that REINFORCE (§5) and CMRT (§6) are likely to improve over the pre-trained model only under the best possible conditions  i.e.  where the pre-trained model is “nearly” correct.	0.9707990782221293
en_trf_robertabase_lg0	Is reinforcement learning an effective approach for neural machine translation	However  full experiments (with 1M steps) exhibit similar trends: where REINFORCE was not close to converging after 50K steps  the same was true after 1M steps. We evaluate the peakiness of a distribution in terms of the probability of the most probable token (the mode)  the total probability of the ten most probable tokens  and the entropy of the distribution (lower entropy indicates more peakiness). 3Based on our NMT experiments  which we assume to be representative of the error rate of other systems.	0.9711119613020964
en_trf_robertabase_lg0	Is reinforcement learning an effective approach for neural machine translation	 Caccia et al (2018) recently observed in the context of language modeling using GANs that per- formance gains similar to those GAN yield can be achieved by decreasing the temperature for the prediction softmax (i.e.  making it peakier).However  they proposed no causes for this effect.Our findings propose an underlying mechanism leading to this trend.	0.9711860028206479
en_trf_robertabase_lg0	Is reinforcement learning an effective approach for neural machine translation	 2.Constant Reward: r is constantly equal to 1  for all tokens.This setting is aimed to  confirm that PKE is not a result of the signal carried by the reward.	0.971320719493384
en_trf_robertabase_lg0	Is reinforcement learning an effective approach for neural machine translation	More sophisticated exploration methods have been extensively studied  for example using measures for the exploratory usefulness of states or actions   or relying on parameter-space noise rather than action-space noise . For MT  an additional challenge is that even effective exploration (sampling diverse sets of obser- vations)  may not be enough  since the state-action space is too large to be effectively covered  with almost all sentences being not rewarding.Recently  diversity-based and multi-goal methods for RL were proposed to tackle similar challenges (Andrychowicz et al  2017; Ghosh et al  2018; Eysen- bach et al  2019).	0.9714793074488831
en_trf_robertabase_lg0	Is reinforcement learning an effective approach for neural machine translation	This tendency is advantageous for generating ﬂuent sentences with high probability  but may also entail slower convergence rates when using RL to fine-tune the model  because RL methods used in text generation sample from the (pretrained) policy distribution  which means they mostly sample what the pretrained model deems to be likely.Since the pretrained model (or policy) is peaky  exploration of other potentially more rewarding tokens will be limited  hampering convergence. Intuitively  REINFORCE increases the probabilities of successful (positively rewarding) observa- tions  weighing updates by how rewarding they were.	0.9717351969829412
en_trf_robertabase_lg0	Is reinforcement learning an effective approach for neural machine translation	We assess the rate of convergence empirically in §5  finding this to be indeed the case. 1Sakaguchi et al (2017) discuss the relation between CMRT and REINFORCE  claiming that CMRT is a  variantAppendix A.1 shows that CMRT does not in fact optimize the same objective. 2Not performing deduplication (e.g. in THUMT ) results in assigning higher relative  weight to high-probability tokens  which may have an adverse effect on convergence rate.	0.9723020092167821
en_trf_robertabase_lg0	Is reinforcement learning an effective approach for neural machine translation	In practice  increasing the learning rate may deteriorate re- sults  as it may cause the system to overfit to the sampled instances.Indeed  when increasing the learning rate in our NMT experiments (see below) by an order of magnitude  early stopping caused the RL procedure to stop without any parameter updates. Figure 2 shows the change in Pθ over the first 50K REINFORCE steps (probabilities are averaged over 100 repetitions)  for a case where ybest was initially the second  third and fourth most probable.	0.9723189353757131
en_trf_robertabase_lg0	Is reinforcement learning an effective approach for neural machine translation	This tradeoff explains the importance of tuning α reported in the literature.In §6 we present simulations with CMRT  showing very similar trends as presented by REINFORCE. k  3 MOTIVATING DISCUSSION  Implementing a stochastic gradient ascent  REINFORCE is guaranteed to converge to a stationary point of R under broad conditions.	0.9724744446286199
en_trf_robertabase_lg1	Is reinforcement learning an effective approach for neural machine translation	Using a pretrained model ameliorates the last problem.But then  these pretrained models are in general quite peaky  and because training is done on-policy – that is  actions are being sampled from the same model being optimized – exploration is inherently limited. Here we argued that  taken together  these challenges result in significant weaknesses for current RL practices for NMT  that may ultimately prevent them from being truly useful.	0.9729224100910053
en_trf_robertabase_lg0	Is reinforcement learning an effective approach for neural machine translation	In principle  such methods allow learning from a more “exploratory” policy.Moreover  a key mo- tivation for using α in CMRT is smoothing; off-policy sampling allows smoothing while keeping convergence guarantees. In its basic form  exploration in REINFORCE relies on stochasticity in the action-selection (in MT  this is due to sampling).	0.9738223052989237
en_trf_robertabase_lg0	Is reinforcement learning an effective approach for neural machine translation	The model was pretrained on WMT2015 training data .Hyperparameters are reported in Appendix A.3. We define one of the tokens in V to be the target token and denote it with ybest.We assign deterministic token reward  this makes learning easier than when relying on approximations and our predictions optimistic.	0.9739914526149284
en_trf_robertabase_lg2	Is reinforcement learning an effective approach for neural machine translation	In fact  our results suggest that RL practices in MT are likely to improve performance only where the pre-trained parameters are already close to yielding the correct translation.Our findings further suggest that observed gains may be due to effects unrelated to the training signal  concretely  changes in the shape of the distribution curve. 1  INTRODUCTION  Reinforcement learning (RL) is an appealing path for advancement in Machine Translation (MT)  as it allows training systems to optimize non-differentiable score functions  common in MT evaluation  as well as tackling the “exposure bias”  in standard training  namely that the model is not exposed during training to incorrectly generated tokens  and is thus unlikely to recover from generating such tokens at test time.	0.9742375383156116
en_trf_robertabase_lg0	Is reinforcement learning an effective approach for neural machine translation	However  not much is known about its convergence rate under the prevailing conditions in NMT. We begin with a qualitative  motivating analysis of these questions.As work on language generation empirically showed  RNNs quickly learn to output very peaky distributions .	0.9761651273765716


en_trf_distilbertbaseuncased_lg
en_trf_distilbertbaseuncased_lg1	Is reinforcement learning an effective approach for neural machine translation	Another contribution of this paper is in showing that CMRT does not optimize the expected reward and is thus theoretically unmotivated. A number of reasons lead us to believe that in our NMT experiments  improvements are not due to the reward function  but to artefacts such as PKE.First  reducing a constant baseline from r  so as to make the expected reward zero  disallows learning.	0.8066168884263681
en_trf_distilbertbaseuncased_lg0	Is reinforcement learning an effective approach for neural machine translation	 Caccia et al (2018) recently observed in the context of language modeling using GANs that per- formance gains similar to those GAN yield can be achieved by decreasing the temperature for the prediction softmax (i.e.  making it peakier).However  they proposed no causes for this effect.Our findings propose an underlying mechanism leading to this trend.	0.8077910210244587
en_trf_distilbertbaseuncased_lg0	Is reinforcement learning an effective approach for neural machine translation	Lately  an especially prominent use for REINFORCE is adversarial training with discrete data  where another network predicts the reward (GAN).For some recent work on RL for NMT  see (Zhang et al  2016; Li et al  2017; Wu et al  2017; Yu et al  2017; Yang et al  2018). 2.2 MINIMUM RISK TRAINING  The term Minimum Risk Training (MRT) is used ambiguously in MT to refer either to the appli- cation of REINFORCE to minimizing the risk (equivalently  to maximizing the expected reward  the negative loss)  or more commonly to a somewhat different estimation method  which we term Con- trastive MRT (CMRT) and turn now to analyzing.	0.8087785905938508
en_trf_distilbertbaseuncased_lg0	Is reinforcement learning an effective approach for neural machine translation	 We conclude by discussing other RL practices in MT which should be avoided for practical and theoretical reasons  and brieﬂy discuss alternative RL approaches that will allow RL to tackle a larger class of errors in pre-trained models (§7). 1  Published as a conference paper at ICLR 2020  2 RL IN MACHINE TRANSLATION  An MT system generates tokens y = (y1  ...  yn) from a vocabulary V one token at a time.The probability of generating yi given preceding tokens y<i is given by Pθ(·|x  y<i)  where x is the source sentence and θ are the model parameters.	0.8121858550426333
en_trf_distilbertbaseuncased_lg1	Is reinforcement learning an effective approach for neural machine translation	Using a pretrained model ameliorates the last problem.But then  these pretrained models are in general quite peaky  and because training is done on-policy – that is  actions are being sampled from the same model being optimized – exploration is inherently limited. Here we argued that  taken together  these challenges result in significant weaknesses for current RL practices for NMT  that may ultimately prevent them from being truly useful.	0.8127317379182771
en_trf_distilbertbaseuncased_lg0	Is reinforcement learning an effective approach for neural machine translation	This tradeoff explains the importance of tuning α reported in the literature.In §6 we present simulations with CMRT  showing very similar trends as presented by REINFORCE. k  3 MOTIVATING DISCUSSION  Implementing a stochastic gradient ascent  REINFORCE is guaranteed to converge to a stationary point of R under broad conditions.	0.8143896262332012
en_trf_distilbertbaseuncased_lg0	Is reinforcement learning an effective approach for neural machine translation	We assess the rate of convergence empirically in §5  finding this to be indeed the case. 1Sakaguchi et al (2017) discuss the relation between CMRT and REINFORCE  claiming that CMRT is a  variantAppendix A.1 shows that CMRT does not in fact optimize the same objective. 2Not performing deduplication (e.g. in THUMT ) results in assigning higher relative  weight to high-probability tokens  which may have an adverse effect on convergence rate.	0.8153144373001585
en_trf_distilbertbaseuncased_lg0	Is reinforcement learning an effective approach for neural machine translation	However  as Edunov et al (2018) point out  this practice may lower results  as it may destabilize training by leading the model to improve over outputs it cannot generalize over  as they are very different from anything the model assigns a high probability to  at the cost of other outputs. 8 CONCLUSION  The standard MT scenario poses several uncommon challenges for RL.First  the action space in MT problems is a high-dimensional discrete space (generally in the size of the vocabulary of the target language or the product thereof for sentences).	0.8158577551294326
en_trf_distilbertbaseuncased_lg1	Is reinforcement learning an effective approach for neural machine translation	Published as a conference paper at ICLR 2020  ON THE WEAKNESSES OF REINFORCEMENT LEARN- ING FOR NEURAL MACHINE TRANSLATION  Leshem Choshen1  Lior Fox2  Zohar Aizenbud1  Omri Abend1 3 1 School of Computer Science and Engineering  2 The Edmond and Lily Safra Center for Brain Sciences 3 Department of Cognitive Sciences The Hebrew University of Jerusalem first.last@mail.huji.ac.il  oabend@cs.huji.ac.il  ABSTRACT  Reinforcement learning (RL) is frequently used to increase performance in text generation tasks  including machine translation (MT)  notably through the use of Minimum Risk Training (MRT) and Generative Adversarial Networks (GAN).However  little is known about what and how these methods learn in the context of MT.We prove that one of the most common RL methods for MT does not optimize the expected reward  as well as show that other methods take an infeasibly long time to converge.	0.8166120045767576
en_trf_distilbertbaseuncased_lg0	Is reinforcement learning an effective approach for neural machine translation	This contrasts with the more common sce- nario studied by contemporary RL methods  which focuses mostly on much smaller discrete action spaces (e.g.  video games (Mnih et al  2015; 2016))  or continuous action spaces of relatively low dimensions (e.g.  simulation of robotic control tasks ).Second  reward for MT is naturally very sparse – almost all possible sentences are “wrong” (hence  not rewarding) in a given context.Finally  it is common in MT to use RL for tuning a pretrained model.	0.8176830004472027
en_trf_distilbertbaseuncased_lg2	Is reinforcement learning an effective approach for neural machine translation	In fact  our results suggest that RL practices in MT are likely to improve performance only where the pre-trained parameters are already close to yielding the correct translation.Our findings further suggest that observed gains may be due to effects unrelated to the training signal  concretely  changes in the shape of the distribution curve. 1  INTRODUCTION  Reinforcement learning (RL) is an appealing path for advancement in Machine Translation (MT)  as it allows training systems to optimize non-differentiable score functions  common in MT evaluation  as well as tackling the “exposure bias”  in standard training  namely that the model is not exposed during training to incorrectly generated tokens  and is thus unlikely to recover from generating such tokens at test time.	0.830200713994691
en_trf_distilbertbaseuncased_lg0	Is reinforcement learning an effective approach for neural machine translation	More sophisticated exploration methods have been extensively studied  for example using measures for the exploratory usefulness of states or actions   or relying on parameter-space noise rather than action-space noise . For MT  an additional challenge is that even effective exploration (sampling diverse sets of obser- vations)  may not be enough  since the state-action space is too large to be effectively covered  with almost all sentences being not rewarding.Recently  diversity-based and multi-goal methods for RL were proposed to tackle similar challenges (Andrychowicz et al  2017; Ghosh et al  2018; Eysen- bach et al  2019).	0.8319848119792984
en_trf_distilbertbaseuncased_lg0	Is reinforcement learning an effective approach for neural machine translation	The model was pretrained on WMT2015 training data .Hyperparameters are reported in Appendix A.3. We define one of the tokens in V to be the target token and denote it with ybest.We assign deterministic token reward  this makes learning easier than when relying on approximations and our predictions optimistic.	0.8404442316296094
en_trf_distilbertbaseuncased_lg0	Is reinforcement learning an effective approach for neural machine translation	In principle  such methods allow learning from a more “exploratory” policy.Moreover  a key mo- tivation for using α in CMRT is smoothing; off-policy sampling allows smoothing while keeping convergence guarantees. In its basic form  exploration in REINFORCE relies on stochasticity in the action-selection (in MT  this is due to sampling).	0.8552735371065769
en_trf_distilbertbaseuncased_lg0	Is reinforcement learning an effective approach for neural machine translation	However  not much is known about its convergence rate under the prevailing conditions in NMT. We begin with a qualitative  motivating analysis of these questions.As work on language generation empirically showed  RNNs quickly learn to output very peaky distributions .	0.8582616597873762


Google_USE
Google_USE0	Is reinforcement learning an effective approach for neural machine translation	In principle  such methods allow learning from a more “exploratory” policy.Moreover  a key mo- tivation for using α in CMRT is smoothing; off-policy sampling allows smoothing while keeping convergence guarantees. In its basic form  exploration in REINFORCE relies on stochasticity in the action-selection (in MT  this is due to sampling).	[[0.505441]]
Google_USE0	Is reinforcement learning an effective approach for neural machine translation	 We conclude by discussing other RL practices in MT which should be avoided for practical and theoretical reasons  and brieﬂy discuss alternative RL approaches that will allow RL to tackle a larger class of errors in pre-trained models (§7). 1  Published as a conference paper at ICLR 2020  2 RL IN MACHINE TRANSLATION  An MT system generates tokens y = (y1  ...  yn) from a vocabulary V one token at a time.The probability of generating yi given preceding tokens y<i is given by Pθ(·|x  y<i)  where x is the source sentence and θ are the model parameters.	[[0.5175532]]
Google_USE0	Is reinforcement learning an effective approach for neural machine translation	These motivations have led to much interest in RL for text generation in general and MT in particular (see §2).Various policy gradient methods have been used  notably REINFORCE (Williams  1992) and variants thereof (e.g.  Ranzato et al  2015; Edunov et al  2018) and Minimum Risk Training (MRT; e.g.  Och  2003; Shen et al  2016).Another popular use of RL is for training GANs (Yang et al  2018; Tevet et al  2018).	[[0.5186564]]
Google_USE0	Is reinforcement learning an effective approach for neural machine translation	In practice  increasing the learning rate may deteriorate re- sults  as it may cause the system to overfit to the sampled instances.Indeed  when increasing the learning rate in our NMT experiments (see below) by an order of magnitude  early stopping caused the RL procedure to stop without any parameter updates. Figure 2 shows the change in Pθ over the first 50K REINFORCE steps (probabilities are averaged over 100 repetitions)  for a case where ybest was initially the second  third and fourth most probable.	[[0.51872337]]
Google_USE0	Is reinforcement learning an effective approach for neural machine translation	This contrasts with the more common sce- nario studied by contemporary RL methods  which focuses mostly on much smaller discrete action spaces (e.g.  video games (Mnih et al  2015; 2016))  or continuous action spaces of relatively low dimensions (e.g.  simulation of robotic control tasks ).Second  reward for MT is naturally very sparse – almost all possible sentences are “wrong” (hence  not rewarding) in a given context.Finally  it is common in MT to use RL for tuning a pretrained model.	[[0.52186567]]
Google_USE0	Is reinforcement learning an effective approach for neural machine translation	However  not much is known about its convergence rate under the prevailing conditions in NMT. We begin with a qualitative  motivating analysis of these questions.As work on language generation empirically showed  RNNs quickly learn to output very peaky distributions .	[[0.52843666]]
Google_USE2	Is reinforcement learning an effective approach for neural machine translation	In fact  our results suggest that RL practices in MT are likely to improve performance only where the pre-trained parameters are already close to yielding the correct translation.Our findings further suggest that observed gains may be due to effects unrelated to the training signal  concretely  changes in the shape of the distribution curve. 1  INTRODUCTION  Reinforcement learning (RL) is an appealing path for advancement in Machine Translation (MT)  as it allows training systems to optimize non-differentiable score functions  common in MT evaluation  as well as tackling the “exposure bias”  in standard training  namely that the model is not exposed during training to incorrectly generated tokens  and is thus unlikely to recover from generating such tokens at test time.	[[0.5296129]]
Google_USE0	Is reinforcement learning an effective approach for neural machine translation	We believe the adoption of such methods is a promising path forward for the application of RL in NLP. 8  Published as a conference paper at ICLR 2020  9 ACKNOWLEDGMENTS  This work was supported by the Israel Science Foundation (grant no.929/17) and by the HUJI Cyber Security Research Center in conjunction with the Israel National Cyber Bureau in the Prime Minister’s Office.	[[0.53120774]]
Google_USE1	Is reinforcement learning an effective approach for neural machine translation	Another contribution of this paper is in showing that CMRT does not optimize the expected reward and is thus theoretically unmotivated. A number of reasons lead us to believe that in our NMT experiments  improvements are not due to the reward function  but to artefacts such as PKE.First  reducing a constant baseline from r  so as to make the expected reward zero  disallows learning.	[[0.53127563]]
Google_USE0	Is reinforcement learning an effective approach for neural machine translation	The model was pretrained on WMT2015 training data .Hyperparameters are reported in Appendix A.3. We define one of the tokens in V to be the target token and denote it with ybest.We assign deterministic token reward  this makes learning easier than when relying on approximations and our predictions optimistic.	[[0.5351854]]
Google_USE0	Is reinforcement learning an effective approach for neural machine translation	Lately  an especially prominent use for REINFORCE is adversarial training with discrete data  where another network predicts the reward (GAN).For some recent work on RL for NMT  see (Zhang et al  2016; Li et al  2017; Wu et al  2017; Yu et al  2017; Yang et al  2018). 2.2 MINIMUM RISK TRAINING  The term Minimum Risk Training (MRT) is used ambiguously in MT to refer either to the appli- cation of REINFORCE to minimizing the risk (equivalently  to maximizing the expected reward  the negative loss)  or more commonly to a somewhat different estimation method  which we term Con- trastive MRT (CMRT) and turn now to analyzing.	[[0.54323196]]
Google_USE1	Is reinforcement learning an effective approach for neural machine translation	We call this the peakiness-effect (PKE)  and show it occurs both in simulations (§4.1) and in full-scale NMT experiments (§4.2). With more iterations  the most-rewarding tokens will be eventually sampled  and gradually gain probability mass.This discussion suggests that training will be extremely sample-inefficient.	[[0.5457208]]
Google_USE0	Is reinforcement learning an effective approach for neural machine translation	 4.2 NMT EXPERIMENTS  We turn to analyzing a real-world application of REINFORCE to NMT.Important differences between this and the previous simula- tions are: (1) it is rare in NMT for REINFORCE to sample from the same conditional distribution more than a handful of times  given the number of source sentences x and sentence prefixes y<i (con- texts); and (2) in NMT Pθ(·|x  y<i) shares parameters between con- texts  which means that updating Pθ for one context may inﬂuence Pθ for another. We follow the same pretraining as in §4.1. We then follow Yang et al (2018) in defining the reward function based on the expected BLEU score.	[[0.55364]]
Google_USE0	Is reinforcement learning an effective approach for neural machine translation	However  as Edunov et al (2018) point out  this practice may lower results  as it may destabilize training by leading the model to improve over outputs it cannot generalize over  as they are very different from anything the model assigns a high probability to  at the cost of other outputs. 8 CONCLUSION  The standard MT scenario poses several uncommon challenges for RL.First  the action space in MT problems is a high-dimensional discrete space (generally in the size of the vocabulary of the target language or the product thereof for sentences).	[[0.5879602]]
Google_USE1	Is reinforcement learning an effective approach for neural machine translation	Published as a conference paper at ICLR 2020  ON THE WEAKNESSES OF REINFORCEMENT LEARN- ING FOR NEURAL MACHINE TRANSLATION  Leshem Choshen1  Lior Fox2  Zohar Aizenbud1  Omri Abend1 3 1 School of Computer Science and Engineering  2 The Edmond and Lily Safra Center for Brain Sciences 3 Department of Cognitive Sciences The Hebrew University of Jerusalem first.last@mail.huji.ac.il  oabend@cs.huji.ac.il  ABSTRACT  Reinforcement learning (RL) is frequently used to increase performance in text generation tasks  including machine translation (MT)  notably through the use of Minimum Risk Training (MRT) and Generative Adversarial Networks (GAN).However  little is known about what and how these methods learn in the context of MT.We prove that one of the most common RL methods for MT does not optimize the expected reward  as well as show that other methods take an infeasibly long time to converge.	[[0.7253188]]
