Q: RLquestiongen: What model is state of the art for natural question generation
A: Graph to sequence (Graph2Seq)

Scores:
Order of scoring: Evan, 
Bert: 1
Alberta: 4
Distilbert: 7
USE: 4

en_trf_bertbaseuncased_lg
en_trf_bertbaseuncased_lg1	What model is state of the art for natural question generation	As we can see  incorporating answer information helps the model identify the answer type of the question to be generated  and thus makes the generated ques- tions more relevant and specific.Also  we find our Graph2Seq model can generate more complete and valid questions compared to the Seq2Seq baseline.We think it is because a Graph2Seq model is able to exploit the rich text structure information better than a Seq2Seq model.	0.7123563380740869
en_trf_bertbaseuncased_lg0	What model is state of the art for natural question generation	G2Sdyn+BERT+RL: what is essential for the successful execution of a project ?Passage: the church operates three hundred sixty schools and institutions overseasGold: how many schools and institutions does the church operate overseas ?G2Ssta w/o BiGGNN (Seq2Seq): how many schools does the church have ?	0.7126931918880435
en_trf_bertbaseuncased_lg0	What model is state of the art for natural question generation	G2Ssta: what is essential for the successful execution of a project ?G2Ssta+BERT: what is essential for the successful execution of a project ?G2Ssta+BERT+RL: what is essential for the successful execution of a project ?	0.7149823149398664
en_trf_bertbaseuncased_lg0	What model is state of the art for natural question generation	On the one hand  an advantage of static graph construc- tion is that useful domain knowledge can be hard-coded into the graph  which can greatly benefit the downstream task.However  it might suffer if there is a lack of prior knowledge for a specific domain knowledge.On the other hand  dynamic graph construction does not need any prior knowledge about the hidden structure of text  and only relies on the attention matrix to capture these structured information  which provides an easy way to achieve a decent performance.	0.7154456838865867
en_trf_bertbaseuncased_lg0	What model is state of the art for natural question generation	 2 AN RL-BASED GENERATOR-EVALUATOR ARCHITECTURE  In this section  we define the question generation task  and then present our RL-based Graph2Seq model for question generation.We first motivate the design  and then present the details of each component as shown in Fig 1. 2.1 PROBLEM FORMULATION  The goal of question generation is to generate natural language questions based on a given form of data  such as knowledge base triples or tables   sentences (Du et al  2017; Song et al  2018a)  or images   where the generated questions need to be answerable from the input data.	0.7157772218556396
en_trf_bertbaseuncased_lg0	What model is state of the art for natural question generation	Lastly  it shows that fine-tuning the model using REINFORCE can improve the quality of the generated questions. 4 RELATED WORK  4.1 NATURAL QUESTION GENERATION  Early works (Mostow & Chen  2009; Heilman & Smith  2010; Heilman  2011) for QG focused on rule-based approaches that rely on heuristic rules or hand-crafted templates  with low generaliz- ability and scalability.Recent attempts have focused on NN-based approaches that do not require  9  Under review as a conference paper at ICLR 2020  manually-designed rules and are end-to-end trainable.	0.7163398987704026
en_trf_bertbaseuncased_lg0	What model is state of the art for natural question generation	Besides  we find that the pretrained BERT embedding has a considerable impact on the performance and fine-tuning BERT embedding even further improves the performance  which demonstrates the power of large-scale pretrained language models. 3.5 CASE STUDY  Table 4: Generated questions on SQuAD split-2 test set.Target answers are underlined.	0.7190897462688749
en_trf_bertbaseuncased_lg0	What model is state of the art for natural question generation	Very recently  researchers have started exploring methods to automat- ically construct a graph of visual objects  or words (Liu et al  2018; Chen et al  2019b) when applying GNNs to non-graph structured data. To the best of our knowledge  we are the first to investigate systematically the performance difference between syntactic-aware static graph construction and semantics-aware dynamic graph construction in the context of question generation. 5 CONCLUSION  We proposed a novel RL based Graph2Seq model for QG  where the answer information is utilized by an effective Deep Alignment Network and a novel bidirectional GNN is proposed to process the directed passage graph.	0.7195535485504028
en_trf_bertbaseuncased_lg0	What model is state of the art for natural question generation	Also  unlike the baseline methods  our model does not rely on any hand-crafted rules or ad-hoc strategies  and is fully end-to-end trainable. As shown in Table 2  we conducted a human evaluation study to assess the quality of the questions generated by our model  the baseline method MPQG+R  and the ground-truth data in terms of syn- tax  semantics and relevance metrics.We can see that our best performing model achieves good results even compared to the ground-truth  and outperforms the strong baseline method MPQG+R.	0.7198819717837638
en_trf_bertbaseuncased_lg0	What model is state of the art for natural question generation	 2.2 DEEP ALIGNMENT NETWORK  Answer information is crucial for generating relevant and high quality questions from a passage.Un- like previous methods that neglect potential semantic relations between passage and answer words  we explicitly model the global interactions among them in the embedding space.To this end  we propose a novel Deep Alignment Network (DAN) component for effectively incorporating answer information into the passage with multiple granularity levels.	0.7210460609878395
en_trf_bertbaseuncased_lg0	What model is state of the art for natural question generation	 Semantic metric as reward function: One drawback of some evaluation metrics like BLEU is that they do not measure meaning  but only reward systems for n-grams that have exact matches in the reference system.To make our reward function more effective and robust  we additionally use word movers distance (WMD) as a semantic reward function fsem.WMD is the state-of-the-art approach to measure the dissimilarity between two sentences based on word embeddings .	0.7211358265814756
en_trf_bertbaseuncased_lg0	What model is state of the art for natural question generation	In this paper  we focus on QG from a given text passage  along with a target answer.We assume that a text passage is a collection of word tokens X p “ txp answer is also a collection of word tokens X a “ txa  N u  and a target Lu. The task of natural question  2  ...  xp  2  ...  xa  1  xp  1  xa  2  Under review as a conference paper at ICLR 2020  Figure 1: Overall architecture of the proposed model.Best viewed in color.	0.7224155130166948
en_trf_bertbaseuncased_lg0	What model is state of the art for natural question generation	Under review as a conference paper at ICLR 2020  REINFORCEMENT LEARNING BASED GRAPH-TO-SEQUENCE MODEL FOR NATURAL QUESTION GENERATION  Anonymous authors Paper under double-blind review  ABSTRACT  Natural question generation (QG) aims to generate questions from a passage and an answer.Previous works on QG either (i) ignore the rich structure informa- tion hidden in text  (ii) solely rely on cross-entropy loss that leads to issues like exposure bias and inconsistency between train/test measurement  or (iii) fail to fully exploit the answer information.To address these limitations  in this paper  we propose a reinforcement learning (RL) based graph-to-sequence (Graph2Seq) model for QG.	0.7280470033798464
en_trf_bertbaseuncased_lg0	What model is state of the art for natural question generation	More importantly  they fail to exploit other important metrics such as syntactic and semantic constraints for guiding high-quality text genera- tion. Early works on neural QG did not take into account the answer information when generating a question.Recent works have started to explore various means of utilizing the answer information.	0.7314775929909899
en_trf_bertbaseuncased_lg0	What model is state of the art for natural question generation	 Passage: for the successful execution of a project   effective planning is essentialGold: what is essential for the successful execution of a project ?G2Ssta w/o BiGGNN (Seq2Seq): what type of planning is essential for the project ?G2Ssta w/o DAN.: what type of planning is essential for the successful execution of a project ?	0.7347587822142162


en_trf_robertabase_lg
en_trf_robertabase_lg0	What model is state of the art for natural question generation	 • We explore both static and dynamic ways of constructing graph from text and are the first  to systematically investigate their performance impacts on a GNN encoder. • The proposed model is end-to-end trainable  achieves new state-of-the-art scores  and out- performs existing methods by a significant margin on the standard SQuAD benchmark for QG.Our human evaluation study also corroborates that the questions generated by our model are more natural (semantically and syntactically) compared to other baselines.	0.9676237827977617
en_trf_robertabase_lg0	What model is state of the art for natural question generation	G2Ssta+BERT+RL: how many schools and institutions does the church operate ?G2Sdyn+BERT+RL: how many schools does the church operate ? In Table 4  we further show a few examples that illustrate the quality of generated text given a pas- sage under different ablated systems.	0.9682543077037281
en_trf_robertabase_lg0	What model is state of the art for natural question generation	On the one hand  an advantage of static graph construc- tion is that useful domain knowledge can be hard-coded into the graph  which can greatly benefit the downstream task.However  it might suffer if there is a lack of prior knowledge for a specific domain knowledge.On the other hand  dynamic graph construction does not need any prior knowledge about the hidden structure of text  and only relies on the attention matrix to capture these structured information  which provides an easy way to achieve a decent performance.	0.968563306625759
en_trf_robertabase_lg0	What model is state of the art for natural question generation	Initially  BLEU-4 and METEOR were designed for evaluating machine translation systems and ROUGE-L was designed for evaluating text summarization systems.Recently  Q-BLEU1 was de- signed for better evaluating question generation systems  which was shown to correlate significantly better with human judgments compared to existing metrics. Besides automatic evaluation metrics  we also conduct a human evaluation study on split-2.	0.9688287437222889
en_trf_robertabase_lg1	What model is state of the art for natural question generation	Also  unlike the baseline methods  our model does not rely on any hand-crafted rules or ad-hoc strategies  and is fully end-to-end trainable. As shown in Table 2  we conducted a human evaluation study to assess the quality of the questions generated by our model  the baseline method MPQG+R  and the ground-truth data in terms of syn- tax  semantics and relevance metrics.We can see that our best performing model achieves good results even compared to the ground-truth  and outperforms the strong baseline method MPQG+R.	0.9689167376005864
en_trf_robertabase_lg1	What model is state of the art for natural question generation	As we can see  incorporating answer information helps the model identify the answer type of the question to be generated  and thus makes the generated ques- tions more relevant and specific.Also  we find our Graph2Seq model can generate more complete and valid questions compared to the Seq2Seq baseline.We think it is because a Graph2Seq model is able to exploit the rich text structure information better than a Seq2Seq model.	0.9694069670811141
en_trf_robertabase_lg0	What model is state of the art for natural question generation	However  these methods typically ignore the hidden structural information associated with a word sequence such as the syntactic parsing tree.Failing to utilize the rich text structure information beyond the simple word sequence may limit the effectiveness of these models for QG. It has been observed that in general  cross-entropy based sequence training has several limitations like exposure bias and inconsistency between train/test measurement (Ranzato et al  2015; Wu et al  2016).	0.9695987002913868
en_trf_robertabase_lg0	What model is state of the art for natural question generation	Although there are early attempts on constructing a graph from a sentence (Xu et al  2018b)  there is no clear an- swer as to the best way of representing text as a graph.We explore both static and dynamic graph construction approaches  and systematically investigate the performance differences between these two methods in the experimental section. Syntax-based static graph construction: We construct a directed and unweighted passage graph based on dependency parsing.	0.9696527212468944
en_trf_robertabase_lg0	What model is state of the art for natural question generation	 Semantic metric as reward function: One drawback of some evaluation metrics like BLEU is that they do not measure meaning  but only reward systems for n-grams that have exact matches in the reference system.To make our reward function more effective and robust  we additionally use word movers distance (WMD) as a semantic reward function fsem.WMD is the state-of-the-art approach to measure the dissimilarity between two sentences based on word embeddings .	0.9698549805277393
en_trf_robertabase_lg1	What model is state of the art for natural question generation	To achieve the third goal  we propose a novel Deep Alignment Network (DAN) for effectively incorporating answer information into the passage at multiple granularity levels. Our main contributions are as follows:  • We propose a novel RL-based Graph2Seq model for natural question generation.To the  best of our knowledge  we are the first to introduce the Graph2Seq architecture for QG.	0.970231547783021
en_trf_robertabase_lg0	What model is state of the art for natural question generation	G2Sdyn+BERT+RL: what is essential for the successful execution of a project ?Passage: the church operates three hundred sixty schools and institutions overseasGold: how many schools and institutions does the church operate overseas ?G2Ssta w/o BiGGNN (Seq2Seq): how many schools does the church have ?	0.9705961721404778
en_trf_robertabase_lg0	What model is state of the art for natural question generation	More importantly  they fail to exploit other important metrics such as syntactic and semantic constraints for guiding high-quality text genera- tion. Early works on neural QG did not take into account the answer information when generating a question.Recent works have started to explore various means of utilizing the answer information.	0.9719274182651386
en_trf_robertabase_lg1	What model is state of the art for natural question generation	Our two-stage training strategy benefits from both cross-entropy based and REINFORCE based sequence training.We also explore both static and dynamic graph construction from text  and systematically investigate and analyze the performance difference between the two.On the benchmark SQuAD dataset  our proposed model outperforms previous state-of-the-art meth- ods by a significant margin and achieve new best results.	0.9736927693191039
en_trf_robertabase_lg0	What model is state of the art for natural question generation	G2Ssta: what is essential for the successful execution of a project ?G2Ssta+BERT: what is essential for the successful execution of a project ?G2Ssta+BERT+RL: what is essential for the successful execution of a project ?	0.9740899201431938
en_trf_robertabase_lg0	What model is state of the art for natural question generation	 Passage: for the successful execution of a project   effective planning is essentialGold: what is essential for the successful execution of a project ?G2Ssta w/o BiGGNN (Seq2Seq): what type of planning is essential for the project ?G2Ssta w/o DAN.: what type of planning is essential for the successful execution of a project ?	0.9752351405903714


en_trf_distilbertbaseuncased_lg
en_trf_distilbertbaseuncased_lg1	What model is state of the art for natural question generation	Very recently  researchers have started exploring methods to automat- ically construct a graph of visual objects  or words (Liu et al  2018; Chen et al  2019b) when applying GNNs to non-graph structured data. To the best of our knowledge  we are the first to investigate systematically the performance difference between syntactic-aware static graph construction and semantics-aware dynamic graph construction in the context of question generation. 5 CONCLUSION  We proposed a novel RL based Graph2Seq model for QG  where the answer information is utilized by an effective Deep Alignment Network and a novel bidirectional GNN is proposed to process the directed passage graph.	0.8233231864918236
en_trf_distilbertbaseuncased_lg0	What model is state of the art for natural question generation	In this paper  we focus on QG from a given text passage  along with a target answer.We assume that a text passage is a collection of word tokens X p “ txp answer is also a collection of word tokens X a “ txa  N u  and a target Lu. The task of natural question  2  ...  xp  2  ...  xa  1  xp  1  xa  2  Under review as a conference paper at ICLR 2020  Figure 1: Overall architecture of the proposed model.Best viewed in color.	0.8236883040910852
en_trf_distilbertbaseuncased_lg0	What model is state of the art for natural question generation	More importantly  they fail to exploit other important metrics such as syntactic and semantic constraints for guiding high-quality text genera- tion. Early works on neural QG did not take into account the answer information when generating a question.Recent works have started to explore various means of utilizing the answer information.	0.8239250021068387
en_trf_distilbertbaseuncased_lg2	What model is state of the art for natural question generation	Further details on human evaluation can be found in Appendix E.  3.3 EXPERIMENTAL RESULTS AND HUMAN EVALUATION  Table 1 shows the automatic evaluation results comparing our proposed models against other state- of-the-art baseline methods.First of all  we can see that both of our full models G2Ssta+BERT+RL and G2Sdyn+BERT+RL achieve the new state-of-the-art scores on both data splits and consistently outperform previous methods by a significant margin.This highlights that our RL-based Graph2Seq model  together with the deep alignment network  successfully addresses the three issues we high- lighted in Sec.	0.8259040277995989
en_trf_distilbertbaseuncased_lg0	What model is state of the art for natural question generation	Lastly  it shows that fine-tuning the model using REINFORCE can improve the quality of the generated questions. 4 RELATED WORK  4.1 NATURAL QUESTION GENERATION  Early works (Mostow & Chen  2009; Heilman & Smith  2010; Heilman  2011) for QG focused on rule-based approaches that rely on heuristic rules or hand-crafted templates  with low generaliz- ability and scalability.Recent attempts have focused on NN-based approaches that do not require  9  Under review as a conference paper at ICLR 2020  manually-designed rules and are end-to-end trainable.	0.826437666489169
en_trf_distilbertbaseuncased_lg0	What model is state of the art for natural question generation	G2Sdyn+BERT+RL: what is essential for the successful execution of a project ?Passage: the church operates three hundred sixty schools and institutions overseasGold: how many schools and institutions does the church operate overseas ?G2Ssta w/o BiGGNN (Seq2Seq): how many schools does the church have ?	0.8273184739341907
en_trf_distilbertbaseuncased_lg0	What model is state of the art for natural question generation	 2.2 DEEP ALIGNMENT NETWORK  Answer information is crucial for generating relevant and high quality questions from a passage.Un- like previous methods that neglect potential semantic relations between passage and answer words  we explicitly model the global interactions among them in the embedding space.To this end  we propose a novel Deep Alignment Network (DAN) component for effectively incorporating answer information into the passage with multiple granularity levels.	0.8294904978314921
en_trf_distilbertbaseuncased_lg1	What model is state of the art for natural question generation	As we can see  incorporating answer information helps the model identify the answer type of the question to be generated  and thus makes the generated ques- tions more relevant and specific.Also  we find our Graph2Seq model can generate more complete and valid questions compared to the Seq2Seq baseline.We think it is because a Graph2Seq model is able to exploit the rich text structure information better than a Seq2Seq model.	0.8329236242877658
en_trf_distilbertbaseuncased_lg0	What model is state of the art for natural question generation	 2 AN RL-BASED GENERATOR-EVALUATOR ARCHITECTURE  In this section  we define the question generation task  and then present our RL-based Graph2Seq model for question generation.We first motivate the design  and then present the details of each component as shown in Fig 1. 2.1 PROBLEM FORMULATION  The goal of question generation is to generate natural language questions based on a given form of data  such as knowledge base triples or tables   sentences (Du et al  2017; Song et al  2018a)  or images   where the generated questions need to be answerable from the input data.	0.833245476577518
en_trf_distilbertbaseuncased_lg0	What model is state of the art for natural question generation	Besides  we find that the pretrained BERT embedding has a considerable impact on the performance and fine-tuning BERT embedding even further improves the performance  which demonstrates the power of large-scale pretrained language models. 3.5 CASE STUDY  Table 4: Generated questions on SQuAD split-2 test set.Target answers are underlined.	0.8336105041626708
en_trf_distilbertbaseuncased_lg1	What model is state of the art for natural question generation	Also  unlike the baseline methods  our model does not rely on any hand-crafted rules or ad-hoc strategies  and is fully end-to-end trainable. As shown in Table 2  we conducted a human evaluation study to assess the quality of the questions generated by our model  the baseline method MPQG+R  and the ground-truth data in terms of syn- tax  semantics and relevance metrics.We can see that our best performing model achieves good results even compared to the ground-truth  and outperforms the strong baseline method MPQG+R.	0.8353172843118363
en_trf_distilbertbaseuncased_lg1	What model is state of the art for natural question generation	Our two-stage training strategy benefits from both cross-entropy based and REINFORCE based sequence training.We also explore both static and dynamic graph construction from text  and systematically investigate and analyze the performance difference between the two.On the benchmark SQuAD dataset  our proposed model outperforms previous state-of-the-art meth- ods by a significant margin and achieve new best results.	0.8426286918752706
en_trf_distilbertbaseuncased_lg1	What model is state of the art for natural question generation	To achieve the third goal  we propose a novel Deep Alignment Network (DAN) for effectively incorporating answer information into the passage at multiple granularity levels. Our main contributions are as follows:  • We propose a novel RL-based Graph2Seq model for natural question generation.To the  best of our knowledge  we are the first to introduce the Graph2Seq architecture for QG.	0.8440119028888996
en_trf_distilbertbaseuncased_lg0	What model is state of the art for natural question generation	G2Ssta: what is essential for the successful execution of a project ?G2Ssta+BERT: what is essential for the successful execution of a project ?G2Ssta+BERT+RL: what is essential for the successful execution of a project ?	0.8458323169088358
en_trf_distilbertbaseuncased_lg0	What model is state of the art for natural question generation	 Passage: for the successful execution of a project   effective planning is essentialGold: what is essential for the successful execution of a project ?G2Ssta w/o BiGGNN (Seq2Seq): what type of planning is essential for the project ?G2Ssta w/o DAN.: what type of planning is essential for the successful execution of a project ?	0.8523708030273175


Google_USE
Google_USE0	What model is state of the art for natural question generation	More importantly  they fail to exploit other important metrics such as syntactic and semantic constraints for guiding high-quality text genera- tion. Early works on neural QG did not take into account the answer information when generating a question.Recent works have started to explore various means of utilizing the answer information.	[[0.4352673]]
Google_USE0	What model is state of the art for natural question generation	Our Graph2Seq model is based on a novel bidirectional gated graph neural network  which extends the original gated graph neural network  by considering both incoming and outgoing edges  and fusing them during the graph embedding learning.To achieve the second goal  we design a hybrid evaluator which is trained by optimizing a mixed objective function that combines both cross-entropy and RL loss.We use not only discrete evaluation metrics like BLEU  but also semantic metrics like word mover’s distance  to encourage both syntactically and semantically valid text generation.	[[0.43618798]]
Google_USE2	What model is state of the art for natural question generation	Further details on human evaluation can be found in Appendix E.  3.3 EXPERIMENTAL RESULTS AND HUMAN EVALUATION  Table 1 shows the automatic evaluation results comparing our proposed models against other state- of-the-art baseline methods.First of all  we can see that both of our full models G2Ssta+BERT+RL and G2Sdyn+BERT+RL achieve the new state-of-the-art scores on both data splits and consistently outperform previous methods by a significant margin.This highlights that our RL-based Graph2Seq model  together with the deep alignment network  successfully addresses the three issues we high- lighted in Sec.	[[0.4402832]]
Google_USE0	What model is state of the art for natural question generation	 2.2 DEEP ALIGNMENT NETWORK  Answer information is crucial for generating relevant and high quality questions from a passage.Un- like previous methods that neglect potential semantic relations between passage and answer words  we explicitly model the global interactions among them in the embedding space.To this end  we propose a novel Deep Alignment Network (DAN) component for effectively incorporating answer information into the passage with multiple granularity levels.	[[0.44211257]]
Google_USE1	What model is state of the art for natural question generation	Also  unlike the baseline methods  our model does not rely on any hand-crafted rules or ad-hoc strategies  and is fully end-to-end trainable. As shown in Table 2  we conducted a human evaluation study to assess the quality of the questions generated by our model  the baseline method MPQG+R  and the ground-truth data in terms of syn- tax  semantics and relevance metrics.We can see that our best performing model achieves good results even compared to the ground-truth  and outperforms the strong baseline method MPQG+R.	[[0.4470324]]
Google_USE0	What model is state of the art for natural question generation	 L “ γLrl ` p1 ´ γqLlm  (11)  3 EXPERIMENTS  We evaluate our proposed model against state-of-the-art methods on the SQuAD dataset .Our full models have two variants G2Ssta+BERT+RL and G2Sdyn+BERT+RL which adopts static graph construction or dynamic graph construction  respectively.For model settings and sensitivity analysis  please refer to Appendix B and C. 1  3.1 BASELINE METHODS  We compare against the following baselines in our experiments: i) SeqCopyNet   ii) NQG++   iii) MPQG+R   iv) AFPQA   v) s2sa-at-mp-gsa   vi) ASs2s   and vii) CGC-QG .	[[0.45028245]]
Google_USE0	What model is state of the art for natural question generation	 However  the existing approaches for QG suffer from several limitations; they (i) ignore the rich structure information hidden in text  (ii) solely rely on cross-entropy loss that leads to issues like exposure bias and inconsistency between train/test measurement  and (iii) fail to fully exploit the answer information.To address these limitations  we propose a reinforcement learning (RL) based graph-to-sequence (Graph2Seq) model for QG as well as deep alignment networks to effectively cope with the QG task.To the best of our knowledge  we are the first to introduce the Graph2Seq architecture to solve the question generation task.	[[0.45157713]]
Google_USE0	What model is state of the art for natural question generation	To achieve the third goal  we propose a novel Deep Alignment Network (DAN) for effectively incorporating answer information into the passage at multiple granularity levels. Our main contributions are as follows:  • We propose a novel RL-based Graph2Seq model for natural question generation.To the  best of our knowledge  we are the first to introduce the Graph2Seq architecture for QG.	[[0.45523703]]
Google_USE0	What model is state of the art for natural question generation	 generation is to generate the best natural language question consisting of a sequence of word tokens ˆY “ ty1  y2  ...  yT u which maximizes the conditional likelihood ˆY “ arg maxY P pY |X p  X aq.Here N   L  and T are the lenghts of the passage  answer and question  respectively.We focus on the problem setting where we have a set of passage (and answers) and target questions pairs  to learn the mapping; existing QG approaches (Du et al  2017; Song et al  2018a; Zhao et al  2018; Kim et al  2018) make a similar assumption.	[[0.45590365]]
Google_USE0	What model is state of the art for natural question generation	Under review as a conference paper at ICLR 2020  REINFORCEMENT LEARNING BASED GRAPH-TO-SEQUENCE MODEL FOR NATURAL QUESTION GENERATION  Anonymous authors Paper under double-blind review  ABSTRACT  Natural question generation (QG) aims to generate questions from a passage and an answer.Previous works on QG either (i) ignore the rich structure informa- tion hidden in text  (ii) solely rely on cross-entropy loss that leads to issues like exposure bias and inconsistency between train/test measurement  or (iii) fail to fully exploit the answer information.To address these limitations  in this paper  we propose a reinforcement learning (RL) based graph-to-sequence (Graph2Seq) model for QG.	[[0.4914538]]
Google_USE0	What model is state of the art for natural question generation	 1  INTRODUCTION  Natural question generation (QG) has many useful applications such as improving the question an- swering task (Chen et al  2017; 2019a) by providing more training data (Tang et al  2017; Yuan et al  2017)  generating practice exercises and assessments for educational purposes (Heilman & Smith  2010; Danon & Last  2017)  and helping dialog systems to kick-start and continue a conver- sation with human users .While many existing works focus on QG from images (Fan et al  2018; Li et al  2018) or knowledge bases (Serban et al  2016; Elsahar et al  2018)  in this work  we focus on QG from text. Conventional methods (Mostow & Chen  2009; Heilman & Smith  2010; Heilman  2011) for QG rely on heuristic rules or hand-crafted templates  leading to the issues of low generalizability and scal- ability.	[[0.49702153]]
Google_USE0	What model is state of the art for natural question generation	Lastly  it shows that fine-tuning the model using REINFORCE can improve the quality of the generated questions. 4 RELATED WORK  4.1 NATURAL QUESTION GENERATION  Early works (Mostow & Chen  2009; Heilman & Smith  2010; Heilman  2011) for QG focused on rule-based approaches that rely on heuristic rules or hand-crafted templates  with low generaliz- ability and scalability.Recent attempts have focused on NN-based approaches that do not require  9  Under review as a conference paper at ICLR 2020  manually-designed rules and are end-to-end trainable.	[[0.500414]]
Google_USE1	What model is state of the art for natural question generation	As we can see  incorporating answer information helps the model identify the answer type of the question to be generated  and thus makes the generated ques- tions more relevant and specific.Also  we find our Graph2Seq model can generate more complete and valid questions compared to the Seq2Seq baseline.We think it is because a Graph2Seq model is able to exploit the rich text structure information better than a Seq2Seq model.	[[0.5008245]]
Google_USE0	What model is state of the art for natural question generation	When question generation is guided by the semantics of an answer  the resulting questions become more relevant and readable.Conceptually  there are three different ways to incorporate the answer information by simply marking the answer location in the passage (Zhou et al  2017; Zhao et al  2018; Liu et al  2019)  or using complex passage-answer matching strategies   or separating answers from passages when applying a Seq2Seq model (Kim et al  2018; Sun et al  2018).However  they neglect potential semantic relations between passage words and answer words  and thus fail to explicitly model the global interactions among them in the embedding space.	[[0.55738413]]
Google_USE0	What model is state of the art for natural question generation	 2 AN RL-BASED GENERATOR-EVALUATOR ARCHITECTURE  In this section  we define the question generation task  and then present our RL-based Graph2Seq model for question generation.We first motivate the design  and then present the details of each component as shown in Fig 1. 2.1 PROBLEM FORMULATION  The goal of question generation is to generate natural language questions based on a given form of data  such as knowledge base triples or tables   sentences (Du et al  2017; Song et al  2018a)  or images   where the generated questions need to be answerable from the input data.	[[0.5683898]]
