en_trf_bertbaseuncased_lg
Which is the best model
Furthermore  we report the classification results of our proposed GIL by using mean and max-pooling mechanisms  respectively.GIL with mean pooling (i.e.  “GIL /w 2 conv layers + mean pooling") can get a better result than the GIL method with max-pooling (i.e.  “GIL /w 2 conv layers + max-pooling")  e.g.  86.2% vs 85.2% on the Cora graph dataset.The reason may be that the graph network with two convolutional layers and the mean pooling mechanism can obtain the optimal graph embeddings  but when increasing the network layers  more parameters of a certain graph model need to be optimized  which may lead to the over-fitting issue.
0.6203900028208551
Which is the best model
In the testing stage  we may take all training nodes and perform the model update according to Eqn (6) like the training process.The updated model is used as the final model and is then fed into Eqn (2) to infer the class labels for those query nodes. 4 MODULES In this section  we instantiate all modules (i.e.  functions) of the aforementioned structure relation.
0.6204926895437002
Which is the best model
The dropout rate is set to 0.5 in the convolution and fully connected layers to avoid over-fitting  and the ReLU unit is leveraged as a nonlinear activation function.We pre-train our proposed GIL model for 200 iterations with the training set  where its initial learning rate  decay factor  and momentum are set to 0.05  0.95  and 0.9  respectively.Here we train the GIL model using the stochastic gradient descent method with the batch size of 100.
0.6256185225063298
Which is the best model
When using structure relations  “GIL /w learning on Vtr" obtains an improvement of 1.9% (over “GCN /w learning on Vtr”)  which can be attributed to the building connection between nodes.The meta-optimization strategy (“GIL /w meta-training from Vtr → Vval" vs “GIL /w learning on Vtr”) has a gain of 2.9%  which indicates that a good inference capability can be learnt through meta-optimization.It is worth noting that  GIL adopts a meta-optimization strategy to learn the inference model  which is a process of migrating  7  Published as a conference paper at ICLR 2020  from a training set to a validation set.
0.6312060808024065
Which is the best model
In IJCAI  pp 2609–2615  2018. Sachin Ravi and Hugo Larochelle.Optimization as a model for few-shot learning.
0.6407311347778408
Which is the best model
In other words  the validation set is only used to teach the model itself how to transfer to unseen data.In contrast  the conventional methods often employ a validation set to tune parameters of a certain model of interest. Table 3: Performance comparisons with several GIL variants and the classical GCN method on the Cora dataset.
0.6436403041153803
Which is the best model
In the extreme case  labels of unlabeled nodes could be determined by those neighbors with the 1 ∼ 2 step reachability.In summary  our proposed GIL method prefers small ratio labeled nodes on the semi-supervised node classification task. Inference learning process: Classification errors of different epochs on the validation set of the Cora dataset can be illustrated in Fig 3.
0.6456181124170579
Which is the best model
 • Path reachability fP (vi  vj  E) −→ Rdp   represents the characteristics of path reachability from vi to vj.As there usually exist multiple traversal paths between two nodes  we choose the function as reachable probabilities of different lengths of walks from vi to vj.More details will be introduced in Section 4.
0.6497621031393012
Which is the best model
Marginalized kernels between labeled graphs. In ICML  pp 321–328  2003. Thomas N. Kipf and Max Welling.
0.6562072824702831
Which is the best model
Note that the last one fr defines on the former two ones  so we consider the cases in Table 4 by adding modules.When not using all modules  only original attributes of nodes are used to predict labels.The case of only using fe belongs to the GCN method  which can achieve 81.5% on the Cora dataset.
0.6608844701429274






en_trf_robertabase_lg
Which is the best model
In the testing stage  we may take all training nodes and perform the model update according to Eqn (6) like the training process.The updated model is used as the final model and is then fed into Eqn (2) to infer the class labels for those query nodes. 4 MODULES In this section  we instantiate all modules (i.e.  functions) of the aforementioned structure relation.
0.9614559044744097
Which is the best model
In ICML  pp 912–919  2003. Chenyi Zhuang and Qiang Ma. Dual graph convolutional networks for graph-based semi-supervised  classification.In WWW  pp 499–508  2018.
0.9614585965982105
Which is the best model
N-gcn: Multi-scale graph  convolution for semi-supervised node classification. arXiv preprint arXiv:1802.08888  2018. James Atwood and Don Towsley.Diffusion-convolutional neural networks.
0.9617157319046222
Which is the best model
In other words  the validation set is only used to teach the model itself how to transfer to unseen data.In contrast  the conventional methods often employ a validation set to tune parameters of a certain model of interest. Table 3: Performance comparisons with several GIL variants and the classical GCN method on the Cora dataset.
0.9617517456508072
Which is the best model
Model-agnostic meta-learning for fast adaptation of  deep networks.In ICML  pp 1126–1135  2017. Will Hamilton  Zhitao Ying  and Jure Leskovec.
0.9620299779231642
Which is the best model
In CVPR  pp 5115–5124  2017. Christopher Morris  Kristian Kersting  and Petra Mutzel.Glocalized weisfeiler-lehman graph kernels:  Global-local feature maps of graphs.
0.9625336887797311
Which is the best model
In the standard protocol  3  Published as a conference paper at ICLR 2020  of prior literatures (Yang et al  2016)  the three node sets share the same label space.We follow but do not restrict this protocol for our proposed method.Given the training and validation node sets  the aim is to predict the node labels of testing nodes by using node attributes as well as edge connections.
0.9627932951627081
Which is the best model
A sophisticated machine learning technique used in most existing methods (Kipf & Welling  2017; Zhou et al  2004) is to choose the optimal classifier (trained on a training set) after checking the performance on the validation set.However  these methods essentially ignore how to extract transferable knowledge from these known labeled nodes to unlabeled nodes  as the graph structure itself implies node connectivity/reachability.Moreover  due to the scarcity of labeled samples  the performance of such a classifier is usually not satisfying.
0.9642085722761409
Which is the best model
In IJCAI  pp 2609–2615  2018. Sachin Ravi and Hugo Larochelle.Optimization as a model for few-shot learning.
0.9658407911474166
Which is the best model
Note that the last one fr defines on the former two ones  so we consider the cases in Table 4 by adding modules.When not using all modules  only original attributes of nodes are used to predict labels.The case of only using fe belongs to the GCN method  which can achieve 81.5% on the Cora dataset.
0.9692078493296902




en_trf_distilbertbaseuncased_lg
Which is the best model
Furthermore  we report the classification results of our proposed GIL by using mean and max-pooling mechanisms  respectively.GIL with mean pooling (i.e.  “GIL /w 2 conv layers + mean pooling") can get a better result than the GIL method with max-pooling (i.e.  “GIL /w 2 conv layers + max-pooling")  e.g.  86.2% vs 85.2% on the Cora graph dataset.The reason may be that the graph network with two convolutional layers and the mean pooling mechanism can obtain the optimal graph embeddings  but when increasing the network layers  more parameters of a certain graph model need to be optimized  which may lead to the over-fitting issue.
0.7480377612141431
Which is the best model
When using structure relations  “GIL /w learning on Vtr" obtains an improvement of 1.9% (over “GCN /w learning on Vtr”)  which can be attributed to the building connection between nodes.The meta-optimization strategy (“GIL /w meta-training from Vtr → Vval" vs “GIL /w learning on Vtr”) has a gain of 2.9%  which indicates that a good inference capability can be learnt through meta-optimization.It is worth noting that  GIL adopts a meta-optimization strategy to learn the inference model  which is a process of migrating  7  Published as a conference paper at ICLR 2020  from a training set to a validation set.
0.7490114852212207
Which is the best model
The dropout rate is set to 0.5 in the convolution and fully connected layers to avoid over-fitting  and the ReLU unit is leveraged as a nonlinear activation function.We pre-train our proposed GIL model for 200 iterations with the training set  where its initial learning rate  decay factor  and momentum are set to 0.05  0.95  and 0.9  respectively.Here we train the GIL model using the stochastic gradient descent method with the batch size of 100.
0.749372092411089
Which is the best model
Marginalized kernels between labeled graphs. In ICML  pp 321–328  2003. Thomas N. Kipf and Max Welling.
0.7515062052637874
Which is the best model
In the standard protocol  3  Published as a conference paper at ICLR 2020  of prior literatures (Yang et al  2016)  the three node sets share the same label space.We follow but do not restrict this protocol for our proposed method.Given the training and validation node sets  the aim is to predict the node labels of testing nodes by using node attributes as well as edge connections.
0.7518548587494518
Which is the best model
In the testing stage  we may take all training nodes and perform the model update according to Eqn (6) like the training process.The updated model is used as the final model and is then fed into Eqn (2) to infer the class labels for those query nodes. 4 MODULES In this section  we instantiate all modules (i.e.  functions) of the aforementioned structure relation.
0.7573500433857987
Which is the best model
In the extreme case  labels of unlabeled nodes could be determined by those neighbors with the 1 ∼ 2 step reachability.In summary  our proposed GIL method prefers small ratio labeled nodes on the semi-supervised node classification task. Inference learning process: Classification errors of different epochs on the validation set of the Cora dataset can be illustrated in Fig 3.
0.7641087296994077
Which is the best model
In other words  the validation set is only used to teach the model itself how to transfer to unseen data.In contrast  the conventional methods often employ a validation set to tune parameters of a certain model of interest. Table 3: Performance comparisons with several GIL variants and the classical GCN method on the Cora dataset.
0.7663656172437061
Which is the best model
In IJCAI  pp 2609–2615  2018. Sachin Ravi and Hugo Larochelle.Optimization as a model for few-shot learning.
0.7689564858620309
Which is the best model
Note that the last one fr defines on the former two ones  so we consider the cases in Table 4 by adding modules.When not using all modules  only original attributes of nodes are used to predict labels.The case of only using fe belongs to the GCN method  which can achieve 81.5% on the Cora dataset.
0.7819935650942139




Google_USE
Which is the best model
Robust and scalable graph-based semisupervised learning. Proceedings of the IEEE  100(9):2624–2638  2012. Zhiling Luo  Ling Liu  Jianwei Yin  Ying Li  and Zhaohui Wu. Deep learning of graphs with ngram convolutional neural networks.
[[0.28264147]]
Which is the best model
 Table 4: Performance comparisons with different mod- ules on the Cora dataset  where fe  fP   and fr denote node representation  path reachability  and structure re- lation  respectively. fP fr fe - - - (cid:88) - - (cid:88) (cid:88) - (cid:88) (cid:88) (cid:88)  Acc.(%) 56.0 81.5 85.0 86.2  Figure 3: Classification errors of different itera- tions on the validation set of the Cora dataset. Module analysis: We evaluate the effectiveness of different modules within our proposed GIL framework  including node representation fe  path reachability fP   and structure relation fr.
[[0.2831314]]
Which is the best model
 Ulrik Brandes  Daniel Delling  Marco Gaertler  Robert Gorke  Martin Hoefer  Zoran Nikoloski  and Dorothea Wagner.On modularity clustering.IEEE transactions on knowledge and data engineering  20(2):172–188  2008.
[[0.285621]]
Which is the best model
 Yujia Li  Daniel Tarlow  Marc Brockschmidt  and Richard Zemel.Gated graph sequence neural  Wei Liu  Junfeng He  and Shih-Fu Chang.Large graph construction for scalable semi-supervised  Wei Liu  Jun Wang  and Shih-Fu Chang.
[[0.28723192]]
Which is the best model
 Given a training set Vtr  we expect that the best performance can be obtained on the validation set Vval after optimizing the model on Vtr.Given a trained/pretrained model Θ = {fe  φw  φr}  we perform iteratively gradient updates on the training set Vtr to obtain the new model  formally   Θ(cid:48) = Θ − α∇ΘLtr(Θ)   where α is the updating rate.Note that  in the computation of class scores  since the reference node and query node can be both from the training set Vtr  we set the computation weight wi→j = 0 if i = j in Eqn (3).
[[0.28814143]]
Which is the best model
Deep convolutional networks on graph-structured data. arXiv preprint arXiv:1506.05163  2015. Jiatao Jiang  Zhen Cui  Chunyan Xu  and Jian Yang.
[[0.28896636]]
Which is the best model
ICML  2016. Bing Yu  Haoteng Yin  and Zhanxing Zhu.Spatio-temporal graph convolutional networks: A deep  learning framework for traffic forecasting.
[[0.28984088]]
Which is the best model
Technical Report 1999-66  1999. Shirui Pan  Ruiqi Hu  Guodong Long  Jing Jiang  Lina Yao  and Chengqi Zhang.Adversarially  regularized graph autoencoder for graph embedding.
[[0.30004212]]
Which is the best model
AI magazine  29(3):93–93  2008. Nino Shervashidze  SVN Vishwanathan  Tobias Petri  Kurt Mehlhorn  and Karsten Borgwardt.Efficient graphlet kernels for large graph comparison.
[[0.30746925]]
Which is the best model
 Jian Du  Shanghang Zhang  Guanhang Wu  José MF Moura  and Soummya Kar.Topology adaptive  graph convolutional networks. arXiv preprint arXiv:1710.10370  2017. Chelsea Finn  Pieter Abbeel  and Sergey Levine.
[[0.3392583]]
