en_trf_bertbaseuncased_lg
Which model proved to be extremely effective?
To bridge the connection between two nodes  we formally define a structure relation by encapsulating node attributes  between-node paths  and local topological structures together  which can make the inference conveniently deduced from one node to another node.For learning the inference process  we further introduce meta-optimization on structure relations from training nodes to validation nodes  such that the learnt graph inference capability can be better self-adapted to testing nodes.Comprehensive evaluations on four benchmark datasets (including Cora  Citeseer  Pubmed  and NELL) demonstrate the superiority of our proposed GIL when compared against state-of-the-art methods on the semi-supervised node classification task.
0.6954008261970294
Which model proved to be extremely effective?
Although graph CNN based methods have achieved considerable capabilities of graph embedding by optimizing filters  they are limited into a conventionally semi-supervised framework and lack of an efficient inference mechanism on graphs.Especially  in the case of few-shot learning  where a small number of training nodes are labeled  this kind of methods would drastically compromise the performance.For example  the Pubmed graph dataset (Sen et al  2008) consists  ∗Corresponding author: Zhen Cui.
0.6966753764375518
Which model proved to be extremely effective?
We can build a structure relation for obtaining the connection between any two graph nodes  where node attributes  between-node paths  and graph structure information can be encapsulated together.For better capturing the transferable knowledge  our method further learns to transfer the mined knowledge from the training samples to the validation set  finally boosting the prediction accuracy of the labels of unlabeled nodes in the testing set.The extensive experimental results demonstrate the effectiveness of our proposed GIL for solving the semi-supervised learning problem  even in the few-shot paradigm.
0.698062957982071
Which model proved to be extremely effective?
Classification errors are rapidly decreasing as the number of iterations increases from the beginning to 400 iterations  while they are with a slow descent from 400 iterations to 1200 iterations.It demonstrates that the learned knowledge from the training samples can be transferred for inferring node category information from these reference labeled nodes.The performance of semi-supervised classification can be further increased by improving the generalized capability of the Graph CNN model.
0.7046877802413091
Which model proved to be extremely effective?
To address these issues  we introduce a meta-learning mechanism (Finn et al  2017; Ravi & Larochelle  2017; Sung et al  2017) to learn to infer node labels on graphs.Specifically  the graph structure  between-node path reachability  and node attributes are jointly modeled into the learning process.Our aim is to learn to infer from labeled nodes to unlabeled nodes  so that the learner can perform better on a validation set and thus classify a testing set more accurately.
0.7063290266005791
Which model proved to be extremely effective?
The structure relations are well defined by jointly considering node attributes  between-node paths  and graph topological structures. • To make the inference model better generalize to test nodes  we introduce a meta-learning procedure to optimize structure relations  which could be the first time for graph node classification to the best of our knowledge. • Comprehensive evaluations on three citation network datasets (including Cora  Citeseer  and Pubmed) and one knowledge graph data (i.e.  NELL) demonstrate the superiority of our proposed GIL in contrast with other state-of-the-art methods on the semi-supervised classification task.
0.7084353661984684
Which model proved to be extremely effective?
These aforementioned works usually boil down to a general classification task  where the model is learnt on a training set and selected by checking a validation set.However  they do not put great efforts on how to learn to infer from one node to another node on a topological graph  especially in the few-shot regime. In this paper  we propose a graph inference learning (GIL) framework to teach the model itself to adaptively infer from reference labeled nodes to those query unlabeled nodes  and finally boost the performance of semi-supervised node classification in the case of a few number of labeled samples.
0.7084462459019396
Which model proved to be extremely effective?
In other words  the validation set is only used to teach the model itself how to transfer to unseen data.In contrast  the conventional methods often employ a validation set to tune parameters of a certain model of interest. Table 3: Performance comparisons with several GIL variants and the classical GCN method on the Cora dataset.
0.7184784588672852
Which model proved to be extremely effective?
Inspired by the recent meta-learning strategy (Finn et al  2017)  we learn to infer the structure relations from a training set to a validation set  which can benefit the generalization capability of the learned model.In other words  our proposed GIL attempts to learn some transferable knowledge underlying in the structure relations from training samples to validation samples  such that the learned structure relations can be better self-adapted to the new testing stage. We summarize the main contributions of this work as three folds:  • We propose a novel graph inference learning framework by building structure relations to infer unknown node labels from those labeled nodes in an end-to-end way.
0.7200541291183746
Which model proved to be extremely effective?
A sophisticated machine learning technique used in most existing methods (Kipf & Welling  2017; Zhou et al  2004) is to choose the optimal classifier (trained on a training set) after checking the performance on the validation set.However  these methods essentially ignore how to extract transferable knowledge from these known labeled nodes to unlabeled nodes  as the graph structure itself implies node connectivity/reachability.Moreover  due to the scarcity of labeled samples  the performance of such a classifier is usually not satisfying.
0.7206236829013498



en_trf_robertabase_lg
Which model proved to be extremely effective?
GIL can outperform the classic GCN by analyzing the accuracies within different between-node steps  which indicates that our proposed GIL has a better reference capability than GCN by using the meta-optimization mechanism from training nodes to validation nodes. (a)  (b)  Figure 2: (a) Performance comparisons within different between-node steps on the Cora dataset.The accuracy equals to the number of correctly classified nodes divided by all testing samples  and is accumulated from step 1 to step k. (b) Performance comparisons with different label rates on the Pubmed dataset.
0.9687887305294938
Which model proved to be extremely effective?
In the standard protocol  3  Published as a conference paper at ICLR 2020  of prior literatures (Yang et al  2016)  the three node sets share the same label space.We follow but do not restrict this protocol for our proposed method.Given the training and validation node sets  the aim is to predict the node labels of testing nodes by using node attributes as well as edge connections.
0.9690455544362423
Which model proved to be extremely effective?
When using structure relations  “GIL /w learning on Vtr" obtains an improvement of 1.9% (over “GCN /w learning on Vtr”)  which can be attributed to the building connection between nodes.The meta-optimization strategy (“GIL /w meta-training from Vtr → Vval" vs “GIL /w learning on Vtr”) has a gain of 2.9%  which indicates that a good inference capability can be learnt through meta-optimization.It is worth noting that  GIL adopts a meta-optimization strategy to learn the inference model  which is a process of migrating  7  Published as a conference paper at ICLR 2020  from a training set to a validation set.
0.9693747527878086
Which model proved to be extremely effective?
To address these issues  we introduce a meta-learning mechanism (Finn et al  2017; Ravi & Larochelle  2017; Sung et al  2017) to learn to infer node labels on graphs.Specifically  the graph structure  between-node path reachability  and node attributes are jointly modeled into the learning process.Our aim is to learn to infer from labeled nodes to unlabeled nodes  so that the learner can perform better on a validation set and thus classify a testing set more accurately.
0.9696689646491464
Which model proved to be extremely effective?
In IJCAI  pp 2609–2615  2018. Sachin Ravi and Hugo Larochelle.Optimization as a model for few-shot learning.
0.9699607618472802
Which model proved to be extremely effective?
In other words  the validation set is only used to teach the model itself how to transfer to unseen data.In contrast  the conventional methods often employ a validation set to tune parameters of a certain model of interest. Table 3: Performance comparisons with several GIL variants and the classical GCN method on the Cora dataset.
0.9702038977578505
Which model proved to be extremely effective?
The structure relations are well defined by jointly considering node attributes  between-node paths  and graph topological structures. • To make the inference model better generalize to test nodes  we introduce a meta-learning procedure to optimize structure relations  which could be the first time for graph node classification to the best of our knowledge. • Comprehensive evaluations on three citation network datasets (including Cora  Citeseer  and Pubmed) and one knowledge graph data (i.e.  NELL) demonstrate the superiority of our proposed GIL in contrast with other state-of-the-art methods on the semi-supervised classification task.
0.9705119318216722
Which model proved to be extremely effective?
Although graph CNN based methods have achieved considerable capabilities of graph embedding by optimizing filters  they are limited into a conventionally semi-supervised framework and lack of an efficient inference mechanism on graphs.Especially  in the case of few-shot learning  where a small number of training nodes are labeled  this kind of methods would drastically compromise the performance.For example  the Pubmed graph dataset (Sen et al  2008) consists  ∗Corresponding author: Zhen Cui.
0.9714374444391071
Which model proved to be extremely effective?
Note that the last one fr defines on the former two ones  so we consider the cases in Table 4 by adding modules.When not using all modules  only original attributes of nodes are used to predict labels.The case of only using fe belongs to the GCN method  which can achieve 81.5% on the Cora dataset.
0.9725618209739307
Which model proved to be extremely effective?
A sophisticated machine learning technique used in most existing methods (Kipf & Welling  2017; Zhou et al  2004) is to choose the optimal classifier (trained on a training set) after checking the performance on the validation set.However  these methods essentially ignore how to extract transferable knowledge from these known labeled nodes to unlabeled nodes  as the graph structure itself implies node connectivity/reachability.Moreover  due to the scarcity of labeled samples  the performance of such a classifier is usually not satisfying.
0.9742019495039367



en_trf_distilbertbaseuncased_lg
Which model proved to be extremely effective?
These aforementioned works usually boil down to a general classification task  where the model is learnt on a training set and selected by checking a validation set.However  they do not put great efforts on how to learn to infer from one node to another node on a topological graph  especially in the few-shot regime. In this paper  we propose a graph inference learning (GIL) framework to teach the model itself to adaptively infer from reference labeled nodes to those query unlabeled nodes  and finally boost the performance of semi-supervised node classification in the case of a few number of labeled samples.
0.7828136170397981
Which model proved to be extremely effective?
To bridge the connection between two nodes  we formally define a structure relation by encapsulating node attributes  between-node paths  and local topological structures together  which can make the inference conveniently deduced from one node to another node.For learning the inference process  we further introduce meta-optimization on structure relations from training nodes to validation nodes  such that the learnt graph inference capability can be better self-adapted to testing nodes.Comprehensive evaluations on four benchmark datasets (including Cora  Citeseer  Pubmed  and NELL) demonstrate the superiority of our proposed GIL when compared against state-of-the-art methods on the semi-supervised node classification task.
0.7837987362392544
Which model proved to be extremely effective?
To address these issues  we introduce a meta-learning mechanism (Finn et al  2017; Ravi & Larochelle  2017; Sung et al  2017) to learn to infer node labels on graphs.Specifically  the graph structure  between-node path reachability  and node attributes are jointly modeled into the learning process.Our aim is to learn to infer from labeled nodes to unlabeled nodes  so that the learner can perform better on a validation set and thus classify a testing set more accurately.
0.7857792606069385
Which model proved to be extremely effective?
The large gain of using the relation module fr (i.e.  from 81.5% to 85.0%) may be contributed to the ability of inference learning on attributes as well as local topology structures which are implicitly encoded in fe.The path information fP can further boost the performance by 1.2%  e.g.  86.2% vs 85.0%.It demonstrates that three different modules of our method can improve the graph inference learning capability.
0.7859484766855952
Which model proved to be extremely effective?
Classification errors are rapidly decreasing as the number of iterations increases from the beginning to 400 iterations  while they are with a slow descent from 400 iterations to 1200 iterations.It demonstrates that the learned knowledge from the training samples can be transferred for inferring node category information from these reference labeled nodes.The performance of semi-supervised classification can be further increased by improving the generalized capability of the Graph CNN model.
0.7900442130644847
Which model proved to be extremely effective?
Although graph CNN based methods have achieved considerable capabilities of graph embedding by optimizing filters  they are limited into a conventionally semi-supervised framework and lack of an efficient inference mechanism on graphs.Especially  in the case of few-shot learning  where a small number of training nodes are labeled  this kind of methods would drastically compromise the performance.For example  the Pubmed graph dataset (Sen et al  2008) consists  ∗Corresponding author: Zhen Cui.
0.7911197434490158
Which model proved to be extremely effective?
The structure relations are well defined by jointly considering node attributes  between-node paths  and graph topological structures. • To make the inference model better generalize to test nodes  we introduce a meta-learning procedure to optimize structure relations  which could be the first time for graph node classification to the best of our knowledge. • Comprehensive evaluations on three citation network datasets (including Cora  Citeseer  and Pubmed) and one knowledge graph data (i.e.  NELL) demonstrate the superiority of our proposed GIL in contrast with other state-of-the-art methods on the semi-supervised classification task.
0.7923694433457633
Which model proved to be extremely effective?
A sophisticated machine learning technique used in most existing methods (Kipf & Welling  2017; Zhou et al  2004) is to choose the optimal classifier (trained on a training set) after checking the performance on the validation set.However  these methods essentially ignore how to extract transferable knowledge from these known labeled nodes to unlabeled nodes  as the graph structure itself implies node connectivity/reachability.Moreover  due to the scarcity of labeled samples  the performance of such a classifier is usually not satisfying.
0.7950016789019433
Which model proved to be extremely effective?
Inspired by the recent meta-learning strategy (Finn et al  2017)  we learn to infer the structure relations from a training set to a validation set  which can benefit the generalization capability of the learned model.In other words  our proposed GIL attempts to learn some transferable knowledge underlying in the structure relations from training samples to validation samples  such that the learned structure relations can be better self-adapted to the new testing stage. We summarize the main contributions of this work as three folds:  • We propose a novel graph inference learning framework by building structure relations to infer unknown node labels from those labeled nodes in an end-to-end way.
0.8018911899291294
Which model proved to be extremely effective?
In other words  the validation set is only used to teach the model itself how to transfer to unseen data.In contrast  the conventional methods often employ a validation set to tune parameters of a certain model of interest. Table 3: Performance comparisons with several GIL variants and the classical GCN method on the Cora dataset.
0.815347604750308
Google_USE
Which model proved to be extremely effective?
 Yujia Li  Daniel Tarlow  Marc Brockschmidt  and Richard Zemel.Gated graph sequence neural  Wei Liu  Junfeng He  and Shih-Fu Chang.Large graph construction for scalable semi-supervised  Wei Liu  Jun Wang  and Shih-Fu Chang.
[[0.2719484]]
Which model proved to be extremely effective?
Classification errors are rapidly decreasing as the number of iterations increases from the beginning to 400 iterations  while they are with a slow descent from 400 iterations to 1200 iterations.It demonstrates that the learned knowledge from the training samples can be transferred for inferring node category information from these reference labeled nodes.The performance of semi-supervised classification can be further increased by improving the generalized capability of the Graph CNN model.
[[0.27381963]]
Which model proved to be extremely effective?
Marginalized kernels between labeled graphs. In ICML  pp 321–328  2003. Thomas N. Kipf and Max Welling.
[[0.27439258]]
Which model proved to be extremely effective?
 Ulrik Brandes  Daniel Delling  Marco Gaertler  Robert Gorke  Martin Hoefer  Zoran Nikoloski  and Dorothea Wagner.On modularity clustering.IEEE transactions on knowledge and data engineering  20(2):172–188  2008.
[[0.2786293]]
Which model proved to be extremely effective?
 (8)  (9)  5 EXPERIMENTS  5.1 EXPERIMENTAL SETTINGS  We evaluate our proposed GIL method on three citation network datasets: Cora  Citeseer  Pubmed (Sen et al  2008)  and one knowledge graph NELL dataset (Carlson et al  2010).The statistical properties of graph data are summarized in Table 1.Following the previous protocol in (Kipf & Welling  2017; Zhuang & Ma  2018)  we split the graph data into a training set  a validation set  and a testing set.
[[0.28885555]]
Which model proved to be extremely effective?
We further improve the inference learning capability of the GIL model for 1200 iterations with the validation set  where the meta-learning rates α and β are both set to 0.001. 6  Published as a conference paper at ICLR 2020  5.2 COMPARISON WITH STATE-OF-THE-ARTS  We compare the GIL approach with several state-of-the-art methods (Monti et al  2017; Kipf & Welling  2017; Zhou et al  2004; Zhuang & Ma  2018) over four graph datasets  including Cora  Citeseer  Pubmed  and NELL.The classification accuracies for all methods are reported in Table 2.
[[0.28980327]]
Which model proved to be extremely effective?
 Methods Clustering (Brandes et al  2008) DeepWalk (Zhou et al  2004) Gaussian (Zhu et al  2003) G-embedding (Yang et al  2016) DCNN (Atwood & Towsley  2016) GCN (Kipf & Welling  2017) MoNet (Monti et al  2017) N-GCN (Abu-El-Haija et al  2018) GAT (Velickovic et al  2018) AGNN (Thekumparampil et al  2018) TAGCN (Du et al  2017) DGCN (Zhuang & Ma  2018) Our GIL  Cora 59.5 67.2 68.0 75.7 76.8 81.5 81.7 83.0 83.0 83.1 83.3 83.5 86.2  Citeseer 60.1 43.2 45.3 64.7 - 70.3 - 72.2 72.5 71.7 72.5 72.6 74.1  Pubmed NELL 70.7 65.3 63.0 77.2 73.0 79.0 78.8 79.5 79.0 79.9 79.0 80.0 83.1  21.8 58.1 26.5 61.9 - 66.0 - - - - - 74.2 78.9  5.3 ANALYSIS  Meta-optimization: As can be seen in Table 3  we report the classification accuracies of semi-supervised classification with several variants of our proposed GIL and the classical GCN method (Kipf & Welling  2017) when evaluating them on the Cora dataset.For analyzing the perfor- mance improvement of our proposed GIL with the graph inference learning process  we report the classification accuracies of GCN (Kipf & Welling  2017) and our proposed GIL on the Cora dataset under two different situations  including “only learning with the training set Vtr" and “with jointly learning on a training set Vtr and a validation set Vval".“GCN /w jointly learning on Vtr & Vval" achieves a better result than “GCN /w learning on Vtr" by 3.6%  which demonstrates that the network performance can be improved by employing validation samples.
[[0.2898631]]
Which model proved to be extremely effective?
A sophisticated machine learning technique used in most existing methods (Kipf & Welling  2017; Zhou et al  2004) is to choose the optimal classifier (trained on a training set) after checking the performance on the validation set.However  these methods essentially ignore how to extract transferable knowledge from these known labeled nodes to unlabeled nodes  as the graph structure itself implies node connectivity/reachability.Moreover  due to the scarcity of labeled samples  the performance of such a classifier is usually not satisfying.
[[0.2899459]]
Which model proved to be extremely effective?
AI magazine  29(3):93–93  2008. Nino Shervashidze  SVN Vishwanathan  Tobias Petri  Kurt Mehlhorn  and Karsten Borgwardt.Efficient graphlet kernels for large graph comparison.
[[0.293073]]
Which model proved to be extremely effective?
Although graph CNN based methods have achieved considerable capabilities of graph embedding by optimizing filters  they are limited into a conventionally semi-supervised framework and lack of an efficient inference mechanism on graphs.Especially  in the case of few-shot learning  where a small number of training nodes are labeled  this kind of methods would drastically compromise the performance.For example  the Pubmed graph dataset (Sen et al  2008) consists  ∗Corresponding author: Zhen Cui.
[[0.31412226]]
