Published as a conference paper at ICLR 2020  SELF: LEARNING TO FILTER NOISY LABELS WITH SELF-ENSEMBLING  Duc Tam Nguyen ∗, Chaithanya Kumar Mummadi ∗†, Thi Phuong Nhung Ngo †, Thi Hoai Phuong Nguyen ‡, Laura Beggel †, Thomas Brox †  ABSTRACT  Deep neural networks (DNNs) have been shown to over-fit a dataset when be- ing trained with noisy labels for a long enough time.To overcome this problem, we present a simple and effective method self-ensemble label filtering (SELF) to progressively filter out the wrong labels during training.Our method improves the task performance by gradually allowing supervision only from the potentially non-noisy (clean) labels and stops learning on the filtered noisy labels.
<EOS>
For the filtering, we form running averages of predictions over the entire training dataset using the network output at different training epochs.We show that these en- semble estimates yield more accurate identification of inconsistent predictions throughout training than the single estimates of the network at the most recent training epoch.While filtered samples are removed entirely from the supervised training loss, we dynamically leverage them via semi-supervised learning in the unsupervised loss.
<EOS>
We demonstrate the positive effect of such an approach on var- ious image classification tasks under both symmetric and asymmetric label noise and at different noise ratios.It substantially outperforms all previous works on noise-aware learning across different datasets and can be applied to a broad set of network architectures. 1  INTRODUCTION  The acquisition of large quantities of a high-quality human annotation is a frequent bottleneck in applying DNNs.
<EOS>
There are two cheap but imperfect alternatives to collect annotation at large scale: crowdsourcing from non-experts and web annotations, particularly for image data where the tags and online query keywords are treated as valid labels.Both these alternatives typically introduce noisy (wrong) labels.While Rolnick et al (2017) empirically demonstrated that DNNs can be surprisingly robust to label noise under certain conditions, Zhang et al (2017) has shown that DNNs have the capacity to memorize the data and will do so eventually when being confronted with too many noisy labels.
<EOS>
Consequently, training DNNs with traditional learning procedures on noisy data strongly deteriorates their ability to generalize – a severe problem.Hence, limiting the inﬂuence of label noise is of great practical importance. A common approach to mitigate the negative inﬂuence of noisy labels is to eliminate them from the training data and train deep learning models just with the clean labels (Fr´enay & Verleysen, 2013).
<EOS>
Employing semi-supervised learning can even counteract the noisy labels (Laine & Aila, 2016; Luo et al, 2018).However, the decision which labels are noisy and which are not is decisive for learning robust models.Otherwise, unfiltered noisy labels still inﬂuence the (supervised) loss and affect the task performance as in these previous works.
<EOS>
They use the entire label set to compute the loss and severely lack a mechanism to identify and filter out the erroneous labels from the labels set. In this paper, we propose a self-ensemble label filtering (SELF) framework that identifies potentially noisy labels during training and keeps the network from receiving supervision from the filtered noisy labels.This allows DNNs to gradually focus on learning from undoubtedly correct samples even with an extreme level of noise in the labels (e.g., 80% noise ratio) and leads to improved performance as the supervision become less noisy.
<EOS>
The key contribution of our work is progressive filtering, i.e.,  ∗Computer Vision Group, University of Freiburg, Germany †Bosch Center for AI, Bosch GmbH, Germany ‡Karlsruhe Institute of Technology, Germany  1  Published as a conference paper at ICLR 2020  (a) Evaluation on CIFAR-10  (b) Evaluation on CIFAR-100  Figure 1: Comparing the performance of SELF with previous works for learning under different (symmetric) label noise ratios on the (a) CIFAR-10 & (b) CIFAR-100 datasets.SELF retains higher robust classification accuracy at all noise levels. leverage the knowledge provided in the network’s output over different training iterations to form a consensus of predictions (self-ensemble predictions) to progressively identify and filter out the noisy labels from the labeled data.
<EOS>
 When learning under label noise, the network receives noisy updates and hence ﬂuctuates strongly.Such conduct of training would impede to learn stable neural representations and further mislead the consensus of the predictions.Therefore, it is essential to incorporate a model with stable training behavior to obtain better estimates from the consensus.
<EOS>
Concretely, we employ the semi-supervised technique as a backbone to our framework to stabilize the learning process of the model.Correctly, we maintain the running average model, such as proposed by Tarvainen & Valpola (2017), a.k.a. the Mean-Teacher model.This model ensemble learning provides a more stable supervisory signal than the noisy model snapshots and provides a stable ground for progressive filtering to filter out potential noisy labels.
<EOS>
Note that this is different from just a mere combination of semi-supervised techniques with a noisy label filtering method. We call our approach self-ensemble label filtering (SELF) - that establishes model ensemble learning as a backbone to form a solid consensus of the self-ensemble predictions to filter out the noisy labels progressively.Our framework allows to compute supervised loss on cleaner subsets rather than the entire noisy labeled data as in previous works.
<EOS>
It further leverages the entire dataset, including the filtered out erroneous samples in the unsupervised loss.To best of our knowledge, we are the first to identify and propose self-ensemble as a principled technique against learning under noisy labels. Our motivation stems from the observation that DNNs start to learn from easy samples in initial phases and gradually adapt to hard ones during training.
<EOS>
When trained on wrongly labeled data, DNNs learn from clean labels at ease and receive inconsistent error signals from the noisy labels before over-fitting to the dataset.The network’s prediction is likely to be consistent on clean samples and inconsistent or oscillates strongly on wrongly labeled samples over different training iterations.Based on this observation, we record the outputs of a single network made on different training epochs and treat them as an ensemble of predictions obtained from different individual networks.
<EOS>
We call these ensembles that are evolved from a single network self-ensemble predictions.Subsequently, we identify the correctly labeled samples via the agreement between the provided label set and our running average of self-ensemble predictions.The samples of ensemble predictions that agree with the provided labels are likely to be consistent and treated as clean samples.
<EOS>
 In summary, our SELF framework stabilizes the training process and improves the generalization ability of DNNs.We evaluate the proposed technique on image classification tasks using CI- FAR10, CIFAR100 & ImageNet.We demonstrate that SELF consistently outperforms the existing approaches on asymmetric and symmetric noise at all noise levels, as shown in Fig 1.
<EOS>
Besides, SELF remains robust towards the choice of the network architecture.Our work is transferable to other tasks without the need to modify the architecture or the primary learning objective. 2  Published as a conference paper at ICLR 2020  Figure 2: Overview of the self-ensemble label filtering (SELF) framework.
<EOS>
The model starts in iteration 0 with training from the noisy label set.During training, the model maintains a self- ensemble, a running average of itself  to provide a stable learning signal.Also, the model collects a self-ensemble prediction (moving-average) for the subsequent filtering.
<EOS>
Once the best model is found, these predictions identify and filter out noisy labels using the original label set L0.The model performs this progressive filtering until there is no more better model.For details see Algorithm 1.
<EOS>
 2 SELF-ENSEMBLE LABEL FILTERING  2.1 OVERVIEW  Fig 2 shows an overview of our proposed approach.In the beginning, we assume that the labels of the training set are noisy.The model attempts to identify correct labels progressively using self- forming ensembles of models and predictions.
<EOS>
Since wrong labels cause strong ﬂuctuations in the model’s predictions, using ensembles is a natural way to counteract noisy labels. Concretely, in each iteration, the model learns from a detected set of potentially correct labels and maintains a running average of model snapshots (realized by the Mean Teacher model Tarvainen & Valpola (2017)).This ensemble model is evaluated on the entire dataset and provides an additional learning signal for training the single models.
<EOS>
Additionally, our framework maintains the running- average of the model’s predictions for the filtering process.The model is trained until we find the best model w.r.t. the performance on the validation set (e.g., by early-stopping).The set of correct labels is detected based on the strategy defined in Sec.
<EOS>
2.2. In the next iteration, we again use all data and the new filtered label set as input for the model training.The iterative training procedure stops when no better model can be found.In the following, we give more details about the combination of this training and filtering procedure.
<EOS>
 2.2 PROGRESSIVE LABEL FILTERING  Progressive detection of correctly labeled samples Our framework Self-Ensemble Label Filter- ing (Algorithm 1) focuses on the detection of certainly correct labels from the provided label set L0.In each iteration i, the model is trained using the label set of potentially correct labels Li. At the end of each iteration, the model determines the next correct label set Li+1 using the filtering strategy described in 2.2 The model stops learning when no improvement was achieved after training on the refined label set Li+1. In other words, in each iteration, the model attempts to learn from the easy, in some sense, obviously correct labels.
<EOS>
However, learning from easy samples also affects similar but harder samples from the same classes.Therefore, by learning from these easy samples, the network can gradually distinguish between hard and wrongly-labeled samples. 3  Published as a conference paper at ICLR 2020  Algorithm 1 SELF: Self-Ensemble Label Filtering pseudocode Require: Dtrain = noisy labeled training set Require: Dval = noisy labeled validation set Require: (x, y) = training stimuli and label Require: α = ensembling momentum, 0 ≤ α ≤ 1  while acc(Mi, Dval) ≥ acc(Mbest, Dval) do  i ← 0 Mi ← train(Dtrain, Dval) Mbest ← Mi zi ← 0  Mbest ← Mi Df ilter ← Dtrain i ← i + 1 for (x, y) in Df ilter do  ˆzi ← Mbest(x) zi ← αzi−1 + (1 − α)ˆzi if y (cid:54)= argmax(zi) then  y ← ∅ in Df ilter  end if end for Mi ← train(Df ilter, Dval)  end while return Mbest  (cid:46) counter to track iterations (cid:46) initial Mean-Teacher ensemble model training (cid:46) set initial model as best model (cid:46) initialize ensemble predictions of all samples (ignored sample index for simplicity) (cid:46) iterate until no best model is found on Dval (cid:46) save the best model (cid:46) set filtered dataset as initial label set  (cid:46) evaluate model output ˆzi (cid:46) accumulate ensemble predictions zi (cid:46) verify agreement of ensemble predictions & label (cid:46) identify it as noisy label & remove from label set  (cid:46) train Mean-Teacher model on filtered label set  Our framework does not focus on repairing all noisy labels.
<EOS>
Although the detection of wrong labels is sometimes easy, finding their correct hidden label might be extremely challenging in case of having many classes.If the noise is sufficiently random, the set of correct labels will be representative to achieve high model performance.Further, in our framework, the label filtering is performed on the original label set L0 from iteration 0.
<EOS>
Clean labels erroneously removed in an earlier iteration (e.g., labels of hard to classify samples) can be reconsidered for model training again in later iterations. Filtering strategy The model can determine the set of potentially correct labels Li based on agree- ment between the label y and its maximal likelihood prediction ˆy|x with Li = {(y, x) | ˆyx = y; ∀(y, x) ∈ L0}.L0 is the label set provided in the beginning, (y, x) are the samples and their respective noisy labels in the iteration i. In other words, the labels are only used for supervised training if in the current epoch, the model predicts the respective label to be the correct class with the highest likelihood.
<EOS>
In practice, our framework does not use ˆy(x) of model snapshots for filtering but a moving-average of the ensemble models and predictions to improve the filtering decision. 2.3 SELF-ENSEMBLE LEARNING  The model’s predictions for noisy samples tend to ﬂuctuate.For example, take a cat wrongly labeled as a tiger.
<EOS>
Other cat samples would encourage the model to predict the given cat image as a cat.Contrary, the wrong label tiger regularly pulls the model back to predict the cat as a tiger.Hence, using the model’s predictions gathered in one single training epoch for filtering is sub-optimal.
<EOS>
Therefore, in our framework SELF, our model relies on ensembles of models and predictions. Model ensemble with Mean Teacher A natural way to form a model ensemble is by using an exponential running average of model snapshots (Fig.3a).
<EOS>
This idea was proposed in Tarvainen & Valpola (2017) for semi-supervised learning and is known as the Mean Teacher model.In our framework, both the mean teacher model and the normal model are evaluated on all data to preserve the consistency between both models.The consistency loss between student and teacher output distribution can be realized with Mean-Square-Error loss or Kullback-Leibler-divergence.
<EOS>
More details for training with the model ensemble can be found in Appendix A.1  Prediction ensemble Additionally, we propose to collect the sample predictions over multiple training epochs: zj = αzj−1 + (1 − α)ˆzj , whereby zj depicts the moving-average prediction of sample k at epoch j, α is a momentum, ˆzj is the model prediction for sample k in epoch j. This scheme is displayed in Fig 3b.For each sample, we store the moving-average predictions, accumulated over the past iterations.Besides having a more stable basis for the filtering step, our proposed procedure also leads to negligible memory and computation overhead.
<EOS>
 4  Published as a conference paper at ICLR 2020  (a) Model ensemble (Mean teacher)  (b) Predictions ensemble  Figure 3: Maintaining the (a) model and (b) predictions ensembles is very effective against noisy model updates.These ensembles are self-forming during the training process as a moving-average of (a) model snapshots or (b) class predictions from previous training steps. Further, due to continuous training of the best model from the previous model, computation time can be significantly reduced, compared to re-training the model from scratch.
<EOS>
On the new filtered dataset, the model must only slowly adapt to the new noise ratio contained in the training set.Depending on the computation budget, a maximal number of iterations for filtering can be set to save time. 3 RELATED WORKS  Reed et al (2014); Azadi et al (2015) performed early works on learning robustly under label noise for deep neural networks.
<EOS>
Recently, Rolnick et al (2017) have shown for classification that deep neural networks come with natural robustness to label noise following a particular random distribution.No modification of the network or the training procedure is required to achieve this robustness.Following this insight, our framework SELF relies on this natural robustness to kickstart the self-ensemble filtering process to extend the robust behavior to more challenging scenarios.
<EOS>
 Laine & Aila (2016); Luo et al (2018) proposed to apply semi-supervised techniques on the data to counteract noise.These and other semi-supervised learning techniques learn from a static, initial set of noisy labels and have no mechanisms to repair labels.Therefore, the supervised losses in their learning objective are typically high until the model strongly overfits to the label noise.
<EOS>
Compared to these works, our framework performs a variant of self-supervised label corrections.The network learns from a dynamic, variable set of labels, which is determined by the network itself.Progressive filtering allows the network to (1) focus on a label set with a significantly lower noise ratio and (2) repair wrong decisions made by itself in an earlier iteration.
<EOS>
 Other works assign weights to potentially wrong labels to reduce the learning signal (Jiang et al, 2017; Ren et al, 2018; Jenni & Favaro, 2018).These approaches tend to assign less extreme weights or hyperparameters that are hard to set.Since the typical classification loss is highly non-linear, a lower weight might still lead to learning from wrong labels.
<EOS>
Compared to these works, the samples in SELF only receive extreme weights: either they are zero or one.Further, SELF focuses only on self-detecting the correct samples, instead of repairing the wrong labels.Typically, the set of correct samples are much easier to detect and are sufficiently representative to achieve high performance.
<EOS>
 Han et al (2018b); Jiang et al (2017) employ two collaborating and simultaneously learning net- works to determine which samples to learn from and which not.However, the second network is free in its predictions and hence hard to tune.Compared to these works, we use ensemble learning as a principled approach to counteract model ﬂuctuations.
<EOS>
In SELF, the second network is extremely restricted and is only composed of running averages of the first network.To realize the second network, we use the mean-teacher model  as a backbone.Compared to their work, our self-ensemble label filtering gradually detects the correct labels and learns from them, so the label set is variable.
<EOS>
Further, we do use not only model ensembles but also an ensemble of predictions to detect correct labels. Other works modify the primary loss function of the classification tasks.Patrini et al (2017) es- timates the noise transition matrix to correct the loss, Han et al (2018a) uses human-in-the-loop, Zhang & Sabuncu (2018); Thulasidasan et al (2019) propose other forms of cross-entropy losses.
<EOS>
The loss modification impedes the transfer of these ideas to other tasks than classification.Compared to these works, our framework SELF does not modify the primary loss.However, many tasks rely on the presence of clean labels such as anomaly detection (Nguyen et al, 2019a) or self-supervised and unsupervised learning (Nguyen et al, 2019b).
<EOS>
The progressive filtering procedure and self-ensemble learning proposed are also applicable in these tasks to counteract noise effectively. 5  Published as a conference paper at ICLR 2020  Table 1: Comparison of classification accuracy when learning under uniform label noise on CIFAR- 10 and CIFAR-100.Following previous works, we compare two evaluation scenarios: with a noisy validation set (top) and with 1000 clean validation samples (bottom).
<EOS>
The best model is marked in bold.Having a small clean validation set improves the model but is not necessary. NOISE RATIO  CIFAR-10  CIFAR-100  40% 60% 80% 40% 60% 80 %  USING NOISY VALIDATION SET  69.66 70.64 78.15 86.06 89.00 89.00 87.13 87.62 83.25 81.85 83.36 85.34 83.27 93.70  90.93 78.00 86.55 86.92 95.10  - - - - - - 82.54 82.70 74.96 74.04 72.84 80.07 74.39 93.15  87.58 - - - 93.77  - - - - 20.00 49.00 64.07 67.92 54.64 29.22 - 53.81 40.09 69.91  70.80 - - - 79.93  51.34 49.10 - 58.01 61.00 68.00 61.77 62.64 31.05 55.95 52.01 53.69 52.88 71.98  68.20 59.00 58.34 61.31 74.76  - - - - - - 53.16 54.04 19.12 47.98 42.27 41.47 42.64 66.21  59.44 - - - 68.35  - - - - 13.00 35.00 29.16 29.60 08.90 23.22  15.00 18.46 42.09  34.06 - - - 46.43  REED-HARD  S-MODEL (GOLDBERGER & BEN-REUVEN, 2016) OPEN-SET WANG ET AL. (2018) RAND.
<EOS>
WEIGHTS  BI-LEVEL-MODEL  MENTORNET  Lq  TRUNC Lq  FORWARD ˆT  CO-TEACHING (HAN ET AL., 2018B) D2L  SL  JOINTOPT  SELF (OURS)  DAC  MENTORNET  RAND.WEIGHTS  REN ET AL  SELF* (OURS)  4 EVALUATION  4.1 EXPERIMENTS DESCRIPTIONS  4.1.1 STRUCTURE OF THE ANALYSIS  USING CLEAN VALIDATION SET (1000 IMAGES)  We evaluate our approach on CIFAR-10, CIFAR-100, an ImageNet-ILSVRC on different noise sce- narios.For CIFAR-10, CIFAR-100, and ImageNet, we consider the typical situation with symmetric and asymmetric label noise.
<EOS>
In the case of the symmetric noise, a label is randomly ﬂipped to another class with probability p. Following previous works, we also consider label ﬂips of semantically sim- ilar classes on CIFAR-10, and pair-wise label ﬂips on CIFAR-100.Finally, we perform studies on the choice of the network architecture and the ablation of the components in our framework.Tab.
<EOS>
6 (Appendix) shows the in-deep analysis of semi-supervised learning strategies combined with recent works.Overall, the proposed framework SELF outperforms all these combinations. 4.1.2 COMPARISONS TO PREVIOUS WORKS  We compare our work to previous methods from Reed-Hard , S-model (Gold- berger & Ben-Reuven, 2016), Wang et al (2018), Rand. weights , Bi-level-model , D2L , SL , Lq , Trunc Lq , Forward ˆT , DAC , Random reweighting , and Learning to reweight .
<EOS>
For co-teaching (Han et al, 2018b), MentorNet , JointOpt , the source codes are available and hence used for evaluation.  and DAC  considered the setting of having a small clean validation set of 1000 and 5000 images respectively.For comparison purposes, we also experiment with a small clean set of 1000 images additionally.
<EOS>
Further, we abandon oracle experiments or methods using additional information to keep the evaluation comparable.For instance, Forward T  uses the true underlying confusion matrix to correct the loss.This information is neither known in typical scenarios nor used by other methods.
<EOS>
 6  Published as a conference paper at ICLR 2020  Table 2: Asymmetric noise on CIFAR-10, CIFAR-100.All methods use Resnet34.CIFAR-10: ﬂip TRUCK → AUTOMOBILE, BIRD → AIRPLANE, DEER → HORSE, CAT↔DOG with prob. p. CIFAR-100: ﬂip class i to (i + 1)%100 with prob. p. SELF retains high performances across all noise ratios and outperforms all previous works.
<EOS>
 CIFAR-10  NOISE RATIO  10%  20%  30%  40%  CCE MAE FORWARD ˆT Lq TRUNC Lq SL JOINTOPT SELF (OURS)  90.69 82.61 90.52 90.91 90.43 88.24 90.12 93.75  88.59 52.93 89.09 89.33 89.45 85.36 89.45 92.76  86.14 50.36 86.79 85.45 87.10 80.64 87.18 92.42  80.11 45.52 83.55 76.74 82.28  -  87.97 89.07  10%  66.54 13.38 45.96 68.36 68.86 65.58 69.61 72.45  CIFAR-100 20%  30%  59.20 11.50 42.46 66.59 66.59 65.14 68.94 70.53  51.40 08.91 38.13 61.45 61.87 63.10 63.99 65.09  40%  42.74 08.20 34.44 47.22 47.66  -  53.71 53.83  Whenever possible, we adopt the reported performance from the corresponding publications.The testing scenarios are kept as similar as possible to enable a fair comparison.All tested scenarios use a noisy validation set with the same noise distribution as the training set unless stated otherwise.
<EOS>
All model performances are reported on the clean test set. Table 3: Effect of the choice of network architecture on classification accuracy on CIFAR-10 & -100 with uniform label noise.SELF is compatible with all tested architectures.
<EOS>
Here * represents baseline accuracy of the architectures that are trained on fully supervised setting at 0% label noise. CIFAR-10  CIFAR-100  CIFAR-10  CIFAR-100  RESNET101  93.89*  81.14*  RESNET34  93.5*  76.76*  NOISE  40% 80% 40% 80%  NOISE  40% 80% 40% 80%  MENTORNET CO-T.SELF  89.00 62.58 92.77  49.00 21.79 64.52  68.00 39.58 69.00  35.00 16.79 39.73  Lq TRUNC Lq FORWARD ˆT SELF  87.13 87.62 83.25 91.13  64.07 67.92 54.64 63.59  61.77 62.64 31.05 66.71  29.16 29.60 8.90 35.56  WRN 28-10  96.21*  81.02*  NOISE  40% 80% 40% 80%  MENTORNET REWEIGHT SELF  88.7 86.02 93.34  46.30 - 67.41  67.50 58.01 72.48  30.10 - 42.06  RESNET26  96.37*  81.20*  NOISE  CO-T.
<EOS>
SELF  40% 80% 40% 80%  81.85 93.70  29.22 69.91  55.95 71.98  23.22 42.09  4.1.3 NETWORKS CONFIGURATION AND TRAINING  For the basic training of self-ensemble model, we use the Mean Teacher model  available on GitHub 1The students and teacher networks are residual networks  with 26 layers with Shake-Shake-regularization (Gastaldi, 2017).We use the Py- Torch  implementation of the network and keep the training settings close to .The network is trained with Stochastic Gradient Descent.
<EOS>
In each filtering iteration, the model is trained for a maximum of 300 epochs, with patience of 50 epochs.For more training details, see the appendix. 4.2 EXPERIMENTS RESULTS  4.2.1 SYMMETRIC LABEL NOISE  CIFAR-10 and 100 Results for typical uniform noise scenarios with noise ratios on CIFAR-10 and CIFAR-100 are shown in Tab.
<EOS>
1.More results are visualized in Fig 1a (CIFAR-10) and Fig 1b (CIFAR-100).Our approach SELF performs robustly in the case of lower noise ratios with up to 60% and outperforms previous works.
<EOS>
Although a strong performance loss occurs at 80% label noise,  1https://github.com/CuriousAI/mean-teacher  7  Published as a conference paper at ICLR 2020  Table 4: Classification accuracy on clean ImageNet validation dataset.The mod- els are trained at 40% label noise and the best model is picked based on the evalu- ation on noisy validation data.Mentornet shows the best previously reported results.
<EOS>
Mentornet* is based on Resnet-101.We chose the smaller Resnext50 model to re- duce the run-time. Accurracy  Mentornet* ResNext Mean-T.
<EOS>
SELF (Ours)  Resnext18 Resnext50 P@1 P@5 P@1 P@5  -  65.10 85.90 - 50.6 75.99 56.25 80.90 58.04 81.82 62.96 85.72 66.92 86.65 71.31 89.92  Table 5: Ablation study on CIFAR-10 and CIFAR- 100.The Resnet baseline was trained on the full noisy label set.Adding progressive filtering im- proves over this baseline.
<EOS>
The Mean Teacher main- tains an ensemble of model snapshots, which helps counteract noise.Having progressive filtering and model ensembles (-MVA-pred.) makes the model more robust but still fails at 80% noise.The full SELF framework additionally uses the prediction ensemble for detection of correct labels.
<EOS>
 NOISE RATIO  RESNET26 FILTERING MEAN-T. - MVA-PRED.SELF (OURS)  CIFAR-10  CIFAR-100 40% 80% 40% 80%  83.20 87.35 93.70 93.77 93.70  41.37 49.58 52.50 57.40 69.91  53.18 61.40 65.85 71.69 71.98  19.92 23.42 26.31 38,61 42.09  SELF still outperforms most of the previous approaches.The experiment SELF* using a 1000 clean validation images shows that the performance loss mostly originates from the progressive filtering relying too strongly on the extremely noisy validation set.
<EOS>
 ImageNet-ILSVRC Tab.4 shows the precision@1 and @5 of various models, given 40% label noise in the training set.Our networks are based on ResNext18 and Resnext50.
<EOS>
Note that MentorNet  uses Resnet101 (P@1: 78.25) , which has higher performance compared to Resnext50 (P@1: 77.8)  on the standard ImageNet validation set. Despite the weaker model, SELF (ResNext50) surpasses the best previously reported results by more than 5% absolute improvement.Even the significantly weaker model ResNext18 outperforms MentorNet, which is based on a more powerful ResNet101 network.
<EOS>
 4.2.2 ASYMMETRIC LABEL NOISE Tab.2 shows more challenging noise scenarios when the noise is not class-symmetric and uniform.Concretely, labels are ﬂipped among semantically similar classes such as CAT and DOG on CIFAR- 10.
<EOS>
On CIFAR-100, each label is ﬂipped to the next class with a probability p. In these scenarios, our framework SELF also retains high performance and only shows a small performance drop at 40% noise.The high label noise resistance of our framework indicates that the proposed self-ensemble filtering process helps the network identify correct samples, even under extreme noise ratios. 4.2.3 EFFECTS OF DIFFERENT ARCHITECTURES Previous works utilize a various set of different architectures, which hinders a fair comparison.
<EOS>
Tab.3 shows the performance of our framework SELF compared to previous approaches.SELF outperforms other works in all scenarios except for CIFAR-10 with 80% noise.
<EOS>
Typical robust learning approaches lead to significant accuracy losses at 40% noise, while SELF still retains high performance.Further, note that SELF allows the network’s performance to remain consistent across the different underlying architectures. 4.2.4 ABLATION STUDY Tab.
<EOS>
5 shows the importance of each component in our framework.See Fig 4a, Fig 4b for experi- ments on more noise ratios.As expected, the Resnet-baseline rapidly breaks down with increasing noise ratios.
<EOS>
Adding self-supervised filtering increases the performance slightly in lower noise ratios.However, the model has to rely on extremely noisy snapshots.Contrary, using a model ensemble alone such as in Mean-Teacher can counteract noise on the noisy dataset CIFAR-10.
<EOS>
On the more challenging CIFAR-100, however, the performance decreases strongly.With self-supervised filter- ing and model ensembles, SELF (without MVA-pred) is more robust and only impairs performance at 80% noise.The last performance boost is given by using moving-average predictions so that the network can reliably detect correctly labeled samples gradually.
<EOS>
 Fig 4 shows the ablation experiments on more noise ratios.The analyses shows that each component in SELF is essential for the model to learn robustly. 8  Published as a conference paper at ICLR 2020  (a) Ablation exps. on CIFAR-10  (b) Ablation exps. on CIFAR-100  Figure 4: Ablation study on the importance of the components in our framework SELF, evaluated on (a) Cifar-10 and (b) Cifar-100 with uniform noise.
<EOS>
Please refer Tab.5 for details of components. Table 6: Analysis of semi-supervised learning (SSL) strategies: entropy learning, mean-teacher combined with recent works.
<EOS>
Our progressive filtering strategy is shown to be effective and per- forms well regardless of the choice of the semi-supervised learning backbone.Overall, the proposed method SELF outperforms all these combinations.Best model in each SSL-category is marked in bold.
<EOS>
Running mean-teacher+ co-teaching using the same configuration is not possible due to memory constraints. NOISE RATIO  CIFAR-10  CIFAR-100  40%  60%  80%  40%  60%  80%  BASELINE MODELS  RESNET26 (GASTALDI, 2017) CO-TEACHING (HAN ET AL., 2018B) JOINTOPT  PROGRESSIVE FILTERING (OURS)  ENTROPY ENTROPY + CO-TEACHING ENTROPY + JOINT-OPT ENTROPY+FILTERING (OURS)  83.20 81.85 83.27 87.35  79.13 84.94 84.44 90.04  72.35 74.04 74.39 75.47  85.98 74.28 75.86 83.88  41.37 29.22 40.09 49.58  46.93 35.16 39.16 52.46  53.18 55.95 52.88 61.40  54.65 55.68 56.73 59.97  44.31 47.98 42.64 50.60  41.34 43.52 43.27 46.45  19.92 23.22 18.46 23.42  21.29 20.5 17.24 23.53  SEMI-SUPERVISED LEARNING WITH ENTROPY LEARNING  SEMI-SUPERVISED LEARNING WITH MEAN-TEACHER  MEAN TEACHER MEAN-TEACHER + JOINTOPT MEAN-TEACHER + FILTERING - SELF (OURS)  93.70 91.40 93.70  90.40 83.62 92.85  52.5 45.12 69.91  65.85 60.09 71.98  54.48 45.92 66.21  26.31 23.54 42.58  4.2.5 SEMI-SUPERVISED LEARNING FOR PROGRESSIVE FILTERING  Tab.6 shows different semi-supervised learning strategies: entropy learning, mean-teacher com- bined with recent works.
<EOS>
Note that Co-Teaching+Mean-Teacher cannot be implemented and run in the same configuration as other experiments, due to memory constraints. The analysis indicates the semi-supervised losses mostly stabilize the baselines, compared to the model without semi-supervised learning.However, Co-teaching and JointOpt sometimes perform worse than the purely semi-supervised model.
<EOS>
This result indicates that their proposed frameworks are not always compatible with semi-supervised losses. The progressive filtering technique is seamlessly compatible with different semi-supervised losses.The filtering outperforms its counterparts when combined with Entropy Learning or Mean-teacher model.
<EOS>
Overall, SELF outperforms all considered combinations. 9  Published as a conference paper at ICLR 2020  5 CONCLUSION  We propose a simple and easy to implement a framework to train robust deep learning models under incorrect or noisy labels.We filter out the training samples that are hard to learn (possibly noisy labeled samples) by leveraging ensemble of predictions of the single network’s output over different training epochs.
<EOS>
Subsequently, we allow clean supervision from the non-hard samples and further leverage additional unsupervised loss from the entire dataset.We show that our framework results in DNN models with superior generalization performance on CIFAR-10, CIFAR-100 & ImageNet and outperforms all previous works under symmetric (uniform) and asymmetric noises.Furthermore, our models remain robust despite the increasing noise ratio and change in network architectures.
<EOS>
