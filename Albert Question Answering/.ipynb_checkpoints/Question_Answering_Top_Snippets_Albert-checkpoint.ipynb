{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1qfQAtRsMVl7"
   },
   "source": [
    "# Reading Comprehension with ALBERT (and similar)\n",
    "\n",
    "Author: [@techno246](https://twitter.com/techno246)\n",
    "\n",
    "Github Repo: https://github.com/spark-ming/albert-qa-demo/\n",
    "\n",
    "Blog Post: https://www.spark64.com/post/machine-comprehension\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Reading comprehension, otherwise known as question answering systems, are one of the tasks that NLP tries to solve. The goal of this task is to be able to answer an arbitary question given a context. For instance, given the following context:\n",
    "\n",
    "> New Zealand (MÄori: Aotearoa) is a sovereign island country in the southwestern Pacific Ocean. It has a total land area of 268,000 square kilometres (103,500 sq mi), and a population of 4.9 million. New Zealand's capital city is Wellington, and its most populous city is Auckland.\n",
    "\n",
    "We ask the question\n",
    "\n",
    "> How many people live in New Zealand?\n",
    "\n",
    "We expect the QA system is to respond with something like this:\n",
    "\n",
    "> 4.9 million\n",
    "\n",
    "Since 2017, transformer models have shown to outperform existing approaches for this task. Many pretrained transformer models exist, including BERT, GPT-2, XLNET. One of the newcomers to the group is ALBERT (A Lite BERT) which was published in September 2019. The research group claims that it outperforms BERT, with much less parameters (shorter training and inference time). \n",
    "\n",
    "This tutorial demonstrates how you can fine-tune ALBERT for the task of QnA and use it for inference. For this tutorial, we will use the transformer library built by [Hugging Face](https://huggingface.co/), which is an extremely nice implementation of the transformer models (including ALBERT) in both TensorFlow and PyTorch. You can  just use a fine-tuned model from their [model repository](https://huggingface.co/models) (which I encourage in general to save money and reduce emissions). However for educational purposes I will also show you how to finetune it yourself so you can adapt it for your own data. \n",
    "\n",
    "Note that the goal of this is not to build an optimised, production ready system, but to demonstrate the concept with as little code as possible. Therefore a lot of code will be retrofitted for this purpose. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sBBHbGvQN5vX"
   },
   "source": [
    "## 1.0 Setup\n",
    "\n",
    "Let's check out what kind of GPU our friends at Google gave us. This notebook should be configured to give you a P100 ðŸ˜ƒ (saved in metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "frTeTcy4WdbY"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D5RImM3oWbrZ"
   },
   "source": [
    "First, we clone the Hugging Face transformer library from Github.\n",
    "\n",
    "\n",
    "Note it's checking out a specific commit only because I've tested this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "QOAoUwBFMQCg",
    "outputId": "053e1ce2-f7c5-440e-e82a-ede83bb9f2d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'transformers'...\n",
      "remote: Enumerating objects: 9, done.\u001b[K\n",
      "remote: Counting objects: 100% (9/9), done.\u001b[K\n",
      "remote: Compressing objects: 100% (9/9), done.\u001b[K\n",
      "remote: Total 21778 (delta 1), reused 1 (delta 0), pack-reused 21769\u001b[K\n",
      "Receiving objects: 100% (21778/21778), 12.94 MiB | 25.10 MiB/s, done.\n",
      "Resolving deltas: 100% (15654/15654), done.\n",
      "Note: checking out 'a3085020ed0d81d4903c50967687192e3101e770'.\n",
      "\n",
      "You are in 'detached HEAD' state. You can look around, make experimental\n",
      "changes and commit them, and you can discard any commits you make in this\n",
      "state without impacting any branches by performing another checkout.\n",
      "\n",
      "If you want to create a new branch to retain commits you create, you may\n",
      "do so (now or later) by using -b with the checkout command again. Example:\n",
      "\n",
      "  git checkout -b <new-branch-name>\n",
      "\n",
      "HEAD is now at a3085020 Added repetition penalty to PPLM example (#2436)\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/huggingface/transformers \\\n",
    "&& cd transformers \\\n",
    "&& git checkout a3085020ed0d81d4903c50967687192e3101e770 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 921
    },
    "colab_type": "code",
    "id": "TRZned-8WJrj",
    "outputId": "63b9d788-d21b-47d0-a117-866f864b3971"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ./transformers\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==2.3.0) (1.17.5)\n",
      "Collecting tokenizers==0.0.11\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/36/7af38d572c935f8e0462ec7b4f7a46d73a2b3b1a938f50a5e8132d5b2dc5/tokenizers-0.0.11-cp36-cp36m-manylinux1_x86_64.whl (3.1MB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.1MB 3.4MB/s \n",
      "\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers==2.3.0) (1.11.15)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==2.3.0) (3.0.12)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==2.3.0) (2.21.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from transformers==2.3.0) (4.28.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==2.3.0) (2019.12.20)\n",
      "Collecting sentencepiece\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.0MB 37.3MB/s \n",
      "\u001b[?25hCollecting sacremoses\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 870kB 39.0MB/s \n",
      "\u001b[?25hRequirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.3.0) (0.3.3)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.3.0) (0.9.4)\n",
      "Requirement already satisfied: botocore<1.15.0,>=1.14.15 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.3.0) (1.14.15)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.3.0) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.3.0) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.3.0) (2019.11.28)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.3.0) (1.24.3)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.3.0) (1.12.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.3.0) (7.0)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.3.0) (0.14.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers==2.3.0) (2.6.1)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers==2.3.0) (0.15.2)\n",
      "Building wheels for collected packages: transformers, sacremoses\n",
      "  Building wheel for transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for transformers: filename=transformers-2.3.0-cp36-none-any.whl size=458557 sha256=6c30f4c4a2d82a89763a98e2ed98d40cd694b02f8f41c76a450f8bad49687bfe\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-hm3s7c37/wheels/23/19/dd/2561a4e47240cf6b307729d58e56f8077dd0c698f5992216cf\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.38-cp36-none-any.whl size=884628 sha256=058a6ce324db52030a25f35f800607627e5faedd1f07d7ccfba1d9fd027ec10d\n",
      "  Stored in directory: /root/.cache/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n",
      "Successfully built transformers sacremoses\n",
      "Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n",
      "Successfully installed sacremoses-0.0.38 sentencepiece-0.1.85 tokenizers-0.0.11 transformers-2.3.0\n",
      "Collecting tensorboardX\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/35/f1/5843425495765c8c2dd0784a851a93ef204d314fc87bcc2bbb9f662a3ad1/tensorboardX-2.0-py2.py3-none-any.whl (195kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 204kB 3.5MB/s \n",
      "\u001b[?25hRequirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (3.10.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.12.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.17.5)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorboardX) (45.2.0)\n",
      "Installing collected packages: tensorboardX\n",
      "Successfully installed tensorboardX-2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install ./transformers\n",
    "!pip install tensorboardX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UHCuzhPptH0M"
   },
   "source": [
    "## 2.0 Train Model\n",
    "\n",
    "This is where we can train our own model. Note you can skip this step if you don't want to wait 1.5 hours!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OaQGsAiWXcnd"
   },
   "source": [
    "### 2.1 Get Training and Evaluation Data\n",
    "\n",
    "The SQuAD dataset contains question/answer pairs to for training the ALBERT model for the QA task. \n",
    "\n",
    "Now get the SQuAD V2.0 dataset. `train-v2.0.json` is for training and `dev-v2.0.json` is for evaluation to see how well your model trained.\n",
    "\n",
    "Read more about this dataset here: https://rajpurkar.github.io/SQuAD-explorer/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dI6e-PfOXSnO"
   },
   "outputs": [],
   "source": [
    "!mkdir dataset \\\n",
    "&& cd dataset \\\n",
    "&& wget https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json \\\n",
    "&& wget https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dZ87q93GDeeL"
   },
   "source": [
    "### 2.2 Run training \n",
    "\n",
    "We can now train the model with the training set. \n",
    "\n",
    "### Notes about parameters:\n",
    "`per_gpu_train_batch_size` specifies the number of training examples per iteration per GPU. *In general*, higher means more accuracy and faster training. However, the biggest limitation is the size of the GPU. 12 is what I use for a GPU with 16GB memory. \n",
    "\n",
    "`save_steps` specifies number of steps before it outputs a checkpoint file. I've increased it to save disk space.\n",
    "\n",
    "`num_train_epochs` I recommend two epochs here. It's currently set to one for the purpose of time\n",
    "\n",
    "`version_2_with_negative` is required for SQuAD V2.0. If training with V1.1, take out this flag\n",
    "\n",
    "Warning: it takes about 1.5 hours to train an epoch! If you don't want to wait this long, feel free to skip this step and note the comment in the code to use a pretrained model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-Eg53t3QXZAb"
   },
   "outputs": [],
   "source": [
    "!export SQUAD_DIR=/content/dataset \\\n",
    "&& python transformers/examples/run_squad.py \\\n",
    "  --model_type albert \\\n",
    "  --model_name_or_path albert-base-v2 \\\n",
    "  --do_train \\\n",
    "  --do_eval \\\n",
    "  --do_lower_case \\\n",
    "  --train_file $SQUAD_DIR/train-v2.0.json \\\n",
    "  --predict_file $SQUAD_DIR/dev-v2.0.json \\\n",
    "  --per_gpu_train_batch_size 12 \\\n",
    "  --learning_rate 3e-5 \\\n",
    "  --num_train_epochs 1.0 \\\n",
    "  --max_seq_length 384 \\\n",
    "  --doc_stride 128 \\\n",
    "  --output_dir /content/model_output \\\n",
    "  --save_steps 1000 \\\n",
    "  --threads 4 \\\n",
    "  --version_2_with_negative "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-JCNRkQwUD56"
   },
   "source": [
    "## 3.0 Setup prediction code\n",
    "\n",
    "Now we can use the Hugging Face library to make predictions using our newly trained model. Note that a lot of the code is pulled from `run_squad.py` in the Hugging Face repository, with all the training parts removed. This modified code allows to run predictions we pass in directly as strings, rather .json format like the training/test set.\n",
    "\n",
    "NOTE if you decided train your own mode, change the flag `use_own_model` to `True`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68,
     "referenced_widgets": [
      "f5bb7649026d4b8992815d357e785dc4",
      "7c9061a64a3a4f739bf138d6c358234a",
      "a55e529f7288442fb74dc1c78411e2d0",
      "7b76064c5b7f4d8293b6351d16f83888",
      "61fcf5796a9647a7b5b0749b73a8f442",
      "ffd05ad915554f55bb868218b0dc0090",
      "a64796515dd9461c9a8200064f0af27c",
      "9c44b6b2fa3145c3bf812af364884e40"
     ]
    },
    "colab_type": "code",
    "id": "qp0Pq9z9Y4S0",
    "outputId": "cfa29afe-3493-4c42-84f0-12efc6889231"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5bb7649026d4b8992815d357e785dc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=234922444, style=ProgressStyle(description_â€¦"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import time\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "from transformers import (\n",
    "    AlbertConfig,\n",
    "    AlbertForQuestionAnswering,\n",
    "    AlbertTokenizer,\n",
    "    squad_convert_examples_to_features\n",
    ")\n",
    "\n",
    "from transformers.data.processors.squad import SquadResult, SquadV2Processor, SquadExample\n",
    "\n",
    "from transformers.data.metrics.squad_metrics import compute_predictions_logits\n",
    "\n",
    "# READER NOTE: Set this flag to use own model, or use pretrained model in the Hugging Face repository\n",
    "use_own_model = False\n",
    "\n",
    "if use_own_model:\n",
    "  model_name_or_path = \"/content/model_output\"\n",
    "else:\n",
    "  model_name_or_path = \"ktrapeznikov/albert-xlarge-v2-squad-v2\"\n",
    "\n",
    "output_dir = \"\"\n",
    "\n",
    "# Config\n",
    "n_best_size = 1\n",
    "max_answer_length = 30\n",
    "do_lower_case = True\n",
    "null_score_diff_threshold = 0.0\n",
    "\n",
    "def to_list(tensor):\n",
    "    return tensor.detach().cpu().tolist()\n",
    "\n",
    "# Setup model\n",
    "config_class, model_class, tokenizer_class = (\n",
    "    AlbertConfig, AlbertForQuestionAnswering, AlbertTokenizer)\n",
    "config = config_class.from_pretrained(model_name_or_path)\n",
    "tokenizer = tokenizer_class.from_pretrained(\n",
    "    model_name_or_path, do_lower_case=True)\n",
    "model = model_class.from_pretrained(model_name_or_path, config=config)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "processor = SquadV2Processor()\n",
    "\n",
    "def run_prediction(question_texts, context_text):\n",
    "    \"\"\"Setup function to compute predictions\"\"\"\n",
    "    examples = []\n",
    "\n",
    "    for i, question_text in enumerate(question_texts):\n",
    "        example = SquadExample(\n",
    "            qas_id=str(i),\n",
    "            question_text=question_text,\n",
    "            context_text=context_text,\n",
    "            answer_text=None,\n",
    "            start_position_character=None,\n",
    "            title=\"Predict\",\n",
    "            is_impossible=False,\n",
    "            answers=None,\n",
    "        )\n",
    "\n",
    "        examples.append(example)\n",
    "\n",
    "    features, dataset = squad_convert_examples_to_features(\n",
    "        examples=examples,\n",
    "        tokenizer=tokenizer,\n",
    "        max_seq_length=384,\n",
    "        doc_stride=128,\n",
    "        max_query_length=64,\n",
    "        is_training=False,\n",
    "        return_dataset=\"pt\",\n",
    "        threads=1,\n",
    "    )\n",
    "\n",
    "    eval_sampler = SequentialSampler(dataset)\n",
    "    eval_dataloader = DataLoader(dataset, sampler=eval_sampler, batch_size=10)\n",
    "\n",
    "    all_results = []\n",
    "\n",
    "    for batch in eval_dataloader:\n",
    "        model.eval()\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            inputs = {\n",
    "                \"input_ids\": batch[0],\n",
    "                \"attention_mask\": batch[1],\n",
    "                \"token_type_ids\": batch[2],\n",
    "            }\n",
    "\n",
    "            example_indices = batch[3]\n",
    "\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "            for i, example_index in enumerate(example_indices):\n",
    "                eval_feature = features[example_index.item()]\n",
    "                unique_id = int(eval_feature.unique_id)\n",
    "\n",
    "                output = [to_list(output[i]) for output in outputs]\n",
    "\n",
    "                start_logits, end_logits = output\n",
    "                result = SquadResult(unique_id, start_logits, end_logits)\n",
    "                all_results.append(result)\n",
    "\n",
    "    output_prediction_file = \"predictions.json\"\n",
    "    output_nbest_file = \"nbest_predictions.json\"\n",
    "    output_null_log_odds_file = \"null_predictions.json\"\n",
    "\n",
    "    predictions = compute_predictions_logits(\n",
    "        examples,\n",
    "        features,\n",
    "        all_results,\n",
    "        n_best_size,\n",
    "        max_answer_length,\n",
    "        do_lower_case,\n",
    "        output_prediction_file,\n",
    "        output_nbest_file,\n",
    "        output_null_log_odds_file,\n",
    "        False,  # verbose_logging\n",
    "        True,  # version_2_with_negative\n",
    "        null_score_diff_threshold,\n",
    "        tokenizer,\n",
    "    )\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nIQOB8vhpcKs"
   },
   "source": [
    "## 4.0 Run predictions\n",
    "\n",
    "Now for the fun part... testing out your model on different inputs. Pretty rudimentary example here. But the possibilities are endless with this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 431
    },
    "colab_type": "code",
    "id": "F-sUrcA5nXTH",
    "outputId": "06fd1e96-bf02-4cc8-c0ea-7a85a117ac7e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "convert squad examples to features: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 247.14it/s]\n",
      "add example index and unique id: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 7037.42it/s]\n",
      "convert squad examples to features: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 271.74it/s]\n",
      "add example index and unique id: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 20008.61it/s]\n",
      "convert squad examples to features: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 253.31it/s]\n",
      "add example index and unique id: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 11309.21it/s]\n",
      "convert squad examples to features: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 233.58it/s]\n",
      "add example index and unique id: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 25811.10it/s]\n",
      "convert squad examples to features: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 247.64it/s]\n",
      "add example index and unique id: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 3298.71it/s]\n",
      "convert squad examples to features: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 288.94it/s]\n",
      "add example index and unique id: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 23221.06it/s]\n",
      "convert squad examples to features: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 351.47it/s]\n",
      "add example index and unique id: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 14873.42it/s]\n",
      "convert squad examples to features: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 245.07it/s]\n",
      "add example index and unique id: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 17242.77it/s]\n",
      "convert squad examples to features: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 314.95it/s]\n",
      "add example index and unique id: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 30840.47it/s]\n",
      "convert squad examples to features: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 480.28it/s]\n",
      "add example index and unique id: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 24105.20it/s]\n",
      "convert squad examples to features: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 573.29it/s]\n",
      "add example index and unique id: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 20128.63it/s]\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "# context = \"New Zealand (MÄori: Aotearoa) is a sovereign island country in the southwestern Pacific Ocean. It has a total land area of 268,000 square kilometres (103,500 sq mi), and a population of 4.9 million. New Zealand's capital city is Wellington, and its most populous city is Auckland.\"\n",
    "contexts = [\"Inspired by the recent meta-learning strategy (Finn et al  2017)  we learn to infer the structure relations from a training set to a validation set  which can benefit the generalization capability of the learned model.\",\n",
    "            \"To make the inference model better generalize to test nodes  we introduce a meta-learning procedure to optimize structure relations  which could be the first time for graph node classification to the best of our knowledge.\",\n",
    "            \"We pre-train our proposed GIL model for 200 iterations with the training set  where its initial learning rate  decay factor  and momentum are set to 0.05  0.95  and 0.9  respectively.\",\n",
    "            \"We summarize the main contributions of this work as three folds:  â€¢ We propose a novel graph inference learning framework by building structure relations to infer unknown node labels from those labeled nodes in an end-to-end way.\",\n",
    "            \"By increasing the length of reachability path  the inference process of the GIL method would become difficult and more graph structure information may be employed in the predicted process.\",\n",
    "            \"In the standard protocol 3 Published as a conference paper at ICLR 2020  of prior literatures (Yang et al  2016)  the three node sets share the same label space.\",\n",
    "            \"We have given the detailed configuration of the relationship regression module in the class-to-node relationship of Section 4.\",\n",
    "            \"Inï¬‚uence of different between-node steps: We compare the classification performance within different between-node steps for our proposed GIL and GCN (Kipf & Welling  2017)  as illustrated in Fig 2(a).\",\n",
    "            \"We follow but do not restrict this protocol for our proposed method.\",\n",
    "            \"The detailed implementation of our model can be found in Section 4.\",\n",
    "            \"The classification accuracies for all methods are reported in Table 2.\"]\n",
    "\n",
    "questions = [\"How many people live in New Zealand?\", \n",
    "             \"Which is the best model?\",\n",
    "             \"What is the model used in this paper?\",\n",
    "             \"How many iterations was this model pretrained\",\n",
    "             \"What was the momentum set to?\",\n",
    "             \"What was the decay set to?\",\n",
    "             \"What was the learning rate set to?\",\n",
    "             \"When was this model used?\"]\n",
    "\n",
    "researchPaper = '/content/graph_inference_learning_for_semi_supervised_classification_3.csv'\n",
    "data = open(\"question_answers_small.txt\", \"w\")\n",
    "data2 = open(\"question_answers.txt\", \"w\")\n",
    "def run_all_snippets(researchPaper, data):\n",
    "  with open(researchPaper) as researchPaperCSV:\n",
    "    researchPaperReader = csv.reader(researchPaperCSV)\n",
    "        # for query in queriesList:\n",
    "        #   queryObj = researchPpr_NLP(query)\n",
    "        #   print(queryObj)\n",
    "    for snippet in researchPaperReader:\n",
    "    #print(snippet)\n",
    "    #EOS tag ignore\n",
    "      if('<EOS>' not in snippet):\n",
    "        snippetStr = \" \"\n",
    "        snippetStr = ' '.join([str(elem) for elem in snippet])\n",
    "        predictions = run_prediction(questions, snippetStr)\n",
    "        data.write(\"\\n\")\n",
    "        data.write(snippetStr + \"\\n\")\n",
    "        for key in predictions.keys():\n",
    "          data.write(questions[int(key)] + ' Answer: ' + predictions[key] + \"\\n\")\n",
    "    data.close()\n",
    "\n",
    "\n",
    "# Run method\n",
    "def small_sample():\n",
    "  for context in contexts:\n",
    "    predictions = run_prediction(questions, context)\n",
    "    # Print results\n",
    "    data.write(\"\\n\")\n",
    "    data.write(context + \"\\n\")\n",
    "    for key in predictions.keys():\n",
    "      data.write(questions[int(key)] + ' Answer: ' + predictions[key] + \"\\n\")\n",
    "  data.close()\n",
    "# run_all_snippets(researchPaper, data2)\n",
    "small_sample()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EXR9iATWahci"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cdb_qlV6aeI_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rkivu8FOqp_8"
   },
   "source": [
    "## 5.0 Next Steps\n",
    "\n",
    "In this tutorial, you learnt how to fine-tune an ALBERT model for the task of question answering, using the SQuAD dataset. Then, you learnt how you can make predictions using the model. \n",
    "\n",
    "We retrofitted `compute_predictions_logits` to make the prediction for the purpose of simplicity and minimising dependencies in the tutorial. Take a peak inside that module to see how it works. If you want to serve this as an API, you will want to strip out a lot of the stuff it's doing (such as writing the predictions to a JSON, etc)\n",
    "\n",
    "You can now turn this into an API by serving it using a web framework. I recommend checking out FastAPI, which is what [Albert Learns to Read](https://littlealbert.now.sh) is built on. \n",
    "\n",
    "Feel free to open an issue in the [Github respository](https://github.com/spark-ming/albert-qa-demo/) for this notebook, or tweet me @techno246 if you have any questions! \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "UHCuzhPptH0M",
    "OaQGsAiWXcnd"
   ],
   "machine_shape": "hm",
   "name": "Question_Answering_Top_Snippets_Albert.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "61fcf5796a9647a7b5b0749b73a8f442": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "7b76064c5b7f4d8293b6351d16f83888": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9c44b6b2fa3145c3bf812af364884e40",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_a64796515dd9461c9a8200064f0af27c",
      "value": "100% 235M/235M [00:04&lt;00:00, 48.1MB/s]"
     }
    },
    "7c9061a64a3a4f739bf138d6c358234a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9c44b6b2fa3145c3bf812af364884e40": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a55e529f7288442fb74dc1c78411e2d0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ffd05ad915554f55bb868218b0dc0090",
      "max": 234922444,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_61fcf5796a9647a7b5b0749b73a8f442",
      "value": 234922444
     }
    },
    "a64796515dd9461c9a8200064f0af27c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f5bb7649026d4b8992815d357e785dc4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a55e529f7288442fb74dc1c78411e2d0",
       "IPY_MODEL_7b76064c5b7f4d8293b6351d16f83888"
      ],
      "layout": "IPY_MODEL_7c9061a64a3a4f739bf138d6c358234a"
     }
    },
    "ffd05ad915554f55bb868218b0dc0090": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
