{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from ProcessText.ipynb\n",
      "['HTMLConverter', 'LAParams', 'PDFPage', 'PDFPageInterpreter', 'PDFResourceManager', 'TextConverter', 'XMLConverter', '__builtins__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', 'get_ipython', 'glob', 'io', 'os', 'pdfToText', 're', 'remove_intext_citations', 'remove_references', 'replace_to_word', 'replace_words', 'snippetProducerSplit', 'snippetToCsv', 'split_sentences', 'sys']\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "import ProcessText\n",
    "import glob\n",
    "import os\n",
    "print(dir(ProcessText))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdfdir = 'C:/Users/Evan/Documents/7180QueryTool/Data/'\n",
    "# txtdir = 'C:/Users/Evan/Documents/7180QueryTool/DataTxt/'\n",
    "txtdir = \"C:/Users/God/git/CS7800/7180QueryTool/DataTxt/\"\n",
    "#csvdir = \"C:/Users/God/git/CS7800/7180QueryTool/DataCsv/\"\n",
    "csvdir = 'C:/Users/Evan/Documents/7180QueryTool/DataCsv/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: %s Which model is best for large batch optimization of bert?\n"
     ]
    }
   ],
   "source": [
    "# Basic user interface; obtain query\n",
    "query = \"Which model is best for large batch optimization of bert?\"\n",
    "print(\"Query: %s\", query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape papers to pdf folder\n",
    "%run -i arxiv_pdf_scraper 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert pdfs to text\n",
    "for pdfPaperName in glob.glob(pdfdir + \"*.pdf\"):\n",
    "    ProcessText.pdfToText(pdfPaperName, txtdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists.\n",
      "File already exists.\n",
      "File already exists.\n",
      "File already exists.\n",
      "File already exists.\n",
      "File already exists.\n",
      "File already exists.\n",
      "File already exists.\n",
      "File already exists.\n"
     ]
    }
   ],
   "source": [
    "# Convert text files to CSV snippets\n",
    "sentenceNum = 3\n",
    "for txtPaperName in glob.glob(txtdir + \"*.txt\"):\n",
    "    ProcessText.snippetToCsv(txtPaperName, sentenceNum, csvdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from RelevantSnippets.ipynb\n",
      "[['Inspired by the recent meta-learning strategy ', ' we learn to infer the structure relations from a training set to a validation set', ' which can benefit the generalization capability of the learned model.In other words', ' our proposed GIL attempts to learn some transferable knowledge underlying in the structure relations from training samples to validation samples', ' such that the learned structure relations can be better self-adapted to the new testing stage. We summarize the main contributions of this work as three folds:  • We propose a novel graph inference learning framework by building structure relations to infer unknown node labels from those labeled nodes in an end-to-end way.'], ['Although graph CNN based methods have achieved considerable capabilities of graph embedding by optimizing filters', ' they are limited into a conventionally semi-supervised framework and lack of an efficient inference mechanism on graphs.Especially', ' in the case of few-shot learning', ' where a small number of training nodes are labeled', ' this kind of methods would drastically compromise the performance.For example', ' the Pubmed graph dataset  consists  ∗Corresponding author: Zhen Cui.'], ['The large gain of using the relation module fr (i.e.', ' from 81.5% to 85.0%) may be contributed to the ability of inference learning on attributes as well as local topology structures which are implicitly encoded in fe.The path information fP can further boost the performance by 1.2%', ' e.g.', ' 86.2% vs 85.0%.It demonstrates that three different modules of our method can improve the graph inference learning capability.'], ['In the standard protocol  3  \\x0cPublished as a conference paper at ICLR 2020  of prior literatures ', ' the three node sets share the same label space.We follow but do not restrict this protocol for our proposed method.Given the training and validation node sets', ' the aim is to predict the node labels of testing nodes by using node attributes as well as edge connections.'], ['The dropout rate is set to 0.5 in the convolution and fully connected layers to avoid over-fitting', ' and the ReLU unit is leveraged as a nonlinear activation function.We pre-train our proposed GIL model for 200 iterations with the training set', ' where its initial learning rate', ' decay factor', ' and momentum are set to 0.05', ' 0.95', ' and 0.9', ' respectively.Here we train the GIL model using the stochastic gradient descent method with the batch size of 100.'], ['This indicates that our proposed GIL can better optimize structure relations and thus improve the network generalization.We further compare our proposed GIL with several existing deep graph embedding methods', ' including graph attention network ', ' dual graph convolutional networks ', ' topology adaptive graph convolutional networks ', ' Multi-scale graph convolution ', ' etc.For example', ' our proposed GIL achieves a very large gain', ' e.g.', ' 86.2% vs 83.3%  on the Cora dataset', ' and 78.9% vs 66.0%  on the NELL dataset.'], ['The structure relations are well defined by jointly considering node attributes', ' between-node paths', ' and graph topological structures. • To make the inference model better generalize to test nodes', ' we introduce a meta-learning procedure to optimize structure relations', ' which could be the first time for graph node classification to the best of our knowledge. • Comprehensive evaluations on three citation network datasets (including Cora', ' Citeseer', ' and Pubmed) and one knowledge graph data (i.e.', ' NELL) demonstrate the superiority of our proposed GIL in contrast with other state-of-the-art methods on the semi-supervised classification task.'], ['To bridge the connection between two nodes', ' we formally define a structure relation by encapsulating node attributes', ' between-node paths', ' and local topological structures together', ' which can make the inference conveniently deduced from one node to another node.For learning the inference process', ' we further introduce meta-optimization on structure relations from training nodes to validation nodes', ' such that the learnt graph inference capability can be better self-adapted to testing nodes.Comprehensive evaluations on four benchmark datasets (including Cora', ' Citeseer', ' Pubmed', ' and NELL) demonstrate the superiority of our proposed GIL when compared against state-of-the-art methods on the semi-supervised node classification task.'], ['These aforementioned works usually boil down to a general classification task', ' where the model is learnt on a training set and selected by checking a validation set.However', ' they do not put great efforts on how to learn to infer from one node to another node on a topological graph', ' especially in the few-shot regime. In this paper', ' we propose a graph inference learning (GIL) framework to teach the model itself to adaptively infer from reference labeled nodes to those query unlabeled nodes', ' and finally boost the performance of semi-supervised node classification in the case of a few number of labeled samples.'], ['Classification errors are rapidly decreasing as the number of iterations increases from the beginning to 400 iterations', ' while they are with a slow descent from 400 iterations to 1200 iterations.It demonstrates that the learned knowledge from the training samples can be transferred for inferring node category information from these reference labeled nodes.The performance of semi-supervised classification can be further increased by improving the generalized capability of the Graph CNN model.'], ['In the testing stage', ' we may take all training nodes and perform the model update according to Eqn (6) like the training process.The updated model is used as the final model and is then fed into Eqn (2) to infer the class labels for those query nodes. 4 MODULES In this section', ' we instantiate all modules (i.e.', ' functions) of the aforementioned structure relation.'], ['In other words', ' the validation set is only used to teach the model itself how to transfer to unseen data.In contrast', ' the conventional methods often employ a validation set to tune parameters of a certain model of interest. Table 3: Performance comparisons with several GIL variants and the classical GCN method on the Cora dataset.'], ['In the extreme case', ' labels of unlabeled nodes could be determined by those neighbors with the 1 ∼ 2 step reachability.In summary', ' our proposed GIL method prefers small ratio labeled nodes on the semi-supervised node classification task. Inference learning process: Classification errors of different epochs on the validation set of the Cora dataset can be illustrated in Fig 3.'], ['To address these issues', ' we introduce a meta-learning mechanism (Finn et al', ' 2017; Ravi & Larochelle', ' 2017; Sung et al', ' 2017) to learn to infer node labels on graphs.Specifically', ' the graph structure', ' between-node path reachability', ' and node attributes are jointly modeled into the learning process.Our aim is to learn to infer from labeled nodes to unlabeled nodes', ' so that the learner can perform better on a validation set and thus classify a testing set more accurately.'], [' Inﬂuence of different between-node steps: We compare the classification performance within different between-node steps for our proposed GIL and GCN ', ' as illustrated in Fig 2(a).The length of between-node steps can be computed with the shortest path between reference nodes and query nodes.When the step between nodes is smaller', ' both GIL and GCN methods can predict the category information for a small part of unlabeled nodes in the testing set.']]\n",
      "[['Increasing the batch size is able to warm-up and stabilize the optimization process ', ' but decreasing the batch size brings chaos to the optimization process and can cause divergence.In our experiments', ' we found a technique that is useful to stabilize the second stage optimization.Because we switched to a different optimization problem', ' it is necessary to re-warm-up the optimization.'], ['Using LARS', ' ResNet-50 can be trained on ImageNet in just a few minutes!However', ' it has been observed that its performance gains are not consistent across tasks.For instance', ' LARS performs poorly for attention models like BERT.'], ['We also provided theoretical analysis for the LAMB optimizer', ' highlighting the cases where it performs better than standard SGD.LAMB achieves a better performance than existing optimizers for a wide range of applications.By using LAMB', ' we are able to scale the batch size of BERT pre-training to 64K without losing accuracy', ' thereby', ' reducing the BERT training time from 3 days to around 76 minutes.'], ['To the best of our knowledge', ' ours is first adaptive solver that can achieve state-of-the-art accuracy for RESNET-50 as adaptive solvers like Adam fail to obtain the accuracy of SGD with momentum for these tasks. 1.1 RELATED WORK  The literature on optimization for machine learning is vast and hence', ' we restrict our attention to the most relevant works here.Earlier works on large batch optimization for machine learning mostly focused on convex models', ' benefiting by a factor of square root of batch size using appropriately large learning rate.'], ['Using LR warm-up and linear scaling', ' Goyal et al (2017) managed to train RESNET-50 with batch size 8192 without loss in generalization performance.However', ' empirical study  shows that learning rate scaling heuristics with the batch size do not hold across all problems or across all batch sizes. More recently', ' to reduce hand-tuning of hyperparameters', ' adaptive learning rates for large batch training garnered significant interests.'], ['More specifically', ' we make the following main contributions in this paper. • Inspired by LARS', ' we investigate a general adaptation strategy specially catered to large  batch learning and provide intuition for the strategy. • Based on the adaptation strategy', ' we develop a new optimization algorithm (LAMB) for achieving adaptivity of learning rate in SGD.'], ['We use the square root of LR scaling rule to automatically adjust learning rate and linear-epoch warmup scheduling.We use TPUv3 in all the experiments.A TPUv3 Pod has 1024 chips and can provide more than 100 petaﬂops performance for mixed precision computing.'], ['There has been recent surge in interest in using large batch stochastic optimization methods to tackle this issue.The most prominent algorithm in this line of research is LARS', ' which by employing layerwise adaptive learning rates trains RESNET on ImageNet in a few minutes.However', ' LARS performs poorly for attention models like BERT', ' indicating that its performance gains are not consistent across tasks.'], ['Furthermore', ' the dependence on L∞ (the maximum of smoothness across dimension) can lead to significantly slow convergence.In the next section', ' we discuss algorithms to circumvent this issue. 3  \\x0cPublished as a conference paper at ICLR 2020  3 ALGORITHMS  In this section', ' we first discuss a general strategy to adapt the learning rate in large batch settings.'], ['While these works demonstrate the feasibility of this strategy for reducing the wall time for training large deep neural networks', ' they also highlight the need for an adaptive learning rate mechanism for large batch learning. Variants of SGD using layerwise adaptive learning rates have been recently proposed to address this problem.The most successful in this line of research is the LARS algorithm ', ' which was initially proposed for training RESNET.'], ['Furthermore', ' we provide convergence analysis for both LARS and LAMB to achieve a stationary point in nonconvex settings.We highlight the benefits of using these methods for large batch settings. • We demonstrate the strong empirical performance of LAMB across several challenging tasks.'], ['The goal of this paper is to investigate and develop optimization techniques to accelerate training large deep neural networks', ' mostly focusing on approaches based on variants of SGD. Methods based on SGD iteratively update the parameters of the model by moving them in a scaled (negative) direction of the gradient calculated on a minibatch.However', ' SGD’s scalability is limited by its inherent sequential nature.'], ['In this paper', ' we first study a principled layerwise adaptation strategy to accelerate training of deep neural networks using large mini-batches.Using this strategy', ' we develop a new layerwise adaptive large batch optimization technique called LAMB; we then provide convergence analysis of LAMB as well as LARS', ' showing convergence to a stationary point in general nonconvex settings.Our empirical results demonstrate the superior performance of LAMB across various tasks such as BERT and RESNET-50 training with very little hyperparameter tuning.'], [' 5  \\x0cPublished as a conference paper at ICLR 2020  4 EXPERIMENTS  We now present empirical results comparing LAMB with existing optimizers on two important large batch training tasks: BERT and RESNET-50 training.We also compare LAMB with existing optimizers for small batch size (< 1K) and small dataset (e.g. CIFAR', ' MNIST) (see Appendix). Experimental Setup.'], ['LARS was earlier proposed for large batch learning for RESNET on ImageNet.In general', ' it is observed that the using (heavy-ball) momentum', ' one can reduce the variance in the stochastic gradients at the cost of little bias.The pseudocode for LARS is provide in Algorithm 1.']]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['A recent work  is a breakthrough to introduce deep learning paradigm into graph matching task', ' which utilizes a neural network to learn the affinity function.The learning procedure is explicitly derived from the factorization of affinity matrix (Zhou & De la Torre', ' 2012)', ' which makes the interpretation of the network behavior possible.However', ' the displacement loss in  measures the pixel-wise transla- tion which is similar to optical-ﬂow ', ' being essentially a regression task instead of combinaotiral optimization.'], ['For the relatively easy linear assignment problem', ' it has been known that Sinkhorn algorithm (Sinkhorn', ' 1964) is the approximate and differentiable version of Hungarian algorithm .The Sinkhorn Network  is de- veloped given known assignment cost', ' whereby doubly-stochastic regulation is performed on input non-negative square matrix.Patrini et al (2018) devise the Sinkhorn AutoEncoder to minimize Wasserstein distance', ' and Emami & Ranka (2018) propose to learning a linear assignment solver via reinforcement learning.'], [' Finally we give a brief qualitative analysis on why Hungarian attention can improve matching loss.As discrete graph matching problem is actually built upon Delta function over permutation vertices (1 at ground-truth matching and 0 otherwise) ', ' learning of graph matching with per- mutation loss is actually to approximate such functions with continuous counterparts.Unfortunately', ' more precise approximation to Delta function will result in higher non-smoothness', ' as discussed in Yu et al (2018).'], ['For permutation prediction', ' DeepPermNet (Santa Cruz et al', ' 2018) adopts the Sinkhorn layer on top of a deep convolutional network.However this method cannot be directly applied for graph matching as it is not invariant to input permutations which is conditioned on a predefined node permutation as reference.In particular', ' existing supervised methods on combi- natorial learning are generally cross-entropy-based.'], ['This fact greatly limits the performance and broad application w.r.t. different problem settings considering its NP-hard nature. +  Recently', ' the seminal work namely deep graph matching (DGM)  is proposed to exploit the high capacity of deep networks for graph matching', ' which achieves state- of-the-art performance.This is in contrast to some early works which incorporate learning strategy  1We assume graphs are of equal size for narrative simplicity.'], ['Though there are some alternatives that deliver discrete solutions by adding more constraints or introducing numerical con- tinuation (Zhou & De la Torre', ' 2012; Yu et al', ' 2018)', ' the main line of methods is to incorporate a sampling procedure (e.g. winner-take-all and Hungarian).Among them', ' the Hungarian algorithm (Kuhn', ' 1955) is a widely adopted', ' for its efficiency and theoretical optimality. However', ' the Hungarian algorithm incurs a gap between training (loss function) and testing stages (Hungarian sampling).'], [' iii) The empirical results on three public benchmarks shows that the two proposed techniques are orthogonal and beneficial to existing techniques.Specifically', ' on the one hand', ' our CIE module can effectively boost the accuracy by exploring the edge attributes which otherwise are not consid- ered in state-of-the-art deep graph matching methods; on the other hand', ' our Hungarian attention mechanism also shows generality and it is complementary to existing graph matching loss. 2 RELATED WORKS  Graph embedding.'], ['In contrast', ' only node embedding is accounted in pre- vious works; ii) a general masking mechanism over the loss function is devised to improve the smoothness of objective learning for graph matching.Using Hungar- ian algorithm', ' it dynamically constructs a structured and sparsely connected layer', ' taking into account the most contributing matching pairs as hard attention.Our approach performs competitively', ' and can also improve state-of-the-art methods as plugin', ' regarding with matching accuracy on three public benchmarks.'], ['While the first setting is for a class-aware situation', ' the second setting is considered for testing the class-agnostic case.Results are shown in Table 3. We see our method surpasses all the competing methods in terms of matching accuracy.'], ['In contrast', ' edge attributes e.g. length and orientation are widely used in traditional graph matching models (Cho et al', ' 2010; Yan et al', ' 2015; Yu et al', ' 2018) for constructing the affinity matrix K. Such a gap shall be filled in the deep graph matching pipeline. Another important consideration refers to the design of loss function.There are mainly two forms in existing deep graph matching works: i) displacement loss  similar to the use in optical ﬂow estimation ; ii) the so-called permutation loss  involving iterative Sinkhorn procedure followed by a cross-entropy loss.'], ['It is similar to the feature transfer strategy in  for sparse correspon- dence', ' which employs a feature merging method analogous to style transfer . As Sinkhorn layer does not necessarily output binary digits', ' we employ Hungarian algorithm (Kuhn', ' 1955) to discretize matching output S in testing.The testing differs from the training due to the Hungarian discretization.'], ['Pointer Net  incorporates cross-entropy loss on learning heuristics for combinatorial problems.Milan et al (2017) propose an objective-based loss', ' where the gradients are only updated if the objective improves after update. Learning for graph matching.'], [' ii) We devise a new mechanism to adjust the loss function based on the Hungarian method which is widely used for linear assignment problem', ' as termed by Hungarian attention.It resorts to dynami- cally generating sparse matching mask according to Hungarian sampling during training', ' rather than approximating Hungarian sampling with a differentiable function.As such', ' the Hungarian attention introduces higher smoothness against traditional loss functions to ease the training.'], [' Peer methods.We compare our method with the following selected counterparts: 1) HARG .This shallow learning method is based on hand-crafted feature and Structured SVM; 2) GMN .'], [' 5 CONCLUSION  We have presented a novel and effective approach for learning based graph matching.On one hand', ' the novelty of our method partially lies in the development of the Hungarian attention', ' which in- trinsically adapts the matching problem.It is further observed from the experiments that Hungarian attention can improve several matching-oriented loss functions', ' which might bring about potential for a series of combinatorial problems.']]\n",
      "[[' Caccia et al (2018) recently observed in the context of language modeling using GANs that per- formance gains similar to those GAN yield can be achieved by decreasing the temperature for the prediction softmax (i.e.', ' making it peakier).However', ' they proposed no causes for this effect.Our findings propose an underlying mechanism leading to this trend.'], ['However', ' as Edunov et al (2018) point out', ' this practice may lower results', ' as it may destabilize training by leading the model to improve over outputs it cannot generalize over', ' as they are very different from anything the model assigns a high probability to', ' at the cost of other outputs. 8 CONCLUSION  The standard MT scenario poses several uncommon challenges for RL.First', ' the action space in MT problems is a high-dimensional discrete space (generally in the size of the vocabulary of the target language or the product thereof for sentences).'], [' Third', ' we show that promoting the target token to be the mode is likely to take a prohibitively long time.The only case we find', ' where improvements are likely', ' is where the target token is among the first 2-3 most probable tokens according to the pretrained model.These findings suggest that REINFORCE (§5) and CMRT (§6) are likely to improve over the pre-trained model only under the best possible conditions', ' i.e.', ' where the pre-trained model is “nearly” correct.'], ['The model was pretrained on WMT2015 training data .Hyperparameters are reported in Appendix A.3. We define one of the tokens in V to be the target token and denote it with ybest.We assign deterministic token reward', ' this makes learning easier than when relying on approximations and our predictions optimistic.'], ['We return to this point in §7.Fur- thermore', ' given their findings', ' it is reasonable to assume that our results are relevant for RL use in other generation tasks', ' whose output space too is discrete', ' high-dimensional and concentrated. 4.1 CONTROLLED SIMULATIONS  We experiment with a 1-layer softmax model', ' that predicts a single token i ∈ V with probability eθi j eθjθ = {θj}j∈V are the model’s parameters.'], ['We call this the peakiness-effect (PKE)', ' and show it occurs both in simulations (§4.1) and in full-scale NMT experiments (§4.2). With more iterations', ' the most-rewarding tokens will be eventually sampled', ' and gradually gain probability mass.This discussion suggests that training will be extremely sample-inefficient.'], ['It is possible that RL has managed to push ybest in some cases between very low ranks (<1', '000) to medium-low ranks (between 10 and 1', '000).However', ' token probabilities in these ranks are so low that it is unlikely to affect the system outputs in any way.This fits well with the results of our simulations that predicted that only the initially top-ranked tokens are likely to change.'], ['This contrasts with the more common sce- nario studied by contemporary RL methods', ' which focuses mostly on much smaller discrete action spaces (e.g.', ' video games (Mnih et al', ' 2015; 2016))', ' or continuous action spaces of relatively low dimensions (e.g.', ' simulation of robotic control tasks ).Second', ' reward for MT is naturally very sparse – almost all possible sentences are “wrong” (hence', ' not rewarding) in a given context.Finally', ' it is common in MT to use RL for tuning a pretrained model.'], ['However', ' being the RL procedure of choice in much recent work we repeat the simulations described in §4 and §5', ' assessing CMRT’s performance in these conditions.We experiment with α = 0.005 and k = 20', ' common settings in the literature', ' and average over 100 trials. Figure 5 shows how the distribution Pθ changes over the course of 50K update steps to θ', ' where ybest is taken to be the second and third initially most probable token (Simulated Reward setting).'], [' The per-token sampling as done in our experiments is more exploratory than beam search ', ' reducing PKE.Furthermore', ' the latter does not sample from the behavior policy', ' but does not properly account for being off-policy in the parameter updates. Adding the reference to the sample S', ' which some implementations allow  may help reduce the problems of never sampling the target tokens.'], ['However', ' not much is known about its convergence rate under the prevailing conditions in NMT. We begin with a qualitative', ' motivating analysis of these questions.As work on language generation empirically showed', ' RNNs quickly learn to output very peaky distributions .'], ['This tradeoff explains the importance of tuning α reported in the literature.In §6 we present simulations with CMRT', ' showing very similar trends as presented by REINFORCE. k  3 MOTIVATING DISCUSSION  Implementing a stochastic gradient ascent', ' REINFORCE is guaranteed to converge to a stationary point of R under broad conditions.'], ['In principle', ' such methods allow learning from a more “exploratory” policy.Moreover', ' a key mo- tivation for using α in CMRT is smoothing; off-policy sampling allows smoothing while keeping convergence guarantees. In its basic form', ' exploration in REINFORCE relies on stochasticity in the action-selection (in MT', ' this is due to sampling).'], ['As in the previous section', ' we use both controlled simulations and NMT experiments. 5.1 CONTROLLED SIMULATIONS  We use the same model and experimental setup described in Section 4.1', ' this time only exploring the Simulated Reward setting', ' as a Constant Reward is not expected to converge to any meaningful θ.Results are averaged over 100 conditional distributions sampled from the pretrained model.'], ['More sophisticated exploration methods have been extensively studied', ' for example using measures for the exploratory usefulness of states or actions ', ' or relying on parameter-space noise rather than action-space noise . For MT', ' an additional challenge is that even effective exploration (sampling diverse sets of obser- vations)', ' may not be enough', ' since the state-action space is too large to be effectively covered', ' with almost all sentences being not rewarding.Recently', ' diversity-based and multi-goal methods for RL were proposed to tackle similar challenges (Andrychowicz et al', ' 2017; Ghosh et al', ' 2018; Eysen- bach et al', ' 2019).']]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Our Graph2Seq model is based on a novel bidirectional gated graph neural network', ' which extends the original gated graph neural network  by considering both incoming and outgoing edges', ' and fusing them during the graph embedding learning.To achieve the second goal', ' we design a hybrid evaluator which is trained by optimizing a mixed objective function that combines both cross-entropy and RL loss.We use not only discrete evaluation metrics like BLEU', ' but also semantic metrics like word mover’s distance  to encourage both syntactically and semantically valid text generation.'], ['On the one hand', ' an advantage of static graph construc- tion is that useful domain knowledge can be hard-coded into the graph', ' which can greatly benefit the downstream task.However', ' it might suffer if there is a lack of prior knowledge for a specific domain knowledge.On the other hand', ' dynamic graph construction does not need any prior knowledge about the hidden structure of text', ' and only relies on the attention matrix to capture these structured information', ' which provides an easy way to achieve a decent performance.'], ['Although there are early attempts on constructing a graph from a sentence (Xu et al', ' 2018b)', ' there is no clear an- swer as to the best way of representing text as a graph.We explore both static and dynamic graph construction approaches', ' and systematically investigate the performance differences between these two methods in the experimental section. Syntax-based static graph construction: We construct a directed and unweighted passage graph based on dependency parsing.'], ['The loss function is defined as: log P pys  Lrl “ prp ˆY q ´ rpY sqq  (9)  ÿ  t |X', ' ys  ătq  t  As we can see', ' if the sampled output has a higher reward than the baseline one', ' we maximize its likelihood', ' and vice versa. One of the key factors for RL is to pick the proper reward function.To take syntactic and semantic constraints into account', ' we consider the following metrics as our reward functions:  Evaluation metric as reward function: We use one of our evaluation metrics', ' BLEU-4', ' as our reward function feval', ' which lets us directly optimize the model towards the evaluation metrics.'], ['We further use a mixed objective function with both syntactic and semantic constraints for guiding text generation.In particular', ' we present a hybrid evaluator with a mixed objective function that combines both cross-entropy loss and RL loss in order to ensure the generation of syntactically and semantically valid text. For the RL part', ' we employ the self-critical sequence training (SCST) algorithm  to directly optimize the evaluation metrics.'], ['In our experiments', ' we also observe that doing both forward and backward message passing in the GNN encoder is beneficial.Surprisingly', ' using GCN  as the graph en- coder (and converting the input graph to an undirected graph) does not provide good performance.In addition', ' fine-tuning the model using REINFORCE can further improve the model performance in all settings (i.e.', ' w/ and w/o BERT)', ' which shows the benefits of directly optimizing the evalu- ation metrics.'], ['Lastly', ' it shows that fine-tuning the model using REINFORCE can improve the quality of the generated questions. 4 RELATED WORK  4.1 NATURAL QUESTION GENERATION  Early works (Mostow & Chen', ' 2009; Heilman & Smith', ' 2010; Heilman', ' 2011) for QG focused on rule-based approaches that rely on heuristic rules or hand-crafted templates', ' with low generaliz- ability and scalability.Recent attempts have focused on NN-based approaches that do not require  9  \\x0cUnder review as a conference paper at ICLR 2020  manually-designed rules and are end-to-end trainable.'], ['Under review as a conference paper at ICLR 2020  REINFORCEMENT LEARNING BASED GRAPH-TO-SEQUENCE MODEL FOR NATURAL QUESTION GENERATION  Anonymous authors Paper under double-blind review  ABSTRACT  Natural question generation (QG) aims to generate questions from a passage and an answer.Previous works on QG either (i) ignore the rich structure informa- tion hidden in text', ' (ii) solely rely on cross-entropy loss that leads to issues like exposure bias and inconsistency between train/test measurement', ' or (iii) fail to fully exploit the answer information.To address these limitations', ' in this paper', ' we propose a reinforcement learning (RL) based graph-to-sequence (Graph2Seq) model for QG.'], [' 2.2 DEEP ALIGNMENT NETWORK  Answer information is crucial for generating relevant and high quality questions from a passage.Un- like previous methods that neglect potential semantic relations between passage and answer words', ' we explicitly model the global interactions among them in the embedding space.To this end', ' we propose a novel Deep Alignment Network (DAN) component for effectively incorporating answer information into the passage with multiple granularity levels.'], [' However', ' the existing approaches for QG suffer from several limitations; they (i) ignore the rich structure information hidden in text', ' (ii) solely rely on cross-entropy loss that leads to issues like exposure bias and inconsistency between train/test measurement', ' and (iii) fail to fully exploit the answer information.To address these limitations', ' we propose a reinforcement learning (RL) based graph-to-sequence (Graph2Seq) model for QG as well as deep alignment networks to effectively cope with the QG task.To the best of our knowledge', ' we are the first to introduce the Graph2Seq architecture to solve the question generation task.'], ['To achieve the third goal', ' we propose a novel Deep Alignment Network (DAN) for effectively incorporating answer information into the passage at multiple granularity levels. Our main contributions are as follows:  • We propose a novel RL-based Graph2Seq model for natural question generation.To the  best of our knowledge', ' we are the first to introduce the Graph2Seq architecture for QG.'], ['As a result', ' they do not always produce the best results on discrete evaluation metrics on sequence generation tasks such as text summerization  or question gener- ation .To cope with these issues', ' some recent QG approaches (Song et al', ' 2017; Kumar et al', ' 2018b) directly optimize evaluation metrics using Reinforcement Learning  1  \\x0cUnder review as a conference paper at ICLR 2020  (RL) (Williams', ' 1992).However', ' existing approaches usually only employ evaluation metrics like BLEU and ROUGE-L as rewards for RL optimization.'], ['As we can see', ' incorporating answer information helps the model identify the answer type of the question to be generated', ' and thus makes the generated ques- tions more relevant and specific.Also', ' we find our Graph2Seq model can generate more complete and valid questions compared to the Seq2Seq baseline.We think it is because a Graph2Seq model is able to exploit the rich text structure information better than a Seq2Seq model.'], ['Besides', ' we find that the pretrained BERT embedding has a considerable impact on the performance and fine-tuning BERT embedding even further improves the performance', ' which demonstrates the power of large-scale pretrained language models. 3.5 CASE STUDY  Table 4: Generated questions on SQuAD split-2 test set.Target answers are underlined.'], ['Very recently', ' researchers have started exploring methods to automat- ically construct a graph of visual objects  or words (Liu et al', ' 2018; Chen et al', ' 2019b) when applying GNNs to non-graph structured data. To the best of our knowledge', ' we are the first to investigate systematically the performance difference between syntactic-aware static graph construction and semantics-aware dynamic graph construction in the context of question generation. 5 CONCLUSION  We proposed a novel RL based Graph2Seq model for QG', ' where the answer information is utilized by an effective Deep Alignment Network and a novel bidirectional GNN is proposed to process the directed passage graph.']]\n",
      "[['Concretely', ' we employ the semi-supervised technique as a backbone to our framework to stabilize the learning process of the model.Correctly', ' we maintain the running average model', ' such as proposed by Tarvainen & Valpola (2017)', ' a.k.a. the Mean-Teacher model.This model ensemble learning provides a more stable supervisory signal than the noisy model snapshots and provides a stable ground for progressive filtering to filter out potential noisy labels.'], ['Once the best model is found', ' these predictions identify and filter out noisy labels using the original label set L0.The model performs this progressive filtering until there is no more better model.For details see Algorithm 1.'], ['In practice', ' our framework does not use ˆy(x) of model snapshots for filtering but a moving-average of the ensemble models and predictions to improve the filtering decision. 2.3 SELF-ENSEMBLE LEARNING  The model’s predictions for noisy samples tend to ﬂuctuate.For example', ' take a cat wrongly labeled as a tiger.'], [' 2 SELF-ENSEMBLE LABEL FILTERING  2.1 OVERVIEW  Fig 2 shows an overview of our proposed approach.In the beginning', ' we assume that the labels of the training set are noisy.The model attempts to identify correct labels progressively using self- forming ensembles of models and predictions.'], ['Note that this is different from just a mere combination of semi-supervised techniques with a noisy label filtering method. We call our approach self-ensemble label filtering (SELF) - that establishes model ensemble learning as a backbone to form a solid consensus of the self-ensemble predictions to filter out the noisy labels progressively.Our framework allows to compute supervised loss on cleaner subsets rather than the entire noisy labeled data as in previous works.'], ['On the new filtered dataset', ' the model must only slowly adapt to the new noise ratio contained in the training set.Depending on the computation budget', ' a maximal number of iterations for filtering can be set to save time. 3 RELATED WORKS  Reed et al (2014); Azadi et al (2015) performed early works on learning robustly under label noise for deep neural networks.'], ['On the more challenging CIFAR-100', ' however', ' the performance decreases strongly.With self-supervised filter- ing and model ensembles', ' SELF (without MVA-pred) is more robust and only impairs performance at 80% noise.The last performance boost is given by using moving-average predictions so that the network can reliably detect correctly labeled samples gradually.'], ['This result indicates that their proposed frameworks are not always compatible with semi-supervised losses. The progressive filtering technique is seamlessly compatible with different semi-supervised losses.The filtering outperforms its counterparts when combined with Entropy Learning or Mean-teacher model.'], ['Recently', ' Rolnick et al (2017) have shown for classification that deep neural networks come with natural robustness to label noise following a particular random distribution.No modification of the network or the training procedure is required to achieve this robustness.Following this insight', ' our framework SELF relies on this natural robustness to kickstart the self-ensemble filtering process to extend the robust behavior to more challenging scenarios.'], ['Further', ' we abandon oracle experiments or methods using additional information to keep the evaluation comparable.For instance', ' Forward T  uses the true underlying confusion matrix to correct the loss.This information is neither known in typical scenarios nor used by other methods.'], ['Adding self-supervised filtering increases the performance slightly in lower noise ratios.However', ' the model has to rely on extremely noisy snapshots.Contrary', ' using a model ensemble alone such as in Mean-Teacher can counteract noise on the noisy dataset CIFAR-10.'], ['We demonstrate the positive effect of such an approach on var- ious image classification tasks under both symmetric and asymmetric label noise and at different noise ratios.It substantially outperforms all previous works on noise-aware learning across different datasets and can be applied to a broad set of network architectures. 1  INTRODUCTION  The acquisition of large quantities of a high-quality human annotation is a frequent bottleneck in applying DNNs.'], ['Typical robust learning approaches lead to significant accuracy losses at 40% noise', ' while SELF still retains high performance.Further', ' note that SELF allows the network’s performance to remain consistent across the different underlying architectures. 4.2.4 ABLATION STUDY Tab.'], ['The progressive filtering procedure and self-ensemble learning proposed are also applicable in these tasks to counteract noise effectively. 5  \\x0cPublished as a conference paper at ICLR 2020  Table 1: Comparison of classification accuracy when learning under uniform label noise on CIFAR- 10 and CIFAR-100.Following previous works', ' we compare two evaluation scenarios: with a noisy validation set (top) and with 1000 clean validation samples (bottom).'], ['Overall', ' SELF outperforms all considered combinations. 9  \\x0cPublished as a conference paper at ICLR 2020  5 CONCLUSION  We propose a simple and easy to implement a framework to train robust deep learning models under incorrect or noisy labels.We filter out the training samples that are hard to learn (possibly noisy labeled samples) by leveraging ensemble of predictions of the single network’s output over different training epochs.']]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['However', ' despite the high representational capacity of DL models', ' the extraction of good features remains challenging.For instance', ' the performance of the learning process can degrade when unrelated tasks are used together (Caruana', ' 1997; Baxter', ' 2000); another detrimental issue may occur when the training of a single model is not balanced properly among multiple tasks . Recent developments in MTRL achieve significant results in feature extraction by means of algorithms specifically developed to address these issues.'], ['We prove this by providing theoretical guarantees that highlight the conditions for which is convenient to share representations among tasks', ' extending the well- known finite-time bounds of Approximate Value-Iteration to the multi-task setting.In addition', ' we complement our analysis by proposing multi-task extensions of three Reinforcement Learning algorithms that we empirically evaluate on widely used Reinforcement Learning benchmarks showing significant improvements over the single-task counterparts in terms of sample efficiency and performance. 1  INTRODUCTION  Multi-Task Learning (MTL) ambitiously aims to learn multiple tasks jointly instead of learning them separately', ' leveraging the assumption that the considered tasks have common properties which can be exploited by Machine Learning (ML) models to generalize the learning of each of them.'], [' the simplifying assumption that the samples are i.i.d.; nevertheless they are useful to show the benefit of our method also in complex scenarios', ' e.g. MuJoCo .We remark that in these experiments we are only interested in showing the benefit of learning multiple tasks with a shared representation w.r.t. learning a single task; therefore', ' we only compare our methods with the single task counterparts', ' ignoring other works on MTRL in literature.Experiments have been developed using the MushroomRL library (D’Eramo et al', ' 2020)', ' and run on an NVIDIA R(cid:13) DGX StationTM and Intel R(cid:13) AI DevCloud.'], ['Despite being an extension of previous works', ' we derive these results to justify our approach showing how the error propagation in AVI/API can theoretically benefit from learning multiple tasks jointly. 2.We leverage these results proposing a neural network architecture', ' for which these bounds hold with minor assumptions', ' that allow us to learn multiple tasks with a single regressor extracting a common representation.'], ['Remarkably', ' as we show in the experimental Section 5', ' it is straightforward to extend RL algorithms to their multi-task variants only through the use of the proposed network architecture', ' without major changes to the algorithms themselves. 5 EXPERIMENTAL RESULTS  To empirically evince the effect described by our bounds', ' we propose an extension of FQI (Ernst et al', ' 2005; Riedmiller', ' 2005)', ' that we call MFQI', ' for which our AVI bounds apply.Then', ' to empirically evaluate our approach in challenging RL problems', ' we introduce multi-task variants of two well-known DRL algorithms: DQN  and DDPG ', ' which we call Multi Deep Q-Network (MDQN) and Multi Deep Deterministic Policy Gradient (MDDPG) respectively.'], ['This introduces a tradeoff between the number of features and number of tasks; however', ' for  5  \\x0cPublished as a conference paper at ICLR 2020  (a) Shared network  (b) FQI vs MFQI  (c) #Task analysis  Figure 1: (a) The architecture of the neural network we propose to learn T tasks simultaneously.The wt block maps each input xt from task µt to a shared set of layers h which extracts a common representation of the tasks.Eventually', ' the shared representation is specialized in block ft and the output yt of the network is computed.'], ['Furthermore', ' a step of the algorithm consists of exactly one step in each task.These are the only minor changes to the vanilla DQN algorithm we introduce', ' while all other aspects', ' such as the use of the target network', ' are not modified.Thus', ' the time complexity of MDQN is considerably lower than vanilla DQN thanks to the learning of T tasks with a single model', ' but at the cost of a higher memory complexity for the collection of samples for each task.'], [' √  1.We derive upper confidence bounds for Approximate Value-Iteration (AVI) and Approximate Policy-Iteration (API)2 (Farahmand', ' 2011) in the MTRL setting', ' and we extend the approx- imation error bounds in Maurer et al (2016) to the case of multiple tasks with different dimensionalities.Then', ' we show how to combine these results resulting in', ' to the best of our knowledge', ' the first proposed extension of the finite-time bounds of AVI/API to MTRL.'], ['In particular', ' the theoretical analysis we provide follows previous results about the theoretical properties of multi-task algorithms.For instance', ' Cavallanti et al (2010) and Maurer (2006) prove the theoretical advantages of MTL based on linear approximation.More in detail', ' Maurer (2006) derives bounds on MTL when a linear approximator is used to extract a shared representation among tasks.'], ['We show an empirical evidence of the consequence of our bounds by means of a variant of Fitted Q-Iteration (FQI) ', ' based on our shared network and for which our bounds apply', ' that we call Multi Fitted Q-Iteration (MFQI).Then', ' we perform an empirical evaluation in challenging RL problems proposing multi- task variants of the Deep Q-Network (DQN)  and Deep Deterministic Policy Gradient (DDPG)  algorithms.These algorithms are practical implementations of the more general AVI/API framework', ' designed to solve complex problems.'], ['Then', ' we run separate instances of FQI with a single task network for each task respectively', ' and one of MFQI considering all the tasks simultaneously.Figure 1(b) shows the L1-norm of the difference between Q∗ and QπK averaged over all the tasks.It is clear how MFQI is able to get much closer to the optimal Q-function', ' thus giving an empirical evidence of the AVI bounds in Theorem 2.'], ['Refer to Appendix B for all the details and our motivations about the experimental settings. 5.1 MULTI FITTED Q-ITERATION  As a first empirical evaluation', ' we consider FQI', ' as an example of an AVI algorithm', ' to show the effect described by our theoretical AVI bounds in experiments.We consider the Car-On-Hill problem as described in Ernst et al (2005)', ' and select four different tasks from it changing the mass of the car and the value of the actions (details in Appendix B).'], ['This intuitive justification for our approach', ' joins the theoretical benefit proven in Section 3.Note that our architecture can be used in any MTRL problem involving a regression process; indeed', ' it can be easily used in value-based methods as a Q-function regressor', ' or in policy search as a policy regressor.In both cases', ' the targets are learned for each task µt in its respective output block ft.'], [' Final remarks The bound for MTRL is derived by composing the results in Theorems 2 and 3', ' and Lemma 4.The results above highlight the advantage of learning a shared representation.The bound in Theorem 2 shows that a small approximation error is critical to improve the convergence towards the optimal action-value function', ' and the bound in Theorem 3 shows that the cost of learning the shared representation at each AVI iteration is mitigated by using multiple tasks.'], ['We have derived our results extending the AVI/API bounds (Farah- mand', ' 2011) to MTRL', ' leveraging the upper bounds on the approximation error in MTL provided in Maurer et al (2016).The results of this analysis show that the error propagation during the AVI/API iterations is reduced according to the number of tasks.Then', ' we proposed a practical way of exploiting this theoretical benefit which consists in an effective way of extracting shared representa- tions of multiple tasks by means of deep neural networks.']]\n",
      "[[' Figure 1: Average reward on 100 episodes after training.Standard error on three random seeds. −68 ± 4 698 ± 10 −66 ± 8 704 ± 6 704 ± 79  train 0 S  4 For all the image-based tasks', ' we implement a version of GAIL that uses deep Q-learning (GAIL-DQL) instead of TRPO as in the original GAIL paper ', ' since Q-learning performs better than TRPO in these environments', ' and because this allows for a head-to-head comparison of SQIL and GAIL: both algorithms use the same underlying RL algorithm', ' but provide the agent with different rewards – SQIL provides constant rewards', ' while GAIL provides learned rewards.'], [' D  3  INTERPRETING SQIL AS REGULARIZED BEHAVIORAL CLONING  To understand why SQIL works', ' we sketch a surprising theoretical result: SQIL is equivalent to a variant of behavioral cloning (BC) that uses regularization to overcome state distribution shift. BC is a simple approach that seeks to imitate the expert’s actions using supervised learning – in particular', ' greedily maximizing the conditional likelihood of the demonstrated actions given the demonstrated states', ' without reasoning about the consequences of actions.Thus', ' when the agent makes small mistakes and enters states that are slightly different from those in the demonstrations', ' the distribution mismatch between the states in the demonstrations and those actually encountered by the agent leads to compounding errors .'], ['In the fourth condition', ' we use RBC to optimize the loss in Equation 8. instead of using SQIL to optimize the loss in line 4 of Algorithm 1.This comparison tests the effect of the additional V (s0) term in RBC vs.SQIL (see Equation 9).'], ['Section A.3 in the appendix contains additional implementation details', ' including values for the hyperparameter λsamp; note that the simple default value of λsamp = 1 works well across a variety of environments. As the imitation policy in line 5 of Algorithm 1 learns to behave more like the expert', ' a growing number of expert-like transitions get added to the buffer samp with an assigned reward of zero.This causes the effective reward for mimicking the expert to decay over time.'], ['We show that SQIL is equivalent to augmenting BC with a regularization term that incorporates information about the state transition dynamics into the imitation policy', ' and thus enables long-horizon imitation. 3.1 PRELIMINARIES  Maximum entropy model of expert behavior.SQIL is built on soft Q-learning', ' which assumes that expert behavior follows the maximum entropy model (Ziebart et al', ' 2010; Levine', ' 2018).'], ['We run experiments in four image-based environments – Car Racing', ' Pong', ' Breakout', ' and Space Invaders – and three low-dimensional environments – Humanoid', ' HalfCheetah', ' and Lunar Lander – to compare SQIL to two prior methods: BC and GAIL.We find that SQIL outperforms BC and achieves com- petitive results compared to GAIL.Our experiments illustrate two key benefits of SQIL: (1) that it can overcome the state distribution shift problem of BC without adversarial training or learning a re- ward function', ' which makes it easier to use', ' e.g.', ' with images', ' and (2) that it is simple to implement using existing Q-learning or off-policy actor-critic algorithms.'], [' train 0 S  In the first variant of SQIL', ' λsamp is set to zero', ' to prevent SQIL from using additional samples drawn from the environment (see line 4 of Algorithm 1).This comparison tests if SQIL really needs to interact with the environment', ' or if it can rely solely on the demonstrations.In the second condition', ' γ is set to zero to prevent SQIL from accessing information about state transitions (see Equation 1 and line 4 of Algorithm 1).'], ['Empirically', ' we show that SQIL out- performs BC and achieves competitive results compared to GAIL', ' on a variety of image-based and low-dimensional tasks in Box2D', ' Atari', ' and MuJoCo.This pa- per is a proof of concept that illustrates how a simple imitation method based on RL with constant rewards can be as effective as more complex methods that use learned rewards. 1  INTRODUCTION  Many sequential decision-making problems can be tackled by imitation learning: an expert demon- strates near-optimal behavior to an agent', ' and the agent attempts to replicate that behavior in novel situations .'], ['The reward function in SQIL also has a clear connection to the sparsity prior in RBC: SQIL imposes the sparsity prior from RBC', ' by training the agent with an extremely sparse reward function – r = +1 at the demonstrations', ' and r = 0 everywhere else.Thus', ' SQIL can be motivated as a practical way to implement the ideas for regularizing BC proposed in Piot et al (2014). The main benefit of using SQIL instead of RBC is that SQIL is trivial to implement', ' since it only requires a few small changes to any standard Q-learning implementation (see Section 2).'], ['We use the standard GAIL-TRPO method as a baseline for all the low-dimensional tasks', ' since TRPO performs better than Q-learning in these environments.The original GAIL method implicitly encodes prior knowledge – namely', ' that terminating an episode is either always desirable or always undesirable.As pointed out in Kostrikov et al (2019)', ' this makes comparisons to alternative methods unfair.'], ['Thus', ' our method can be applied in environments with stochastic dynamics and continuous states', ' where the demonstrated states are not necessarily reachable by the agent.We call this method soft Q imitation learning (SQIL). The main contribution of this paper is SQIL: a simple and general imitation learning algorithm that is effective in MDPs with high-dimensional', ' continuous observations and unknown dynamics.'], ['We accomplish this by giving the agent a constant reward of r = +1 for matching the demonstrated action in a demonstrated state', ' and a constant reward of r = 0 for all other behavior.Our method', ' which we call soft Q imitation learn- ing (SQIL)', ' can be implemented with a handful of minor modifications to any standard Q-learning or off-policy actor-critic algorithm.Theoretically', ' we show that SQIL can be interpreted as a regularized variant of BC that uses a sparsity prior to encourage long-horizon imitation.'], ['In Section 3.3', ' we show that this approach recovers an algorithm similar to SQIL. D  3.2 REGULARIZED BEHAVIORAL CLONING  Under the maximum entropy model described in Section 3.1', ' expert behavior is driven by a reward function', ' a soft Q function that computes expected future returns', ' and a policy that takes actions with high soft Q values.In the previous section', ' we used these assumptions to represent the imitation policy in terms of a model of the soft Q function Qθ (Equation 5).'], ['Extending SQIL to MDPs with a continuous action space is also easy', ' since we can simply replace Q-learning  5  \\x0cPublished as a conference paper at ICLR 2020  with an off-policy actor-critic method  (see Section 4.3).Given the difficulty of implementing deep RL algorithms correctly ', ' this ﬂexibility makes SQIL more practical to use', ' since it can be built on top of existing implementations of deep RL algorithms.Furthermore', ' the ablation study in Section 4.4 suggests that SQIL actually performs better than RBC.'], [' Summary.We contribute the SQIL algorithm: a general method for learning to imitate an expert given action demonstrations and access to the environment.Simulation experiments on tasks with high-dimensional', ' continuous observations and unknown dynamics show that our method outper- forms BC and achieves competitive results compared to GAIL', ' while being simple to implement on top of existing off-policy RL algorithms.']]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[' 2.2 GRAPH POOLING  Several advanced pooling techniques are proposed recently for graph models', ' such as SORTPOOL', ' TOPKPOOL', ' DIFFPOOL', ' and SAGPOOL', ' and achieve great performance on multiple benchmark datasets.All of SORTPOOL ', ' TOPKPOOL (Gao & Ji', ' 2019a)', ' and SAG- POOL  learn to select important nodes from the original graph and use these nodes to build a new graph.They share the similar idea to learn a sorting vector based on node representa- tions using GCNs', ' which indicates the importance of different nodes.'], ['All these methods are proposed for node classification tasks and the CRFs are incorporated in different ways.Different from existing work', ' our STRUCTPOOL is proposed for graph pooling operation and the energy is optimized via mean field approximation.All operations in our STRUCTPOOL can be realized by GNN operations so that our STRUCTPOOL can be easily used in any GNNs and trained in an end-to-end fashion.'], ['We propose a novel graph pooling technique', ' known as STRUCTPOOL', ' which is developed based on the conditional random fields.We consider the graph pooling as a node clustering problem and employ the CRF to build relationships between the assignments of different nodes.In addition', ' we generalize our method by incorporating the graph topological information so that our method can control the pairwise clique set in our CRFs.'], ['We employ the mean field approximation to compute the assignments and describe how to incorporate it in graph networks.Then the networks can be  1  \\x0cPublished as a conference paper at ICLR 2020  trained in an end-to-end fashion.Experiments show that our proposed STRUCTPOOL outperforms existing methods significantly and consistently.'], ['GRAPHSAGE  is an inductive framework which generates node embeddings by sampling and aggregating features from local neighbors', ' and it employs global mean pooling.SET2SET  proposes an aggregation method to replace the global pooling operations in deep graph networks.DGCNN  proposes a pooling strategy named SORTPOOL which sorts all nodes  6  \\x0cPublished as a conference paper at ICLR 2020  Table 2: Comparisons between different pooling techniques under the same framework.'], ['Several advanced graph pooling methods', ' such as SORTPOOL ', ' TOPKPOOL (Gao & Ji', ' 2019a)', ' DIFFPOOL ', ' and SAGPOOL  ', ' are recently proposed and achieve promising performance on graph classification tasks.However', ' none of them explicitly models the relationships among different nodes and thus may ig- nore important structural information.We argue that such information is important and should be explicitly captured in graph pooling.'], ['We report the results in Table 2 and the best results are shown in bold.Obviously', ' our method achieves the best performance on five of six datasets', ' and significantly outperforms all comparing pooling techniques.For the dataset EN- ZYMES', ' our obtained result is competitive since SAGPOOL only slightly outperforms our proposed method by 0.34%.'], ['We argue that such information is important and develop a novel graph pooling technique', ' know as the STRUCTPOOL', ' in this work.We consider the graph pooling as a node clustering problem', ' which requires the learning of a cluster assignment ma- trix.We propose to formulate it as a structured prediction problem and employ conditional random fields to capture the relationships among the assignments of different nodes.'], ['In addition', ' the graph kernel method WL obtains the best performance on COLLAB dataset and none of these deep models can achieve similar performance.Our model can obtain competitive performance compared with the second best model.This is because many graphs in COLLAB only have simple structures and deep models may be too complex to capture them.'], ['To this end', ' we propose a novel structured graph pooling technique', ' known as STRUCTPOOL', ' which generates the assignment matrix by considering the feature matrix X and the relationships between the assignments of different nodes.We propose to formulate this as a conditional random field (CRF) problem.The CRFs model a set of random variables with a Markov Random Field (MRF)', ' conditioned on a global observation .'], ['However', ' DIFFPOOL does not explicitly consider such high-order structural relationships', ' which we believe are important for graph pooling.In this work', ' we propose a novel structured graph pooling technique', ' known as the STRUCTPOOL', ' for effectively learning high-level graph representations.Different from exist- ing methods', ' our method explicitly captures high-order structural relationships between different nodes via conditional random fields.'], [' In this work', ' we propose a novel graph pooling technique', ' known as the STRUCTPOOL', ' that formu- lates graph pooling as a structured prediction problem.Following DIFFPOOL ', ' we consider graph pooling as a node clustering problem', ' and each cluster corresponds to a node in the new graph after pooling.Intuitively', ' two nodes with similar features should have a higher probability of being assigned to the same cluster.'], [' 3 STRUCTURED GRAPH POOLING  3.1 GRAPH POOLING VIA NODE CLUSTERING  Even though pooling techniques are shown to facilitate the training of deep models and improve their performance significantly in many image and NLP tasks (Yu & Koltun', ' 2016; Springenberg et al', ' 2014)', ' local pooling operations cannot be directly applied to graph tasks.The reason is there is no spatial locality information among graph nodes.Global max/average pooling operations can be employed for graph tasks but they may lead to information loss', ' due to largely reducing the size of representations trivially.'], ['Notably', ' our method can even outperform other approaches when m = 1.Furthermore', ' we evaluate the running time of our STRUCTPOOL and compare it with DIFFPOOL.For 500 graphs from large-scale dataset D&D', ' we set i = j = 3 and show the aver- aging time cost to perform pooling for each graph.'], [' 5  \\x0cPublished as a conference paper at ICLR 2020  Table 1: Classification results for six benchmark datasets.Note that none of these deep methods can outperform the traditional method WL on COLLAB.We believe the reason is the graphs in COLLAB only have single-layer structures while deep models are too complex to capture them.']]\n"
     ]
    }
   ],
   "source": [
    "# Obtain best snippets for the query\n",
    "import RelevantSnippets\n",
    "for csvPaperName in glob.glob(csvdir + \"*.csv\"):\n",
    "    relevantSnippets = RelevantSnippets.returnRelevant(csvPaperName, query)\n",
    "    print(relevantSnippets + \"\\n\")\n",
    "    # From snippet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From snippets, return answer"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
