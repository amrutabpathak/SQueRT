{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from ProcessText.ipynb\n"
     ]
    }
   ],
   "source": [
    "# pip install pdfminer.six\n",
    "# pip install ipynb\n",
    "\n",
    "# Import statements go here\n",
    "\n",
    "import import_ipynb\n",
    "import ProcessText\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape papers to pdf folder\n",
    "%run -i 'arxiv_pdf_scraper.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "GRAPH INFERENCE LEARNING FOR SEMI-SUPERVISED\n",
      "CLASSIFICATION\n",
      "\n",
      "Chunyan Xu, Zhen Cui∗, Xiaobin Hong, Tong Zhang, and Jian Yang\n",
      "School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China\n",
      "{cyx,zhen.cui,xbhong,tong.zhang,csjyang}@njust.edu.cn\n",
      "\n",
      "Wei Liu\n",
      "Tencent AI Lab, China\n",
      "wl2223@columbia.edu\n",
      "\n",
      "ABSTRACT\n",
      "\n",
      "In this work, we address semi-supervised classiﬁcation of graph data, where the\n",
      "categories of those unlabeled nodes are inferred from labeled nodes as well as\n",
      "graph structures. Recent works often solve this problem via advanced graph\n",
      "convolution in a conventionally supervised manner, but the performance could\n",
      "degrade signiﬁcantly when labeled data is scarce. To this end, we propose a\n",
      "Graph Inference Learning (GIL) framework to boost the performance of semi-\n",
      "supervised node classiﬁcation by learning the inference of node labels on graph\n",
      "topology. To bridge the connection between two nodes, we formally deﬁne a\n",
      "structure relation by encapsulating node attributes, between-node paths, and local\n",
      "topological structures together, which can make the inference conveniently deduced\n",
      "from one node to another node. For learning the inference process, we further\n",
      "introduce meta-optimization on structure relations from training nodes to validation\n",
      "nodes, such that the learnt graph inference capability can be better self-adapted to\n",
      "testing nodes. Comprehensive evaluations on four benchmark datasets (including\n",
      "Cora, Citeseer, Pubmed, and NELL) demonstrate the superiority of our proposed\n",
      "GIL when compared against state-of-the-art methods on the semi-supervised node\n",
      "classiﬁcation task.\n",
      "\n",
      "1\n",
      "\n",
      "INTRODUCTION\n",
      "\n",
      "Graph, which comprises a set of vertices/nodes together with connected edges, is a formal structural\n",
      "representation of non-regular data. Due to the strong representation ability, it accommodates many\n",
      "potential applications, e.g., social network (Orsini et al., 2017), world wide data (Page et al., 1999),\n",
      "knowledge graph (Xu et al., 2017), and protein-interaction network (Borgwardt et al., 2007). Among\n",
      "these, semi-supervised node classiﬁcation on graphs is one of the most interesting also popular topics.\n",
      "Given a graph in which some nodes are labeled, the aim of semi-supervised classiﬁcation is to infer\n",
      "the categories of those remaining unlabeled nodes by using various priors of the graph.\n",
      "\n",
      "While there have been numerous previous works (Brandes et al., 2008; Zhou et al., 2004; Zhu\n",
      "et al., 2003; Yang et al., 2016; Zhao et al., 2019) devoted to semi-supervised node classiﬁcation\n",
      "based on explicit graph Laplacian regularizations, it is hard to efﬁciently boost the performance of\n",
      "label prediction due to the strict assumption that connected nodes are likely to share the same label\n",
      "information. With the progress of deep learning on grid-shaped images/videos (He et al., 2016),\n",
      "a few of graph convolutional neural networks (CNN) based methods, including spectral (Kipf &\n",
      "Welling, 2017) and spatial methods (Niepert et al., 2016; Pan et al., 2018; Yu et al., 2018), have\n",
      "been proposed to learn local convolution ﬁlters on graphs in order to extract more discriminative\n",
      "node representations. Although graph CNN based methods have achieved considerable capabilities\n",
      "of graph embedding by optimizing ﬁlters, they are limited into a conventionally semi-supervised\n",
      "framework and lack of an efﬁcient inference mechanism on graphs. Especially, in the case of few-shot\n",
      "learning, where a small number of training nodes are labeled, this kind of methods would drastically\n",
      "compromise the performance. For example, the Pubmed graph dataset (Sen et al., 2008) consists\n",
      "\n",
      "∗Corresponding author: Zhen Cui.\n",
      "\n",
      "1\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "Figure 1: The illustration of our proposed GIL framework. For the problem of graph node labeling, the category\n",
      "information of these unlabeled nodes depends on the similarity computation between a query node (e.g., vj)\n",
      "and these labeled reference nodes (e.g., vi). We consider the similarity from three points: node attributes,\n",
      "the consistency of local topological structures (i.e., the circle with dashed line), and the between-node path\n",
      "reachability (i.e., the red wave line from vi to vj). Speciﬁcally, the local structures as well as node attributes are\n",
      "encoded as high-level features with graph convolution, while the between-node path reachability is abstracted as\n",
      "reachable probabilities of random walks. To better make the inference generalize to test nodes, we introduce a\n",
      "meta-learning strategy to optimize the structure relations learning from training nodes to validation nodes.\n",
      "\n",
      "of 19,717 nodes and 44,338 edges, but only 0.3% nodes are labeled for the semi-supervised node\n",
      "classiﬁcation task. These aforementioned works usually boil down to a general classiﬁcation task,\n",
      "where the model is learnt on a training set and selected by checking a validation set. However, they\n",
      "do not put great efforts on how to learn to infer from one node to another node on a topological graph,\n",
      "especially in the few-shot regime.\n",
      "\n",
      "In this paper, we propose a graph inference learning (GIL) framework to teach the model itself to\n",
      "adaptively infer from reference labeled nodes to those query unlabeled nodes, and ﬁnally boost the\n",
      "performance of semi-supervised node classiﬁcation in the case of a few number of labeled samples.\n",
      "Given an input graph, GIL attempts to infer the unlabeled nodes from those observed nodes by\n",
      "building between-node relations. The between-node relations are structured as the integration of\n",
      "node attributes, connection paths, and graph topological structures. It means that the similarity\n",
      "between two nodes is decided from three aspects: the consistency of node attributes, the consistency\n",
      "of local topological structures, and the between-node path reachability, as shown in Fig. 1. The local\n",
      "structures anchored around each node as well as the attributes of nodes therein are jointly encoded\n",
      "with graph convolution (Defferrard et al., 2016) for the sake of high-level feature extraction. For the\n",
      "between-node path reachability, we adopt the random walk algorithm to obtain the characteristics\n",
      "from a labeled reference node vi to a query unlabeled node vj in a given graph. Based on the\n",
      "computed node representation and between-node reachability, the structure relations can be obtained\n",
      "by computing the similar scores/relationships from reference nodes to unlabeled nodes in a graph.\n",
      "Inspired by the recent meta-learning strategy (Finn et al., 2017), we learn to infer the structure\n",
      "relations from a training set to a validation set, which can beneﬁt the generalization capability of the\n",
      "learned model. In other words, our proposed GIL attempts to learn some transferable knowledge\n",
      "underlying in the structure relations from training samples to validation samples, such that the learned\n",
      "structure relations can be better self-adapted to the new testing stage.\n",
      "\n",
      "We summarize the main contributions of this work as three folds:\n",
      "\n",
      "• We propose a novel graph inference learning framework by building structure relations to\n",
      "infer unknown node labels from those labeled nodes in an end-to-end way. The structure\n",
      "relations are well deﬁned by jointly considering node attributes, between-node paths, and\n",
      "graph topological structures.\n",
      "\n",
      "• To make the inference model better generalize to test nodes, we introduce a meta-learning\n",
      "procedure to optimize structure relations, which could be the ﬁrst time for graph node\n",
      "classiﬁcation to the best of our knowledge.\n",
      "\n",
      "• Comprehensive evaluations on three citation network datasets (including Cora, Citeseer,\n",
      "and Pubmed) and one knowledge graph data (i.e., NELL) demonstrate the superiority of\n",
      "our proposed GIL in contrast with other state-of-the-art methods on the semi-supervised\n",
      "classiﬁcation task.\n",
      "\n",
      "2\n",
      "\n",
      "   (b) The process of Graph inference learning.  We extract the local representation from the local subgraph (the circle with dashed line     The red wave line denote the node reachability from     to     .  dt th hbilit f  d t th d   \f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "2 RELATED WORK\n",
      "\n",
      "Graph CNNs: With the rapid development of deep learning methods, various graph convolution\n",
      "neural networks (Kashima et al., 2003; Morris et al., 2017; Shervashidze et al., 2009; Yanardag\n",
      "& Vishwanathan, 2015; Jiang et al., 2019; Zhang et al., 2020) have been exploited to analyze\n",
      "the irregular graph-structured data. For better extending general convolutional neural networks to\n",
      "graph domains, two broad strategies have been proposed, including spectral and spatial convolution\n",
      "methods. Speciﬁcally, spectral ﬁltering methods (Henaff et al., 2015; Kipf & Welling, 2017) develop\n",
      "convolution-like operators in the spectral domain, and then perform a series of spectral ﬁlters\n",
      "by decomposing the graph Laplacian. Unfortunately, the spectral-based approaches often lead\n",
      "to a high computational complex due to the operation of eigenvalue decomposition, especially\n",
      "for a large number of graph nodes. To alleviate this computation burden, local spectral ﬁltering\n",
      "methods (Defferrard et al., 2016) are then proposed by parameterizing the frequency responses as a\n",
      "Chebyshev polynomial approximation. Another type of graph CNNs, namely spatial methods (Li et al.,\n",
      "2016; Niepert et al., 2016), can perform the ﬁltering operation by deﬁning the spatial structures of\n",
      "adjacent vertices. Various approaches can be employed to aggregate or sort neighboring vertices, such\n",
      "as diffusion CNNs (Atwood & Towsley, 2016), GraphSAGE (Hamilton et al., 2017), PSCN (Niepert\n",
      "et al., 2016), and NgramCNN (Luo et al., 2017). From the perspective of data distribution, recently,\n",
      "the Gaussian induced convolution model (Jiang et al., 2019) is proposed to disentangle the aggregation\n",
      "process through encoding adjacent regions with Gaussian mixture model.\n",
      "\n",
      "Semi-supervised node classiﬁcation: Among various graph-related applications, semi-supervised\n",
      "node classiﬁcation has gained increasing attention recently, and various approaches have been\n",
      "proposed to deal with this problem, including explicit graph Laplacian regularization and graph-\n",
      "embedding approaches. Several classic algorithms with graph Laplacian regularization contain\n",
      "the label propagation method using Gaussian random ﬁelds (Zhu et al., 2003), the regularization\n",
      "framework by relying on the local/global consistency (Zhou et al., 2004), and the random walk-\n",
      "based sampling algorithm for acquiring the context information (Yang et al., 2016). To further\n",
      "address scalable semi-supervised learning issues (Liu et al., 2012), the Anchor Graph regularization\n",
      "approach (Liu et al., 2010) is proposed to scale linearly with the number of graph nodes and then\n",
      "applied to massive-scale graph datasets. Several graph convolution network methods (Abu-El-Haija\n",
      "et al., 2018; Du et al., 2017; Thekumparampil et al., 2018; Velickovic et al., 2018; Zhuang & Ma,\n",
      "2018) are then developed to obtain discriminative representations of input graphs. For example, Kipf\n",
      "et al. (Kipf & Welling, 2017) proposed a scalable graph CNN model, which can scale linearly in the\n",
      "number of graph edges and learn graph representations by encoding both local graph structures and\n",
      "node attributes. Graph attention networks (GAT) (Velickovic et al., 2018) are proposed to compute\n",
      "hidden representations of each node for attending to its neighbors with a self-attention strategy.\n",
      "By jointly considering the local- and global-consistency information, dual graph convolutional\n",
      "networks (Zhuang & Ma, 2018) are presented to deal with semi-supervised node classiﬁcation. The\n",
      "critical difference between our proposed GIL and those previous semi-supervised node classiﬁcation\n",
      "methods is to adopt a graph inference strategy by deﬁning structure relations on graphs and then\n",
      "leverage a meta optimization mechanism to learn an inference model, which could be the ﬁrst time to\n",
      "the best of our knowledge, while the existing graph CNNs take semi-supervised node classiﬁcation\n",
      "as a general classiﬁcation task.\n",
      "\n",
      "3 THE PROPOSED MODEL\n",
      "3.1 PROBLEM DEFINITION\n",
      "\n",
      "Formally, we denote an undirected/directed graph as G = {V, E, X , Y}, where V = {vi}n\n",
      "i=1 is the\n",
      "ﬁnite set of n (or |V|) vertices, E ∈ Rn×n deﬁnes the adjacency relationships (i.e., edges) between\n",
      "vertices representing the topology of G, X ∈ Rn×d records the explicit/implicit attributes/signals of\n",
      "vertices, and Y ∈ Rn is the vertex labels of C classes. The edge Eij = E(vi, vj) = 0 if and only if\n",
      "vertices vi, vj are not connected, otherwise Eij (cid:54)= 0. The attribute matrix X is attached to the vertex\n",
      "set V, whose i-th row Xvi (or Xi·) represents the attribute of the i-th vertex vi. It means that vi ∈ V\n",
      "carries a vector of d-dimensional signals. Associated with each node vi ∈ V, there is a discrete label\n",
      "yi ∈ {1, 2, · · · , C}.\n",
      "\n",
      "We consider the task of semi-supervised node classiﬁcation over graph data, where only a small\n",
      "number of vertices are labeled for the model learning, i.e., |VLabel| (cid:28) |V|. Generally, we have three\n",
      "node sets: a training set Vtr, a validation set Vval, and a testing set Vte. In the standard protocol\n",
      "\n",
      "3\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "of prior literatures (Yang et al., 2016), the three node sets share the same label space. We follow\n",
      "but do not restrict this protocol for our proposed method. Given the training and validation node\n",
      "sets, the aim is to predict the node labels of testing nodes by using node attributes as well as edge\n",
      "connections. A sophisticated machine learning technique used in most existing methods (Kipf &\n",
      "Welling, 2017; Zhou et al., 2004) is to choose the optimal classiﬁer (trained on a training set) after\n",
      "checking the performance on the validation set. However, these methods essentially ignore how to\n",
      "extract transferable knowledge from these known labeled nodes to unlabeled nodes, as the graph\n",
      "structure itself implies node connectivity/reachability. Moreover, due to the scarcity of labeled\n",
      "samples, the performance of such a classiﬁer is usually not satisfying. To address these issues,\n",
      "we introduce a meta-learning mechanism (Finn et al., 2017; Ravi & Larochelle, 2017; Sung et al.,\n",
      "2017) to learn to infer node labels on graphs. Speciﬁcally, the graph structure, between-node path\n",
      "reachability, and node attributes are jointly modeled into the learning process. Our aim is to learn to\n",
      "infer from labeled nodes to unlabeled nodes, so that the learner can perform better on a validation set\n",
      "and thus classify a testing set more accurately.\n",
      "\n",
      "3.2 STRUCTURE RELATION\n",
      "For convenient inference, we speciﬁcally build a structure relation between two nodes on the topology\n",
      "graph. The labeled vertices (in a training set) are viewed as the reference nodes, and their information\n",
      "can be propagated into those unlabeled vertices for improving the label prediction accuracy. Formally,\n",
      "given a reference node vi ∈ VLabel, we deﬁne the score of a query node vj similar to vi as\n",
      "\n",
      "(1)\n",
      "where Gvi and Gvj may be understood as the centralized subgraphs around vi and vj, respectively.\n",
      "fe, fr, fP are three abstract functions that we explain as follows:\n",
      "\n",
      "si→j = fr(fe(Gvi ), fe(Gvj ), fP (vi, vj, E)),\n",
      "\n",
      "• Node representation fe(Gvi) −→ Rdv , encodes the local representation of the centralized\n",
      "subgraph Gvi around node vi, and may thus be understood as a local ﬁlter function on graphs.\n",
      "This function should not only take the signals of nodes therein as input, but also consider the\n",
      "local topological structure of the subgraph for more accurate similarity computation. To this\n",
      "end, we perform the spectral graph convolution on subgraphs to learn discriminative node\n",
      "features, analogous to the pixel-level feature extraction from convolution maps of gridded\n",
      "images. The details of feature extraction fe are described in Section 4.\n",
      "\n",
      "• Path reachability fP (vi, vj, E) −→ Rdp , represents the characteristics of path reachability\n",
      "from vi to vj. As there usually exist multiple traversal paths between two nodes, we choose\n",
      "the function as reachable probabilities of different lengths of walks from vi to vj. More\n",
      "details will be introduced in Section 4.\n",
      "\n",
      "• Structure relation fr(Rdv , Rdv , Rdp ) −→ R, is a relational function computing the score\n",
      "of vj similar to vi. This function is not exchangeable for different orders of two nodes,\n",
      "due to the asymmetric reachable relationship fP . If necessary, we may easily revise it as a\n",
      "symmetry function, e.g., summarizing two traversal directions. The score function depends\n",
      "on triple inputs: the local representations extracted from the subgraphs w.r.t. fe(Gvi) and\n",
      "fe(Gvj ), respectively, and the path reachability from vi to vj.\n",
      "\n",
      "In semi-supervised node classiﬁcation, we take the training node set Vtr as the reference samples, and\n",
      "the validation set Vval as the query samples during the training stage. Given a query node vj ∈ Vval,\n",
      "we can derive the class similarity score of vj w.r.t. the c-th (c = 1, · · · , C) category by weighting\n",
      "the reference samples Cc = {vk|yvk = c}. Formally, we can further revise Eqn. (1) and deﬁne the\n",
      "class-to-node relationship function as\n",
      "\n",
      "sCc→j = φr(FCc→vj\n",
      "\n",
      "wi→j · fe(Gvi), fe(Gvj )),\n",
      "\n",
      "(cid:88)\n",
      "\n",
      "vi∈Cc\n",
      "\n",
      "s.t. wi→j = φw(fP (vi, vj, E)),\n",
      "\n",
      "(3)\n",
      "where the function φw maps a reachable vector fP (vi, vj, E) into a weight value, and the function φr\n",
      "computes the similar score between vj and the c-th class nodes. The normalization factor FCc→vj of\n",
      "the c-th category w.r.t. vj is deﬁned as\n",
      "\n",
      "(2)\n",
      "\n",
      "(4)\n",
      "\n",
      "For the relation function φr and the weight function φw, we may choose some subnetworks to\n",
      "instantiate them in practice. The detailed implementation of our model can be found in Section 4.\n",
      "\n",
      "FCc→vj =\n",
      "\n",
      "(cid:80)\n",
      "\n",
      "1\n",
      "\n",
      ".\n",
      "\n",
      "vi∈Cc\n",
      "\n",
      "wi→j\n",
      "\n",
      "4\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "3.3\n",
      "\n",
      "INFERENCE LEARNING\n",
      "\n",
      "According to the class-to-node relationship function in Eqn. (2), given a query node vj, we can obtain\n",
      "a score vector sC→j = [sC1→j, · · · , sCC →j](cid:124) ∈ RC after computing the relations to all classes . The\n",
      "indexed category with the maximum score is assumed to be the estimated label. Thus, we can deﬁne\n",
      "the loss function based on cross entropy as follows:\n",
      "\n",
      "L = −\n",
      "\n",
      "yj,c log ˆyCc→j,\n",
      "\n",
      "(cid:88)\n",
      "\n",
      "C\n",
      "(cid:88)\n",
      "\n",
      "vj\n",
      "\n",
      "c=1\n",
      "\n",
      "(5)\n",
      "\n",
      "(6)\n",
      "\n",
      "(7)\n",
      "\n",
      "where yj,c is a binary indicator (i.e., 0 or 1) of class label c for node vj, and the softmax operation\n",
      "is imposed on sCc→j, i.e., ˆyCc→j = exp(sCc→j)/ (cid:80)C\n",
      "k=1 exp(sCk→j). Other error functions may be\n",
      "chosen as the loss function, e.g., mean square error. In the regime of general classiﬁcation, the cross\n",
      "entropy loss is a standard one that performs well.\n",
      "\n",
      "Given a training set Vtr, we expect that the best performance can be obtained on the validation set\n",
      "Vval after optimizing the model on Vtr. Given a trained/pretrained model Θ = {fe, φw, φr}, we\n",
      "perform iteratively gradient updates on the training set Vtr to obtain the new model, formally,\n",
      "\n",
      "Θ(cid:48) = Θ − α∇ΘLtr(Θ),\n",
      "\n",
      "where α is the updating rate. Note that, in the computation of class scores, since the reference node\n",
      "and query node can be both from the training set Vtr, we set the computation weight wi→j = 0 if\n",
      "i = j in Eqn. (3). After several iterates of gradient descent on Vtr, we expect a better performance on\n",
      "the validation set Vval, i.e., min\n",
      "Θ\n",
      "\n",
      "Lval(Θ(cid:48)). Thus, we can perform the gradient update as follows\n",
      "\n",
      "where β is the learning rate of meta optimization (Finn et al., 2017).\n",
      "\n",
      "Θ = Θ − β∇ΘLval(Θ(cid:48)),\n",
      "\n",
      "During the training process, we may perform batch sampling from training nodes and validation\n",
      "nodes, instead of taking all one time. In the testing stage, we may take all training nodes and perform\n",
      "the model update according to Eqn. (6) like the training process. The updated model is used as the\n",
      "ﬁnal model and is then fed into Eqn. (2) to infer the class labels for those query nodes.\n",
      "\n",
      "4 MODULES\n",
      "In this section, we instantiate all modules (i.e., functions) of the aforementioned structure relation.\n",
      "The implementation details can be found in the following.\n",
      "\n",
      "Node Representation fe(Gvi): The local representation at vertex vi can be extracted by performing\n",
      "the graph convolution operation on subgraph Gvi . Similar to gridded images/videos, on which local\n",
      "convolution kernels are deﬁned as multiple lattices with various receptive ﬁelds, the spectral graph\n",
      "convolution is used to encode the local representations of an input graph in our work.\n",
      "\n",
      "Given a graph sample G = {V, E, X }, the normalized graph Laplacian matrix is L = In −\n",
      "D−1/2ED−1/2 = UΛUT , with a diagonal matrix of its eigenvalues Λ. The spectral graph convo-\n",
      "lution can be deﬁned as the multiplication of signal X with a ﬁlter gθ(Λ) = diag(θ) parameterized\n",
      "by θ in the Fourier domain: conv(X ) = gθ(L) ∗ X = Ugθ(Λ)UT X , where parameter θ ∈ Rn\n",
      "is a vector of Fourier coefﬁcients. To reduce the computational complexity and obtain the local\n",
      "information, we use an approximate local ﬁlter of the Chebyshev polynomial (Defferrard et al.,\n",
      "2016), gθ(Λ) = (cid:80)K−1\n",
      "k=0 θkTk(ˆΛ), where parameter θ ∈ RK is a vector of Chebyshev coefﬁcients\n",
      "and Tk(ˆΛ) ∈ Rn×n is the Chebyshev polynomial of order k evaluated at ˆΛ = 2Λ/λmax − In,\n",
      "a diagonal matrix of scaled eigenvalues. The graph ﬁltering operation can then be expressed as\n",
      "gθ(Λ) ∗ X = (cid:80)K−1\n",
      "k=0 θkTk(ˆL)X , where Tk(ˆL) ∈ Rn×n is the Chebyshev polynomial of order k\n",
      "evaluated at the scaled Laplacian ˆL = 2L/λmax − In. Further, we can construct multi-scale receptive\n",
      "ﬁelds for each vertex based on the Laplacian matrix L, where each receptive ﬁeld records hopping\n",
      "neighborhood relationships around the reference vertex vi, and forms a local centralized subgraph.\n",
      "\n",
      "Path Reachability fP (vi, vj, E): Here we compute the probabilities of paths from vertex i to vertex\n",
      "j by employing random walks on graphs, which refers to traversing the graph from vi to vj according\n",
      "to the probability matrix P. For the input graph G with n vertices, the random-walk transition matrix\n",
      "\n",
      "5\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "Datasets Nodes\n",
      "2,708\n",
      "Cora\n",
      "3,327\n",
      "Citeseer\n",
      "19,717\n",
      "Pubmed\n",
      "NELL\n",
      "65,755\n",
      "\n",
      "Edges\n",
      "5,429\n",
      "4,732\n",
      "44,338\n",
      "266,144\n",
      "\n",
      "Classes\n",
      "7\n",
      "6\n",
      "3\n",
      "210\n",
      "\n",
      "Features\n",
      "1,433\n",
      "3,703\n",
      "500\n",
      "5,414\n",
      "\n",
      "Label Rates\n",
      "0.052\n",
      "0.036\n",
      "0.003\n",
      "0.001\n",
      "\n",
      "Table 1: The properties (especially for label rate) of various graph datasets used for the semi-supervised\n",
      "classiﬁcation task.\n",
      "\n",
      "can be deﬁned as P = D−1E, where D ∈ Rn×n is the diagonal degree matrix with Dii = (cid:80)\n",
      "That is to say, each element Pij is the probability of going from vertex i to vertex j in one step.\n",
      "\n",
      "i Eij.\n",
      "\n",
      "The sequence of nodes from vertex i to vertex j is a random walk on the graph, which can be modeled\n",
      "as a classical Markov chain by considering the set of graph vertices. To represent this formulation,\n",
      "we show that P t\n",
      "ij is the probability of getting from vertex vi to vertex vj in t steps. This fact is easily\n",
      "exhibited by considering a t-step path from vertex vi to vertex vj as ﬁrst taking a single step to some\n",
      "vertex h, and then taking t − 1 steps to vj. The transition probability P t in t steps can be formulated\n",
      "as\n",
      "\n",
      "P t\n",
      "\n",
      "ij =\n",
      "\n",
      "PihP t−1\n",
      "h,j\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Pij\n",
      "(cid:88)\n",
      "\n",
      "h\n",
      "\n",
      "if t = 1\n",
      "if t > 1 ,\n",
      "\n",
      "where each matrix entry P t\n",
      "steps. Finally, the node reachability from vi to vj can be written as a dp-dimensional vector:\n",
      "\n",
      "ij denotes the probability of starting at vertex i and ending at vertex j in t\n",
      "\n",
      "ij, . . . , P dp\n",
      "ij ],\n",
      "where dp refers to the step length of the longest path from vi to vj.\n",
      "\n",
      "fP (vi, vj, E) = [Pij, P 2\n",
      "\n",
      "Class-to-Node Relationship sCc→j: To deﬁne the node relationship si→j from vi to vj, we simulta-\n",
      "neously consider the property of path reachability fP (vi, vj, E), local representations fe(Gvi), and\n",
      "fe(Gvj ) of nodes vi, vj. The function φw(fP (vi, vj, E)) in Eqn. (3), which is to map the reachable\n",
      "vector fP (vi, vj, E) ∈ Rdp into a weight value, can be implemented with two 16-dimensional fully\n",
      "connected layers in our experiments. The computed value wi→j can be further used to weight the\n",
      "local features at node vi, fe(Gvi) ∈ Rdv . For obtaining the similar score between vj and the c-th\n",
      "class nodes Cc in Eqn. (2), we perform a concatenation of two input features, where one refers to the\n",
      "weighted features of vertex vi, and another is the local features of vertex vj. One fully connected\n",
      "layer (w.r.t. φr) with C-dimensions is ﬁnally adopted to obtain the relation regression score.\n",
      "\n",
      "(8)\n",
      "\n",
      "(9)\n",
      "\n",
      "5 EXPERIMENTS\n",
      "\n",
      "5.1 EXPERIMENTAL SETTINGS\n",
      "\n",
      "We evaluate our proposed GIL method on three citation network datasets: Cora, Citeseer, Pubmed (Sen\n",
      "et al., 2008), and one knowledge graph NELL dataset (Carlson et al., 2010). The statistical properties\n",
      "of graph data are summarized in Table 1. Following the previous protocol in (Kipf & Welling, 2017;\n",
      "Zhuang & Ma, 2018), we split the graph data into a training set, a validation set, and a testing set.\n",
      "Taking into account the graph convolution and pooling modules, we may alternately stack them into\n",
      "a multi-layer Graph convolutional network. The GIL model consists of two graph convolution layers,\n",
      "each of which is followed by a mean-pooling layer, a class-to-node relationship regression module,\n",
      "and a ﬁnal softmax layer. We have given the detailed conﬁguration of the relationship regression\n",
      "module in the class-to-node relationship of Section 4. The parameter dp in Eqn. (9) is set to the\n",
      "mean length of between-node reachability paths in the input graph. The channels of the 1-st and\n",
      "2-nd convolutional layers are set to 128 and 256, respectively. The scale of the respective ﬁled is 2\n",
      "in both convolutional layers. The dropout rate is set to 0.5 in the convolution and fully connected\n",
      "layers to avoid over-ﬁtting, and the ReLU unit is leveraged as a nonlinear activation function. We\n",
      "pre-train our proposed GIL model for 200 iterations with the training set, where its initial learning\n",
      "rate, decay factor, and momentum are set to 0.05, 0.95, and 0.9, respectively. Here we train the GIL\n",
      "model using the stochastic gradient descent method with the batch size of 100. We further improve\n",
      "the inference learning capability of the GIL model for 1200 iterations with the validation set, where\n",
      "the meta-learning rates α and β are both set to 0.001.\n",
      "\n",
      "6\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "5.2 COMPARISON WITH STATE-OF-THE-ARTS\n",
      "\n",
      "We compare the GIL approach with several state-of-the-art methods (Monti et al., 2017; Kipf &\n",
      "Welling, 2017; Zhou et al., 2004; Zhuang & Ma, 2018) over four graph datasets, including Cora,\n",
      "Citeseer, Pubmed, and NELL. The classiﬁcation accuracies for all methods are reported in Table 2.\n",
      "Our proposed GIL can signiﬁcantly outperform these graph Laplacian regularized methods on four\n",
      "graph datasets, including Deep walk (Zhou et al., 2004), modularity clustering (Brandes et al., 2008),\n",
      "Gaussian ﬁelds (Zhu et al., 2003), and graph embedding (Yang et al., 2016) methods. For example,\n",
      "we can achieve much higher performance than the deepwalk method (Zhou et al., 2004), e.g., 43.2%\n",
      "vs 74.1% on the Citeseer dataset, 65.3% vs 83.1% on the Pubmed dataset, and 58.1% vs 78.9% on the\n",
      "NELL dataset. We ﬁnd that the graph embedding method (Yang et al., 2016), which has considered\n",
      "both label information and graph structure during sampling, can obtain lower accuracies than our\n",
      "proposed GIL by 9.4% on the Citeseer dataset and 10.5% on the Cora dataset, respectively. This\n",
      "indicates that our proposed GIL can better optimize structure relations and thus improve the network\n",
      "generalization. We further compare our proposed GIL with several existing deep graph embedding\n",
      "methods, including graph attention network (Velickovic et al., 2018), dual graph convolutional\n",
      "networks (Zhuang & Ma, 2018), topology adaptive graph convolutional networks (Du et al., 2017),\n",
      "Multi-scale graph convolution (Abu-El-Haija et al., 2018), etc. For example, our proposed GIL\n",
      "achieves a very large gain, e.g., 86.2% vs 83.3% (Du et al., 2017) on the Cora dataset, and 78.9%\n",
      "vs 66.0% (Kipf & Welling, 2017) on the NELL dataset. We evaluate our proposed GIL method on\n",
      "a large graph dataset with a lower label rate, which can signiﬁcantly outperform existing baselines\n",
      "on the Pubmed dataset: 3.1% over DGCN (Zhuang & Ma, 2018), 4.1% over classic GCN (Kipf &\n",
      "Welling, 2017) and TAGCN (Du et al., 2017), 3.2% over AGNN (Thekumparampil et al., 2018), and\n",
      "3.6% over N-GCN (Abu-El-Haija et al., 2018). It demonstrates that our proposed GIL performs very\n",
      "well on various graph datasets by building the graph inference learning process, where the limited\n",
      "label information and graph structures can be well employed in the predicted framework.\n",
      "\n",
      "Table 2: Performance comparisons of semi-supervised classiﬁcation methods.\n",
      "\n",
      "Methods\n",
      "Clustering (Brandes et al., 2008)\n",
      "DeepWalk (Zhou et al., 2004)\n",
      "Gaussian (Zhu et al., 2003)\n",
      "G-embedding (Yang et al., 2016)\n",
      "DCNN (Atwood & Towsley, 2016)\n",
      "GCN (Kipf & Welling, 2017)\n",
      "MoNet (Monti et al., 2017)\n",
      "N-GCN (Abu-El-Haija et al., 2018)\n",
      "GAT (Velickovic et al., 2018)\n",
      "AGNN (Thekumparampil et al., 2018)\n",
      "TAGCN (Du et al., 2017)\n",
      "DGCN (Zhuang & Ma, 2018)\n",
      "Our GIL\n",
      "\n",
      "Cora\n",
      "59.5\n",
      "67.2\n",
      "68.0\n",
      "75.7\n",
      "76.8\n",
      "81.5\n",
      "81.7\n",
      "83.0\n",
      "83.0\n",
      "83.1\n",
      "83.3\n",
      "83.5\n",
      "86.2\n",
      "\n",
      "Citeseer\n",
      "60.1\n",
      "43.2\n",
      "45.3\n",
      "64.7\n",
      "-\n",
      "70.3\n",
      "-\n",
      "72.2\n",
      "72.5\n",
      "71.7\n",
      "72.5\n",
      "72.6\n",
      "74.1\n",
      "\n",
      "Pubmed NELL\n",
      "70.7\n",
      "65.3\n",
      "63.0\n",
      "77.2\n",
      "73.0\n",
      "79.0\n",
      "78.8\n",
      "79.5\n",
      "79.0\n",
      "79.9\n",
      "79.0\n",
      "80.0\n",
      "83.1\n",
      "\n",
      "21.8\n",
      "58.1\n",
      "26.5\n",
      "61.9\n",
      "-\n",
      "66.0\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "74.2\n",
      "78.9\n",
      "\n",
      "5.3 ANALYSIS\n",
      "\n",
      "Meta-optimization: As can be seen in Table 3, we report the classiﬁcation accuracies of\n",
      "semi-supervised classiﬁcation with several variants of our proposed GIL and the classical GCN\n",
      "method (Kipf & Welling, 2017) when evaluating them on the Cora dataset. For analyzing the perfor-\n",
      "mance improvement of our proposed GIL with the graph inference learning process, we report the\n",
      "classiﬁcation accuracies of GCN (Kipf & Welling, 2017) and our proposed GIL on the Cora dataset\n",
      "under two different situations, including “only learning with the training set Vtr\" and “with jointly\n",
      "learning on a training set Vtr and a validation set Vval\". “GCN /w jointly learning on Vtr & Vval\"\n",
      "achieves a better result than “GCN /w learning on Vtr\" by 3.6%, which demonstrates that the network\n",
      "performance can be improved by employing validation samples. When using structure relations,\n",
      "“GIL /w learning on Vtr\" obtains an improvement of 1.9% (over “GCN /w learning on Vtr”), which\n",
      "can be attributed to the building connection between nodes. The meta-optimization strategy (“GIL /w\n",
      "meta-training from Vtr → Vval\" vs “GIL /w learning on Vtr”) has a gain of 2.9%, which indicates\n",
      "that a good inference capability can be learnt through meta-optimization. It is worth noting that, GIL\n",
      "adopts a meta-optimization strategy to learn the inference model, which is a process of migrating\n",
      "\n",
      "7\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "from a training set to a validation set. In other words, the validation set is only used to teach the\n",
      "model itself how to transfer to unseen data. In contrast, the conventional methods often employ a\n",
      "validation set to tune parameters of a certain model of interest.\n",
      "\n",
      "Table 3: Performance comparisons with several GIL variants and the classical GCN method on the Cora dataset.\n",
      "\n",
      "GCN (Kipf & Welling, 2017)\n",
      "\n",
      "Methods\n",
      "\n",
      "GIL\n",
      "\n",
      "GIL+mean pooling\n",
      "\n",
      "GIL+2 conv. layers\n",
      "\n",
      "/w learning on Vtr\n",
      "/w jointly learning on Vtr & Vval\n",
      "/w learning on Vtr\n",
      "/w meta-train Vtr → Vval\n",
      "/w 1 conv. layer\n",
      "/w 2 conv. layers\n",
      "/w 3 conv. layers\n",
      "/w max-pooling\n",
      "/w mean pooling\n",
      "\n",
      "Acc. (%)\n",
      "81.4\n",
      "84.0\n",
      "83.3\n",
      "86.2\n",
      "84.5\n",
      "86.2\n",
      "85.4\n",
      "85.2\n",
      "86.2\n",
      "\n",
      "Network settings: We explore the effectiveness of our proposed GIL with the same mean pooling\n",
      "mechanism, but with different numbers of convolutional layers, i.e., “GIL + mean pooling\" with one,\n",
      "two, and three convolutional layers, respectively. As can be seen in Table 3, the proposed GIL with\n",
      "two convolutional layers can obtain a better performance on the Cora data than the other two network\n",
      "settings (i.e., GIL with one or three convolutional layers). For example, the performance of ‘GIL /w\n",
      "1 conv. layer + mean pooling\" is slightly decreased by 1.7% over “GIL /w 2 conv. layers + mean\n",
      "pooling\" on the Cora dataset. Furthermore, we report the classiﬁcation results of our proposed GIL\n",
      "by using mean and max-pooling mechanisms, respectively. GIL with mean pooling (i.e., “GIL /w\n",
      "2 conv layers + mean pooling\") can get a better result than the GIL method with max-pooling (i.e.,\n",
      "“GIL /w 2 conv layers + max-pooling\"), e.g., 86.2% vs 85.2% on the Cora graph dataset. The reason\n",
      "may be that the graph network with two convolutional layers and the mean pooling mechanism can\n",
      "obtain the optimal graph embeddings, but when increasing the network layers, more parameters of a\n",
      "certain graph model need to be optimized, which may lead to the over-ﬁtting issue.\n",
      "\n",
      "Inﬂuence of different between-node steps: We compare the classiﬁcation performance within\n",
      "different between-node steps for our proposed GIL and GCN (Kipf & Welling, 2017), as illustrated in\n",
      "Fig. 2(a). The length of between-node steps can be computed with the shortest path between reference\n",
      "nodes and query nodes. When the step between nodes is smaller, both GIL and GCN methods can\n",
      "predict the category information for a small part of unlabeled nodes in the testing set. The reason\n",
      "may be that the node category information may be disturbed by its nearest neighboring nodes with\n",
      "different labels and fewer nodes are within 1 or 2 steps in the testing set. The GIL and GCN methods\n",
      "can infer the category information for a part of unlabeled nodes by adopting node attributes, when\n",
      "two nodes are not connected in the graph (i.e., step=∞). By increasing the length of reachability\n",
      "path, the inference process of the GIL method would become difﬁcult and more graph structure\n",
      "information may be employed in the predicted process. GIL can outperform the classic GCN by\n",
      "analyzing the accuracies within different between-node steps, which indicates that our proposed GIL\n",
      "has a better reference capability than GCN by using the meta-optimization mechanism from training\n",
      "nodes to validation nodes.\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 2: (a) Performance comparisons within different between-node steps on the Cora dataset. The accuracy\n",
      "equals to the number of correctly classiﬁed nodes divided by all testing samples, and is accumulated from step 1\n",
      "to step k. (b) Performance comparisons with different label rates on the Pubmed dataset.\n",
      "\n",
      "8\n",
      "\n",
      "1357911step0.00.20.40.60.8accuracyour GILGCNlabel rate0.30%0.60%0.90%1.20%1.50%1.80%GCN0.7920.7970.8050.8240.8290.834GIL(ours)0.8170.8240.8310.8360.8380.8421x2x3x4x5x6x77.0%79.0%81.0%83.0%85.0%1x2x3x4x5x6xGCNGIL(ours)Label rates Accuracy \f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "Inﬂuence of different label rates: We also explore the performance comparisons of the GIL method\n",
      "with different label rates, and the detailed results on the Pubmed dataset can be shown in Fig. 2(b).\n",
      "When label rates increase by multiplication, the performances of GIL and GCN are improved, but the\n",
      "relative gain becomes narrow. The reason is that, the reachable path lengths between unlabeled nodes\n",
      "and labeled nodes will be reduced with the increase of labeled nodes, which will weaken the effect\n",
      "of inference learning. In the extreme case, labels of unlabeled nodes could be determined by those\n",
      "neighbors with the 1 ∼ 2 step reachability. In summary, our proposed GIL method prefers small ratio\n",
      "labeled nodes on the semi-supervised node classiﬁcation task.\n",
      "\n",
      "Inference learning process: Classiﬁcation errors of different epochs on the validation set of the\n",
      "Cora dataset can be illustrated in Fig. 3. Classiﬁcation errors are rapidly decreasing as the number of\n",
      "iterations increases from the beginning to 400 iterations, while they are with a slow descent from 400\n",
      "iterations to 1200 iterations. It demonstrates that the learned knowledge from the training samples\n",
      "can be transferred for inferring node category information from these reference labeled nodes. The\n",
      "performance of semi-supervised classiﬁcation can be further increased by improving the generalized\n",
      "capability of the Graph CNN model.\n",
      "\n",
      "Table 4: Performance comparisons with different mod-\n",
      "ules on the Cora dataset, where fe, fP , and fr denote\n",
      "node representation, path reachability, and structure re-\n",
      "lation, respectively.\n",
      "\n",
      "fP\n",
      "fr\n",
      "fe\n",
      "-\n",
      "-\n",
      "-\n",
      "(cid:88) -\n",
      "-\n",
      "(cid:88) (cid:88) -\n",
      "(cid:88) (cid:88) (cid:88)\n",
      "\n",
      "Acc.(%)\n",
      "56.0\n",
      "81.5\n",
      "85.0\n",
      "86.2\n",
      "\n",
      "Figure 3: Classiﬁcation errors of different itera-\n",
      "tions on the validation set of the Cora dataset.\n",
      "\n",
      "Module analysis: We evaluate the effectiveness of different modules within our proposed GIL\n",
      "framework, including node representation fe, path reachability fP , and structure relation fr. Note\n",
      "that the last one fr deﬁnes on the former two ones, so we consider the cases in Table 4 by adding\n",
      "modules. When not using all modules, only original attributes of nodes are used to predict labels.\n",
      "The case of only using fe belongs to the GCN method, which can achieve 81.5% on the Cora dataset.\n",
      "The large gain of using the relation module fr (i.e., from 81.5% to 85.0%) may be contributed to the\n",
      "ability of inference learning on attributes as well as local topology structures which are implicitly\n",
      "encoded in fe. The path information fP can further boost the performance by 1.2%, e.g., 86.2% vs\n",
      "85.0%. It demonstrates that three different modules of our method can improve the graph inference\n",
      "learning capability.\n",
      "\n",
      "Computational complexity: For the computational complexity of our proposed GIL, the cost is\n",
      "mainly spent on the computations of node representation, between-node reachability, and class-to-\n",
      "node relationship, which are about O((ntr + nte) ∗ e ∗ din ∗ dout), O((ntr + nte) ∗ e ∗ P ), and\n",
      "O(ntr ∗ nted2\n",
      "out), respectively. ntr and nte refer to the numbers of training and testing nodes, din\n",
      "and dout denote the input and output dimensions of node representation, e is about the average degree\n",
      "of graph node, and P is the step length of node reachability. Compared with those classic Graph\n",
      "CNNs (Kipf & Welling, 2017), our proposed GIL has a slightly higher cost due to an extra inference\n",
      "learning process, but can complete the testing stage with several seconds on these benchmark datasets.\n",
      "\n",
      "6 CONCLUSION\n",
      "\n",
      "In this work, we tackled the semi-supervised node classiﬁcation task with a graph inference learning\n",
      "method, which can better predict the categories of these unlabeled nodes in an end-to-end framework.\n",
      "We can build a structure relation for obtaining the connection between any two graph nodes, where\n",
      "node attributes, between-node paths, and graph structure information can be encapsulated together.\n",
      "For better capturing the transferable knowledge, our method further learns to transfer the mined\n",
      "knowledge from the training samples to the validation set, ﬁnally boosting the prediction accuracy\n",
      "of the labels of unlabeled nodes in the testing set. The extensive experimental results demonstrate\n",
      "the effectiveness of our proposed GIL for solving the semi-supervised learning problem, even in\n",
      "the few-shot paradigm. In the future, we would extend the graph inference method to handle more\n",
      "graph-related tasks, such as graph generation and social network analysis.\n",
      "\n",
      "9\n",
      "\n",
      "the number of iterations error \f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "ACKNOWLEDGMENT\n",
      "\n",
      "This work was supported by the National Natural Science Foundation of China (Nos. 61972204,\n",
      "61906094, U1713208), the Natural Science Foundation of Jiangsu Province (Grant Nos. BK20191283\n",
      "and BK20190019), and Tencent AI Lab Rhino-Bird Focused Research Program (No. JR201922).\n",
      "\n",
      "REFERENCES\n",
      "\n",
      "2001, 2016.\n",
      "\n",
      "Sami Abu-El-Haija, Amol Kapoor, Bryan Perozzi, and Joonseok Lee. N-gcn: Multi-scale graph\n",
      "\n",
      "convolution for semi-supervised node classiﬁcation. arXiv preprint arXiv:1802.08888, 2018.\n",
      "\n",
      "James Atwood and Don Towsley. Diffusion-convolutional neural networks. In NeurIPS, pp. 1993–\n",
      "\n",
      "Karsten M Borgwardt, Hans-Peter Kriegel, SVN Vishwanathan, and Nicol N Schraudolph. Graph ker-\n",
      "nels for disease outcome prediction from protein-protein interaction networks. Paciﬁc Symposium\n",
      "on Biocomputing Paciﬁc Symposium on Biocomputing, pp. 4–15, 2007.\n",
      "\n",
      "Ulrik Brandes, Daniel Delling, Marco Gaertler, Robert Gorke, Martin Hoefer, Zoran Nikoloski,\n",
      "and Dorothea Wagner. On modularity clustering. IEEE transactions on knowledge and data\n",
      "engineering, 20(2):172–188, 2008.\n",
      "\n",
      "Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr Settles, Estevam R. Hruschka Jr., and Tom M.\n",
      "\n",
      "Mitchell. Toward an architecture for never-ending language learning. In AAAI, 2010.\n",
      "\n",
      "Michaël Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on\n",
      "\n",
      "graphs with fast localized spectral ﬁltering. In NeurIPS, pp. 3844–3852, 2016.\n",
      "\n",
      "Jian Du, Shanghang Zhang, Guanhang Wu, José MF Moura, and Soummya Kar. Topology adaptive\n",
      "\n",
      "graph convolutional networks. arXiv preprint arXiv:1710.10370, 2017.\n",
      "\n",
      "Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of\n",
      "\n",
      "deep networks. In ICML, pp. 1126–1135, 2017.\n",
      "\n",
      "Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. In\n",
      "\n",
      "NeurIPS, pp. 1025–1035, 2017.\n",
      "\n",
      "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image\n",
      "\n",
      "recognition. In CVPR, pp. 770–778, 2016.\n",
      "\n",
      "Mikael Henaff, Joan Bruna, and Yann LeCun. Deep convolutional networks on graph-structured data.\n",
      "\n",
      "arXiv preprint arXiv:1506.05163, 2015.\n",
      "\n",
      "Jiatao Jiang, Zhen Cui, Chunyan Xu, and Jian Yang. Gaussian-induced convolution for graphs. In\n",
      "\n",
      "AAAI, volume 33, pp. 4007–4014, 2019.\n",
      "\n",
      "Hisashi Kashima, Koji Tsuda, and Akihiro Inokuchi. Marginalized kernels between labeled graphs.\n",
      "\n",
      "In ICML, pp. 321–328, 2003.\n",
      "\n",
      "Thomas N. Kipf and Max Welling. Semi-supervised classiﬁcation with graph convolutional networks.\n",
      "\n",
      "In ICLR, 2017.\n",
      "\n",
      "networks. ICLR, 2016.\n",
      "\n",
      "learning. In ICML, 2010.\n",
      "\n",
      "Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel. Gated graph sequence neural\n",
      "\n",
      "Wei Liu, Junfeng He, and Shih-Fu Chang. Large graph construction for scalable semi-supervised\n",
      "\n",
      "Wei Liu, Jun Wang, and Shih-Fu Chang. Robust and scalable graph-based semisupervised learning.\n",
      "\n",
      "Proceedings of the IEEE, 100(9):2624–2638, 2012.\n",
      "\n",
      "Zhiling Luo, Ling Liu, Jianwei Yin, Ying Li, and Zhaohui Wu. Deep learning of graphs with ngram\n",
      "convolutional neural networks. IEEE Transactions on Knowledge and Data Engineering, 29(10):\n",
      "2125–2139, 2017.\n",
      "\n",
      "10\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "Federico Monti, Davide Boscaini, Jonathan Masci, Emanuele Rodola, Jan Svoboda, and Michael M\n",
      "Bronstein. Geometric deep learning on graphs and manifolds using mixture model cnns. In CVPR,\n",
      "pp. 5115–5124, 2017.\n",
      "\n",
      "Christopher Morris, Kristian Kersting, and Petra Mutzel. Glocalized weisfeiler-lehman graph kernels:\n",
      "\n",
      "Global-local feature maps of graphs. In ICDM, pp. 327–336. IEEE, 2017.\n",
      "\n",
      "Mathias Niepert, Mohamed Ahmed, and Konstantin Kutzkov. Learning convolutional neural networks\n",
      "\n",
      "for graphs. In ICML, pp. 2014–2023, 2016.\n",
      "\n",
      "Francesco Orsini, Daniele Baracchi, and Paolo Frasconi. Shift aggregate extract networks. arXiv\n",
      "\n",
      "preprint arXiv:1703.05537, 2017.\n",
      "\n",
      "Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry Winograd. The pagerank citation ranking:\n",
      "\n",
      "Bringing order to the web. Technical Report 1999-66, 1999.\n",
      "\n",
      "Shirui Pan, Ruiqi Hu, Guodong Long, Jing Jiang, Lina Yao, and Chengqi Zhang. Adversarially\n",
      "\n",
      "regularized graph autoencoder for graph embedding. In IJCAI, pp. 2609–2615, 2018.\n",
      "\n",
      "Sachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. In ICLR, 2017.\n",
      "\n",
      "Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-Rad.\n",
      "\n",
      "Collective classiﬁcation in network data. AI magazine, 29(3):93–93, 2008.\n",
      "\n",
      "Nino Shervashidze, SVN Vishwanathan, Tobias Petri, Kurt Mehlhorn, and Karsten Borgwardt.\n",
      "Efﬁcient graphlet kernels for large graph comparison. In Artiﬁcial Intelligence and Statistics, pp.\n",
      "488–495, 2009.\n",
      "\n",
      "Flood Sung, Li Zhang, Tao Xiang, Timothy Hospedales, and Yongxin Yang. Learning to learn:\n",
      "\n",
      "Meta-critic networks for sample efﬁcient learning. arXiv preprint arXiv:1706.09529, 2017.\n",
      "\n",
      "Kiran K Thekumparampil, Chong Wang, Sewoong Oh, and Li-Jia Li. Attention-based graph neural\n",
      "\n",
      "network for semi-supervised learning. arXiv preprint arXiv:1803.03735, 2018.\n",
      "\n",
      "Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, and Yoshua\n",
      "\n",
      "Bengio. Graph attention networks. ICLR, 2018.\n",
      "\n",
      "Danfei Xu, Yuke Zhu, Christopher B Choy, and Li Fei-Fei. Scene graph generation by iterative\n",
      "\n",
      "message passing. In CVPR, pp. 5410–5419, 2017.\n",
      "\n",
      "Pinar Yanardag and SVN Vishwanathan. Deep graph kernels. In SIGKDD, pp. 1365–1374, 2015.\n",
      "\n",
      "Zhilin Yang, William W Cohen, and Ruslan Salakhutdinov. Revisiting semi-supervised learning with\n",
      "\n",
      "graph embeddings. ICML, 2016.\n",
      "\n",
      "Bing Yu, Haoteng Yin, and Zhanxing Zhu. Spatio-temporal graph convolutional networks: A deep\n",
      "\n",
      "learning framework for trafﬁc forecasting. In IJCAI, pp. 3634–3640, 2018.\n",
      "\n",
      "Tong Zhang, Zhen Cui, Chunyan Xu, Wenming Zheng, and Jian Yang. Variational pathway reasoning\n",
      "\n",
      "for eeg emotion recognition. In AAAI, 2020.\n",
      "\n",
      "Wenting Zhao, Zhen Cui, Chunyan Xu, Chengzheng Li, Tong Zhang, and Jian Yang. Hashing graph\n",
      "\n",
      "convolution for node classiﬁcation. In CIKM, 2019.\n",
      "\n",
      "Dengyong Zhou, Olivier Bousquet, Thomas N Lal, Jason Weston, and Bernhard Schölkopf. Learning\n",
      "\n",
      "with local and global consistency. In NeurIPS, pp. 321–328, 2004.\n",
      "\n",
      "Xiaojin Zhu, Zoubin Ghahramani, and John D Lafferty. Semi-supervised learning using gaussian\n",
      "\n",
      "ﬁelds and harmonic functions. In ICML, pp. 912–919, 2003.\n",
      "\n",
      "Chenyi Zhuang and Qiang Ma. Dual graph convolutional networks for graph-based semi-supervised\n",
      "\n",
      "classiﬁcation. In WWW, pp. 499–508, 2018.\n",
      "\n",
      "11\n",
      "\n",
      "\f",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "LARGE BATCH OPTIMIZATION FOR DEEP LEARNING:\n",
      "TRAINING BERT IN 76 MINUTES\n",
      "\n",
      "Yang You2, Jing Li1, Sashank Reddi1, Jonathan Hseu1, Sanjiv Kumar1, Srinadh Bhojanapalli1\n",
      "Xiaodan Song1, James Demmel2, Kurt Keutzer2, Cho-Jui Hsieh1,3\n",
      "Yang You was a student researcher at Google Brain. This project was done when he was at Google Brain.\n",
      "Google1, UC Berkeley2, UCLA3\n",
      "{youyang, demmel, keutzer}@cs.berkeley.edu, {jingli, sashank, jhseu, sanjivk, bsrinadh, xiaodansong, chojui}@google.com\n",
      "\n",
      "ABSTRACT\n",
      "\n",
      "Training large deep neural networks on massive datasets is computationally very\n",
      "challenging. There has been recent surge in interest in using large batch stochastic\n",
      "optimization methods to tackle this issue. The most prominent algorithm in this\n",
      "line of research is LARS, which by employing layerwise adaptive learning rates\n",
      "trains RESNET on ImageNet in a few minutes. However, LARS performs poorly for\n",
      "attention models like BERT, indicating that its performance gains are not consistent\n",
      "across tasks. In this paper, we ﬁrst study a principled layerwise adaptation strategy\n",
      "to accelerate training of deep neural networks using large mini-batches. Using this\n",
      "strategy, we develop a new layerwise adaptive large batch optimization technique\n",
      "called LAMB; we then provide convergence analysis of LAMB as well as LARS,\n",
      "showing convergence to a stationary point in general nonconvex settings. Our\n",
      "empirical results demonstrate the superior performance of LAMB across various\n",
      "tasks such as BERT and RESNET-50 training with very little hyperparameter tuning.\n",
      "In particular, for BERT training, our optimizer enables use of very large batch sizes\n",
      "of 32868 without any degradation of performance. By increasing the batch size to\n",
      "the memory limit of a TPUv3 Pod, BERT training time can be reduced from 3 days\n",
      "to just 76 minutes (Table 1). The LAMB implementation is available online1.\n",
      "\n",
      "1\n",
      "\n",
      "INTRODUCTION\n",
      "\n",
      "With the advent of large scale datasets, training large deep neural networks, even using computation-\n",
      "ally efﬁcient optimization methods like Stochastic gradient descent (SGD), has become particularly\n",
      "challenging. For instance, training state-of-the-art deep learning models like BERT and ResNet-50\n",
      "takes 3 days on 16 TPUv3 chips and 29 hours on 8 Tesla P100 gpus respectively (Devlin et al., 2018;\n",
      "He et al., 2016). Thus, there is a growing interest to develop optimization solutions to tackle this\n",
      "critical issue. The goal of this paper is to investigate and develop optimization techniques to accelerate\n",
      "training large deep neural networks, mostly focusing on approaches based on variants of SGD.\n",
      "\n",
      "Methods based on SGD iteratively update the parameters of the model by moving them in a scaled\n",
      "(negative) direction of the gradient calculated on a minibatch. However, SGD’s scalability is limited\n",
      "by its inherent sequential nature. Owing to this limitation, traditional approaches to improve SGD\n",
      "training time in the context of deep learning largely resort to distributed asynchronous setup (Dean\n",
      "et al., 2012; Recht et al., 2011). However, the implicit staleness introduced due to the asynchrony\n",
      "limits the parallelization of the approach, often leading to degraded performance. The feasibility of\n",
      "computing gradient on large minibatches in parallel due to recent hardware advances has seen the\n",
      "resurgence of simply using synchronous SGD with large minibatches as an alternative to asynchronous\n",
      "SGD. However, naïvely increasing the batch size typically results in degradation of generalization\n",
      "performance and reduces computational beneﬁts (Goyal et al., 2017).\n",
      "\n",
      "Synchronous SGD on large minibatches beneﬁts from reduced variance of the stochastic gradients\n",
      "used in SGD. This allows one to use much larger learning rates in SGD, typically of the order square\n",
      "root of the minibatch size. Surprisingly, recent works have demonstrated that up to certain minibatch\n",
      "sizes, linear scaling of the learning rate with minibatch size can be used to further speed up the\n",
      "\n",
      "1https://github.com/tensorflow/addons/blob/master/tensorflow_addons/\n",
      "\n",
      "optimizers/lamb.py\n",
      "\n",
      "1\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "training Goyal et al. (2017). These works also elucidate two interesting aspects to enable the use of\n",
      "linear scaling in large batch synchronous SGD: (i) linear scaling of learning rate is harmful during the\n",
      "initial phase; thus, a hand-tuned warmup strategy of slowly increasing the learning rate needs to be\n",
      "used initially, and (ii) linear scaling of learning rate can be detrimental beyond a certain batch size.\n",
      "Using these tricks, Goyal et al. (2017) was able to drastically reduce the training time of ResNet-50\n",
      "model from 29 hours to 1 hour using a batch size of 8192. While these works demonstrate the\n",
      "feasibility of this strategy for reducing the wall time for training large deep neural networks, they\n",
      "also highlight the need for an adaptive learning rate mechanism for large batch learning.\n",
      "\n",
      "Variants of SGD using layerwise adaptive learning rates have been recently proposed to address this\n",
      "problem. The most successful in this line of research is the LARS algorithm (You et al., 2017), which\n",
      "was initially proposed for training RESNET. Using LARS, ResNet-50 can be trained on ImageNet in\n",
      "just a few minutes! However, it has been observed that its performance gains are not consistent across\n",
      "tasks. For instance, LARS performs poorly for attention models like BERT. Furthermore, theoretical\n",
      "understanding of the adaptation employed in LARS is largely missing. To this end, we study and\n",
      "develop new approaches specially catered to the large batch setting of our interest.\n",
      "\n",
      "Contributions. More speciﬁcally, we make the following main contributions in this paper.\n",
      "\n",
      "• Inspired by LARS, we investigate a general adaptation strategy specially catered to large\n",
      "\n",
      "batch learning and provide intuition for the strategy.\n",
      "\n",
      "• Based on the adaptation strategy, we develop a new optimization algorithm (LAMB) for\n",
      "achieving adaptivity of learning rate in SGD. Furthermore, we provide convergence analysis\n",
      "for both LARS and LAMB to achieve a stationary point in nonconvex settings. We highlight\n",
      "the beneﬁts of using these methods for large batch settings.\n",
      "\n",
      "• We demonstrate the strong empirical performance of LAMB across several challenging tasks.\n",
      "Using LAMB we scale the batch size in training BERT to more than 32k without degrading\n",
      "the performance; thereby, cutting the time down from 3 days to 76 minutes. Ours is the ﬁrst\n",
      "work to reduce BERT training wall time to less than couple of hours.\n",
      "\n",
      "• We also demonstrate the efﬁciency of LAMB for training state-of-the-art image classiﬁcation\n",
      "models like RESNET. To the best of our knowledge, ours is ﬁrst adaptive solver that can\n",
      "achieve state-of-the-art accuracy for RESNET-50 as adaptive solvers like Adam fail to obtain\n",
      "the accuracy of SGD with momentum for these tasks.\n",
      "\n",
      "1.1 RELATED WORK\n",
      "\n",
      "The literature on optimization for machine learning is vast and hence, we restrict our attention to the\n",
      "most relevant works here. Earlier works on large batch optimization for machine learning mostly\n",
      "focused on convex models, beneﬁting by a factor of square root of batch size using appropriately large\n",
      "learning rate. Similar results can be shown for nonconvex settings wherein using larger minibatches\n",
      "improves the convergence to stationary points; albeit at the cost of extra computation. However,\n",
      "several important concerns were raised with respect to generalization and computational performance\n",
      "in large batch nonconvex settings. It was observed that training with extremely large batch was\n",
      "difﬁcult (Keskar et al., 2016; Hoffer et al., 2017). Thus, several prior works carefully hand-tune\n",
      "training hyper-parameters, like learning rate and momentum, to avoid degradation of generalization\n",
      "performance (Goyal et al., 2017; Li, 2017; You et al., 2018; Shallue et al., 2018).\n",
      "\n",
      "(Krizhevsky, 2014) empirically found that simply scaling the learning rate linearly with respect to\n",
      "batch size works better up to certain batch sizes. To avoid optimization instability due to linear scaling\n",
      "of learning rate, Goyal et al. (2017) proposed a highly hand-tuned learning rate which involves a\n",
      "warm-up strategy that gradually increases the LR to a larger value and then switching to the regular\n",
      "LR policy (e.g. exponential or polynomial decay). Using LR warm-up and linear scaling, Goyal et al.\n",
      "(2017) managed to train RESNET-50 with batch size 8192 without loss in generalization performance.\n",
      "However, empirical study (Shallue et al., 2018) shows that learning rate scaling heuristics with the\n",
      "batch size do not hold across all problems or across all batch sizes.\n",
      "\n",
      "More recently, to reduce hand-tuning of hyperparameters, adaptive learning rates for large batch\n",
      "training garnered signiﬁcant interests. Several recent works successfully scaled the batch size to large\n",
      "values using adaptive learning rates without degrading the performance, thereby, ﬁnishing RESNET-\n",
      "50 training on ImageNet in a few minutes (You et al., 2018; Iandola et al., 2016; Codreanu et al.,\n",
      "2017; Akiba et al., 2017; Jia et al., 2018; Smith et al., 2017; Martens & Grosse, 2015; Devarakonda\n",
      "\n",
      "2\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "et al., 2017; Mikami et al., 2018; Osawa et al., 2018; You et al., 2019; Yamazaki et al., 2019). To the\n",
      "best of our knowledge, the fastest training result for RESNET-50 on ImageNet is due to Ying et al.\n",
      "(2018), who achieve 76+% top-1 accuracy. By using the LARS optimizer and scaling the batch size to\n",
      "32K on a TPUv3 Pod, Ying et al. (2018) was able to train RESNET-50 on ImageNet in 2.2 minutes.\n",
      "However, it was empirically observed that none of these performance gains hold in other tasks such\n",
      "as BERT training (see Section 4).\n",
      "\n",
      "2 PRELIMINARIES\n",
      "\n",
      "Notation. For any vector xt ∈ Rd, either xt,j or [xt]j are used to denote its jth coordinate where\n",
      "j ∈ [d]. Let I be the d × d identity matrix, and let I = [I1, I2, ..., Ih] be its decomposition into column\n",
      "submatrices Ii = d × dh. For x ∈ Rd, let x(i) be the block of variables corresponding to the columns\n",
      "of Ii i.e., x(i) = I(cid:62)\n",
      "i x ∈ Rdi for i = {1, 2, · · · , h}. For any function f : Rd → R, we use ∇if (x) to\n",
      "denote the gradient with respect to x(i). For any vectors u, v ∈ Rd, we use u2 and u/v to denote\n",
      "elementwise square and division operators respectively. We use (cid:107).(cid:107) and (cid:107).(cid:107)1 to denote l2-norm and\n",
      "l1-norm of a vector respectively.\n",
      "\n",
      "We start our discussion by formally stating the problem setup. In this paper, we study nonconvex\n",
      "stochastic optimization problems of the form\n",
      "\n",
      "f (x) := Es∼P[(cid:96)(x, s)] +\n",
      "\n",
      "(cid:107)x(cid:107)2,\n",
      "\n",
      "min\n",
      "x∈Rd\n",
      "\n",
      "λ\n",
      "2\n",
      "\n",
      "(1)\n",
      "\n",
      "where (cid:96) is a smooth (possibly nonconvex) function and P is a probability distribution on the domain\n",
      "S ⊂ Rk. Here, x corresponds to model parameters, (cid:96) is the loss function and P is an unknown data\n",
      "distribution.\n",
      "We assume function (cid:96)(x) is Li-smooth with respect to x(i), i.e., there exists a constant Li such that\n",
      "\n",
      "(cid:107)∇i(cid:96)(x, s) − ∇i(cid:96)(y, s)(cid:107) ≤ Li(cid:107)x(i) − y(i)(cid:107),\n",
      "\n",
      "∀ x, y ∈ Rd, and s ∈ S,\n",
      "\n",
      "(2)\n",
      "\n",
      "for all i ∈ [h]. We use L = (L1, · · · , Lh)(cid:62) to denote the h-dimensional vector of Lipschitz constants.\n",
      "We use L∞ and Lavg to denote maxi Li and (cid:80)\n",
      "Li\n",
      "h respectively. We assume the following bound\n",
      "i for all x ∈ Rd and i ∈ [h].\n",
      "on the variance in stochastic gradients: E(cid:107)∇i(cid:96)(x, s) − ∇if (x)(cid:107)2 ≤ σ2\n",
      "Furthermore, we also assume E(cid:107)[∇(cid:96)(x, s)]i − [∇f (x)]i(cid:107)2 ≤ ˜σ2\n",
      "i for all x ∈ Rd and i ∈ [d]. We use\n",
      "σ = (σ1, · · · , σh)(cid:62) and ˜σ = (˜σ1, · · · , ˜σd)(cid:62) to denote the vectors of standard deviations of stochastic\n",
      "gradient per layer and per dimension respectively. Finally, we assume that the gradients are bounded\n",
      "i.e., [∇l(x, s)]j ≤ G for all i ∈ [d], x ∈ Rd and s ∈ S. Note that such assumptions are typical in the\n",
      "analysis of stochastic ﬁrst-order methods (cf. (Ghadimi & Lan, 2013a; Ghadimi et al., 2014)).\n",
      "\n",
      "i\n",
      "\n",
      "Stochastic gradient descent (SGD) is one of the simplest ﬁrst-order algorithms for solving problem in\n",
      "Equation 1. The update at the tth iteration of SGD is of the following form:\n",
      "\n",
      "xt+1 = xt − ηt\n",
      "\n",
      "∇(cid:96)(xt, st) + λxt,\n",
      "\n",
      "(SGD)\n",
      "\n",
      "1\n",
      "|St|\n",
      "\n",
      "(cid:88)\n",
      "\n",
      "st∈St\n",
      "\n",
      "where St is set of b random samples drawn from the distribution P. For very large batch settings, the\n",
      "following is a well-known result for SGD.\n",
      "\n",
      "Theorem 1 ((Ghadimi & Lan, 2013b)). With large batch b = T and using appropriate learning rate,\n",
      "we have the following for the iterates of SGD:\n",
      "\n",
      "E (cid:2)(cid:107)∇f (xa)(cid:107)2(cid:3) ≤ O\n",
      "\n",
      "(cid:18) (f (x1) − f (x∗))L∞\n",
      "\n",
      "(cid:107)σ(cid:107)2\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "T\n",
      "\n",
      "+\n",
      "\n",
      "T\n",
      "\n",
      ".\n",
      "\n",
      "where x∗ is an optimal solution to the problem in equation 1 and xa is an iterate uniformly randomly\n",
      "chosen from {x1, · · · , xT }.\n",
      "\n",
      "However, tuning the learning rate ηt in SGD, especially in large batch settings, is difﬁcult in practice.\n",
      "Furthermore, the dependence on L∞ (the maximum of smoothness across dimension) can lead to\n",
      "signiﬁcantly slow convergence. In the next section, we discuss algorithms to circumvent this issue.\n",
      "\n",
      "3\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "3 ALGORITHMS\n",
      "\n",
      "In this section, we ﬁrst discuss a general strategy to adapt the learning rate in large batch settings.\n",
      "Using this strategy, we discuss two speciﬁc algorithms in the later part of the section. Since our\n",
      "primary focus is on deep learning, our discussion is centered around training a h-layer neural network.\n",
      "\n",
      "General Strategy. Suppose we use an iterative base algorithm A (e.g. SGD or ADAM) in the small\n",
      "batch setting with the following layerwise update rule:\n",
      "\n",
      "where ut is the update made by A at time step t. We propose the following two changes to the update\n",
      "for large batch settings:\n",
      "\n",
      "xt+1 = xt + ηtut,\n",
      "\n",
      "1. The update is normalized to unit l2-norm. This is ensured by modifying the update to the\n",
      "form ut/(cid:107)ut(cid:107). Throughout this paper, such a normalization is done layerwise i.e., the update\n",
      "for each layer is ensured to be unit l2-norm.\n",
      "\n",
      "2. The learning rate is scaled by φ((cid:107)xt(cid:107)) for some function φ : R+ → R+. Similar to the\n",
      "\n",
      "normalization, such a scaling is done layerwise.\n",
      "\n",
      "Suppose the base algorithm A is SGD, then the modiﬁcation results in the following update rule:\n",
      "\n",
      "x(i)\n",
      "t+1 = x(i)\n",
      "\n",
      "t − ηt\n",
      "\n",
      "φ((cid:107)x(i)\n",
      "t (cid:107))\n",
      "(cid:107)g(i)\n",
      "t (cid:107)\n",
      "\n",
      "g(i)\n",
      "t\n",
      "\n",
      ",\n",
      "\n",
      "(3)\n",
      "\n",
      "and g(i)\n",
      "t\n",
      "\n",
      "for all layers i ∈ [h] and where x(i)\n",
      "are the parameters and the gradients of the ith layer at\n",
      "t\n",
      "time step t. The normalization modiﬁcation is similar to one typically used in normalized gradient\n",
      "descent except that it is done layerwise. Note that the modiﬁcation leads to a biased gradient update;\n",
      "however, in large-batch settings, it can be shown that this bias is small. It is intuitive that such a\n",
      "normalization provides robustness to exploding gradients (where the gradient can be arbitrarily large)\n",
      "and plateaus (where the gradient can be arbitrarily small). Normalization of this form essentially\n",
      "ignores the size of the gradient and is particularly useful in large batch settings where the direction of\n",
      "the gradient is largely preserved.\n",
      "\n",
      "The scaling term involving φ ensures that the norm of the update is of the same order as that of\n",
      "the parameter. We found that this typically ensures faster convergence in deep neural networks.\n",
      "In practice, we observed that a simple function of φ(z) = min{max{z, γl}, γu} works well. It is\n",
      "instructive to consider the case where φ(z) = z. In this scenario, the overall change in the learning\n",
      "rate is (cid:107)x(i)\n",
      "t (cid:107)\n",
      "(cid:107)g(i)\n",
      "t (cid:107)\n",
      "gradient (see equation 2). We now discuss different instantiations of the strategy discussed above. In\n",
      "particular, we focus on two algorithms: LARS (3.1) and the proposed method, LAMB (3.2).\n",
      "\n",
      ", which can also be interpreted as an estimate on the inverse of Lipschitz constant of the\n",
      "\n",
      "3.1 LARS ALGORITHM\n",
      "\n",
      "The ﬁrst instantiation of the general strategy is LARS algorithm (You et al., 2017), which is obtained\n",
      "by using momentum optimizer as the base algorithm A in the framework. LARS was earlier proposed\n",
      "for large batch learning for RESNET on ImageNet. In general, it is observed that the using (heavy-ball)\n",
      "momentum, one can reduce the variance in the stochastic gradients at the cost of little bias. The\n",
      "pseudocode for LARS is provide in Algorithm 1.\n",
      "\n",
      "We now provide convergence analysis for LARS in general nonconvex setting stated in this paper. For\n",
      "the sake of simplicity, we analyze the case where β1 = 0 and λ = 0 in Algorithm 1. However, our\n",
      "analysis should extend to the general case as well. We will defer all discussions about the convergence\n",
      "rate to the end of the section.\n",
      "\n",
      "Theorem 2. Let ηt = η =\n",
      "where αl, αu > 0. Then for xt generated using LARS (Algorithm 1), we have the following bound\n",
      "\n",
      "for all t ∈ [T ], b = T , αl ≤ φ(v) ≤ αu for all v > 0\n",
      "\n",
      "u(cid:107)L(cid:107)1T\n",
      "\n",
      "α2\n",
      "\n",
      "(cid:113) 2(f (x1)−f (x∗))\n",
      "\n",
      "(cid:32)\n",
      "\n",
      "(cid:34)\n",
      "\n",
      "E\n",
      "\n",
      "1\n",
      "√\n",
      "h\n",
      "\n",
      "h\n",
      "(cid:88)\n",
      "\n",
      "i=1\n",
      "\n",
      "(cid:35)(cid:33)2\n",
      "\n",
      "(cid:18) (f (x1) − f (x∗))Lavg\n",
      "\n",
      "(cid:107)∇if (xa)(cid:107)\n",
      "\n",
      "≤ O\n",
      "\n",
      "T\n",
      "\n",
      "+\n",
      "\n",
      "(cid:19)\n",
      "\n",
      ",\n",
      "\n",
      "(cid:107)σ(cid:107)2\n",
      "1\n",
      "T h\n",
      "\n",
      "where x∗ is an optimal solution to the problem in equation 1 and xa is an iterate uniformly randomly\n",
      "chosen from {x1, · · · , xT }.\n",
      "\n",
      "4\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "Algorithm 2 LAMB\n",
      "\n",
      "Input: x1 ∈ Rd, learning rate {ηt}T\n",
      "0 < β1, β2 < 1, scaling function φ, (cid:15) > 0\n",
      "Set m0 = 0, v0 = 0\n",
      "for t = 1 to T do\n",
      "\n",
      "t=1, parameters\n",
      "\n",
      "∇(cid:96)(xt, st).\n",
      "\n",
      "Algorithm 1 LARS\n",
      "\n",
      "Input: x1 ∈ Rd, learning rate {ηt}T\n",
      "0 < β1 < 1, scaling function φ, (cid:15) > 0\n",
      "Set m0 = 0\n",
      "for t = 1 to T do\n",
      "\n",
      "t=1, parameter\n",
      "\n",
      "(cid:80)\n",
      "\n",
      "Draw b samples St from P\n",
      "Compute gt = 1\n",
      "∇(cid:96)(xt, st)\n",
      "st∈St\n",
      "|St|\n",
      "mt = β1mt−1 + (1 − β1)(gt + λxt)\n",
      "x(i)\n",
      "t+1 = x(i)\n",
      "\n",
      "t − ηt\n",
      "\n",
      "m(i)\n",
      "t\n",
      "\n",
      "φ((cid:107)x\n",
      "\n",
      "(i)\n",
      "t (cid:107))\n",
      "(i)\n",
      "t (cid:107)\n",
      "\n",
      "(cid:107)m\n",
      "\n",
      "for all i ∈ [h]\n",
      "\n",
      "end for\n",
      "\n",
      "(cid:80)\n",
      "\n",
      "Draw b samples St from P.\n",
      "Compute gt = 1\n",
      "st∈St\n",
      "|St|\n",
      "mt = β1mt−1 + (1 − β1)gt\n",
      "vt = β2vt−1 + (1 − β2)g2\n",
      "t\n",
      "mt = mt/(1 − βt\n",
      "1)\n",
      "vt = vt/(1 − βt\n",
      "2)\n",
      "Compute ratio rt = mt√\n",
      "x(i)\n",
      "t+1 = x(i)\n",
      "\n",
      "t − ηt\n",
      "\n",
      "vt+(cid:15)\n",
      "(i)\n",
      "t (cid:107))\n",
      "(i)\n",
      "t (cid:107)\n",
      "\n",
      "φ((cid:107)x\n",
      "(i)\n",
      "t +λx\n",
      "\n",
      "(cid:107)r\n",
      "\n",
      "end for\n",
      "\n",
      "(r(i)\n",
      "\n",
      "t + λx(i)\n",
      "t )\n",
      "\n",
      "3.2 LAMB ALGORITHM\n",
      "\n",
      "The second instantiation of the general strategy is obtained by using ADAM as the base algorithm A.\n",
      "ADAM optimizer is popular in deep learning community and has shown to have good performance\n",
      "for training state-of-the-art language models like BERT. Unlike LARS, the adaptivity of LAMB is\n",
      "two-fold: (i) per dimension normalization with respect to the square root of the second moment used\n",
      "in ADAM and (ii) layerwise normalization obtained due to layerwise adaptivity. The pseudocode for\n",
      "LAMB is provided in Algorithm 2. When β1 = 0 and β2 = 0, the algorithm reduces to be Sign SGD\n",
      "where the learning rate is scaled by square root of the layer dimension (Bernstein et al., 2018).\n",
      "\n",
      "The following result provides convergence rate for LAMB in general nonconvex settings. Similar to\n",
      "the previous case, we focus on the setting where β1 = 0 and λ = 0. As before, our analysis extends\n",
      "to the general case; however, the calculations become messy.\n",
      "\n",
      "(cid:113) 2(f (x1)−f (x∗))\n",
      "\n",
      "for all t ∈ [T ], b = T , di = d/h for all i ∈ [h], and\n",
      "Theorem 3. Let ηt = η =\n",
      "αl ≤ φ(v) ≤ αu for all v > 0 where αl, αu > 0. Then for xt generated using LAMB (Algorithm 2),\n",
      "we have the following bounds:\n",
      "\n",
      "u(cid:107)L(cid:107)1T\n",
      "\n",
      "α2\n",
      "\n",
      "1. When β2 = 0, we have\n",
      "\n",
      "2. When β2 > 0, we have\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "E\n",
      "\n",
      "(cid:20) 1\n",
      "√\n",
      "d\n",
      "\n",
      "(cid:21)(cid:19)2\n",
      "\n",
      "(cid:18) (f (x1) − f (x∗))Lavg\n",
      "\n",
      "(cid:107)∇f (xa)(cid:107)1\n",
      "\n",
      "≤ O\n",
      "\n",
      "T\n",
      "\n",
      "+\n",
      "\n",
      "(cid:19)\n",
      "\n",
      ",\n",
      "\n",
      "(cid:107)˜σ(cid:107)2\n",
      "1\n",
      "T h\n",
      "\n",
      "E[(cid:107)∇f (xa)(cid:107)2] ≤ O\n",
      "\n",
      "(cid:32)(cid:115)\n",
      "\n",
      "G2d\n",
      "\n",
      "h(1 − β2)\n",
      "\n",
      "(cid:34)(cid:114)\n",
      "\n",
      "×\n",
      "\n",
      "2(f (x1) − f (x∗))(cid:107)L(cid:107)1\n",
      "\n",
      "T\n",
      "\n",
      "(cid:35)(cid:33)\n",
      "\n",
      ",\n",
      "\n",
      "+\n",
      "\n",
      "(cid:107)˜σ(cid:107)1√\n",
      "T\n",
      "\n",
      "where x∗ is an optimal solution to the problem in equation 1 and xa is an iterate uniformly randomly\n",
      "chosen from {x1, · · · , xT }.\n",
      "\n",
      "Discussion on convergence rates. We ﬁrst start our discussion with the comparison of convergence\n",
      "rate of LARS with that of SGD (Theorem 1). The convergence rates of LARS and SGD differ in\n",
      "two ways: (1) the convergence criterion is (E[(cid:80)h\n",
      "i=1 (cid:107)∇if (cid:107)])2 as opposed to E[(cid:107)∇f (cid:107)2] in SGD and\n",
      "(2) the dependence on L and σ in the convergence rate. Brieﬂy, the convergence rate of LARS is\n",
      "better than SGD when the gradient is denser than curvature and stochasticity. This convergence rate\n",
      "comparison is similar in spirit to the one obtained in (Bernstein et al., 2018). Assuming that the\n",
      "convergence criterion in Theorem 1 and Theorem 2 is of similar order (which happens when gradients\n",
      "are fairly dense), convergence rate of LARS and LAMB depend on Lavg instead of L∞ and are thus,\n",
      "signiﬁcantly better than that of SGD. A more quantitative comparison is provided in Section C of\n",
      "the Appendix. The comparison of LAMB (with β2 = 0) with SGD is along similar lines. We obtain\n",
      "slightly worse rates for the case where β2 > 0; although, we believe that its behavior should be better\n",
      "than the case β2 = 0. We leave this investigation to future work.\n",
      "\n",
      "5\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "4 EXPERIMENTS\n",
      "\n",
      "We now present empirical results comparing LAMB with existing optimizers on two important\n",
      "large batch training tasks: BERT and RESNET-50 training. We also compare LAMB with existing\n",
      "optimizers for small batch size (< 1K) and small dataset (e.g. CIFAR, MNIST) (see Appendix).\n",
      "\n",
      "Experimental Setup. To demonstrate its robustness, we use very minimal hyperparameter tuning for\n",
      "the LAMB optimizer. Thus, it is possible to achieve better results by further tuning the hyperparameters.\n",
      "The parameters β1 and β2 in Algorithm 2 are set to 0.9 and 0.999 respectively in all our experiments;\n",
      "we only tune the learning rate. We use a polynomially decaying learning rate of ηt = η0 ×(1−t/T ) in\n",
      "Algorithm 2), which is the same as in BERT baseline. This setting also works for all other applications\n",
      "in this paper. Furthermore, for BERT and RESNET-50 training, we did not tune the hyperparameters\n",
      "of LAMB while increasing the batch size. We use the square root of LR scaling rule to automatically\n",
      "adjust learning rate and linear-epoch warmup scheduling. We use TPUv3 in all the experiments. A\n",
      "TPUv3 Pod has 1024 chips and can provide more than 100 petaﬂops performance for mixed precision\n",
      "computing. To make sure we are comparing with solid baselines, we use grid search to tune the\n",
      "hyper-parameters for ADAM, ADAGRAD, ADAMW (ADAM with weight decay), and LARS. We also\n",
      "tune weight decay for ADAMW. All the hyperparameter tuning settings are reported in the Appendix.\n",
      "Due to space constraints, several experimental details are relegated to the Appendix.\n",
      "\n",
      "4.1 BERT TRAINING\n",
      "\n",
      "We ﬁrst discuss empirical results for speeding up BERT training. For this experiment, we use the same\n",
      "dataset as Devlin et al. (2018), which is a concatenation of Wikipedia and BooksCorpus with 2.5B\n",
      "and 800M words respectively. We speciﬁcally focus on the SQuAD task2 in this paper. The F1 score\n",
      "on SQuAD-v1 is used as the accuracy metric in our experiments. All our comparisons are with respect\n",
      "to the baseline BERT model by Devlin et al. (2018). To train BERT, Devlin et al. (2018) ﬁrst train the\n",
      "model for 900k iterations using a sequence length of 128 and then switch to a sequence length of\n",
      "512 for the last 100k iterations. This results in a training time of around 3 days on 16 TPUv3 chips.\n",
      "The baseline BERT model3 achieves a F1 score of 90.395. To ensure a fair comparison, we follow\n",
      "the same SQuAD ﬁne-tune procedure of Devlin et al. (2018) without modifying any conﬁguration\n",
      "(including number of epochs and hyperparameters). As noted earlier, we could get even better results\n",
      "by changing the ﬁne-tune conﬁguration. For instance, by just slightly changing the learning rate in\n",
      "the ﬁne-tune stage, we can obtain a higher F1 score of 91.688 for the batch size of 16K using LAMB.\n",
      "We report a F1 score of 91.345 in Table 1, which is the score obtained for the untuned version. Below\n",
      "we describe two different training choices for training BERT and discuss the corresponding speedups.\n",
      "\n",
      "For the ﬁrst choice, we maintain the same training procedure as the baseline except for changing the\n",
      "training optimizer to LAMB. We run with the same number of epochs as the baseline but with batch\n",
      "size scaled from 512 to 32K. The choice of 32K batch size (with sequence length 512) is mainly\n",
      "due to memory limits of TPU Pod. Our results are shown in Table 1. By using the LAMB optimizer,\n",
      "we are able to achieve a F1 score of 91.460 in 15625 iterations for a batch size of 32768 (14063\n",
      "iterations for sequence length 128 and 1562 iterations for sequence length 512). With 32K batch size,\n",
      "we reduce BERT training time from 3 days to around 100 minutes. We achieved 49.1 times speedup\n",
      "by 64 times computational resources (76.7% efﬁciency). We consider the speedup is great because we\n",
      "use the synchronous data-parallelism. There is a communication overhead coming from transferring\n",
      "of the gradients over the interconnect. For RESNET-50, researchers are able to achieve 90% scaling\n",
      "efﬁciency because RESNET-50 has much fewer parameters (# parameters is equal to #gradients) than\n",
      "BERT (25 million versus 300 million).\n",
      "\n",
      "To obtain further improvements, we use the Mixed-Batch Training procedure with LAMB. Recall\n",
      "that BERT training involves two stages: the ﬁrst 9/10 of the total epochs use a sequence length of 128,\n",
      "while the last 1/10 of the total epochs use a sequence length of 512. For the second stage training,\n",
      "which involves a longer sequence length, due to memory limits, a maximum batch size of only\n",
      "32768 can be used on a TPUv3 Pod. However, we can potentially use a larger batch size for the\n",
      "ﬁrst stage because of a shorter sequence length. In particular, the batch size can be increased to\n",
      "131072 for the ﬁrst stage. However, we did not observe any speedup by increasing the batch size from\n",
      "65536 to 131072 for the ﬁrst stage, thus, we restrict the batch size to 65536 for this stage. By using\n",
      "this strategy, we are able to make full utilization of the hardware resources throughout the training\n",
      "\n",
      "2https://rajpurkar.github.io/SQuAD-explorer/\n",
      "3Pre-trained BERT model can be downloaded from https://github.com/google-research/bert\n",
      "\n",
      "6\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "Table 1: We use the F1 score on SQuAD-v1 as the accuracy metric. The baseline F1 score is the\n",
      "score obtained by the pre-trained model (BERT-Large) provided on BERT’s public repository (as of\n",
      "February 1st, 2019). We use TPUv3s in our experiments. We use the same setting as the baseline: the\n",
      "ﬁrst 9/10 of the total epochs used a sequence length of 128 and the last 1/10 of the total epochs used\n",
      "a sequence length of 512. All the experiments run the same number of epochs. Dev set means the test\n",
      "data. It is worth noting that we can achieve better results by manually tuning the hyperparameters.\n",
      "The data in this table is collected from the untuned version.\n",
      "\n",
      "Solver\n",
      "\n",
      "batch size\n",
      "\n",
      "steps\n",
      "\n",
      "F1 score on dev set TPUs\n",
      "\n",
      "Time\n",
      "\n",
      "Baseline\n",
      "LAMB\n",
      "LAMB\n",
      "LAMB\n",
      "LAMB\n",
      "LAMB\n",
      "LAMB\n",
      "LAMB\n",
      "\n",
      "512\n",
      "512\n",
      "1k\n",
      "2k\n",
      "4k\n",
      "8k\n",
      "16k\n",
      "32k\n",
      "\n",
      "1000k\n",
      "1000k\n",
      "500k\n",
      "250k\n",
      "125k\n",
      "62500\n",
      "31250\n",
      "15625\n",
      "\n",
      "LAMB\n",
      "\n",
      "64k/32k\n",
      "\n",
      "8599\n",
      "\n",
      "90.395\n",
      "91.752\n",
      "91.761\n",
      "91.946\n",
      "91.137\n",
      "91.263\n",
      "91.345\n",
      "91.475\n",
      "\n",
      "90.584\n",
      "\n",
      "16\n",
      "16\n",
      "32\n",
      "64\n",
      "128\n",
      "256\n",
      "512\n",
      "1024\n",
      "\n",
      "1024\n",
      "\n",
      "81.4h\n",
      "82.8h\n",
      "43.2h\n",
      "21.4h\n",
      "693.6m\n",
      "390.5m\n",
      "200.0m\n",
      "101.2m\n",
      "\n",
      "76.19m\n",
      "\n",
      "procedure. Increasing the batch size is able to warm-up and stabilize the optimization process (Smith\n",
      "et al., 2017), but decreasing the batch size brings chaos to the optimization process and can cause\n",
      "divergence. In our experiments, we found a technique that is useful to stabilize the second stage\n",
      "optimization. Because we switched to a different optimization problem, it is necessary to re-warm-up\n",
      "the optimization. Instead of decaying the learning rate at the second stage, we ramp up the learning\n",
      "rate from zero again in the second stage (re-warm-up). As with the ﬁrst stage, we decay the learning\n",
      "rate after the re-warm-up phase. With this method, we only need 8599 iterations and ﬁnish BERT\n",
      "training in 76 minutes (100.2% efﬁciency).\n",
      "\n",
      "Comparison with ADAMW and LARS. To ensure that our approach is compared to a solid\n",
      "baseline for the BERT training, we tried three different strategies for tuning ADAMW: (1) ADAMW\n",
      "with default hyperparameters (see Devlin et al. (2018)) (2) ADAMW with the same hyperparameters\n",
      "as LAMB, and (3) ADAMW with tuned hyperparameters. ADAMW stops scaling at the batch size of\n",
      "16K because it is not able to achieve the target F1 score (88.1 vs 90.4). The tuning information of\n",
      "ADAMW is shown in the Appendix. For 64K/32K mixed-batch training, even after extensive tuning\n",
      "of the hyperparameters, we fail to get any reasonable result with ADAMW optimizer. We conclude\n",
      "that ADAMW does not work well in large-batch BERT training or is at least hard to tune. We also\n",
      "observe that LAMB performs better than LARS for all batch sizes (see Table 2).\n",
      "\n",
      "Table 2: LAMB achieves a higher performance (F1 score) than LARS for all the batch sizes. The\n",
      "baseline achieves a F1 score of 90.390. Thus, LARS stops scaling at the batch size of 16K.\n",
      "\n",
      "Batch Size\n",
      "\n",
      "512\n",
      "\n",
      "1K\n",
      "\n",
      "2K\n",
      "\n",
      "4K\n",
      "\n",
      "8K\n",
      "\n",
      "16K\n",
      "\n",
      "32K\n",
      "\n",
      "LARS\n",
      "LAMB\n",
      "\n",
      "90.717\n",
      "91.752\n",
      "\n",
      "90.369\n",
      "91.761\n",
      "\n",
      "90.748\n",
      "91.946\n",
      "\n",
      "90.537\n",
      "91.137\n",
      "\n",
      "90.548\n",
      "91.263\n",
      "\n",
      "89.589\n",
      "91.345\n",
      "\n",
      "diverge\n",
      "91.475\n",
      "\n",
      "4.2\n",
      "\n",
      "IMAGENET TRAINING WITH RESNET-50.\n",
      "\n",
      "ImageNet training with ResNet-50 is an industry standard metric that is being used in MLPerf4.\n",
      "The baseline can get 76.3% top-1 accuracy in 90 epochs (Goyal et al., 2017). All the successful\n",
      "implementations are based on momentum SGD (He et al., 2016; Goyal et al., 2017) or LARS optimizer\n",
      "(Ying et al., 2018; Jia et al., 2018; Mikami et al., 2018; You et al., 2018; Yamazaki et al., 2019).\n",
      "Before our study, we did not ﬁnd any paper reporting a state-of-the-art accuracy achieved by ADAM,\n",
      "\n",
      "4https://mlperf.org/\n",
      "\n",
      "7\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "ADAGRAD, or ADAMW optimizer. In our experiments, even with comprehensive hyper-parameter\n",
      "tuning, ADAGRAD/ADAM/ADAMW (with batch size 16K) only achieves 55.38%/66.04%/67.27%\n",
      "top-1 accuracy. After adding learning rate scheme of Goyal et al. (2017), the top-1 accuracy of\n",
      "ADAGRAD/ADAM/ADAMW was improved to 72.0%/73.48%/73.07%. However, they are still much\n",
      "lower than 76.3%. The details of the tuning information are in the Appendix. Table 3 shows that\n",
      "LAMB can achieve the target accuracy. Beyond a batch size of 8K, LAMB’s accuracy is higher than\n",
      "the momentum. LAMB’s accuracy is also slightly better than LARS. At a batch size of 32K, LAMB\n",
      "achieves 76.4% top-1 accuracy while LARS achieves 76.3%. At a batch size of 2K, LAMB is able to\n",
      "achieve 77.11% top-1 accuracy while LARS achieves 76.6%.\n",
      "\n",
      "Table 3: Top-1 validation accuracy of ImageNet/RESNET-50 training at the batch size of 16K (90\n",
      "epochs). The performance of momentum was reported by (Goyal et al., 2017). + means adding the\n",
      "learning rate scheme of Goyal et al. (2017) to the optimizer: (1) 5-epoch warmup to stablize the initial\n",
      "stage; and (2) multiply the learning rate by 0.1 at 30th, 60th, and 80th epoch. The target accuracy is\n",
      "around 0.763 (Goyal et al., 2017). All the adaptive solvers were comprehensively tuned. The tuning\n",
      "information was in the Appendix.\n",
      "adagrad/adagrad+\n",
      "\n",
      "adamw/adamw+ momentum\n",
      "\n",
      "adam/adam+\n",
      "\n",
      "optimizer\n",
      "\n",
      "lamb\n",
      "\n",
      "Accuracy\n",
      "\n",
      "0.5538/0.7201\n",
      "\n",
      "0.6604/0.7348\n",
      "\n",
      "0.6727/0.7307\n",
      "\n",
      "0.7520\n",
      "\n",
      "0.7666\n",
      "\n",
      "4.3 HYPERPARAMETERS FOR SCALING THE BATCH SIZE\n",
      "\n",
      "For BERT and ImageNet training, we did not tune the hyperparameters of LAMB optimizer when\n",
      "increasing the batch size. We use the square root LR scaling rule and linear-epoch warmup scheduling\n",
      "to automatically adjust learning rate. The details can be found in Tables 4 and 5\n",
      "\n",
      "Table 4: Untuned LAMB for BERT training across different batch sizes (ﬁxed #epochs). We use\n",
      "square root LR scaling and linear-epoch warmup. For example, batch size 32K needs to ﬁnish 15625\n",
      "iterations. It uses 0.2×15625 = 3125 iterations for learning rate warmup. BERT’s baseline achieved a\n",
      "F1 score of 90.395. We can achieve an even higher F1 score if we manually tune the hyperparameters.\n",
      "\n",
      "Batch Size\n",
      "\n",
      "512\n",
      "\n",
      "Learning Rate\n",
      "Warmup Ratio\n",
      "\n",
      "F1 score\n",
      "\n",
      "Exact Match\n",
      "\n",
      "5\n",
      "\n",
      "1\n",
      "\n",
      "320\n",
      "\n",
      "91.752\n",
      "85.090\n",
      "\n",
      "23.0×103\n",
      "\n",
      "22.5×103\n",
      "\n",
      "22.0×103\n",
      "\n",
      "21.5×103\n",
      "\n",
      "21.0×103\n",
      "\n",
      "20.5×103\n",
      "\n",
      "20.0×103\n",
      "\n",
      "1K\n",
      "5\n",
      "\n",
      "1\n",
      "\n",
      "160\n",
      "\n",
      "2K\n",
      "5\n",
      "\n",
      "1\n",
      "80\n",
      "\n",
      "4K\n",
      "5\n",
      "\n",
      "1\n",
      "40\n",
      "\n",
      "8K\n",
      "5\n",
      "\n",
      "1\n",
      "20\n",
      "\n",
      "16K\n",
      "\n",
      "32K\n",
      "\n",
      "5\n",
      "\n",
      "1\n",
      "10\n",
      "\n",
      "5\n",
      "\n",
      "1\n",
      "5\n",
      "\n",
      "91.761\n",
      "85.260\n",
      "\n",
      "91.946\n",
      "85.355\n",
      "\n",
      "91.137\n",
      "84.172\n",
      "\n",
      "91.263\n",
      "84.901\n",
      "\n",
      "91.345\n",
      "84.816\n",
      "\n",
      "91.475\n",
      "84.939\n",
      "\n",
      "Table 5: Untuned LAMB for ImageNet training with RESNET-50 for different batch sizes (90 epochs).\n",
      "We use square root LR scaling and linear-epoch warmup. The baseline Goyal et al. (2017) gets 76.3%\n",
      "top-1 accuracy in 90 epochs. Stanford DAWN Bench (Coleman et al., 2017) baseline achieves 93%\n",
      "top-5 accuracy. LAMB achieves both of them. LAMB can achieve an even higher accuracy if we\n",
      "manually tune the hyperparameters.\n",
      "\n",
      "Batch Size\n",
      "\n",
      "Learning Rate\n",
      "Warmup Epochs\n",
      "Top-5 Accuracy\n",
      "Top-1 Accuracy\n",
      "\n",
      "512\n",
      "\n",
      "4\n",
      "\n",
      "23.0×100\n",
      "0.3125\n",
      "0.9335\n",
      "0.7696\n",
      "\n",
      "1K\n",
      "4\n",
      "\n",
      "22.5×100\n",
      "0.625\n",
      "0.9349\n",
      "0.7706\n",
      "\n",
      "2K\n",
      "4\n",
      "\n",
      "1.25\n",
      "0.9353\n",
      "0.7711\n",
      "\n",
      "22.0×100\n",
      "\n",
      "21.5×100\n",
      "\n",
      "21.0×100\n",
      "\n",
      "20.5×100\n",
      "\n",
      "20.0×100\n",
      "\n",
      "4K\n",
      "4\n",
      "\n",
      "2.5\n",
      "\n",
      "8K\n",
      "4\n",
      "\n",
      "5\n",
      "\n",
      "16K\n",
      "\n",
      "4\n",
      "\n",
      "10\n",
      "\n",
      "32K\n",
      "\n",
      "4\n",
      "\n",
      "20\n",
      "\n",
      "0.9332\n",
      "0.7692\n",
      "\n",
      "0.9331\n",
      "0.7689\n",
      "\n",
      "0.9322\n",
      "0.7666\n",
      "\n",
      "0.9308\n",
      "0.7642\n",
      "\n",
      "5 CONCLUSION\n",
      "\n",
      "Large batch techniques are critical to speeding up deep neural network training. In this paper, we\n",
      "propose the LAMB optimizer, which supports adaptive elementwise updating and layerwise learning\n",
      "\n",
      "8\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "rates. Furthermore, LAMB is a general purpose optimizer that works for both small and large batches.\n",
      "We also provided theoretical analysis for the LAMB optimizer, highlighting the cases where it\n",
      "performs better than standard SGD. LAMB achieves a better performance than existing optimizers for\n",
      "a wide range of applications. By using LAMB, we are able to scale the batch size of BERT pre-training\n",
      "to 64K without losing accuracy, thereby, reducing the BERT training time from 3 days to around 76\n",
      "minutes. LAMB is also the ﬁrst large batch adaptive solver that can achieve state-of-the-art accuracy\n",
      "on ImageNet training with RESNET-50.\n",
      "\n",
      "6 ACKNOWLEDGEMENT\n",
      "We want to thank the comments from George Dahl and Jeff Dean. We want to thank Michael Banﬁeld,\n",
      "Dehao Chen, Youlong Cheng, Sameer Kumar, and Zak Stone for TPU Pod support.\n",
      "\n",
      "REFERENCES\n",
      "\n",
      "Takuya Akiba, Shuji Suzuki, and Keisuke Fukuda. Extremely large minibatch sgd: Training resnet-50\n",
      "\n",
      "on imagenet in 15 minutes. arXiv preprint arXiv:1711.04325, 2017.\n",
      "\n",
      "Yoshua Bengio. Practical recommendations for gradient-based training of deep architectures. In\n",
      "\n",
      "Neural networks: Tricks of the trade, pp. 437–478. Springer, 2012.\n",
      "\n",
      "Jeremy Bernstein, Yu-Xiang Wang, Kamyar Azizzadenesheli, and Anima Anandkumar. signsgd:\n",
      "\n",
      "compressed optimisation for non-convex problems. CoRR, abs/1802.04434, 2018.\n",
      "\n",
      "Valeriu Codreanu, Damian Podareanu, and Vikram Saletore. Scale out for large minibatch sgd:\n",
      "Residual network training on imagenet-1k with improved accuracy and reduced time to train. arXiv\n",
      "preprint arXiv:1711.04291, 2017.\n",
      "\n",
      "Cody Coleman, Deepak Narayanan, Daniel Kang, Tian Zhao, Jian Zhang, Luigi Nardi, Peter Bailis,\n",
      "Kunle Olukotun, Chris Ré, and Matei Zaharia. Dawnbench: An end-to-end deep learning bench-\n",
      "mark and competition. Training, 100(101):102, 2017.\n",
      "\n",
      "Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao, Andrew Senior,\n",
      "Paul Tucker, Ke Yang, Quoc V Le, et al. Large scale distributed deep networks. In Advances in\n",
      "neural information processing systems, pp. 1223–1231, 2012.\n",
      "\n",
      "Aditya Devarakonda, Maxim Naumov, and Michael Garland. Adabatch: Adaptive batch sizes for\n",
      "\n",
      "training deep neural networks. arXiv preprint arXiv:1712.02029, 2017.\n",
      "\n",
      "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep\n",
      "\n",
      "bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\n",
      "\n",
      "Timothy Dozat. Incorporating nesterov momentum into adam. 2016.\n",
      "\n",
      "Saeed Ghadimi and Guanghui Lan. Stochastic ﬁrst- and zeroth-order methods for nonconvex\n",
      "stochastic programming. SIAM Journal on Optimization, 23(4):2341–2368, 2013a. doi: 10.1137/\n",
      "120880811.\n",
      "\n",
      "Saeed Ghadimi and Guanghui Lan. Stochastic ﬁrst-and zeroth-order methods for nonconvex stochastic\n",
      "\n",
      "programming. SIAM Journal on Optimization, 23(4):2341–2368, 2013b.\n",
      "\n",
      "Saeed Ghadimi, Guanghui Lan, and Hongchao Zhang. Mini-batch stochastic approximation methods\n",
      "for nonconvex stochastic composite optimization. Mathematical Programming, 155(1-2):267–305,\n",
      "2014.\n",
      "\n",
      "Priya Goyal, Piotr Dollár, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola,\n",
      "Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet\n",
      "in 1 hour. arXiv preprint arXiv:1706.02677, 2017.\n",
      "\n",
      "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image\n",
      "recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,\n",
      "pp. 770–778, 2016.\n",
      "\n",
      "Elad Hoffer, Itay Hubara, and Daniel Soudry. Train longer, generalize better: closing the generalization\n",
      "\n",
      "gap in large batch training of neural networks. arXiv preprint arXiv:1705.08741, 2017.\n",
      "\n",
      "9\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "Forrest N Iandola, Matthew W Moskewicz, Khalid Ashraf, and Kurt Keutzer. Firecaffe: near-linear\n",
      "acceleration of deep neural network training on compute clusters. In Proceedings of the IEEE\n",
      "Conference on Computer Vision and Pattern Recognition, pp. 2592–2600, 2016.\n",
      "\n",
      "Xianyan Jia, Shutao Song, Wei He, Yangzihao Wang, Haidong Rong, Feihu Zhou, Liqiang Xie,\n",
      "Zhenyu Guo, Yuanzhou Yang, Liwei Yu, et al. Highly scalable deep learning training system with\n",
      "mixed-precision: Training imagenet in four minutes. arXiv preprint arXiv:1807.11205, 2018.\n",
      "\n",
      "Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter\n",
      "Tang. On large-batch training for deep learning: Generalization gap and sharp minima. arXiv\n",
      "preprint arXiv:1609.04836, 2016.\n",
      "\n",
      "Alex Krizhevsky. One weird trick for parallelizing convolutional neural networks. arXiv preprint\n",
      "\n",
      "Mu Li. Scaling Distributed Machine Learning with System and Algorithm Co-design. PhD thesis,\n",
      "\n",
      "arXiv:1404.5997, 2014.\n",
      "\n",
      "Intel, 2017.\n",
      "\n",
      "James Martens and Roger Grosse. Optimizing neural networks with kronecker-factored approximate\n",
      "\n",
      "curvature. In International conference on machine learning, pp. 2408–2417, 2015.\n",
      "\n",
      "Hiroaki Mikami, Hisahiro Suganuma, Yoshiki Tanaka, Yuichi Kageyama, et al. Imagenet/resnet-50\n",
      "\n",
      "training in 224 seconds. arXiv preprint arXiv:1811.05233, 2018.\n",
      "\n",
      "Yurii E Nesterov. A method for solving the convex programming problem with convergence rate o\n",
      "\n",
      "(1/kˆ 2). In Dokl. akad. nauk Sssr, volume 269, pp. 543–547, 1983.\n",
      "\n",
      "Kazuki Osawa, Yohei Tsuji, Yuichiro Ueno, Akira Naruse, Rio Yokota, and Satoshi Matsuoka.\n",
      "Second-order optimization method for large mini-batch: Training resnet-50 on imagenet in 35\n",
      "epochs. arXiv preprint arXiv:1811.12019, 2018.\n",
      "\n",
      "Benjamin Recht, Christopher Re, Stephen Wright, and Feng Niu. Hogwild: A lock-free approach to\n",
      "parallelizing stochastic gradient descent. In Advances in neural information processing systems,\n",
      "pp. 693–701, 2011.\n",
      "\n",
      "Christopher J Shallue, Jaehoon Lee, Joe Antognini, Jascha Sohl-Dickstein, Roy Frostig, and George E\n",
      "Dahl. Measuring the effects of data parallelism on neural network training. arXiv preprint\n",
      "arXiv:1811.03600, 2018.\n",
      "\n",
      "Samuel L Smith, Pieter-Jan Kindermans, and Quoc V Le. Don’t decay the learning rate, increase the\n",
      "\n",
      "batch size. arXiv preprint arXiv:1711.00489, 2017.\n",
      "\n",
      "Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. On the importance of initialization\n",
      "and momentum in deep learning. In International conference on machine learning, pp. 1139–1147,\n",
      "2013.\n",
      "\n",
      "Masafumi Yamazaki, Akihiko Kasagi, Akihiro Tabuchi, Takumi Honda, Masahiro Miwa, Naoto\n",
      "Fukumoto, Tsuguchika Tabaru, Atsushi Ike, and Kohta Nakashima. Yet another accelerated sgd:\n",
      "Resnet-50 training on imagenet in 74.7 seconds. arXiv preprint arXiv:1903.12650, 2019.\n",
      "\n",
      "Chris Ying, Sameer Kumar, Dehao Chen, Tao Wang, and Youlong Cheng. Image classiﬁcation at\n",
      "\n",
      "supercomputer scale. arXiv preprint arXiv:1811.06992, 2018.\n",
      "\n",
      "Yang You, Igor Gitman, and Boris Ginsburg. Scaling sgd batch size to 32k for imagenet training.\n",
      "\n",
      "arXiv preprint arXiv:1708.03888, 2017.\n",
      "\n",
      "Yang You, Zhao Zhang, Cho-Jui Hsieh, James Demmel, and Kurt Keutzer. Imagenet training in\n",
      "minutes. In Proceedings of the 47th International Conference on Parallel Processing, pp. 1. ACM,\n",
      "2018.\n",
      "\n",
      "Yang You, Jonathan Hseu, Chris Ying, James Demmel, Kurt Keutzer, and Cho-Jui Hsieh. Large-batch\n",
      "\n",
      "training for lstm and beyond. arXiv preprint arXiv:1901.08256, 2019.\n",
      "\n",
      "10\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "APPENDIX\n",
      "\n",
      "A PROOF OF THEOREM 2\n",
      "\n",
      "Proof. We analyze the convergence of LARS for general minibatch size here. Recall that the update\n",
      "of LARS is the following\n",
      "\n",
      "x(i)\n",
      "t+1 = x(i)\n",
      "\n",
      "t − ηtφ((cid:107)x(i)\n",
      "\n",
      "t (cid:107))\n",
      "\n",
      "g(i)\n",
      "t\n",
      "(cid:107)g(i)\n",
      "t (cid:107)\n",
      "\n",
      ",\n",
      "\n",
      "for all i ∈ [h]. For simplicity of notation, we reason the\n",
      "\n",
      "Since the function f is L-smooth, we have the following:\n",
      "\n",
      "f (xt+1) ≤ f (xt) + (cid:104)∇if (xt), x(i)\n",
      "\n",
      "t+1 − x(i)\n",
      "\n",
      "t (cid:105) +\n",
      "\n",
      "(cid:107)x(i)\n",
      "\n",
      "t+1 − x(i)\n",
      "\n",
      "t (cid:107)2\n",
      "\n",
      "= f (xt) − ηt\n",
      "\n",
      "φ((cid:107)x(i)\n",
      "\n",
      "t (cid:107)) ×\n",
      "\n",
      "[∇if (xt)]j ×\n",
      "\n",
      "≤ f (xt) − ηt\n",
      "\n",
      "φ((cid:107)x(i)\n",
      "\n",
      "t (cid:107)) ×\n",
      "\n",
      "[∇if (xt)]j ×\n",
      "\n",
      "(cid:32)\n",
      "\n",
      "(cid:32)\n",
      "\n",
      "h\n",
      "(cid:88)\n",
      "\n",
      "Liη2\n",
      "\n",
      "t φ2((cid:107)x(i)\n",
      "\n",
      "t (cid:107))\n",
      "\n",
      "+\n",
      "\n",
      "i=1\n",
      "\n",
      "2\n",
      "\n",
      "−\n",
      "\n",
      "[∇if (xt)]j\n",
      "(cid:107)∇if (xt)(cid:107)\n",
      "\n",
      "+\n",
      "\n",
      "[∇if (xt)]j\n",
      "(cid:107)∇if (xt)(cid:107)\n",
      "\n",
      "(cid:33)(cid:33)\n",
      "\n",
      "+\n",
      "\n",
      "t α2\n",
      "η2\n",
      "u\n",
      "2\n",
      "\n",
      "(cid:107)L(cid:107)1\n",
      "\n",
      "= f (xt) − ηt\n",
      "\n",
      "φ((cid:107)x(i)\n",
      "\n",
      "t (cid:107)) × (cid:107)∇if (xt)(cid:107) − ηt\n",
      "\n",
      "[∇if (xt)]j ×\n",
      "\n",
      "(cid:32) g(i)\n",
      "t,j\n",
      "(cid:107)g(i)\n",
      "t (cid:107)\n",
      "\n",
      "−\n",
      "\n",
      "[∇if (xt)]j\n",
      "(cid:107)∇if (xt)(cid:107)\n",
      "\n",
      "+\n",
      "\n",
      "t α2\n",
      "η2\n",
      "u\n",
      "2\n",
      "\n",
      "(cid:107)L(cid:107)1\n",
      "\n",
      "(cid:33)(cid:33)\n",
      "\n",
      "(4)\n",
      "\n",
      "h\n",
      "(cid:88)\n",
      "\n",
      "i=1\n",
      "\n",
      "Li\n",
      "2\n",
      "\n",
      "(cid:33)\n",
      "\n",
      "g(i)\n",
      "t,j\n",
      "(cid:107)g(i)\n",
      "t (cid:107)\n",
      "(cid:32) g(i)\n",
      "t,j\n",
      "(cid:107)g(i)\n",
      "t (cid:107)\n",
      "(cid:32)\n",
      "di(cid:88)\n",
      "\n",
      "h\n",
      "(cid:88)\n",
      "\n",
      "i=1\n",
      "\n",
      "j=1\n",
      "\n",
      "The ﬁrst inequality follows from the lipschitz continuous nature of the gradient. Let ∆(i)\n",
      "∇if (xt). Then the above inequality can be rewritten in the following manner:\n",
      "\n",
      "t = g(i)\n",
      "\n",
      "t −\n",
      "\n",
      "f (xt+1) ≤ f (xt) − ηt\n",
      "\n",
      "φ((cid:107)x(i)\n",
      "\n",
      "t (cid:107))(cid:107)∇if (xt)(cid:107)\n",
      "\n",
      "h\n",
      "(cid:88)\n",
      "\n",
      "i=1\n",
      "\n",
      "− ηt\n",
      "\n",
      "h\n",
      "(cid:88)\n",
      "\n",
      "di(cid:88)\n",
      "\n",
      "i=1\n",
      "\n",
      "j=1\n",
      "\n",
      "(cid:32)\n",
      "\n",
      "φ((cid:107)x(i)\n",
      "\n",
      "t (cid:107)) ×\n",
      "\n",
      "[∇if (xt)]j ×\n",
      "\n",
      "(cid:32) (∆(i)\n",
      "(cid:107)∆(i)\n",
      "\n",
      "t,j + [∇if (xt)]j)\n",
      "t + ∇if (xt)(cid:107)\n",
      "\n",
      "(cid:33)(cid:33)\n",
      "\n",
      "−\n",
      "\n",
      "[∇if (xt)]j\n",
      "(cid:107)∇if (xt)(cid:107)\n",
      "\n",
      "+\n",
      "\n",
      "t α2\n",
      "η2\n",
      "u\n",
      "2\n",
      "\n",
      "(cid:107)L(cid:107)1\n",
      "\n",
      "= f (xt) − ηt\n",
      "\n",
      "φ((cid:107)x(i)\n",
      "\n",
      "t (cid:107))(cid:107)∇if (xt)(cid:107)\n",
      "\n",
      "− ηt\n",
      "\n",
      "φ((cid:107)x(i)\n",
      "\n",
      "t (cid:107)) ×\n",
      "\n",
      "(cid:32)\n",
      "\n",
      "(cid:104)∆(i)\n",
      "\n",
      "t + ∇if (xt), ∇if (xt)(cid:105)\n",
      "(cid:107)∆(i)\n",
      "\n",
      "t + ∇if (xt)(cid:107)\n",
      "\n",
      "− (cid:107)∇if (xt)(cid:107)\n",
      "\n",
      "+\n",
      "\n",
      "(cid:107)L(cid:107)1\n",
      "\n",
      "(cid:33)\n",
      "\n",
      "η2\n",
      "t α2\n",
      "u\n",
      "2\n",
      "\n",
      "= f (xt) − ηt\n",
      "\n",
      "φ((cid:107)x(i)\n",
      "\n",
      "t (cid:107))(cid:107)∇if (xt)(cid:107)\n",
      "\n",
      "+ ηt\n",
      "\n",
      "φ((cid:107)x(i)\n",
      "\n",
      "t (cid:107)) ×\n",
      "\n",
      "(cid:32)\n",
      "\n",
      "(cid:107)∇if (xt)(cid:107)(cid:107)∆(i)\n",
      "\n",
      "t + ∇if (xt)(cid:107) − (cid:104)∆(i)\n",
      "t + ∇if (xt)(cid:107)\n",
      "\n",
      "(cid:107)∆(i)\n",
      "\n",
      "t + ∇if (xt), ∇if (xt)(cid:105)\n",
      "\n",
      "(cid:33)\n",
      "\n",
      "+\n",
      "\n",
      "η2\n",
      "t α2\n",
      "u\n",
      "2\n",
      "\n",
      "(cid:107)L(cid:107)1\n",
      "\n",
      "= f (xt) − ηt\n",
      "\n",
      "φ((cid:107)x(i)\n",
      "\n",
      "t (cid:107))(cid:107)∇if (xt)(cid:107) +\n",
      "\n",
      "η2\n",
      "t α2\n",
      "u\n",
      "2\n",
      "\n",
      "(cid:107)L(cid:107)1\n",
      "\n",
      "+ ηt\n",
      "\n",
      "φ((cid:107)x(i)\n",
      "\n",
      "t (cid:107)) ×\n",
      "\n",
      "h\n",
      "(cid:88)\n",
      "\n",
      "i=1\n",
      "\n",
      "(cid:32)\n",
      "\n",
      "(cid:107)∇if (xt)(cid:107)(cid:107)∆(i)\n",
      "\n",
      "t + ∇if (xt)(cid:107) − (cid:107)∆(i)\n",
      "\n",
      "t + ∇if (xt)(cid:107)2 + (cid:104)∆(i)\n",
      "\n",
      "t\n",
      "\n",
      ", ∆(i)\n",
      "\n",
      "t + ∇if (xt)(cid:105)\n",
      "\n",
      "(cid:33)\n",
      "\n",
      ".\n",
      "\n",
      "(cid:107)∆(i)\n",
      "\n",
      "t + ∇if (xt)(cid:107)\n",
      "\n",
      "(5)\n",
      "\n",
      "h\n",
      "(cid:88)\n",
      "\n",
      "di(cid:88)\n",
      "\n",
      "i=1\n",
      "\n",
      "j=1\n",
      "\n",
      "h\n",
      "(cid:88)\n",
      "\n",
      "di(cid:88)\n",
      "\n",
      "i=1\n",
      "\n",
      "j=1\n",
      "\n",
      "h\n",
      "(cid:88)\n",
      "\n",
      "i=1\n",
      "\n",
      "h\n",
      "(cid:88)\n",
      "\n",
      "i=1\n",
      "\n",
      "h\n",
      "(cid:88)\n",
      "\n",
      "i=1\n",
      "\n",
      "h\n",
      "(cid:88)\n",
      "\n",
      "i=1\n",
      "\n",
      "h\n",
      "(cid:88)\n",
      "\n",
      "i=1\n",
      "\n",
      "h\n",
      "(cid:88)\n",
      "\n",
      "i=1\n",
      "\n",
      "11\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "Using Cauchy-Schwarz inequality in the above inequality, we have:\n",
      "\n",
      "f (xt+1) ≤ f (xt) − ηt\n",
      "\n",
      "φ((cid:107)x(i)\n",
      "\n",
      "t (cid:107))(cid:107)∇if (xt)(cid:107)\n",
      "\n",
      "h\n",
      "(cid:88)\n",
      "\n",
      "i=1\n",
      "\n",
      "h\n",
      "(cid:88)\n",
      "\n",
      "i=1\n",
      "\n",
      "h\n",
      "(cid:88)\n",
      "\n",
      "i=1\n",
      "\n",
      "h\n",
      "(cid:88)\n",
      "\n",
      "i=1\n",
      "\n",
      "h\n",
      "(cid:88)\n",
      "\n",
      "i=1\n",
      "\n",
      "h\n",
      "(cid:88)\n",
      "\n",
      "i=1\n",
      "\n",
      "h\n",
      "(cid:88)\n",
      "\n",
      "i=1\n",
      "\n",
      "+ ηt\n",
      "\n",
      "φ((cid:107)x(i)\n",
      "\n",
      "t (cid:107)) ×\n",
      "\n",
      "(cid:107)∇if (xt)(cid:107) − (cid:107)∆(i)\n",
      "\n",
      "t + ∇if (xt)(cid:107) + (cid:107)∆(i)\n",
      "t (cid:107)\n",
      "\n",
      "(cid:16)\n",
      "\n",
      "(cid:17)\n",
      "\n",
      "+\n",
      "\n",
      "η2\n",
      "t α2\n",
      "u\n",
      "2\n",
      "\n",
      "(cid:107)L(cid:107)1\n",
      "\n",
      "≤ f (xt) − ηt\n",
      "\n",
      "φ((cid:107)x(i)\n",
      "\n",
      "t (cid:107))(cid:107)∇if (xt)(cid:107) + 2ηt\n",
      "\n",
      "φ((cid:107)x(i)\n",
      "\n",
      "t (cid:107)) × (cid:107)∆(i)\n",
      "\n",
      "t (cid:107) +\n",
      "\n",
      "η2\n",
      "t α2\n",
      "u\n",
      "2\n",
      "\n",
      "(cid:107)L(cid:107)1\n",
      "\n",
      "Taking expectation, we obtain the following:\n",
      "\n",
      "E[f (xt+1)] ≤ f (xt) − ηt\n",
      "\n",
      "φ((cid:107)x(i)\n",
      "\n",
      "t (cid:107))(cid:107)∇if (xt)(cid:107) + 2ηt\n",
      "\n",
      "φ((cid:107)x(i)\n",
      "\n",
      "t (cid:107)) × E[(cid:107)∆(i)\n",
      "\n",
      "t (cid:107)] +\n",
      "\n",
      "≤ f (xt) − ηtαl\n",
      "\n",
      "(cid:107)∇if (xt)(cid:107) + 2ηtαu\n",
      "\n",
      "(cid:107)σ(cid:107)1√\n",
      "b\n",
      "\n",
      "+\n",
      "\n",
      "t α2\n",
      "η2\n",
      "u\n",
      "2\n",
      "\n",
      "(cid:107)L(cid:107)1.\n",
      "\n",
      "η2\n",
      "t α2\n",
      "u\n",
      "2\n",
      "\n",
      "(cid:107)L(cid:107)1\n",
      "\n",
      "(6)\n",
      "\n",
      "Summing the above inequality for t = 1 to T and using telescoping sum, we have the following\n",
      "inequality:\n",
      "\n",
      "E[f (xT +1)] ≤ f (x1) − ηαl\n",
      "\n",
      "E[(cid:107)∇if (xt)(cid:107)] + 2ηT\n",
      "\n",
      "αu(cid:107)σ(cid:107)1√\n",
      "\n",
      "+\n",
      "\n",
      "b\n",
      "\n",
      "η2α2\n",
      "uT\n",
      "2\n",
      "\n",
      "(cid:107)L(cid:107)1.\n",
      "\n",
      "T\n",
      "(cid:88)\n",
      "\n",
      "h\n",
      "(cid:88)\n",
      "\n",
      "t=1\n",
      "\n",
      "i=1\n",
      "\n",
      "Rearranging the terms of the above inequality, and dividing by ηT αl, we have:\n",
      "\n",
      "1\n",
      "T\n",
      "\n",
      "T\n",
      "(cid:88)\n",
      "\n",
      "h\n",
      "(cid:88)\n",
      "\n",
      "t=1\n",
      "\n",
      "i=1\n",
      "\n",
      "E[(cid:107)∇if (xt)(cid:107)] ≤\n",
      "\n",
      "f (x1) − E[f (xT +1)]\n",
      "\n",
      "T ηαl\n",
      "\n",
      "2αu(cid:107)σ(cid:107)1\n",
      "\n",
      "+\n",
      "\n",
      "√\n",
      "\n",
      "+\n",
      "\n",
      "bαl\n",
      "\n",
      "ηα2\n",
      "u\n",
      "2αl\n",
      "\n",
      "(cid:107)L(cid:107)1\n",
      "\n",
      "f (x1) − f (x∗)\n",
      "\n",
      "≤\n",
      "\n",
      "T ηαl\n",
      "\n",
      "+\n",
      "\n",
      "2αu(cid:107)σ(cid:107)1\n",
      "\n",
      "√\n",
      "\n",
      "αl\n",
      "\n",
      "b\n",
      "\n",
      "+\n",
      "\n",
      "ηα2\n",
      "u\n",
      "2αl\n",
      "\n",
      "(cid:107)L(cid:107)1.\n",
      "\n",
      "B PROOF OF THEOREM 3\n",
      "\n",
      "Proof. We analyze the convergence of LAMB for general minibatch size here. Recall that the update\n",
      "of LAMB is the following\n",
      "\n",
      "t+1 = x(i)\n",
      "x(i)\n",
      "\n",
      "t − ηtφ((cid:107)x(i)\n",
      "\n",
      "t (cid:107))\n",
      "\n",
      "r(i)\n",
      "t\n",
      "(cid:107)r(i)\n",
      "t (cid:107)\n",
      "\n",
      ",\n",
      "\n",
      "for all i ∈ [h]. For simplicity of notation, we reason the\n",
      "\n",
      "Since the function f is L-smooth, we have the following:\n",
      "\n",
      "f (xt+1) ≤ f (xt) + (cid:104)∇if (xt), x(i)\n",
      "\n",
      "t+1 − x(i)\n",
      "\n",
      "t (cid:105) +\n",
      "\n",
      "(cid:107)x(i)\n",
      "\n",
      "t+1 − x(i)\n",
      "\n",
      "t (cid:107)2\n",
      "\n",
      "= f (xt) −ηt\n",
      "\n",
      "φ((cid:107)x(i)\n",
      "\n",
      "t (cid:107)) ×\n",
      "\n",
      "[∇if (xt)]j ×\n",
      "\n",
      "h\n",
      "(cid:88)\n",
      "\n",
      "di(cid:88)\n",
      "\n",
      "i=1\n",
      "\n",
      "j=1\n",
      "\n",
      "(cid:124)\n",
      "\n",
      "r(i)\n",
      "t,j\n",
      "(cid:107)r(i)\n",
      "t (cid:107)\n",
      "\n",
      "(cid:33)\n",
      "\n",
      "(cid:125)\n",
      "\n",
      "+\n",
      "\n",
      "h\n",
      "(cid:88)\n",
      "\n",
      "i=1\n",
      "\n",
      "Liα2\n",
      "uη2\n",
      "t\n",
      "2\n",
      "\n",
      "(7)\n",
      "\n",
      "h\n",
      "(cid:88)\n",
      "\n",
      "i=1\n",
      "\n",
      "Li\n",
      "2\n",
      "\n",
      "(cid:32)\n",
      "\n",
      "(cid:123)(cid:122)\n",
      "T1\n",
      "\n",
      "12\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "The above inequality simply follows from the lipschitz continuous nature of the gradient. We bound\n",
      "term T1 in the following manner:\n",
      "\n",
      "T1 ≤ −ηt\n",
      "\n",
      "φ((cid:107)x(i)\n",
      "\n",
      "t (cid:107)) ×\n",
      "\n",
      "[∇if (xt)]j ×\n",
      "\n",
      "(cid:32)\n",
      "\n",
      "(cid:33)\n",
      "\n",
      "r(i)\n",
      "t,j\n",
      "(cid:107)r(i)\n",
      "t (cid:107)\n",
      "\n",
      "≤ −ηt\n",
      "\n",
      "(cid:114) 1 − β2\n",
      "G2di\n",
      "\n",
      "(cid:16)\n",
      "\n",
      "φ((cid:107)x(i)\n",
      "\n",
      "t (cid:107)) × [∇if (xt)]j × g(i)\n",
      "\n",
      "t,j\n",
      "\n",
      "(cid:17)\n",
      "\n",
      "h\n",
      "(cid:88)\n",
      "\n",
      "di(cid:88)\n",
      "\n",
      "i=1\n",
      "\n",
      "j=1\n",
      "\n",
      "h\n",
      "(cid:88)\n",
      "\n",
      "di(cid:88)\n",
      "\n",
      "i=1\n",
      "\n",
      "j=1\n",
      "\n",
      "φ((cid:107)x(i)\n",
      "\n",
      "t (cid:107)) × [∇if (xt)]j ×\n",
      "\n",
      "1(sign(∇if (xt)]j) (cid:54)= sign(r(i)\n",
      "\n",
      "t,j ))\n",
      "\n",
      "(cid:33)\n",
      "\n",
      "r(i)\n",
      "t,j\n",
      "(cid:107)r(i)\n",
      "t (cid:107)\n",
      "\n",
      "(cid:33)\n",
      "\n",
      "r(i)\n",
      "t,j\n",
      "(cid:107)r(i)\n",
      "t (cid:107)\n",
      "\n",
      "− ηt\n",
      "\n",
      "h\n",
      "(cid:88)\n",
      "\n",
      "di(cid:88)\n",
      "\n",
      "(cid:32)\n",
      "\n",
      "i=1\n",
      "\n",
      "j=1\n",
      "\n",
      "h\n",
      "(cid:88)\n",
      "\n",
      "di(cid:88)\n",
      "\n",
      "i=1\n",
      "\n",
      "j=1\n",
      "\n",
      "(cid:16)\n",
      "\n",
      "(cid:114) 1\n",
      "di\n",
      "\n",
      "− ηt\n",
      "\n",
      "h\n",
      "(cid:88)\n",
      "\n",
      "di(cid:88)\n",
      "\n",
      "(cid:32)\n",
      "\n",
      "i=1\n",
      "\n",
      "j=1\n",
      "\n",
      "This follows from the fact that (cid:107)r(i)\n",
      "as follows:\n",
      "\n",
      "t (cid:107) ≤\n",
      "\n",
      "(cid:113) di\n",
      "1−β2\n",
      "\n",
      "√\n",
      "\n",
      "and\n",
      "\n",
      "vt ≤ G. If β2 = 0, then T1 can be bounded\n",
      "\n",
      "T1 ≤ −ηt\n",
      "\n",
      "φ((cid:107)x(i)\n",
      "\n",
      "t (cid:107)) × |[∇if (xt)]j|\n",
      "\n",
      "(cid:17)\n",
      "\n",
      "φ((cid:107)x(i)\n",
      "\n",
      "t (cid:107)) × [∇if (xt)]j ×\n",
      "\n",
      "1(sign(∇if (xt)]j) (cid:54)= sign(r(i)\n",
      "\n",
      "t,j ))\n",
      "\n",
      "The rest of the proof for β2 = 0 is similar to argument for the case β2 > 0, which is shown below.\n",
      "Taking expectation, we have the following:\n",
      "\n",
      "E[T1] ≤ −ηt\n",
      "\n",
      "h\n",
      "(cid:88)\n",
      "\n",
      "di(cid:88)\n",
      "\n",
      "i=1\n",
      "\n",
      "j=1\n",
      "\n",
      "(cid:114) 1 − β2\n",
      "G2di\n",
      "\n",
      "(cid:104)\n",
      "φ((cid:107)x(i)\n",
      "\n",
      "E\n",
      "\n",
      "t (cid:107)) ×\n",
      "\n",
      "(cid:16)\n",
      "\n",
      "[∇if (xt)]j × g(i)\n",
      "t,j\n",
      "\n",
      "(cid:17)(cid:105)\n",
      "\n",
      "h\n",
      "(cid:88)\n",
      "\n",
      "di(cid:88)\n",
      "\n",
      "(cid:34)\n",
      "φ((cid:107)x(i)\n",
      "\n",
      "E\n",
      "\n",
      "− ηt\n",
      "\n",
      "(cid:32)\n",
      "\n",
      "t (cid:107)) ×\n",
      "\n",
      "[∇if (xt)]j ×\n",
      "\n",
      "1(sign(∇if (xt)]j) (cid:54)= sign(g(i)\n",
      "\n",
      "t,j ))\n",
      "\n",
      "(cid:35)\n",
      "\n",
      "≤ −ηt\n",
      "\n",
      "h\n",
      "(cid:88)\n",
      "\n",
      "di(cid:88)\n",
      "\n",
      "i=1\n",
      "\n",
      "j=1\n",
      "\n",
      "i=1\n",
      "\n",
      "j=1\n",
      "\n",
      "(cid:114) 1 − β2\n",
      "G2di\n",
      "\n",
      "(cid:104)(cid:16)\n",
      "\n",
      "E\n",
      "\n",
      "φ((cid:107)x(i)\n",
      "\n",
      "t (cid:107)) × [∇if (xt)]j × g(i)\n",
      "\n",
      "t,j\n",
      "\n",
      "(cid:33)\n",
      "\n",
      "r(i)\n",
      "t,j\n",
      "(cid:107)r(i)\n",
      "t (cid:107)\n",
      "\n",
      "(cid:17)(cid:105)\n",
      "\n",
      "E\n",
      "\n",
      "(cid:104)\n",
      "αu|[∇if (xt)]j|1(sign(∇if (xt)]j) (cid:54)= sign(g(i)\n",
      "\n",
      "(cid:105)\n",
      "\n",
      "t,j ))\n",
      "\n",
      "h\n",
      "(cid:88)\n",
      "\n",
      "di(cid:88)\n",
      "\n",
      "+ ηt\n",
      "\n",
      "i=1\n",
      "\n",
      "j=1\n",
      "\n",
      "(cid:114) 1 − β2\n",
      "G2di\n",
      "\n",
      "h\n",
      "(cid:88)\n",
      "\n",
      "di(cid:88)\n",
      "\n",
      "i=1\n",
      "\n",
      "j=1\n",
      "\n",
      "− ηt\n",
      "\n",
      "h\n",
      "(cid:88)\n",
      "\n",
      "di(cid:88)\n",
      "\n",
      "i=1\n",
      "\n",
      "j=1\n",
      "\n",
      "≤ −ηt\n",
      "\n",
      "(cid:104)\n",
      "φ((cid:107)x(i)\n",
      "\n",
      "E\n",
      "\n",
      "t (cid:107)) ×\n",
      "\n",
      "(cid:16)\n",
      "\n",
      "[∇if (xt)]j × g(i)\n",
      "t,j\n",
      "\n",
      "(cid:17)(cid:105)\n",
      "\n",
      "αu|[∇if (xt)]j|P(sign(∇if (xt)]j) (cid:54)= sign(g(i)\n",
      "\n",
      "t,j ))\n",
      "\n",
      "(8)\n",
      "\n",
      "(9)\n",
      "\n",
      "Using the bound on the probability that the signs differ, we get:\n",
      "\n",
      "E[T1] ≤ −ηtαl\n",
      "\n",
      "(cid:107)∇f (xt)(cid:107)2 + ηtαu\n",
      "\n",
      "(cid:114)\n",
      "\n",
      "h(1 − β2)\n",
      "\n",
      "G2d\n",
      "\n",
      "h\n",
      "(cid:88)\n",
      "\n",
      "di(cid:88)\n",
      "\n",
      "i=1\n",
      "\n",
      "j=1\n",
      "\n",
      "σi,j√\n",
      "b\n",
      "\n",
      ".\n",
      "\n",
      "Substituting the above bound on T1 in equation 7, we have the following bound:\n",
      "\n",
      "E[f (xt+1)] ≤ f (xt) − ηtαl\n",
      "\n",
      "(cid:107)∇f (xt)(cid:107)2 + ηtαu\n",
      "\n",
      "(cid:107)˜σ(cid:107)1√\n",
      "b\n",
      "\n",
      "+\n",
      "\n",
      "t α2\n",
      "η2\n",
      "u(cid:107)L(cid:107)1\n",
      "2\n",
      "\n",
      "(10)\n",
      "\n",
      "(cid:114)\n",
      "\n",
      "h(1 − β2)\n",
      "\n",
      "G2d\n",
      "\n",
      "13\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "Algorithm 3 N-LAMB\n",
      "\n",
      "Input: x1 ∈ Rd, learning rate {ηt}T\n",
      "t=1, parame-\n",
      "ters 0 < β1, β2 < 1, scaling function φ, (cid:15) > 0,\n",
      "parameters 0 < {βt\n",
      "Set m0 = 0, v0 = 0\n",
      "for t = 1 to T do\n",
      "\n",
      "t=1 < 1\n",
      "\n",
      "1}T\n",
      "\n",
      "Algorithm 4 NN-LAMB\n",
      "\n",
      "Input: x1 ∈ Rd, learning rate {ηt}T\n",
      "t=1, parameters\n",
      "0 < β1, β2 < 1, scaling function φ, (cid:15) > 0, parame-\n",
      "ters 0 < {βt\n",
      "Set m0 = 0, v0 = 0\n",
      "for t = 1 to T do\n",
      "\n",
      "t=1 < 1\n",
      "\n",
      "1}T\n",
      "\n",
      "∇(cid:96)(xt, st).\n",
      "\n",
      "∇(cid:96)(xt, st).\n",
      "\n",
      "(cid:80)\n",
      "\n",
      "+ (1−βt\n",
      "1−Πt\n",
      "\n",
      "Draw b samples St from P.\n",
      "Compute gt = 1\n",
      "st∈St\n",
      "|St|\n",
      "mt = β1mt−1 + (1 − β1)gt\n",
      "ˆm = βt+1\n",
      "1 mt\n",
      "1)gt\n",
      "i=1βi\n",
      "1−Πt+1\n",
      "i=1 βi\n",
      "1\n",
      "1\n",
      "vt = β2vt−1 + (1 − β2)g2\n",
      "t\n",
      "ˆv = β2vt\n",
      "1−βt\n",
      "2\n",
      "Compute ratio rt = ˆm√\n",
      "x(i)\n",
      "t+1 = x(i)\n",
      "\n",
      "t − ηt\n",
      "\n",
      "ˆv+(cid:15)\n",
      "(i)\n",
      "t (cid:107))\n",
      "(i)\n",
      "t (cid:107)\n",
      "\n",
      "φ((cid:107)x\n",
      "(i)\n",
      "t +λx\n",
      "\n",
      "(cid:107)r\n",
      "\n",
      "(cid:80)\n",
      "\n",
      "+ (1−βt\n",
      "1−Πt\n",
      "\n",
      "Draw b samples St from P.\n",
      "Compute gt = 1\n",
      "st∈St\n",
      "|St|\n",
      "mt = β1mt−1 + (1 − β1)gt\n",
      "ˆm = βt+1\n",
      "1 mt\n",
      "1)gt\n",
      "i=1βi\n",
      "1−Πt+1\n",
      "i=1 βi\n",
      "1\n",
      "1\n",
      "vt = β2vt−1 + (1 − β2)g2\n",
      "t\n",
      "ˆv = βt+1\n",
      "2)g2\n",
      "+ (1−βt\n",
      "vt\n",
      "t\n",
      "1−Πt\n",
      "i=1βi\n",
      "1−Πt+1\n",
      "2\n",
      "Compute ratio rt = ˆm√\n",
      "x(i)\n",
      "t+1 = x(i)\n",
      "\n",
      "t − ηt\n",
      "\n",
      "i=1 βi\n",
      "2\n",
      "\n",
      "ˆv+(cid:15)\n",
      "(i)\n",
      "t (cid:107))\n",
      "(i)\n",
      "t (cid:107)\n",
      "\n",
      "φ((cid:107)x\n",
      "(i)\n",
      "t +λx\n",
      "\n",
      "(cid:107)r\n",
      "\n",
      "2\n",
      "\n",
      "end for\n",
      "\n",
      "end for\n",
      "\n",
      "(r(i)\n",
      "\n",
      "t + λxt)\n",
      "\n",
      "(r(i)\n",
      "\n",
      "t + λxt)\n",
      "\n",
      "Summing the above inequality for t = 1 to T and using telescoping sum, we have the following\n",
      "inequality:\n",
      "\n",
      "E[f (xT +1)] ≤ f (x1) − ηtαl\n",
      "\n",
      "E[(cid:107)∇f (xt)(cid:107)2] + ηT αu\n",
      "\n",
      "(cid:114)\n",
      "\n",
      "h(1 − β2)\n",
      "\n",
      "G2d\n",
      "\n",
      "T\n",
      "(cid:88)\n",
      "\n",
      "t=1\n",
      "\n",
      "(cid:107)˜σ(cid:107)1√\n",
      "b\n",
      "\n",
      "+\n",
      "\n",
      "η2α2\n",
      "uT\n",
      "2\n",
      "\n",
      "(cid:107)L(cid:107)1.\n",
      "\n",
      "Rearranging the terms of the above inequality, and dividing by ηT αl, we have:\n",
      "\n",
      "(cid:114)\n",
      "\n",
      "h(1 − β2)\n",
      "\n",
      "G2d\n",
      "\n",
      "1\n",
      "T\n",
      "\n",
      "T\n",
      "(cid:88)\n",
      "\n",
      "t=1\n",
      "\n",
      "E[(cid:107)∇f (xt)(cid:107)2] ≤\n",
      "\n",
      "f (x1) − E[f (xT +1)]\n",
      "\n",
      "T ηαl\n",
      "\n",
      "f (x1) − f (x∗)\n",
      "\n",
      "≤\n",
      "\n",
      "T ηαl\n",
      "\n",
      "+\n",
      "\n",
      "αu(cid:107)˜σ(cid:107)1\n",
      "αl\n",
      "b\n",
      "\n",
      "√\n",
      "\n",
      "+\n",
      "\n",
      "√\n",
      "\n",
      "αu(cid:107)˜σ(cid:107)1\n",
      "b\n",
      "αl\n",
      "ηα2\n",
      "u\n",
      "2αl\n",
      "\n",
      "+\n",
      "\n",
      "+\n",
      "\n",
      "(cid:107)L(cid:107)1\n",
      "\n",
      "η\n",
      "2\n",
      "\n",
      "(cid:107)L(cid:107)1.\n",
      "\n",
      "C COMPARISON OF CONVERGENCE RATES OF LARS AND SGD\n",
      "\n",
      "Inspired by the comparison used by (Bernstein et al., 2018) for comparing SIGN SGD with SGD, we\n",
      "deﬁne the following quantities:\n",
      "\n",
      "(cid:33)2\n",
      "\n",
      "(cid:107)∇if (xt)(cid:107)\n",
      "\n",
      "=\n",
      "\n",
      "(cid:32) h\n",
      "(cid:88)\n",
      "\n",
      "i=1\n",
      "\n",
      "ψ(∇f (xt))d(cid:107)∇f (xt)(cid:107)2\n",
      "\n",
      "ψgd(cid:107)∇f (xt)(cid:107)2\n",
      "\n",
      "≥\n",
      "\n",
      "h\n",
      "\n",
      "Then LARS convergence rate can be written in the following manner:\n",
      "\n",
      "(E[(cid:107)∇f (xa)(cid:107))2 ≤ O\n",
      "\n",
      "(cid:18) (f (x1) − f (x∗))L∞\n",
      "\n",
      "ψL\n",
      "ψ2\n",
      "g\n",
      "\n",
      "+\n",
      "\n",
      "(cid:107)σ(cid:107)2\n",
      "\n",
      "T\n",
      "\n",
      "(cid:19)\n",
      "\n",
      ".\n",
      "\n",
      "ψ2\n",
      "σ\n",
      "ψ2\n",
      "g\n",
      "\n",
      "If ψL (cid:28) ψ2\n",
      "we gain over SGD. Otherwise, SGD’s upper bound on convergence rate is better.\n",
      "\n",
      "g then LARS (i.e., gradient is more denser than curvature or stochasticity),\n",
      "\n",
      "g and ψσ (cid:28) ψ2\n",
      "\n",
      "(cid:107)L(cid:107)2\n",
      "\n",
      "1 ≤\n",
      "\n",
      "(cid:107)σ(cid:107)2\n",
      "\n",
      "1 =\n",
      "\n",
      "ψLd2(cid:107)L(cid:107)2\n",
      "∞\n",
      "\n",
      "h2\n",
      "\n",
      "ψσd(cid:107)σ(cid:107)2\n",
      "\n",
      ".\n",
      "\n",
      "h\n",
      "\n",
      "h\n",
      "\n",
      "T\n",
      "\n",
      "14\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "Figure 1: This ﬁgure shows N-LAMB and NN-LAMB can achieve a comparable accuracy compared\n",
      "to LAMB optimizer. Their performances are much better than momentum solver. The result of\n",
      "momentum optimizer was reported by Goyal et al. (2017). For Nadam, we use the learning rate recipe\n",
      "of (Goyal et al., 2017): (1) 5-epoch warmup to stablize the initial stage; and (2) multiply the learning\n",
      "rate by 0.1 at 30th, 60th, and 80th epoch. The target accuracy is around 0.763 (Goyal et al., 2017).\n",
      "We also tuned the learning rate of Nadam in {1e-4, 2e-4, ..., 9e-4, 1e-3, 2e-3, ..., 9e-3, 1e-2}.\n",
      "\n",
      "D N-LAMB: NESTEROV MOMENTUM FOR LAMB\n",
      "\n",
      "Sutskever et al. (2013) report that Nesterov’s accelerated gradient (NAG) proposed by Nesterov (1983)\n",
      "is conceptually and empirically better than the regular momentum method for convex, non-stochastic\n",
      "objectives. Dozat (2016) incorporated Nesterov’s momentum into Adam optimizer and proposed\n",
      "the Nadam optimizer. Speciﬁcally, only the ﬁrst moment of Adam was modiﬁed and the second\n",
      "moment of Adam was unchanged. The results on several applications (Word2Vec, Image Recognition,\n",
      "and LSTM Language Model) showed that Nadam optimizer improves the speed of convergence\n",
      "and the quality of the learned models. We also tried using Nesterov’s momentum to replace the\n",
      "regular momentum of LAMB optimizer’s ﬁrst moment. In this way, we got a new algorithm named\n",
      "as N-LAMB (Nesterov LAMB). The complete algorithm is in Algorithm 3. We can also Nesterov’s\n",
      "momentum to replace the regular momentum of LAMB optimizer’s second moment. We refer to this\n",
      "algorithm as NN-LAMB (Nesterov’s momentum for both the ﬁrst moment and the second moment).\n",
      "The details of NN-LAMB were shown in Algorithm 4.\n",
      "\n",
      "Dozat (2016) suggested the best performance of Nadam was achieved by β1 = 0.975, β2 = 0.999, and\n",
      "(cid:15) = 1e-8. We used the same settings for N-LAMB and NN-LAMB. We scaled the batch size to 32K\n",
      "for ImageNet training with ResNet-50. Our experimental results show that N-LAMB and NN-LAMB\n",
      "can achieve a comparable accuracy compared to LAMB optimizer. Their performances are much\n",
      "better than momentum solver (Figure 1).\n",
      "\n",
      "E LAMB WITH LEARNING RATE CORRECTION\n",
      "\n",
      "There are two operations at each iteration in original Adam optimizer (let us call it adam-correction):\n",
      "\n",
      "mt = mt/(1 − βt\n",
      "1)\n",
      "\n",
      "vt = vt/(1 − βt\n",
      "2)\n",
      "It has an impact on the learning rate by ηt := ηt∗(cid:112)(1 − βt\n",
      "1). According to our experimental\n",
      "results, adam-correction essentially has the same effect as learning rate warmup (see Figure 2). The\n",
      "warmup function often was implemented in the modern deep learning system. Thus, we can remove\n",
      "adam-correction from the LAMB optimizer. We did not observe any drop in the test or validation\n",
      "accuracy for BERT and ImageNet training.\n",
      "\n",
      "2)/(1 − βt\n",
      "\n",
      "15\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "Figure 2: The ﬁgure shows that adam-correction has the same effect as learning rate warmup. We\n",
      "removed adam-correction from the LAMB optimizer. We did not observe any drop in the test or\n",
      "validation accuracy for BERT and ImageNet training.\n",
      "\n",
      "Figure 3: We tried different norms in LAMB optimizer. However, we did not observe a signiﬁcant\n",
      "difference in the validation accuracy of ImageNet training with ResNet-50. We use L2 norm as the\n",
      "default.\n",
      "\n",
      "F LAMB WITH DIFFERENT NORMS\n",
      "\n",
      "We need to compute the matrix/tensor norm for each layer when we do the parameter updating in\n",
      "the LAMB optimizer. We tried different norms in LAMB optimizer. However, we did not observe\n",
      "a signiﬁcant difference in the validation accuracy of ImageNet training with ResNet-50. In our\n",
      "experiments, the difference in validation accuracy is less than 0.1 percent (Figure 3). We use L2 norm\n",
      "as the default.\n",
      "\n",
      "G REGULAR BATCH SIZES FOR SMALL DATASETS: MNIST AND CIFAR-10.\n",
      "\n",
      "According to DAWNBench, DavidNet (a custom 9-layer Residual ConvNet) is the fastest model\n",
      "for CIFAR-10 dataset (as of April 1st, 2019)5. The baseline uses the momentum SGD optimizer.\n",
      "Table 6 and Figure 4 show the test accuracy of CIFAR-10 training with DavidNet. The PyTorch\n",
      "implementation (momentum SGD optimizer) on GPUs was reported on Standford DAWNBench’s\n",
      "website, which achieves 94.06% in 24 epochs. The Tensorﬂow implementation (momentum SGD\n",
      "optimizer) on TPU achieves a 93.72% accuracy in 24 epochs6. We use the implementation of\n",
      "TensorFlow on TPUs. LAMB optimizer is able to achieve 94.08% test accuracy in 24 epochs, which\n",
      "is better than other adaptive optimizers and momentum SGD. Even on the smaller tasks like MNIST\n",
      "training with LeNet, LAMB is able to achieve a better accuracy than existing solvers (Table 7).\n",
      "\n",
      "5https://dawn.cs.stanford.edu/benchmark/CIFAR10/train.html\n",
      "6https://github.com/fenwickslab/dl_tutorials/blob/master/tutorial3_cifar10_davidnet_ﬁx.ipynb\n",
      "\n",
      "16\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "Figure 4: LAMB is better than the existing solvers (batch size = 512). We make sure all the solvers are\n",
      "carefully tuned. The learning rate tuning space of Adam, AdamW, Adagrad and LAMB is {0.0001,\n",
      "0.0002, 0.0004, 0.0006, 0.0008, 0.001, 0.002, 0.004, 0.006, 0.008, 0.01, 0.02, 0.04, 0.06, 0.08, 0.1,\n",
      "0.2, 0.4, 0.6, 0.8, 1, 2, 4, 6, 8, 10, 15, 20, 25, 30, 35, 40, 45, 50}. The momentum optimizer was tuned\n",
      "by the baseline implementer. The weight decay term of AdamW was tuned by {0.0001, 0.001, 0.01,\n",
      "0.1, 1.0}.\n",
      "\n",
      "Table 6: CIFAR-10 training with DavidNet (batch size = 512). All of them run 24 epochs and ﬁnish\n",
      "the training under one minute on one cloud TPU. We make sure all the solvers are carefully tuned.\n",
      "The learning rate tuning space of Adam, AdamW, Adagrad and LAMB is {0.0001, 0.0002, 0.0004,\n",
      "0.0006, 0.0008, 0.001, 0.002, 0.004, 0.006, 0.008, 0.01, 0.02, 0.04, 0.06, 0.08, 0.1, 0.2, 0.4, 0.6, 0.8,\n",
      "1, 2, 4, 6, 8, 10, 15, 20, 25, 30, 35, 40, 45, 50}. The momentum optimizer was tuned by the baseline\n",
      "implementer. The weight decay term of AdamW was tuned by {0.0001, 0.001, 0.01, 0.1, 1.0}.\n",
      "\n",
      "Optimizer\n",
      "\n",
      "ADAGRAD ADAM ADAMW momentum LAMB\n",
      "\n",
      "Test Accuracy\n",
      "\n",
      "0.9074\n",
      "\n",
      "0.9225\n",
      "\n",
      "0.9271\n",
      "\n",
      "0.9372\n",
      "\n",
      "0.9408\n",
      "\n",
      "H IMPLEMENTATION DETAILS AND ADDITIONAL RESULTS\n",
      "\n",
      "There are several hyper-parameters in LAMB optimizer. Although users do not need to tune them,\n",
      "we explain them to help users to have a better understanding. β1 is used for decaying the running\n",
      "average of the gradient. β2 is used for decaying the running average of the square of gradient. The\n",
      "default setting for other parameters: weight decay rate λ=0.01, β1=0.9, β2=0.999, (cid:15)=1e-6. We did not\n",
      "tune β1 and β2. However, our experiments show that tuning them may get a higher accuracy.\n",
      "\n",
      "Based on our experience, learning rate is the most important hyper-parameter that affects the learning\n",
      "efﬁciency and ﬁnal accuracy. Bengio (2012) suggests that it is often the single most important\n",
      "hyper-parameter and that it always should be tuned. Thus, to make sure we have a solid baseline, we\n",
      "carefully tune the learning rate of ADAM, ADAMW, ADAGRAD, and momentum SGD\n",
      "\n",
      "In our experiments, we found that the validation loss is not reliable for large-batch training. A lower\n",
      "validation loss does not necessarily lead to a higher validation accuracy (Figure 5). Thus, we use the\n",
      "test/val accuracy or F1 score on dev set to evaluate the optimizers.\n",
      "\n",
      "H.0.1 BERT\n",
      "\n",
      "Table 8 shows some of the tuning information from BERT training with ADAMW optimizer. ADAMW\n",
      "stops scaling at the batch size of 16K. The target F1 score is 90.5. LAMB achieves a F1 score of\n",
      "91.345. The table shows the tuning information of ADAMW. In Table 8, we report the best F1 score\n",
      "we observed from our experiments.\n",
      "\n",
      "The loss curves of BERT training by LAMB for different batch sizes are shown in Figure 6. We\n",
      "observe that the loss curves are almost identical to each other, which means our optimizer scales well\n",
      "with the batch size.\n",
      "\n",
      "The training loss curve of BERT mixed-batch pre-training with LAMB is shown in Figure 7. This\n",
      "ﬁgure shows that LAMB can make the training converge smoothly at the batch size of 64K.\n",
      "\n",
      "Figure 8 shows that we can achieve 76.8% scaling efﬁciency by scaling the batch size (49.1 times\n",
      "speedup by 64 times computational resources) and 101.8% scaling efﬁciency with mixed-batch (65.2\n",
      "times speedup by 64 times computational resources)\n",
      "\n",
      "17\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "Table 7: Test Accuracy by MNIST training with LeNet (30 epochs for Batch Size = 1024). The\n",
      "tuning space of learning rate for all the optimizers is {0.0001, 0.001, 0.01, 0.1}. We use the same\n",
      "learning rate warmup and decay schedule for all of them.\n",
      "\n",
      "Optimizer\n",
      "\n",
      "Momentum Addgrad ADAM ADAMW LAMB\n",
      "\n",
      "Average accuracy over 5 runs\n",
      "\n",
      "0.9933\n",
      "\n",
      "0.9928\n",
      "\n",
      "0.9936\n",
      "\n",
      "0.9941\n",
      "\n",
      "0.9945\n",
      "\n",
      "Figure 5: Our experiments show that even the validation loss is not reliable in the large-scale training.\n",
      "A lower validation loss may lead to a worse accuracy. Thus, we use the test/val accuracy or F1 score\n",
      "on dev set to evaluate the optimizers.\n",
      "\n",
      "H.0.2\n",
      "\n",
      "IMAGENET\n",
      "\n",
      "Figures 9 - 14 show the LAMB trust ratio at different iterations for ImageNet training with ResNet-50.\n",
      "From these ﬁgures we can see that these ratios are very different from each other for different layers.\n",
      "LAMB uses the trust ratio to help the slow learners to train faster.\n",
      "\n",
      "H.1 BASELINE TUNING DETAILS FOR IMAGENET TRAINING WITH RESNET-50\n",
      "\n",
      "If you are not interested in the baseline tuning details, please skip this section.\n",
      "\n",
      "Goyal et al. (2017) suggested a proper learning rate warmup and decay scheme may help improve\n",
      "the ImageNet classiﬁcation accuracy. We included these techniques in Adam/AdamW/AdaGrad\n",
      "tuning. Speciﬁcally, we use the learning rate recipe of Goyal et al. (2017): (1) 5-epoch warmup\n",
      "to stablize the initial stage; and (2) multiply the learning rate by 0.1 at 30th, 60th, and 80th\n",
      "epoch. The target accuracy is around 76.3% (Goyal et al., 2017). There techniques help to im-\n",
      "prove the accuracy of Adam/AdamW/AdaGrad to around 73%. However, even with these techniques,\n",
      "Adam/AdamW/AdaGrad stil can not achieve the target validation accuracy.\n",
      "\n",
      "To make sure our baseline is solid, we carefully tuned the hyper-parameters. Table 9 shows the tuning\n",
      "information of standard Adagrad. Table 10 shows the tuning information of adding the learning rate\n",
      "scheme of Goyal et al. (2017) to standard Adagrad. Table 11 shows the tuning information of standard\n",
      "Adam. Table shows the tuning information of adding the learning rate scheme of Goyal et al. (2017)\n",
      "to standard Adam. It is tricky to tune the AdamW optimizer since both the L2 regularization and\n",
      "weight decay have the effect on the performance. Thus we have four tuning sets.\n",
      "\n",
      "The ﬁrst tuning set is based on AdamW with default L2 regularization. We tune the learning rate and\n",
      "weight decay. The tuning information is in Figures 13, 14, 15, and 16.\n",
      "\n",
      "The second tuning set is based on AdamW with disabled L2 regularization. We tune the learning rate\n",
      "and weight decay. The tuning information is in Figures 17, 18, 19, and 20.\n",
      "\n",
      "18\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "Table 8: ADAMW stops scaling at the batch size of 16K. The target F1 score is 90.5. LAMB achieves\n",
      "a F1 score of 91.345. The table shows the tuning information of ADAMW. In this table, we report the\n",
      "best F1 score we observed from our experiments.\n",
      "LR\n",
      "\n",
      "batch size warmup steps\n",
      "\n",
      "last step infomation\n",
      "\n",
      "F1 score on dev set\n",
      "\n",
      "Solver\n",
      "\n",
      "ADAMW\n",
      "ADAMW\n",
      "ADAMW\n",
      "ADAMW\n",
      "ADAMW\n",
      "ADAMW\n",
      "ADAMW\n",
      "ADAMW\n",
      "ADAMW\n",
      "\n",
      "16K\n",
      "16K\n",
      "16K\n",
      "16K\n",
      "16K\n",
      "16K\n",
      "16K\n",
      "16K\n",
      "16K\n",
      "\n",
      "0.05×31250\n",
      "0.05×31250\n",
      "0.05×31250\n",
      "0.10×31250\n",
      "0.10×31250\n",
      "0.10×31250\n",
      "0.20×31250\n",
      "0.20×31250\n",
      "0.20×31250\n",
      "\n",
      "0.0001\n",
      "0.0002\n",
      "0.0003\n",
      "0.0001\n",
      "0.0002\n",
      "0.0003\n",
      "0.0001\n",
      "0.0002\n",
      "0.0003\n",
      "\n",
      "loss=8.04471, step=28126\n",
      "loss=7.89673, step=28126\n",
      "loss=8.35102, step=28126\n",
      "loss=2.01419, step=31250\n",
      "loss=1.04689, step=31250\n",
      "loss=8.05845, step=20000\n",
      "loss=1.53706, step=31250\n",
      "loss=1.15500, step=31250\n",
      "loss=1.48798, step=31250\n",
      "\n",
      "diverged\n",
      "diverged\n",
      "diverged\n",
      "86.034\n",
      "88.540\n",
      "diverged\n",
      "85.231\n",
      "88.110\n",
      "85.653\n",
      "\n",
      "Figure 6: This ﬁgure shows the training loss curve of LAMB optimizer. We just want to use this ﬁgure\n",
      "to show that LAMB can make the training converge smoothly. Even if we scale the batch size to the\n",
      "extremely large cases, the loss curves are almost identical to each other.\n",
      "\n",
      "Then we add the learning rate scheme of Goyal et al. (2017) to AdamW and refer to it as AdamW+.\n",
      "\n",
      "The third tuning set is based on AdamW+ with default L2 regularization. We tune the learning rate\n",
      "and weight decay. The tuning information is Figure 21 and 22.\n",
      "\n",
      "The fourth tuning set is based on AdamW+ with disabled L2 regularization. We tune the learning rate\n",
      "and weight decay. The tuning information is in Figures 23, 24, 25.\n",
      "\n",
      "Based on our comprehensive tuning results, we conclude the existing adaptive solvers do not perform\n",
      "well on ImageNet training or at least it is hard to tune them.\n",
      "\n",
      "19\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "Figure 7: This ﬁgure shows the training loss curve of LAMB optimizer. This ﬁgure shows that LAMB\n",
      "can make the training converge smoothly at the extremely large batch size (e.g. 64K).\n",
      "\n",
      "Figure 8: We achieve 76.8% scaling efﬁciency (49 times speedup by 64 times computational resources)\n",
      "and 101.8% scaling efﬁciency with a mixed, scaled batch size (65.2 times speedup by 64 times\n",
      "computational resources). 1024-mixed means the mixed-batch training on 1024 TPUs.\n",
      "\n",
      "Figure 9: The LAMB trust ratio.\n",
      "\n",
      "20\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "Figure 10: The LAMB trust ratio.\n",
      "\n",
      "Figure 11: The LAMB trust ratio.\n",
      "\n",
      "Figure 12: The LAMB trust ratio.\n",
      "\n",
      "Figure 13: The LAMB trust ratio.\n",
      "\n",
      "21\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "Figure 14: The LAMB trust ratio.\n",
      "\n",
      "Table 9: The accuracy information of tuning default AdaGrad optimizer for ImageNet training with\n",
      "ResNet-50 (batch size = 16384, 90 epochs, 7038 iterations).\n",
      "\n",
      "Learning Rate Top-1 Validation Accuracy\n",
      "\n",
      "0.0001\n",
      "0.001\n",
      "0.002\n",
      "0.004\n",
      "0.008\n",
      "0.010\n",
      "0.020\n",
      "0.040\n",
      "0.080\n",
      "0.100\n",
      "0.200\n",
      "0.400\n",
      "0.800\n",
      "1.000\n",
      "2.000\n",
      "4.000\n",
      "6.000\n",
      "8.000\n",
      "10.00\n",
      "12.00\n",
      "14.00\n",
      "16.00\n",
      "18.00\n",
      "20.00\n",
      "30.00\n",
      "40.00\n",
      "50.00\n",
      "\n",
      "0.0026855469\n",
      "0.015563965\n",
      "0.022684732\n",
      "0.030924479\n",
      "0.04486084\n",
      "0.054158527\n",
      "0.0758667\n",
      "0.1262614\n",
      "0.24037679\n",
      "0.27357993\n",
      "0.458313\n",
      "0.553833\n",
      "0.54103595\n",
      "0.5489095\n",
      "0.47680664\n",
      "0.5295207\n",
      "0.36950684\n",
      "0.31081137\n",
      "0.30670166\n",
      "0.3091024\n",
      "0.3227946\n",
      "\n",
      "0.0063680015\n",
      "0.11287435\n",
      "0.21602376\n",
      "0.08315023\n",
      "0.0132039385\n",
      "0.0009969076\n",
      "\n",
      "22\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "Table 10: The accuracy information of tuning AdaGrad optimizer for ImageNet training with ResNet-\n",
      "50 (batch size = 16384, 90 epochs, 7038 iterations). We use the learning rate recipe of (Goyal et al.,\n",
      "2017): (1) 5-epoch warmup to stablize the initial stage; and (2) multiply the learning rate by 0.1 at\n",
      "30th, 60th, and 80th epoch. The target accuracy is around 0.763 (Goyal et al., 2017).\n",
      "\n",
      "Learning Rate Top-1 Validation Accuracy\n",
      "\n",
      "0.0001\n",
      "0.001\n",
      "0.002\n",
      "0.004\n",
      "0.008\n",
      "0.010\n",
      "0.020\n",
      "0.040\n",
      "0.080\n",
      "0.100\n",
      "0.200\n",
      "0.400\n",
      "0.800\n",
      "1.000\n",
      "2.000\n",
      "4.000\n",
      "6.000\n",
      "8.000\n",
      "10.00\n",
      "12.00\n",
      "14.00\n",
      "16.00\n",
      "18.00\n",
      "20.00\n",
      "30.00\n",
      "40.00\n",
      "50.00\n",
      "\n",
      "0.0011189779\n",
      "0.00793457\n",
      "0.012573242\n",
      "0.019022623\n",
      "0.027079264\n",
      "0.029012045\n",
      "0.0421346\n",
      "0.06618246\n",
      "0.10970052\n",
      "0.13429768\n",
      "0.26550293\n",
      "0.41918945\n",
      "0.5519816\n",
      "0.58614093\n",
      "0.67252606\n",
      "0.70306396\n",
      "0.709493\n",
      "0.7137858\n",
      "0.71797687\n",
      "0.7187703\n",
      "0.72007245\n",
      "0.7194214\n",
      "0.7149251\n",
      "0.71293133\n",
      "0.70458984\n",
      "0.69085693\n",
      "0.67976886\n",
      "\n",
      "23\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "Table 11: The accuracy information of tuning default Adam optimizer for ImageNet training with\n",
      "ResNet-50 (batch size = 16384, 90 epochs, 7038 iterations). The target accuracy is around 0.763\n",
      "(Goyal et al., 2017).\n",
      "\n",
      "Learning Rate Top-1 Validation Accuracy\n",
      "\n",
      "Table 12: The accuracy information of tuning Adam optimizer for ImageNet training with ResNet-50\n",
      "(batch size = 16384, 90 epochs, 7038 iterations). We use the learning rate recipe of (Goyal et al.,\n",
      "2017): (1) 5-epoch warmup to stablize the initial stage; and (2) multiply the learning rate by 0.1 at\n",
      "30th, 60th, and 80th epoch. The target accuracy is around 0.763 (Goyal et al., 2017).\n",
      "\n",
      "Learning Rate Top-1 Validation Accuracy\n",
      "\n",
      "0.0001\n",
      "0.0002\n",
      "0.0004\n",
      "0.0006\n",
      "0.0008\n",
      "0.001\n",
      "0.002\n",
      "0.004\n",
      "0.006\n",
      "0.008\n",
      "0.010\n",
      "\n",
      "0.0001\n",
      "0.0002\n",
      "0.0004\n",
      "0.0006\n",
      "0.0008\n",
      "0.001\n",
      "0.002\n",
      "0.004\n",
      "0.006\n",
      "0.008\n",
      "0.010\n",
      "0.012\n",
      "0.014\n",
      "0.016\n",
      "0.020\n",
      "0.040\n",
      "0.060\n",
      "0.080\n",
      "0.100\n",
      "\n",
      "0.5521\n",
      "0.6089\n",
      "0.6432\n",
      "0.6465\n",
      "0.6479\n",
      "0.6604\n",
      "0.6408\n",
      "0.5687\n",
      "0.5165\n",
      "0.4812\n",
      "0.3673\n",
      "\n",
      "0.410319\n",
      "0.55263263\n",
      "0.6455485\n",
      "0.6774495\n",
      "0.6996867\n",
      "0.71010333\n",
      "0.73476154\n",
      "0.73286945\n",
      "0.72648114\n",
      "0.72214764\n",
      "0.71466064\n",
      "0.7081502\n",
      "0.6993001\n",
      "0.69108075\n",
      "0.67997235\n",
      "0.58658856\n",
      "0.51090497\n",
      "0.45174155\n",
      "0.40297446\n",
      "\n",
      "24\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "Table 13: The accuracy information of tuning default AdamW optimizer for ImageNet training with\n",
      "ResNet-50 (batch size = 16384, 90 epochs, 7038 iterations). The target accuracy is around 0.763\n",
      "(Goyal et al., 2017).\n",
      "\n",
      "learning rate weight decay L2 regularization Top-1 Validation Accuracy\n",
      "\n",
      "0.0001\n",
      "0.0002\n",
      "0.0004\n",
      "0.0006\n",
      "0.0008\n",
      "0.001\n",
      "0.002\n",
      "0.004\n",
      "0.006\n",
      "0.008\n",
      "0.010\n",
      "0.012\n",
      "0.014\n",
      "0.016\n",
      "0.018\n",
      "0.020\n",
      "0.025\n",
      "0.030\n",
      "0.040\n",
      "0.050\n",
      "\n",
      "0.00001\n",
      "0.00001\n",
      "0.00001\n",
      "0.00001\n",
      "0.00001\n",
      "0.00001\n",
      "0.00001\n",
      "0.00001\n",
      "0.00001\n",
      "0.00001\n",
      "0.00001\n",
      "0.00001\n",
      "0.00001\n",
      "0.00001\n",
      "0.00001\n",
      "0.00001\n",
      "0.00001\n",
      "0.00001\n",
      "0.00001\n",
      "0.00001\n",
      "\n",
      "default (0.01)\n",
      "default (0.01)\n",
      "default (0.01)\n",
      "default (0.01)\n",
      "default (0.01)\n",
      "default (0.01)\n",
      "default (0.01)\n",
      "default (0.01)\n",
      "default (0.01)\n",
      "default (0.01)\n",
      "default (0.01)\n",
      "default (0.01)\n",
      "default (0.01)\n",
      "default (0.01)\n",
      "default (0.01)\n",
      "default (0.01)\n",
      "default (0.01)\n",
      "default (0.01)\n",
      "default (0.01)\n",
      "default (0.01)\n",
      "\n",
      "0.53312176\n",
      "0.5542806\n",
      "0.48769125\n",
      "0.46317545\n",
      "0.40903726\n",
      "0.42401123\n",
      "0.33870444\n",
      "0.12339274\n",
      "0.122924805\n",
      "0.08099365\n",
      "0.016764322\n",
      "0.032714844\n",
      "0.018147787\n",
      "0.0066731772\n",
      "0.010294597\n",
      "0.008260091\n",
      "0.008870442\n",
      "0.0064493814\n",
      "0.0018107096\n",
      "0.003540039\n",
      "\n",
      "25\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "Table 14: The accuracy information of tuning default AdamW optimizer for ImageNet training with\n",
      "ResNet-50 (batch size = 16384, 90 epochs, 7038 iterations). The target accuracy is around 0.763\n",
      "(Goyal et al., 2017).\n",
      "\n",
      "learning rate weight decay L2 regularization Top-1 Validation Accuracy\n",
      "\n",
      "0.0001\n",
      "0.0002\n",
      "0.0004\n",
      "0.0006\n",
      "0.0008\n",
      "0.001\n",
      "0.002\n",
      "0.004\n",
      "0.006\n",
      "0.008\n",
      "0.010\n",
      "0.012\n",
      "0.014\n",
      "0.016\n",
      "0.018\n",
      "0.020\n",
      "0.025\n",
      "0.030\n",
      "0.040\n",
      "0.050\n",
      "\n",
      "0.0001\n",
      "0.0001\n",
      "0.0001\n",
      "0.0001\n",
      "0.0001\n",
      "0.0001\n",
      "0.0001\n",
      "0.0001\n",
      "0.0001\n",
      "0.0001\n",
      "0.0001\n",
      "0.0001\n",
      "0.0001\n",
      "0.0001\n",
      "0.0001\n",
      "0.0001\n",
      "0.0001\n",
      "0.0001\n",
      "0.0001\n",
      "0.0001\n",
      "\n",
      "default (0.01)\n",
      "default (0.01)\n",
      "default (0.01)\n",
      "default (0.01)\n",
      "default (0.01)\n",
      "default (0.01)\n",
      "default (0.01)\n",
      "default (0.01)\n",
      "default (0.01)\n",
      "default (0.01)\n",
      "default (0.01)\n",
      "default (0.01)\n",
      "default (0.01)\n",
      "default (0.01)\n",
      "default (0.01)\n",
      "default (0.01)\n",
      "default (0.01)\n",
      "default (0.01)\n",
      "default (0.01)\n",
      "default (0.01)\n",
      "\n",
      "0.55489093\n",
      "0.56514484\n",
      "0.4986979\n",
      "0.47595215\n",
      "0.44685873\n",
      "0.41029868\n",
      "0.2808024\n",
      "0.08111572\n",
      "0.068115234\n",
      "0.057922363\n",
      "0.05222575\n",
      "0.017313639\n",
      "0.029785156\n",
      "0.016540527\n",
      "0.00575765\n",
      "0.0102335615\n",
      "0.0060831704\n",
      "0.0036417644\n",
      "0.0010782877\n",
      "0.0037638347\n",
      "\n",
      "26\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "Table 15: The accuracy information of tuning default AdamW optimizer for ImageNet training with\n",
      "ResNet-50 (batch size = 16384, 90 epochs, 7038 iterations). The target accuracy is around 0.763\n",
      "(Goyal et al., 2017).\n",
      "\n",
      "learning rate weight decay L2 regularization Top-1 Validation Accuracy\n",
      "\n",
      "0.0001\n",
      "0.0002\n",
      "0.0004\n",
      "0.0006\n",
      "0.0008\n",
      "0.001\n",
      "0.002\n",
      "0.004\n",
      "0.006\n",
      "0.008\n",
      "0.010\n",
      "0.012\n",
      "0.014\n",
      "0.016\n",
      "0.018\n",
      "0.020\n",
      "0.025\n",
      "0.030\n",
      "0.040\n",
      "0.050\n",
      "\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "\n",
      "default (0.01)\n",
      "default (0.01)\n",
      "default (0.01)\n",
      "default (0.01)\n",
      "default (0.01)\n",
      "default (0.01)\n",
      "default (0.01)\n",
      "default (0.01)\n",
      "default (0.01)\n",
      "default (0.01)\n",
      "default (0.01)\n",
      "default (0.01)\n",
      "default (0.01)\n",
      "default (0.01)\n",
      "default (0.01)\n",
      "default (0.01)\n",
      "default (0.01)\n",
      "default (0.01)\n",
      "default (0.01)\n",
      "default (0.01)\n",
      "\n",
      "0.21142578\n",
      "0.4289144\n",
      "0.13537598\n",
      "0.33803305\n",
      "0.32611084\n",
      "0.22194417\n",
      "0.1833903\n",
      "0.08256022\n",
      "0.020507812\n",
      "0.018269857\n",
      "0.007507324\n",
      "0.020080566\n",
      "0.010762532\n",
      "0.0021362305\n",
      "0.007954915\n",
      "0.005859375\n",
      "0.009724935\n",
      "0.0019124349\n",
      "0.00390625\n",
      "0.0009969076\n",
      "\n",
      "27\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "Table 16: The accuracy information of tuning default AdamW optimizer for ImageNet training with\n",
      "ResNet-50 (batch size = 16384, 90 epochs, 7038 iterations). The target accuracy is around 0.763\n",
      "(Goyal et al., 2017).\n",
      "\n",
      "learning rate weight decay L2 regularization Top-1 Validation Accuracy\n",
      "\n",
      "0.0001\n",
      "0.0002\n",
      "0.0004\n",
      "0.0006\n",
      "0.0008\n",
      "0.001\n",
      "0.002\n",
      "0.004\n",
      "0.006\n",
      "0.008\n",
      "0.010\n",
      "0.012\n",
      "0.014\n",
      "0.016\n",
      "0.018\n",
      "0.020\n",
      "0.025\n",
      "0.030\n",
      "0.040\n",
      "0.050\n",
      "\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "\n",
      "0.0009765625\n",
      "0.0009969076\n",
      "0.0010172526\n",
      "0.0009358724\n",
      "0.0022379558\n",
      "0.001566569\n",
      "0.009480794\n",
      "0.0033569336\n",
      "0.0029907227\n",
      "0.0018513998\n",
      "0.009134929\n",
      "0.0022176106\n",
      "0.0040690103\n",
      "0.0017293295\n",
      "0.00061035156\n",
      "0.0022379558\n",
      "0.0017089844\n",
      "0.0014241537\n",
      "0.0020345051\n",
      "0.0012817383\n",
      "\n",
      "default (0.01)\n",
      "default (0.01)\n",
      "default (0.01)\n",
      "default (0.01)\n",
      "default (0.01)\n",
      "default (0.01)\n",
      "default (0.01)\n",
      "default (0.01)\n",
      "default (0.01)\n",
      "default (0.01)\n",
      "default (0.01)\n",
      "default (0.01)\n",
      "default (0.01)\n",
      "default (0.01)\n",
      "default (0.01)\n",
      "default (0.01)\n",
      "default (0.01)\n",
      "default (0.01)\n",
      "default (0.01)\n",
      "default (0.01)\n",
      "\n",
      "28\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "Table 17: The accuracy information of tuning default AdamW optimizer for ImageNet training with\n",
      "ResNet-50 (batch size = 16384, 90 epochs, 7038 iterations). The target accuracy is around 0.763\n",
      "(Goyal et al., 2017).\n",
      "\n",
      "learning rate weight decay L2 regularization Top-1 Validation Accuracy\n",
      "\n",
      "0.0001\n",
      "0.0002\n",
      "0.0004\n",
      "0.0006\n",
      "0.0008\n",
      "0.001\n",
      "0.002\n",
      "0.004\n",
      "0.006\n",
      "0.008\n",
      "0.010\n",
      "0.012\n",
      "0.014\n",
      "0.016\n",
      "0.018\n",
      "0.020\n",
      "0.025\n",
      "0.030\n",
      "0.040\n",
      "0.050\n",
      "\n",
      "0.00001\n",
      "0.00001\n",
      "0.00001\n",
      "0.00001\n",
      "0.00001\n",
      "0.00001\n",
      "0.00001\n",
      "0.00001\n",
      "0.00001\n",
      "0.00001\n",
      "0.00001\n",
      "0.00001\n",
      "0.00001\n",
      "0.00001\n",
      "0.00001\n",
      "0.00001\n",
      "0.00001\n",
      "0.00001\n",
      "0.00001\n",
      "0.00001\n",
      "\n",
      "0.48917642\n",
      "0.58152264\n",
      "0.63460284\n",
      "0.64849854\n",
      "0.6598918\n",
      "0.6662801\n",
      "0.67266846\n",
      "0.6692708\n",
      "0.6573079\n",
      "0.6639404\n",
      "0.65230304\n",
      "0.6505534\n",
      "0.64990234\n",
      "0.65323895\n",
      "0.67026776\n",
      "0.66086835\n",
      "0.65425617\n",
      "0.6476237\n",
      "0.55478925\n",
      "0.61869305\n",
      "\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "\n",
      "29\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "Table 18: The accuracy information of tuning default AdamW optimizer for ImageNet training with\n",
      "ResNet-50 (batch size = 16384, 90 epochs, 7038 iterations). The target accuracy is around 0.763\n",
      "(Goyal et al., 2017).\n",
      "\n",
      "learning rate weight decay L2 regularization Top-1 Validation Accuracy\n",
      "\n",
      "0.0001\n",
      "0.0002\n",
      "0.0004\n",
      "0.0006\n",
      "0.0008\n",
      "0.001\n",
      "0.002\n",
      "0.004\n",
      "0.006\n",
      "0.008\n",
      "0.010\n",
      "0.012\n",
      "0.014\n",
      "0.016\n",
      "0.018\n",
      "0.020\n",
      "0.025\n",
      "0.030\n",
      "0.040\n",
      "0.050\n",
      "\n",
      "0.0001\n",
      "0.0001\n",
      "0.0001\n",
      "0.0001\n",
      "0.0001\n",
      "0.0001\n",
      "0.0001\n",
      "0.0001\n",
      "0.0001\n",
      "0.0001\n",
      "0.0001\n",
      "0.0001\n",
      "0.0001\n",
      "0.0001\n",
      "0.0001\n",
      "0.0001\n",
      "0.0001\n",
      "0.0001\n",
      "0.0001\n",
      "0.0001\n",
      "\n",
      "0.5033366\n",
      "0.5949707\n",
      "0.62561035\n",
      "0.6545207\n",
      "0.66326904\n",
      "0.6677043\n",
      "0.67244464\n",
      "0.6702881\n",
      "0.66033936\n",
      "0.66426593\n",
      "0.66151935\n",
      "0.6545817\n",
      "0.65509033\n",
      "0.6529338\n",
      "0.65651447\n",
      "0.65334064\n",
      "0.655009\n",
      "0.64552814\n",
      "0.6425374\n",
      "0.5988159\n",
      "\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "\n",
      "30\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "Table 19: The accuracy information of tuning default AdamW optimizer for ImageNet training with\n",
      "ResNet-50 (batch size = 16384, 90 epochs, 7038 iterations). The target accuracy is around 0.763\n",
      "(Goyal et al., 2017).\n",
      "\n",
      "learning rate weight decay L2 regularization Top-1 Validation Accuracy\n",
      "\n",
      "0.0001\n",
      "0.0002\n",
      "0.0004\n",
      "0.0006\n",
      "0.0008\n",
      "0.001\n",
      "0.002\n",
      "0.004\n",
      "0.006\n",
      "0.008\n",
      "0.010\n",
      "0.012\n",
      "0.014\n",
      "0.016\n",
      "0.018\n",
      "0.020\n",
      "0.025\n",
      "0.030\n",
      "0.040\n",
      "0.050\n",
      "\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "\n",
      "0.4611206\n",
      "\n",
      "0.0076293945\n",
      "0.29233804\n",
      "0.57295734\n",
      "0.5574748\n",
      "0.5988566\n",
      "0.586263\n",
      "0.62076825\n",
      "0.61503094\n",
      "0.4697876\n",
      "0.619751\n",
      "0.54243976\n",
      "0.5429077\n",
      "0.55281574\n",
      "0.5819295\n",
      "0.5938924\n",
      "0.541097\n",
      "0.45890298\n",
      "0.56193036\n",
      "0.5279134\n",
      "\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "\n",
      "31\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "Table 20: The accuracy information of tuning default AdamW optimizer for ImageNet training with\n",
      "ResNet-50 (batch size = 16384, 90 epochs, 7038 iterations). The target accuracy is around 0.763\n",
      "(Goyal et al., 2017).\n",
      "\n",
      "learning rate weight decay L2 regularization Top-1 Validation Accuracy\n",
      "\n",
      "0.0001\n",
      "0.0002\n",
      "0.0004\n",
      "0.0006\n",
      "0.0008\n",
      "0.001\n",
      "0.002\n",
      "0.004\n",
      "0.006\n",
      "0.008\n",
      "0.010\n",
      "0.012\n",
      "0.014\n",
      "0.016\n",
      "0.018\n",
      "0.020\n",
      "0.025\n",
      "0.030\n",
      "0.040\n",
      "0.050\n",
      "\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "\n",
      "0.0009969076\n",
      "0.0008951823\n",
      "0.00095621747\n",
      "0.0012817383\n",
      "0.016886393\n",
      "0.038146973\n",
      "0.0015258789\n",
      "0.0014241537\n",
      "0.081441246\n",
      "0.028116861\n",
      "0.011820476\n",
      "0.08138021\n",
      "0.010111491\n",
      "0.0041910806\n",
      "0.0038248699\n",
      "0.002746582\n",
      "0.011555989\n",
      "0.0065104165\n",
      "0.016438803\n",
      "0.007710775\n",
      "\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "\n",
      "32\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "Table 21: The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet-\n",
      "50 (batch size = 16384, 90 epochs, 7038 iterations). We use the learning rate recipe of (Goyal et al.,\n",
      "2017): (1) 5-epoch warmup to stablize the initial stage; and (2) multiply the learning rate by 0.1 at\n",
      "30th, 60th, and 80th epoch. The target accuracy is around 0.763 (Goyal et al., 2017).\n",
      "\n",
      "learning rate weight decay L2 regularization Top-1 Validation Accuracy\n",
      "\n",
      "0.0001\n",
      "0.0002\n",
      "0.0004\n",
      "0.0006\n",
      "0.0008\n",
      "0.001\n",
      "0.002\n",
      "0.004\n",
      "0.006\n",
      "0.008\n",
      "0.0001\n",
      "0.0002\n",
      "0.0004\n",
      "0.0006\n",
      "0.0008\n",
      "0.001\n",
      "0.002\n",
      "0.004\n",
      "0.006\n",
      "0.008\n",
      "\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "\n",
      "default (0.01)\n",
      "default (0.01)\n",
      "default (0.01)\n",
      "default (0.01)\n",
      "default (0.01)\n",
      "default (0.01)\n",
      "default (0.01)\n",
      "default (0.01)\n",
      "default (0.01)\n",
      "default (0.01)\n",
      "default (0.01)\n",
      "default (0.01)\n",
      "default (0.01)\n",
      "default (0.01)\n",
      "default (0.01)\n",
      "default (0.01)\n",
      "default (0.01)\n",
      "default (0.01)\n",
      "default (0.01)\n",
      "default (0.01)\n",
      "\n",
      "0.0009969076\n",
      "0.0009969076\n",
      "0.0009969076\n",
      "0.0009358724\n",
      "0.0009969076\n",
      "0.0009765625\n",
      "0.0010172526\n",
      "0.0010172526\n",
      "0.0010172526\n",
      "0.0010172526\n",
      "0.0010172526\n",
      "0.0010172526\n",
      "0.0010172526\n",
      "0.0009969076\n",
      "0.0010172526\n",
      "0.0010172526\n",
      "0.0010172526\n",
      "0.0038452148\n",
      "0.011881511\n",
      "0.0061442056\n",
      "\n",
      "33\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "Table 22: The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet-\n",
      "50 (batch size = 16384, 90 epochs, 7038 iterations). We use the learning rate recipe of (Goyal et al.,\n",
      "2017): (1) 5-epoch warmup to stablize the initial stage; and (2) multiply the learning rate by 0.1 at\n",
      "30th, 60th, and 80th epoch. The target accuracy is around 0.763 (Goyal et al., 2017).\n",
      "\n",
      "learning rate weight decay L2 regularization Top-1 Validation Accuracy\n",
      "\n",
      "0.0001\n",
      "0.0002\n",
      "0.0004\n",
      "0.0006\n",
      "0.0008\n",
      "0.001\n",
      "0.002\n",
      "0.004\n",
      "0.006\n",
      "0.008\n",
      "0.0001\n",
      "0.0002\n",
      "0.0004\n",
      "0.0006\n",
      "0.0008\n",
      "0.001\n",
      "0.002\n",
      "0.004\n",
      "0.006\n",
      "0.008\n",
      "\n",
      "0.0001\n",
      "0.0001\n",
      "0.0001\n",
      "0.0001\n",
      "0.0001\n",
      "0.0001\n",
      "0.0001\n",
      "0.0001\n",
      "0.0001\n",
      "0.0001\n",
      "0.00001\n",
      "0.00001\n",
      "0.00001\n",
      "0.00001\n",
      "0.00001\n",
      "0.00001\n",
      "0.00001\n",
      "0.00001\n",
      "0.00001\n",
      "0.00001\n",
      "\n",
      "default (0.01)\n",
      "default (0.01)\n",
      "default (0.01)\n",
      "default (0.01)\n",
      "default (0.01)\n",
      "default (0.01)\n",
      "default (0.01)\n",
      "default (0.01)\n",
      "default (0.01)\n",
      "default (0.01)\n",
      "default (0.01)\n",
      "default (0.01)\n",
      "default (0.01)\n",
      "default (0.01)\n",
      "default (0.01)\n",
      "default (0.01)\n",
      "default (0.01)\n",
      "default (0.01)\n",
      "default (0.01)\n",
      "default (0.01)\n",
      "\n",
      "0.3665975\n",
      "0.5315755\n",
      "0.6369222\n",
      "0.6760457\n",
      "0.69557697\n",
      "0.7076009\n",
      "0.73065186\n",
      "0.72806805\n",
      "0.72161865\n",
      "\n",
      "0.71816\n",
      "\n",
      "0.49804688\n",
      "0.6287028\n",
      "0.6773885\n",
      "0.67348224\n",
      "0.6622111\n",
      "0.6468709\n",
      "0.5846761\n",
      "0.4868978\n",
      "0.34969077\n",
      "0.31193033\n",
      "\n",
      "34\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "Table 23: The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet-\n",
      "50 (batch size = 16384, 90 epochs, 7038 iterations). We use the learning rate recipe of (Goyal et al.,\n",
      "2017): (1) 5-epoch warmup to stablize the initial stage; and (2) multiply the learning rate by 0.1 at\n",
      "30th, 60th, and 80th epoch. The target accuracy is around 0.763 (Goyal et al., 2017).\n",
      "\n",
      "learning rate weight decay L2 regularization Top-1 Validation Accuracy\n",
      "\n",
      "0.0001\n",
      "0.0002\n",
      "0.0004\n",
      "0.0006\n",
      "0.0008\n",
      "0.001\n",
      "0.002\n",
      "0.004\n",
      "0.006\n",
      "0.008\n",
      "0.0001\n",
      "0.0002\n",
      "0.0004\n",
      "0.0006\n",
      "0.0008\n",
      "0.001\n",
      "0.002\n",
      "0.004\n",
      "0.006\n",
      "0.008\n",
      "\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "\n",
      "0.0010172526\n",
      "0.0009765625\n",
      "0.0010172526\n",
      "0.0009969076\n",
      "0.0010172526\n",
      "0.0009765625\n",
      "0.0009969076\n",
      "0.0009969076\n",
      "0.0009765625\n",
      "0.0010172526\n",
      "0.0009765625\n",
      "0.0010172526\n",
      "0.0010172526\n",
      "0.0010172526\n",
      "0.0010172526\n",
      "0.0009969076\n",
      "0.0010579427\n",
      "0.0016886393\n",
      "0.019714355\n",
      "0.1329956\n",
      "\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "\n",
      "35\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "Table 24: The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet-\n",
      "50 (batch size = 16384, 90 epochs, 7038 iterations). We use the learning rate recipe of (Goyal et al.,\n",
      "2017): (1) 5-epoch warmup to stablize the initial stage; and (2) multiply the learning rate by 0.1 at\n",
      "30th, 60th, and 80th epoch. The target accuracy is around 0.763 (Goyal et al., 2017).\n",
      "\n",
      "learning rate weight decay L2 regularization Top-1 Validation Accuracy\n",
      "\n",
      "0.28515625\n",
      "0.44055176\n",
      "0.56815594\n",
      "0.6234741\n",
      "0.6530762\n",
      "0.6695964\n",
      "0.70048016\n",
      "\n",
      "0.71698\n",
      "\n",
      "0.72021484\n",
      "0.7223918\n",
      "0.72017413\n",
      "0.72058105\n",
      "0.7188924\n",
      "0.71695966\n",
      "0.7154134\n",
      "0.71358234\n",
      "0.7145386\n",
      "0.7114258\n",
      "0.7066447\n",
      "0.70284015\n",
      "\n",
      "0.0001\n",
      "0.0002\n",
      "0.0004\n",
      "0.0006\n",
      "0.0008\n",
      "0.001\n",
      "0.002\n",
      "0.004\n",
      "0.006\n",
      "0.008\n",
      "0.010\n",
      "0.012\n",
      "0.014\n",
      "0.016\n",
      "0.018\n",
      "0.020\n",
      "0.025\n",
      "0.030\n",
      "0.040\n",
      "0.050\n",
      "\n",
      "0.0001\n",
      "0.0001\n",
      "0.0001\n",
      "0.0001\n",
      "0.0001\n",
      "0.0001\n",
      "0.0001\n",
      "0.0001\n",
      "0.0001\n",
      "0.0001\n",
      "0.0001\n",
      "0.0001\n",
      "0.0001\n",
      "0.0001\n",
      "0.0001\n",
      "0.0001\n",
      "0.0001\n",
      "0.0001\n",
      "0.0001\n",
      "0.0001\n",
      "\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "\n",
      "36\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "Table 25: The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet-\n",
      "50 (batch size = 16384, 90 epochs, 7038 iterations). We use the learning rate recipe of (Goyal et al.,\n",
      "2017): (1) 5-epoch warmup to stablize the initial stage; and (2) multiply the learning rate by 0.1 at\n",
      "30th, 60th, and 80th epoch. The target accuracy is around 0.763 (Goyal et al., 2017).\n",
      "\n",
      "learning rate weight decay L2 regularization Top-1 Validation Accuracy\n",
      "\n",
      "0.31247965\n",
      "0.4534912\n",
      "0.57765704\n",
      "0.6277669\n",
      "0.65321857\n",
      "0.6682129\n",
      "0.69938153\n",
      "0.7095947\n",
      "0.710612\n",
      "0.70857745\n",
      "0.7094116\n",
      "0.70717365\n",
      "0.7109375\n",
      "0.7058309\n",
      "0.7052409\n",
      "0.7064412\n",
      "0.7035319\n",
      "0.6994629\n",
      "0.6972656\n",
      "0.6971232\n",
      "\n",
      "0.0001\n",
      "0.0002\n",
      "0.0004\n",
      "0.0006\n",
      "0.0008\n",
      "0.001\n",
      "0.002\n",
      "0.004\n",
      "0.006\n",
      "0.008\n",
      "0.010\n",
      "0.012\n",
      "0.014\n",
      "0.016\n",
      "0.018\n",
      "0.020\n",
      "0.025\n",
      "0.030\n",
      "0.040\n",
      "0.050\n",
      "\n",
      "0.00001\n",
      "0.00001\n",
      "0.00001\n",
      "0.00001\n",
      "0.00001\n",
      "0.00001\n",
      "0.00001\n",
      "0.00001\n",
      "0.00001\n",
      "0.00001\n",
      "0.00001\n",
      "0.00001\n",
      "0.00001\n",
      "0.00001\n",
      "0.00001\n",
      "0.00001\n",
      "0.00001\n",
      "0.00001\n",
      "0.00001\n",
      "0.00001\n",
      "\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "disable\n",
      "\n",
      "37\n",
      "\n",
      "\f",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "LEARNING DEEP GRAPH MATCHING VIA CHANNEL-\n",
      "INDEPENDENT EMBEDDING AND HUNGARIAN ATTEN-\n",
      "TION\n",
      "\n",
      "Tianshu Yu†, Runzhong Wang‡, Junchi Yan‡, Baoxin Li†\n",
      "†Arizona State University\n",
      "‡Shanghai Jiao Tong University\n",
      "{tianshuy,baoxin.li}@asu.edu\n",
      "{runzhong.wang,yanjunchi}@sjtu.edu.cn\n",
      "\n",
      "ABSTRACT\n",
      "\n",
      "Graph matching aims to establishing node-wise correspondence between two\n",
      "graphs, which is a classic combinatorial problem and in general NP-complete. Un-\n",
      "til very recently, deep graph matching methods start to resort to deep networks to\n",
      "achieve unprecedented matching accuracy. Along this direction, this paper makes\n",
      "two complementary contributions which can also be reused as plugin in existing\n",
      "works: i) a novel node and edge embedding strategy which stimulates the multi-\n",
      "head strategy in attention models and allows the information in each channel to\n",
      "be merged independently. In contrast, only node embedding is accounted in pre-\n",
      "vious works; ii) a general masking mechanism over the loss function is devised to\n",
      "improve the smoothness of objective learning for graph matching. Using Hungar-\n",
      "ian algorithm, it dynamically constructs a structured and sparsely connected layer,\n",
      "taking into account the most contributing matching pairs as hard attention. Our\n",
      "approach performs competitively, and can also improve state-of-the-art methods\n",
      "as plugin, regarding with matching accuracy on three public benchmarks.\n",
      "\n",
      "1\n",
      "\n",
      "INTRODUCTION\n",
      "\n",
      "Without loss of generality, we consider the bijection problem for graph matching: given graph G1\n",
      "and G2 of equal size n, graph matching seeks to ﬁnd the one-vs-one node correspondence1:\n",
      "\n",
      "max\n",
      "\n",
      "x(cid:62)Kx\n",
      "\n",
      "x\n",
      "\n",
      "s.t. Px = 1\n",
      "\n",
      "(1)\n",
      "\n",
      "where x = vec(X) ∈ {0, 1}n2\n",
      "which is the column-wise vectorized form of the permutation ma-\n",
      "trix X that encodes the node-to-node correspondence between two graphs, and K ∈ Rn2×n2\n",
      "is\n",
      "the so-called afﬁnity matrix2, respectively. Note P is a selection matrix encoding the one-to-one\n",
      "correspondence constraint. This problem is called Lawler’s QAP (Lawler, 1963) and has attracted\n",
      "enormous attention for its generally NP-complete (Hartmanis, 1982) challenge, as well as a wide\n",
      "spectrum of applications in computer vision, graphics, machine learning and operational research\n",
      "etc. In particular, Koopmans-Beckmann’s QAP (Loiola et al., 2007) with objective tr(X(cid:62)F1XF2)\n",
      "is a special case of Eq. (1), which can be converted to Lawler’s QAP by K = F2 ⊗ F1 and Fi refers\n",
      "to the weighted adjacency matrix. A series of solvers haven been developed to solve graph match-\n",
      "ing problem (Leordeanu & Hebert, 2005; Cho et al., 2010; Bernard et al., 2018; Yan et al., 2015;\n",
      "Yu et al., 2018). All these methods are based on deterministic optimization, which are conditioned\n",
      "with pre-deﬁned afﬁnity matrix and no learning paradigm is involved. This fact greatly limits the\n",
      "performance and broad application w.r.t. different problem settings considering its NP-hard nature.\n",
      "\n",
      "+\n",
      "\n",
      "Recently, the seminal work namely deep graph matching (DGM) (Zanﬁr & Sminchisescu, 2018) is\n",
      "proposed to exploit the high capacity of deep networks for graph matching, which achieves state-\n",
      "of-the-art performance. This is in contrast to some early works which incorporate learning strategy\n",
      "\n",
      "1We assume graphs are of equal size for narrative simplicity. One can easily handle unbalanced graph size\n",
      "\n",
      "by adding dummy nodes as a common protocol in graph matching literature (Cho et al., 2010).\n",
      "\n",
      "2Aia:jb typically encodes the afﬁnity between pair (i, j) and (a, b) where node i, j ∈ G1 and a, b ∈ G2.\n",
      "\n",
      "1\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "separately in local stages (Caetano et al., 2009; Cho et al., 2013). On the other hand, Graph Convolu-\n",
      "tional Networks (GCN) (Kipf & Welling, 2017) brings about new capability on tasks over graph-like\n",
      "data, as it naturally integrates the intrinsic graph structure in a general updating rule:\n",
      "\n",
      "H(l+1) = σ\n",
      "\n",
      "(cid:16) ˆAH(l)W(l)(cid:17)\n",
      "\n",
      "(2)\n",
      "\n",
      "where ˆA is the normalized connectivity matrix. H(l) and W(l) are the features and weights at layer\n",
      "l, respectively. Node embedding is updated by aggregation from 1-neighboring nodes, which is akin\n",
      "to the convolution operator in CNN. By taking advantages of both DGM and GCN, Wang et al.\n",
      "(2019) and Zhang & Lee (2019) incorporate permutation loss instead of displacement loss in (Zanﬁr\n",
      "& Sminchisescu, 2018), with notable improvement across both synthetic and real data.\n",
      "\n",
      "Note that Eq. (1) involves both node and edge information, which exactly correspond to the diag-\n",
      "onal and off-diagonal elements in K, respectively. Edges can carry informative multi-dimensional\n",
      "attributes (namely weights) which are fundamental to graph matching. However existing embed-\n",
      "ding based graph matching methods (Wang et al., 2019; Xu et al., 2019) are focused on the explicit\n",
      "modeling of node level features, whereby the edges are only used as topological node connection for\n",
      "message passing in GCN. Besides, edge attributes are neither well modeled in the embedding-free\n",
      "model (Zanﬁr & Sminchisescu, 2018) since the edge information is derived from the concatena-\n",
      "tion of node features. To our best knowledge, there is no deep graph matching method explicitly\n",
      "incorporating edge attributes. In contrast, edge attributes e.g.\n",
      "length and orientation are widely\n",
      "used in traditional graph matching models (Cho et al., 2010; Yan et al., 2015; Yu et al., 2018) for\n",
      "constructing the afﬁnity matrix K. Such a gap shall be ﬁlled in the deep graph matching pipeline.\n",
      "\n",
      "Another important consideration refers to the design of loss function. There are mainly two forms in\n",
      "existing deep graph matching works: i) displacement loss (Zanﬁr & Sminchisescu, 2018) similar to\n",
      "the use in optical ﬂow estimation (Ren et al., 2017); ii) the so-called permutation loss (Wang et al.,\n",
      "2019) involving iterative Sinkhorn procedure followed by a cross-entropy loss. Results in (Wang\n",
      "et al., 2019) show the latter is an effective improvement against the former regression based loss.\n",
      "However, we argue that the continuous Sinkhorn procedure (in training stage) is yet an unnatural\n",
      "approximation to Hungarian sampling (in testing stage) for discretization. If the network is equipped\n",
      "with a continuous loss function (e.g. cross-entropy), we argue that the training process will make a\n",
      "great “meaningless effort” to enforce some network output digits of the ﬁnal matching matrix into\n",
      "binary and neglect the resting digits which might have notable impact on accuracy.\n",
      "\n",
      "This paper strikes an endeavor on the above two gaps and makes the following main contributions:\n",
      "\n",
      "i) We propose a new approach for edge embedding via channel-wise operation, namely channel-\n",
      "independent embedding (CIE). The hope is to effectively explore the edge attribute and simulate\n",
      "the multi-head strategy in attention models (Veliˇckovi´c et al., 2018) by decoupling the calculations\n",
      "parallel and orthogonal to channel direction. In fact, edge attribute information has not been consid-\n",
      "ered in existing embedding based graph matching methods (Wang et al., 2019; Xu et al., 2019).\n",
      "\n",
      "ii) We devise a new mechanism to adjust the loss function based on the Hungarian method which is\n",
      "widely used for linear assignment problem, as termed by Hungarian attention. It resorts to dynami-\n",
      "cally generating sparse matching mask according to Hungarian sampling during training, rather than\n",
      "approximating Hungarian sampling with a differentiable function. As such, the Hungarian attention\n",
      "introduces higher smoothness against traditional loss functions to ease the training.\n",
      "\n",
      "iii) The empirical results on three public benchmarks shows that the two proposed techniques are\n",
      "orthogonal and beneﬁcial to existing techniques. Speciﬁcally, on the one hand, our CIE module\n",
      "can effectively boost the accuracy by exploring the edge attributes which otherwise are not consid-\n",
      "ered in state-of-the-art deep graph matching methods; on the other hand, our Hungarian attention\n",
      "mechanism also shows generality and it is complementary to existing graph matching loss.\n",
      "\n",
      "2 RELATED WORKS\n",
      "\n",
      "Graph embedding. To handle graph-like data, early works adopt recursive neural networks (RNNs)\n",
      "treating input as directed acyclic graphs (Sperduti & Starita, 1997; Frasconi et al., 1998). Gori et al.\n",
      "(2005); Scarselli et al. (2008) generalized early models to graph neural networks (GNNs) so as to be\n",
      "directly applied on cyclic, directed or undirected graphs. Li et al. (2016) further improved this line\n",
      "\n",
      "2\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "of model by replacing standard RNNs with gated recurrent units (GRUs) (Cho et al., 2013). Inspired\n",
      "by the great success of convolutional neural networks (CNNs) (Simonyan & Zisserman, 2014; He\n",
      "et al., 2016), researchers have made tremendous effort on applying convolution operator to graphs\n",
      "(Bruna et al., 2014; Kipf & Welling, 2017; Gong & Cheng, 2019). Bruna et al. (2014) deﬁned\n",
      "a convolution operator in Fourier domain which is obtained by performing eigen-decomposition\n",
      "on graph Laplacian. However, such convolution will affect the whole spatial domain once taking\n",
      "inverse Fourier transformation. This method was improved by Chebyshev expansion to approximate\n",
      "ﬁlters (Defferrard et al., 2016). Kipf & Welling (2017) propose a graph convolutional operator\n",
      "over 1-neighbor nodes derived from graph spectral theory, which is invariant to node permutation\n",
      "and achieved signiﬁcant performance on semi-supervised learning tasks. There are series of works\n",
      "following GCN, such as GraphSAGE (Hamilton et al., 2017), GAT (Veliˇckovi´c et al., 2018) and\n",
      "MPNN (Gilmer et al., 2017). Refer to (Cai et al., 2018) for a more comprehensive survey.\n",
      "\n",
      "While the aforementioned models are focused on learning node state/embedding, a parallel line of\n",
      "work seek to learn edge embedding by taking into account the information carried on edges (Li et al.,\n",
      "2016; Gilmer et al., 2017; Gong & Cheng, 2019). Edges are intrinsic portion of graphs, and thus\n",
      "edge embedding can be essential to reveal the relation among nodes. Gilmer et al. (2017) introduce\n",
      "a general embedding network incorporating edge information and node-edge information merging,\n",
      "and a serious of works fall into this framework e.g. Gated GNN (Li et al., 2016), Tensor GNN\n",
      "(Sch¨utt et al., 2017) and EGNN (Gong & Cheng, 2019). An improved version is devised in Chen\n",
      "et al. (2019) by interpreting this framework as maximizing mutual information across layers.\n",
      "\n",
      "Loss for combinatorial learning. For the relatively easy linear assignment problem, it has been\n",
      "known that Sinkhorn algorithm (Sinkhorn, 1964) is the approximate and differentiable version of\n",
      "Hungarian algorithm (Mena et al., 2017). The Sinkhorn Network (Adams & Zemel, 2011) is de-\n",
      "veloped given known assignment cost, whereby doubly-stochastic regulation is performed on input\n",
      "non-negative square matrix. Patrini et al. (2018) devise the Sinkhorn AutoEncoder to minimize\n",
      "Wasserstein distance, and Emami & Ranka (2018) propose to learning a linear assignment solver\n",
      "via reinforcement learning. For permutation prediction, DeepPermNet (Santa Cruz et al., 2018)\n",
      "adopts the Sinkhorn layer on top of a deep convolutional network. However this method cannot be\n",
      "directly applied for graph matching as it is not invariant to input permutations which is conditioned\n",
      "on a predeﬁned node permutation as reference. In particular, existing supervised methods on combi-\n",
      "natorial learning are generally cross-entropy-based. Pointer Net (Vinyals et al., 2015) incorporates\n",
      "cross-entropy loss on learning heuristics for combinatorial problems. Milan et al. (2017) propose an\n",
      "objective-based loss, where the gradients are only updated if the objective improves after update.\n",
      "\n",
      "Learning for graph matching. The early effort (Caetano et al., 2009) aims to incorporate learning\n",
      "to graph matching. The key is to learn a more effective afﬁnity function with given correspon-\n",
      "dence as supervision. While the ability by only learning afﬁnity is limited, Cho et al. (2013) pro-\n",
      "pose a matching function learning paradigm using histogram-based attributes with Structured-SVM\n",
      "(Tsochantaridis et al., 2005). A recent work (Zanﬁr & Sminchisescu, 2018) is a breakthrough to\n",
      "introduce deep learning paradigm into graph matching task, which utilizes a neural network to learn\n",
      "the afﬁnity function. The learning procedure is explicitly derived from the factorization of afﬁnity\n",
      "matrix (Zhou & De la Torre, 2012), which makes the interpretation of the network behavior possible.\n",
      "However, the displacement loss in (Zanﬁr & Sminchisescu, 2018) measures the pixel-wise transla-\n",
      "tion which is similar to optical-ﬂow (Dosovitskiy et al., 2015), being essentially a regression task\n",
      "instead of combinaotiral optimization. Seeing this limitation, Wang et al. (2019) employ element-\n",
      "wise binary cross-entropy, termed as permutation loss. This loss has proved capable of capturing\n",
      "the combinatorial nature rather than pixel offset, and achieves improvement over displacement loss.\n",
      "Node embedding is also used in (Wang et al., 2019) to explore the structure information.\n",
      "\n",
      "3 THE PROPOSED LEARNING APPROACH FOR GRAPH MATCHING\n",
      "\n",
      "3.1 APPROACH OVERVIEW\n",
      "\n",
      "An overall structure of our approach is illustrated in Fig. 1. In line with (Wang et al., 2019), we em-\n",
      "ploy VGG16 (Simonyan & Zisserman, 2014) to extract features from input images and bi-linearly\n",
      "interpolate the features at key points (provided by datasets). We concatenate lower-level (Relu4 2)\n",
      "and higher-level (Relu5 1) features to incorporate local and contextual information. For an image\n",
      "with k key points, the feature is denoted as H ∈ Rk×d, where d is the feature dimension. Unless\n",
      "\n",
      "3\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "Figure 1: Architecture overview of the proposed deep graph matching networks that consist of the\n",
      "proposed channel-independent embedding and Hungarian attention layer over the loss function.\n",
      "otherwise speciﬁed, the adjacency matrix A ∈ Rk×k is consequentially constructed via Delaunay\n",
      "triangulation (Delaunay et al., 1934), which is a widely adopted strategy to produce sparsely con-\n",
      "nected graph. To introduce more rich edge information, we also generate k × k m-dimensional edge\n",
      "features E ∈ Rm×k×k. E can be initialized with some basic edge information (e.g.\n",
      "length and\n",
      "angle and other attributes) or a commutative function Eij = p(Hi, Hj) = p(Hj, Hi) ∈ Rm, where\n",
      "Hi refers to the feature of node i. Note for directed graph, the commutative property is not required.\n",
      "\n",
      "The features H and E, together with the adjacency A, are then fed into GNN module. Pairs of\n",
      "features are processed in a Siamese fashion (Bromley et al., 1994). Standard GCN’s message passing\n",
      "rule simply updates node embedding as shown in Eq. (2). In contrast, each GNN layer in our model\n",
      "computes a new pair of node and edge embeddings simultaneously:\n",
      "\n",
      "0 and W l\n",
      "\n",
      "H(l+1) = fi(H(l), E(l), A; W l\n",
      "\n",
      "(3)\n",
      "where W l\n",
      "1 are the learnable parameters at layer l. The edge information is essential to\n",
      "provide structural feature enhancing graph matching. We initialize H(0) = H and E(0) = E in\n",
      "our setting. We will discuss the details of functions f and g in Sec. 3.2. Following state-of-the-art\n",
      "work (Wang et al., 2019), we also compute the cross-graph afﬁnity followed by a column/row-wise\n",
      "softmax activation and a Sinkhorn layer (Adams & Zemel, 2011):\n",
      "\n",
      "0), E(l+1) = g(H(l), E(l), A; W l\n",
      "1)\n",
      "\n",
      "Mij = exp\n",
      "\n",
      "τ H(cid:62)\n",
      "\n",
      "(1)iΛH(2)j\n",
      "\n",
      ", S = Sinkhorn(M)\n",
      "\n",
      "(cid:16)\n",
      "\n",
      "(cid:17)\n",
      "\n",
      "Note here M ∈ Rk×k is the node-level similarity matrix encoding similarity between two graphs,\n",
      "differing from the edege-level afﬁnity matrix K in Eq. 1. τ is the weighting parameter of similarity,\n",
      "Λ contains learnable parameters and H(1)i is the node i’s embedding from graph G1. The output\n",
      "S ∈ [0, 1]k×k, S1 = 1, S(cid:62)1 = 1 is a so-called doubly-stochastic matrix. Here Sinkhorn(·) denotes\n",
      "the following update iteratively to project M into doubly stochastic polygon:\n",
      "\n",
      "M(t+1) = M(t) −\n",
      "\n",
      "M(t)11(cid:62) −\n",
      "\n",
      "11(cid:62)M(t) +\n",
      "\n",
      "1\n",
      "n\n",
      "\n",
      "1\n",
      "n2 11(cid:62)M(t)11(cid:62) −\n",
      "\n",
      "1\n",
      "n\n",
      "\n",
      "11(cid:62)\n",
      "\n",
      "1\n",
      "n\n",
      "\n",
      "The Sinkhorn layer is shown to be an approximation of Hungarian algorithm which produces discrete\n",
      "matching output (Kuhn, 1955). As there are only matrix multiplication and normalization operators\n",
      "involved in Sinkhorn layer, it is differentiable. In practice, Eq. (5) converges rapidly within 10\n",
      "iterations for decades of nodes. Less iterations involved, more precise back-propagated gradients\n",
      "can be achieved. We employ a cross-graph node embedding strategy following (Wang et al., 2019):\n",
      "\n",
      "H(l)\n",
      "\n",
      "(1) = fc\n",
      "\n",
      "(cid:16)\n",
      "cat(H(l)\n",
      "\n",
      "(cid:17)\n",
      "(1), SH(l)\n",
      "(2))\n",
      "\n",
      ", H(l)\n",
      "\n",
      "(2) = fc\n",
      "\n",
      "cat(H(l)\n",
      "\n",
      "(2), S(cid:62)H(l)\n",
      "(2))\n",
      "\n",
      "(cid:16)\n",
      "\n",
      "(cid:17)\n",
      "\n",
      "where fc is a network and cat(·, ·) is the concatenation operator. H(i) is the node feature of graph i.\n",
      "This procedure seeks to merge similar features from another graph into the node feature in current\n",
      "graph. It is similar to the feature transfer strategy in (Aberman et al., 2018) for sparse correspon-\n",
      "dence, which employs a feature merging method analogous to style transfer (Li et al., 2017).\n",
      "\n",
      "As Sinkhorn layer does not necessarily output binary digits, we employ Hungarian algorithm (Kuhn,\n",
      "1955) to discretize matching output S in testing. The testing differs from the training due to the\n",
      "Hungarian discretization. We introduce a novel attention-like mechanism termed as Hungarian\n",
      "attention, along with existing loss functions (will be detailed in Sec. 3.3). The ﬁnal training loss is\n",
      "as follows, where SG and H correspond to binary true matching and Hungarian attention loss.\n",
      "\n",
      "(4)\n",
      "\n",
      "(5)\n",
      "\n",
      "(6)\n",
      "\n",
      "(7)\n",
      "\n",
      "min H(S, SG)\n",
      "\n",
      "4\n",
      "\n",
      "CNNGaussian\tkernelimage\t1initial\tnode\tembeddinginitial\tedge\tembeddingCIEnode\tembeddingedge\tembeddingcross\tgraphnode\tembeddingedge\tembeddingnode\tembeddingedge\tembeddingCIECNNGaussian\tkernelinitial\tnode\tembeddinginitial\tedge\tembeddingCIEnode\tembeddingedge\tembeddingcross\tgraphnode\tembeddingedge\tembeddingCIEnode\tembeddingedge\tembeddingaffinityaffinityimage\t2similarity\tmatrixdoubly-stochastic\tmatrixSinkhornHungarian\tattentionground\ttruth\tmatchingloss\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "Figure 2: Illustration of the proposed CIE layer for embedding based deep graph matching. The\n",
      "operation “Linear” refers to the linear mapping, e.g. H(l)\n",
      "\n",
      "w → W(l)\n",
      "\n",
      "w in Eq (9).\n",
      "\n",
      "2 H(l)\n",
      "\n",
      "3.2 CHANNEL-INDEPENDENT EMBEDDING\n",
      "\n",
      "We detail the updating rule in Eq. (3). We propose a method to merge edge features into node\n",
      "features and perform matching on nodes. Edge information acts an important role in modeling\n",
      "relational data, whereby such relation can be complex thus should be encoded with high-dimensional\n",
      "feature. To this end, Gilmer et al. (2017) introduce a general embedding layer, which takes node and\n",
      "edge features and outputs a message to node v, then fuses the message and the current embedding:\n",
      "\n",
      "m(l)\n",
      "\n",
      "v = σ\n",
      "\n",
      "ft (Evw) H(l)\n",
      "\n",
      "w + W(l)H(l)\n",
      "\n",
      ", H(t+1)\n",
      "\n",
      "v\n",
      "\n",
      "= ut\n",
      "\n",
      "H(t)\n",
      "\n",
      "v , m(l)\n",
      "v\n",
      "\n",
      "(8)\n",
      "\n",
      "(cid:16)\n",
      "\n",
      "(cid:17)\n",
      "\n",
      "(cid:33)\n",
      "\n",
      "(cid:32)\n",
      "\n",
      "(cid:88)\n",
      "\n",
      "w∈Nv\n",
      "\n",
      "v and H(l)\n",
      "\n",
      "where Evw is the feature corresponding to edge (v, w). In the realization of Eq. (8) (Gilmer et al.,\n",
      "2017), m(l)\n",
      "v are fed to GRU (Cho et al., 2014) as a sequential input. There are several\n",
      "variants which take into account speciﬁc tasks (Li et al., 2016; Sch¨utt et al., 2017; Chen et al., 2019).\n",
      "Among these, Li et al. (2016) generates a transformation matrix for each edge and Sch¨utt et al.\n",
      "(2017) resorts to merge embedding via fully connected neural networks. While edge-wise merging\n",
      "is straightforward, the representation ability is also limited. On the other hand, fully connected\n",
      "merging strategy will result in high computational cost and instability for back-propagation. To\n",
      "address these issues, we propose to merge embedding in a channel-wise fashion, which is termed as\n",
      "Channel-Independent Embedding (CIE). Concretely, the updating rule is written as:\n",
      "\n",
      "H(l+1)\n",
      "\n",
      "v\n",
      "\n",
      "= σ\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "(cid:16)\n",
      "\n",
      "(cid:88)\n",
      "\n",
      "(cid:16)\n",
      "\n",
      "w∈Nv\n",
      "\n",
      "ΓN\n",
      "(cid:124)\n",
      "\n",
      "W(l)\n",
      "\n",
      "vw ◦ W(l)\n",
      "1 E(l)\n",
      "(cid:123)(cid:122)\n",
      "\n",
      "2 H(l)\n",
      "w\n",
      "\n",
      "channel-wise operator/function\n",
      "(cid:17)\n",
      "\n",
      "\n",
      "\n",
      "(cid:17)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "(cid:125)\n",
      "\n",
      "E(l+1)\n",
      "\n",
      "vw = σ\n",
      "\n",
      "W(l)\n",
      "\n",
      "1 E(l)\n",
      "vw\n",
      "\n",
      "(cid:16)\n",
      "\n",
      "+ σ\n",
      "\n",
      "W(l)\n",
      "\n",
      "0 H(l)\n",
      "v\n",
      "\n",
      "(cid:17)\n",
      "\n",
      "(9)\n",
      "\n",
      "(10)\n",
      "\n",
      "where ΓN(· ◦ ·) is a channel-wise operator/function (above the underbrace), and it performs calcu-\n",
      "lation per-channel and the output channel dimension is the same as input. The second σ(·) term is\n",
      "the message a node passes to itself, which is necessary in keeping the node information contextually\n",
      "consistent through each CIE layer. In this fashion, CIE is thus a procedure to aggregate node and\n",
      "2 H(l)\n",
      "edge embedding in each channel independently, which requires the dimensions of node (W(l)\n",
      "w )\n",
      "and edge (W(l)\n",
      "vw) representations to be equal. Similarly, we also propose an corresponding up-\n",
      "dating rule of edge embedding by substituting Eq. (10):\n",
      "\n",
      "1 E(l)\n",
      "\n",
      "E(l+1)\n",
      "\n",
      "vw = σ\n",
      "\n",
      "(cid:16)\n",
      "\n",
      "(cid:16)\n",
      "\n",
      "ΓE\n",
      "\n",
      "W(l)\n",
      "\n",
      "1 E(l)\n",
      "\n",
      "vw ◦ h\n",
      "\n",
      "H(l)\n",
      "\n",
      "v , H(l)\n",
      "w\n",
      "\n",
      "(cid:16)\n",
      "\n",
      "(cid:17)(cid:17)(cid:17)\n",
      "\n",
      "(cid:16)\n",
      "\n",
      "+ σ\n",
      "\n",
      "(cid:17)\n",
      "\n",
      "W(l)\n",
      "\n",
      "1 E(l)\n",
      "vw\n",
      "\n",
      "(11)\n",
      "\n",
      "where h(·, ·) is commutative h(X, Y) = h(Y, X). Eq. (11) is supplementary to Eq. (9).\n",
      "\n",
      "Fig. 2 shows a schematic diagram of CIE layer, which is motivated from two perspectives. First,\n",
      "CIE is motivated by counterparts in CNN (Qiu et al., 2017; Tran et al., 2018) which decouple a 3D\n",
      "convolution into two 2D ones (e.g. a 3 × 3 × 3 convolution can be decomposed to a 1 × 3 × 3 and\n",
      "a 3 × 1 × 1 convolutions). In this sense, the number of parameters can be signiﬁcantly reduced.\n",
      "As shown in Fig. 2, node and edge embedding is ﬁrst manipulated along the channel direction via\n",
      "a linear layer, then operated via ΓN and ΓE orthogonal to the channel direction. Instead of merging\n",
      "node and edge as a whole, CIE layer decouples it into two operations. Second, CIE is also motivated\n",
      "by the triumph of multi-head structure (e.g. graph attention (Veliˇckovi´c et al., 2018)), the key of\n",
      "\n",
      "5\n",
      "\n",
      "𝐄(#)𝐇(#)LinearLinearΓ’Γ(𝐄(#)*)𝐇(#)*)𝑚×𝑘×𝑘𝑚×𝑘𝑚channels𝜎𝜎\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "Figure 3: A working example illustrating of our proposed Hungarian attention pipeline starting from\n",
      "similarity matrix. Sinkhorn algorithm solves similarity matrix into a doubly-stochastic matrix in a\n",
      "differentiable way. A discrete permutation matrix is further obtained via Hungarian algorithm. Our\n",
      "proposed Hungarian attention, taking the ground truth matching matrix into account, focuses on\n",
      "the “important” digits either labeled true or being mis-classiﬁed. The output matrix is obtained by\n",
      "attention pooling from doubly-stochastic matrix, where we compute a loss on it.\n",
      "which is to conduct unit calculation multiple times and concatenate the results. Multi-head proved\n",
      "effective to further improve the performance since it is capable of capturing information at different\n",
      "scales or aspects. Traditional neural node-edge message passing algorithms (Gilmer et al., 2017; Li\n",
      "et al., 2016; Sch¨utt et al., 2017) typically produce a uniﬁed transformation matrix for all the channels.\n",
      "On the other hand, in Eq. (9) (10) and (11), one can consider that the basic operator in each channel\n",
      "is repeated d times in a multi-head fashion. The cross-channel information exchange, as signiﬁed in\n",
      "Eq. (9) (10) and (11), only happens before the channel-wise operator (i.e. weights W(l)\n",
      "i as the cross-\n",
      "channel matrices). The main difference between CIE and traditional multi-head approaches e.g.\n",
      "(Veliˇckovi´c et al., 2018) is that CIE assumes the channel-independence of two embedded features\n",
      "(node and edge), while traditional ones only take one input under head-independence assumption.\n",
      "\n",
      "3.3 HUNGARIAN ATTENTION MECHANISM\n",
      "\n",
      "For most graph matching algorithms, the output is in a continuous domain. Though there are some\n",
      "alternatives that deliver discrete solutions by adding more constraints or introducing numerical con-\n",
      "tinuation (Zhou & De la Torre, 2012; Yu et al., 2018), the main line of methods is to incorporate a\n",
      "sampling procedure (e.g. winner-take-all and Hungarian). Among them, the Hungarian algorithm\n",
      "(Kuhn, 1955) is a widely adopted, for its efﬁciency and theoretical optimality.\n",
      "\n",
      "However, the Hungarian algorithm incurs a gap between training (loss function) and testing stages\n",
      "(Hungarian sampling). We compare the permutation loss (Wang et al., 2019) for concrete analysis:\n",
      "\n",
      "LCE = −\n",
      "\n",
      "(cid:88)\n",
      "\n",
      "(cid:0)SG\n",
      "\n",
      "ij log Sij + (cid:0)1 − SG\n",
      "\n",
      "ij\n",
      "\n",
      "(cid:1) log (1 − Sij)(cid:1)\n",
      "\n",
      "(12)\n",
      "\n",
      "i∈G1,j∈G2\n",
      "\n",
      "Note Eq. (12) is an element-wise version of binary cross-entropy. During training, this loss tends\n",
      "to drag the digits in S into binary format and is likely trapped to local optima. This is because this\n",
      "loss will back-propagate the gradients of training samples that are easy to learn in the early training\n",
      "stage. In later iterations, this loss is then hard to give up the digits that have become binary. In fact,\n",
      "the similar phenomenon is also investigated in the focal loss (Lin et al., 2017) in comparison to the\n",
      "traditional cross-entropy loss. During the testing stage, however, the Hungarian algorithm has no\n",
      "preference on the case if digits in S are close to 0 − 1 or not. It binarizes S anyway. Therefore, the\n",
      "effort of Eq. (12) to drag S into binary might be meaningless.\n",
      "\n",
      "This issue is likely to be solved by integrating Hungarian algorithm during the training stage. Un-\n",
      "fortunately, Hungarian algorithm is undifferentiable and its behavior is difﬁcult to mimic with a\n",
      "differentiable counterpart. In this paper, instead of ﬁnding a continuous approximation of Hungar-\n",
      "ian algorithm, we treat it as a black box and dynamically generate network structure (sparse link)\n",
      "\n",
      "6\n",
      "\n",
      "012345abcdef1.41.73.83.80.80.81.33.60.90.82.16.72.50.44.31.20.71.00.41.81.91.12.73.81.11.50.50.40.63.60.80.72.72.15.51.0similarity matrix012345abcdef0.20.10.20.40.10.00.10.30.10.10.10.30.30.00.30.10.10.10.10.20.10.10.20.20.20.20.10.10.10.30.10.10.20.20.40.0doubly-stochastic matrix012345abcdef0.00.00.01.00.00.00.01.00.00.00.00.01.00.00.00.00.00.00.00.01.00.00.00.00.00.00.00.00.01.00.00.00.00.01.00.0permutation matrix012345abcdef0.00.00.01.00.00.01.00.00.00.00.00.00.01.00.00.00.00.00.00.01.00.00.00.00.00.00.00.00.01.00.00.00.00.01.00.0ground truth matrix012345abcdef0.00.00.01.00.00.01.01.00.00.00.00.01.01.00.00.00.00.00.00.01.00.00.00.00.00.00.00.00.01.00.00.00.00.01.00.0attention activation012345abcdef0.00.00.00.40.00.00.10.30.00.00.00.00.30.00.00.00.00.00.00.00.10.00.00.00.00.00.00.00.00.30.00.00.00.00.40.0output matrixSinkhornHungarianAttention PoolingHungarian Attention\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "Table 1: Accuracy on Pascal VOC (best in bold). White and gray background refer to results on test-\n",
      "ing and training, respectively. Compared methods include GMN (Zanﬁr & Sminchisescu, 2018),\n",
      "GAT (Veliˇckovi´c et al., 2018), EPN (Gong & Cheng, 2019), PCA/PIA (Wang et al., 2019).\n",
      "\n",
      "method aero bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv Ave\n",
      "GMN-D 31.9 47.2 51.9 40.8 68.7 72.2 53.6 52.8 34.6 48.6 72.3 47.7 54.8 51.0\n",
      "38.6 75.1 49.5 45.0 83.0 86.3 55.3\n",
      "34.1 77.5 57.1 53.6 83.2 88.6 57.9\n",
      "GMN-P 31.1 46.2 58.2 45.9 70.6 76.4 61.2 61.7 35.5 53.7 58.9 57.5 56.9 49.3\n",
      "39.5 82.0 66.9 50.1 78.5 90.3 63.6\n",
      "GAT-P 46.4 60.5 60.9 51.8 79.0 70.9 62.7 70.1 39.7 63.9 66.2 63.8 65.8 62.8\n",
      "GAT-H 47.2 61.6 63.2 53.3 79.7 70.1 65.3 70.5 38.4 64.7 62.9 65.1 66.2 62.5\n",
      "41.1 78.8 67.1 61.6 81.4 91.0 64.6\n",
      "39.9 80.5 66.7 45.5 77.6 90.6 63.2\n",
      "EPN-P 47.6 65.2 62.2 52.7 77.8 69.5 63.4 69.6 37.8 62.8 63.6 63.9 64.6 61.9\n",
      "PIA-D 39.7 57.7 58.6 47.2 74.0 74.5 62.1 66.6 33.6 61.7 65.4 58.0 67.1 58.9\n",
      "41.9 77.7 64.7 50.5 81.8 89.9 61.6\n",
      "PIA-P 41.5 55.8 60.9 51.9 75.0 75.8 59.6 65.2 33.3 65.9 62.8 62.7 67.7 62.1\n",
      "42.9 80.2 64.3 59.5 82.7 90.1 63.0\n",
      "44.9 77.5 67.4 57.5 86.7 90.9 63.8\n",
      "PCA-P 40.9 55.0 65.8 47.9 76.9 77.9 63.5 67.4 33.7 65.5 63.6 61.3 68.9 62.8\n",
      "40.5 84.7 66.1 47.9 80.5 91.1 64.6\n",
      "PCA-H 49.8 60.7 63.9 52.6 79.8 72.5 63.8 71.2 38.4 62.5 71.7 65.4 66.6 62.5\n",
      "PCA+-P 46.6 61.0 62.3 53.9 78.2 72.5 64.4 70.5 39.0 63.5 74.8 65.2 65.0 61.6\n",
      "40.8 83.2 67.1 50.5 79.6 91.6 64.6\n",
      "CIE2-P 50.9 65.5 68.0 57.0 81.0 75.9 70.3 73.4 41.1 66.7 53.2 68.3 68.4 63.5\n",
      "45.3 84.8 69.7 57.2 79.8 91.6 66.9\n",
      "CIE2-H 51.2 68.4 69.5 57.3 82.5 73.5 69.5 74.0 40.3 67.8 60.0 69.7 70.3 65.1\n",
      "44.7 86.9 70.7 57.3 84.2 92.2 67.4\n",
      "46.1 85.1 70.4 61.6 80.7 91.7 68.1\n",
      "CIE1-P 52.1 69.4 69.9 58.9 80.6 76.3 71.0 74.2 41.1 68.0 60.4 69.7 70.7 65.1\n",
      "44.8 85.2 69.9 65.4 85.2 92.4 68.9\n",
      "CIE1-H 51.2 69.2 70.1 55.0 82.8 72.8 69.0 74.2 39.6 68.8 71.8 70.0 71.8 66.8\n",
      "66.5 99.1 80.7 99.7 98.2 97.0 88.2\n",
      "PCA-P 75.8 99.2 83.3 74.7 98.7 96.3 74.3 87.8 80.9 85.7 100.0 83.7 83.8 98.7\n",
      "46.1 94.8 72.7 93.6 93.7 91.6 76.2\n",
      "CIE1-P 56.5 84.0 73.5 58.0 91.5 81.1 67.8 76.8 46.4 72.2 98.0 73.9 73.6 77.9\n",
      "CIE1-H 59.4 88.1 75.9 58.0 94.3 81.9 69.4 78.9 49.5 78.2 99.7 78.1 78.0 82.1\n",
      "47.4 95.8 75.7 97.6 96.0 91.1 78.7\n",
      "\n",
      "according to its output. Concretely, the sparse link is calculated as:\n",
      "\n",
      "Z = Atten (cid:0)Hungarian(S), SG(cid:1) = P ∪ Q\n",
      "\n",
      "(13)\n",
      "\n",
      "where the attention mechanism Atten is fulﬁlled by an element-wise “logic OR” function. Fig. 3\n",
      "shows an example of Hungarian attention procedure, and Eq. (13) highlights the most contributing\n",
      "digit locations: positive digits P = S where Hungarian agrees with the ground-truth; negative digits\n",
      "Q = Hungarian(S) \\ SG where Hungarian differs from ground-truth. While GT (positive digits)\n",
      "naturally points out the digits that must be considered, negative ones indicate the digits that most\n",
      "hinder the matching (most impeding ones among all mis-matchings). Thus we need only minimize\n",
      "the loss at Z, without considering the rest of digits. As we note that this mechanism only focuses on\n",
      "a small portion of the matching matrix which is analogous to producing hard attention, we term it\n",
      "Hungarian attention. Now that with the attention mask Z, the Hungarian attention loss becomes:\n",
      "\n",
      "HCE = −\n",
      "\n",
      "(cid:88)\n",
      "\n",
      "Zij\n",
      "\n",
      "(cid:0)SG\n",
      "\n",
      "ij log Sij + (cid:0)1 − SG\n",
      "\n",
      "ij\n",
      "\n",
      "(cid:1) log (1 − Sij)(cid:1)\n",
      "\n",
      "(14)\n",
      "\n",
      "i∈G1,j∈G2\n",
      "\n",
      "Note that Hungarian attention mechanism can also be applied to other loss functions once the match-\n",
      "ing score is calculated in an element-wise fashion. Our experiment also studies Hungarian attention\n",
      "loss when casted on focal loss (Lin et al., 2017) and a speciﬁcally designed margin loss.\n",
      "\n",
      "Finally we give a brief qualitative analysis on why Hungarian attention can improve matching loss.\n",
      "As discrete graph matching problem is actually built upon Delta function over permutation vertices\n",
      "(1 at ground-truth matching and 0 otherwise) (Yu et al., 2018), learning of graph matching with per-\n",
      "mutation loss is actually to approximate such functions with continuous counterparts. Unfortunately,\n",
      "more precise approximation to Delta function will result in higher non-smoothness, as discussed in\n",
      "Yu et al. (2018). For highly non-smooth objective, the network is more likely trapped at local optima.\n",
      "Hungarian attention, however, focuses on a small portion of the output locations, thus does not care\n",
      "about if most of the output digits are in {0, 1}. In this sense, Hungarian attention allows moderate\n",
      "smoothness of the objective, thus optimizer with momentum is likely to avoid local optima.\n",
      "\n",
      "4 EXPERIMENTS\n",
      "\n",
      "Experiments are conducted on three benchmarks widely used for learning-based graph matching:\n",
      "CUB2011 dataset (Welinder et al., 2010) following the protocol in (Choy et al., 2016), Pascal VOC\n",
      "keypoint matching (Everingham et al., 2010; Bourdev & Malik, 2009) which is challenging and\n",
      "Willow Object Class dataset (Cho et al., 2013). Mean matching accuracy is adopted for evaluation:\n",
      "\n",
      "Acc =\n",
      "\n",
      "1\n",
      "k\n",
      "\n",
      "i∈G1,j∈G2\n",
      "\n",
      "(cid:88)\n",
      "\n",
      "AND (cid:0)Hungarian(S)ij, SG\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "ij\n",
      "\n",
      "(15)\n",
      "\n",
      "The algorithm abbreviation is in the form “X-Y”, where “X” and “Y” refer to the network structure\n",
      "(e.g. CIE) and loss function (e.g. Hungarian attention loss), respectively. Speciﬁcally, D, P and H\n",
      "\n",
      "7\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "correspond to displacement used in (Zanﬁr & Sminchisescu, 2018), permutation as adopted in (Wang\n",
      "et al., 2019) and Hungarian attention over permutation loss devised by this paper, respectively.\n",
      "\n",
      "Peer methods. We compare our method with the following selected counterparts: 1) HARG (Cho\n",
      "et al., 2013). This shallow learning method is based on hand-crafted feature and Structured SVM;\n",
      "2) GMN (Zanﬁr & Sminchisescu, 2018). This is a seminal work incorporating graph matching and\n",
      "deep learning, and the solver is upon spectral matching (Leordeanu & Hebert, 2005). While the\n",
      "loss of this method is displacement loss, we also report the results of GMN by replacing its loss\n",
      "with permutation loss (GMN-P); 3) PIA/PCA (Wang et al., 2019). PCA and PIA correspond to\n",
      "the algorithms with and without cross-graph node embedding, respectively. Readers are referred\n",
      "to Wang et al. (2019) for more details; We further replace the GNN layer in our framework with:\n",
      "4) GAT (Veliˇckovi´c et al., 2018). Graph attention network is an attention mechanism on graphs,\n",
      "which reweights the embedding according to attention score; 5) EPN (Gong & Cheng, 2019). This\n",
      "method exploits multi-dimensional edge embedding and can further be applied on directed graphs.\n",
      "The edge dimension is set to 32 in our experiments. Finally, we term our network structure CIE for\n",
      "short. To investigate the capacity of edge embedding update, we also devise a version without edge\n",
      "embedding, in which connectivity is initialized as reciprocal of the edge length then normalized,\n",
      "rather than A. This model is called PCA+ since the node embedding strategy follows PCA.\n",
      "\n",
      "Implementation details. As the node number of each graph might vary, we add dummy nodes for\n",
      "each graph pair such that the node number reaches the maximal graph size in a mini-batch in line\n",
      "with the protocol in (Wang et al., 2019). In either training or testing stages, these dummy nodes will\n",
      "not be updated or counted. The activation function in Eq. (9) (10) and (11) is set as Relu (Nair &\n",
      "Hinton, 2010) in all experiments. Speciﬁcally, the node and edge embedding is implemented by:\n",
      "\n",
      "H(l+1)\n",
      "\n",
      "·q\n",
      "\n",
      "= σ\n",
      "\n",
      "(cid:18)(cid:18)\n",
      "\n",
      "A (cid:12)\n",
      "\n",
      "(cid:16)\n",
      "\n",
      "W(l)\n",
      "\n",
      "1 E(l)(cid:17)\n",
      "\n",
      "(cid:19) (cid:16)\n",
      "\n",
      "·q\n",
      "\n",
      "E(l+1)\n",
      "\n",
      "·q\n",
      "\n",
      "= σ\n",
      "\n",
      "(cid:18)(cid:12)\n",
      "(cid:16)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "W(l)\n",
      "\n",
      "0 H(l)(cid:17)\n",
      "\n",
      "(cid:16)\n",
      "\n",
      "W(l)\n",
      "\n",
      "0 H(l)(cid:17)(cid:62)\n",
      "\n",
      "(cid:9)\n",
      "\n",
      "·q\n",
      "\n",
      "W(l)\n",
      "\n",
      "2 H(l)(cid:17)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "(cid:12) E(l)\n",
      "·q\n",
      "\n",
      "·q\n",
      "\n",
      "·q\n",
      "(cid:19)\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "(cid:18)(cid:16)\n",
      "\n",
      "W(l)\n",
      "\n",
      "0 H(l)(cid:17)\n",
      "\n",
      "+ σ\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "(cid:18)(cid:16)\n",
      "\n",
      "W(l)\n",
      "\n",
      "1 E(l)(cid:17)\n",
      "\n",
      "+ σ\n",
      "\n",
      "·q\n",
      "(cid:19)\n",
      "\n",
      "·q\n",
      "\n",
      "(16a)\n",
      "\n",
      "(16b)\n",
      "\n",
      "where (cid:12) and (cid:9) refer to element-wise product and pairwise difference, respectively. H·q is the\n",
      "qth channel of H. In CIE1 setting, only node-level merging Eq. (16a) is considered and the edge\n",
      "feature is updated as Eq. (10). In CIE2 setting, we also replace the edge update Eq. (11) with Eq.\n",
      "(16b). Note edge embedding is used in both CIE1 and CIE2 and note PCA-H can be regarded as\n",
      "the pure node embedding version of our approach. The edge feature is initiated as reciprocal of the\n",
      "edge length. For training, batch size is set to 8. We employ SGD optimizer (Bottou, 2010) with\n",
      "momentum 0.9. Two CIE layers are stacked after VGG16.\n",
      "\n",
      "CUB2011 test CUB2011 consists of 11,788 images from 200 kinds of birds with 15 annotated\n",
      "parts. We randomly sample image pairs from the dataset following the implementation released by\n",
      "Choy et al. (2016). We do not use the pre-alignment of poses during testing, because their alignment\n",
      "result is not publicly available. Therefore, there exists signiﬁcant variation in pose, articulation and\n",
      "appearance across images, in both training and testing phase. Images are cropped around bounding\n",
      "box and resized to 256 × 256 before fed into the network. Instead of evaluating the performance\n",
      "in a retrieval fashion (Zanﬁr & Sminchisescu, 2018), we directly evaluate the matching accuracy\n",
      "since the semantic key-points are pre-given. We test two settings: 1) intra-class. During training,\n",
      "we randomly sample images, with each pair sampled from the same category (out of 200 bird cate-\n",
      "gories). In testing, 2,000 image pairs (100 pairs for each category) are sampled; 2) cross-class. We\n",
      "analogously sample image pairs without considering the category information and 5,000 randomly\n",
      "sampled image pairs are employed for testing. While the ﬁrst setting is for a class-aware situation,\n",
      "the second setting is considered for testing the class-agnostic case. Results are shown in Table 3.\n",
      "\n",
      "We see our method surpasses all the competing methods in terms of matching accuracy. Besides,\n",
      "almost all the selected algorithms can reach over 90% accuracy, indicating that this dataset contains\n",
      "mostly “easy” learning samples.\n",
      "In this case, the Hungarian attention can slightly improve the\n",
      "performance since easy gradients agree with descending trend of the loss on the whole dataset.\n",
      "\n",
      "Pascal VOC test The Pascal VOC dataset with Key-point annotation (Bourdev & Malik, 2009)\n",
      "contains 7,020 training images and 1,682 testing images with 20 classes in total. To the best of\n",
      "our knowledge, this is the largest and most challenging dataset for graph matching in computer vi-\n",
      "sion. Each image is cropped around its object bounding box and is resized to 256 × 256. The node\n",
      "\n",
      "8\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "(a) Accuracy/loss vs. training epoch.\n",
      "\n",
      "(b) Ablation study by Hungarian attention.\n",
      "\n",
      "Figure 4: Performance study on Pascal VOC. Note in (a) the loss is calculated on all matching digits\n",
      "for both CIE1-P and CIE1-H. Note around 10th epoch, the accuracy of CIE1-P almost reaches the\n",
      "highest, but the loss keeps descending until 30th epoch. This indicates that in most of the latter\n",
      "epochs, P-loss performs “meaningless” back-propagation to drag the output to binary. H-loss, by\n",
      "accommodating smoothness, can emphasize most contributing digits and achieves higher accuracy.\n",
      "size of this dataset varies from 6 to 23 and there are various scale, pose and illumination perturba-\n",
      "tions. Experimental results are summarized in Table 1. We see in either setting, CIE signiﬁcantly\n",
      "outperforms all peer algorithms. Speciﬁcally, CIE1-H achieves the best performance and has 0.8%\n",
      "improvement w.r.t. average accuracy over CIE1-P. For each class, CIE1-H and CIE1-P carve up most\n",
      "of the top performance. We also note that CIE1-H has a close performance on “table” compared\n",
      "with GMN-D. Since P-loss is naturally not as robust as D-loss on symmetric objects, P-loss showed\n",
      "great degradation over D-loss on “table” (as discussed in (Wang et al., 2019)). However, with the\n",
      "help of Hungarian link, H-loss can maintain relatively high accuracy despite natural ﬂaw of P-loss.\n",
      "This observation indicates that H-loss can focus on “difﬁcult” examples. We also note that CIE1\n",
      "produces better results against CIE2, which implies that updating edge embedding is less effective\n",
      "compared to a singleton node updating strategy. We can also see from Table 1 that PCA-P has much\n",
      "higher performance on training samples than CIE1-H, which is to the contrary of the result on testing\n",
      "samples. This might indicate that PCA-P overﬁts the training samples.\n",
      "\n",
      "Accuracy/loss vs. training epoch. We further show the typical training behavior of P-loss and H-\n",
      "loss on Pascal VOC dataset in Fig. 4. 30 epochs are involved in a whole training process. Accuracy\n",
      "is evaluated on testing samples after each epoch while loss is the average loss value within each\n",
      "epoch. In the early training stage, the loss of CIE1-P immediately drops. On the other hand, CIE1-H\n",
      "hesitates for several epochs to ﬁnd the most effective descending direction. On the late stage, we\n",
      "observe that even though P-loss (Eq. (12)) calculates much more digits than H-loss (Eq. (14)), the\n",
      "loss values are opposite. This counter-intuitive fact strongly indicates that P-loss makes meaningless\n",
      "effort, which is not helpful to improve the performance, at late stage. The proposed H-loss, on the\n",
      "other hand, is capable of avoiding easy but meaningless gradients.\n",
      "\n",
      "Effect of Hungarian attention mechanism. We also conduct experiments to show the improve-\n",
      "ment of Hungarian attention over several loss functions (with and without Hungarian attention):\n",
      "Hungarian attention is applied on Focal loss (Focal) (Lin et al., 2017) as:\n",
      "\n",
      "(cid:40)\n",
      "\n",
      "Lfocal =\n",
      "\n",
      "−αZij(1 − Sij)γ log(Sij),\n",
      "−(1 − α)ZijSγ\n",
      "\n",
      "SG\n",
      "ij log(1 − Sij), SG\n",
      "\n",
      "ij = 1\n",
      "ij = 0\n",
      "\n",
      "where controlling parameters α = 0.75 and γ = 2 in our setting. We also design a margin loss\n",
      "(Margin) with Hungarian attention under a max-margin rule. Note we insert the Hungarian attention\n",
      "mask Zij into Eq. (17) and Eq. (18) based on the vanilla forms.\n",
      "\n",
      "Lmargin =\n",
      "\n",
      "(cid:26)Zij × max(1 − Sij − β, 0), SG\n",
      "SG\n",
      "\n",
      "Zij × max(Sij − β, 0),\n",
      "\n",
      "ij = 1\n",
      "ij = 0\n",
      "\n",
      "(17)\n",
      "\n",
      "(18)\n",
      "\n",
      "where we set the margin value β = 0.2. Loss of Eq. (18) is valid because after Softmax and\n",
      "Sinkhorn operations, Sij ∈ [0, 1]. We also show permutation loss (Perm) (Wang et al., 2019).\n",
      "Result can be found in Fig. 4 (b) whereby the average accuracy on Pascal VOC is reported. All the\n",
      "settings are under CIE1. For either loss, the proposed Hungarian attention can further enhance the\n",
      "accuracy, which is further visualized by a pair of matching results under P-loss and H-loss in Fig. 5.\n",
      "\n",
      "9\n",
      "\n",
      "051015202530Epoch0.20.250.30.350.40.450.50.550.60.650.7Accuracy0.511.522.53LossAcc: CIE1-PAcc: CIE1-HLoss: CIE1-PLoss: CIE1-HFocalMarginPerm0.60.620.640.660.680.7Accuracyno Hungwith Hung\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "(a) Reference Image\n",
      "\n",
      "(b) P-loss: 7/10\n",
      "\n",
      "(c) H-loss: 8/10\n",
      "\n",
      "Figure 5: Visualization of a matching result: 10 key points in each image with 7 and 8 correct\n",
      "matchings dispalyed, respectively. Different colors across images indicate node correspondence.\n",
      "The larger size of dot, the larger is the predicted value Sij. (a) The reference image. (b) Result on\n",
      "the target image from CIE1-P. (c) Result on the target image from CIE1-H. We see though H-loss\n",
      "i.e. Hungarian attention loss outputs smaller predicted values, it delivers a more accurate matching.\n",
      "\n",
      "Table 2: Accuracy (%) on Willow Object.\n",
      "\n",
      "face\n",
      "method\n",
      "91.2\n",
      "HARG\n",
      "GMN-V\n",
      "98.1\n",
      "GMN-W 99.3\n",
      "100.0\n",
      "PCA-V\n",
      "PCA-W 100.0\n",
      "CIE-V\n",
      "99.9\n",
      "CIE-W 100.0\n",
      "\n",
      "mbike\n",
      "44.4\n",
      "65.0\n",
      "71.4\n",
      "69.8\n",
      "76.7\n",
      "71.5\n",
      "90.0\n",
      "\n",
      "car\n",
      "58.4\n",
      "72.9\n",
      "74.3\n",
      "78.6\n",
      "84.0\n",
      "75.4\n",
      "82.2\n",
      "\n",
      "duck wbottle\n",
      "55.2\n",
      "74.3\n",
      "82.8\n",
      "82.4\n",
      "93.5\n",
      "73.2\n",
      "81.2\n",
      "\n",
      "66.6\n",
      "70.5\n",
      "76.7\n",
      "95.1\n",
      "96.9\n",
      "97.6\n",
      "97.6\n",
      "\n",
      "intra-class\n",
      "\n",
      "Table 3: Accuracy (%) on CUB.\n",
      "method\n",
      "cross-class\n",
      "GMN-D\n",
      "GMN-P\n",
      "GAT-P\n",
      "PCA-P\n",
      "PCA-H\n",
      "CIE-P\n",
      "CIE-H\n",
      "\n",
      "89.9\n",
      "90.8\n",
      "93.4\n",
      "93.5\n",
      "93.5\n",
      "93.8\n",
      "94.2\n",
      "\n",
      "89.6\n",
      "90.4\n",
      "93.2\n",
      "92.9\n",
      "93.7\n",
      "94.1\n",
      "94.4\n",
      "\n",
      "Willow Object Class test We test the transfer ability on Willow Object Class (Cho et al., 2013).\n",
      "It contains 256 images3 of 5 categories in total, with three categories (face, duck and winebottle)\n",
      "collected from Caltech-256 and resting two (car and motorbike) from Pascal VOC 2007. This dataset\n",
      "is considered to have bias compared with Pascal VOC since images in the same category are with\n",
      "relatively ﬁxed pose and background is much cleaner. We crop the object inside its bounding box and\n",
      "resize it to 256 × 256 as CNN input. While HARG is trained from scratch following the protocol\n",
      "in (Cho et al., 2013), all the resting counterparts are either directly pre-trained from the previous\n",
      "section or ﬁne-tuned upon the pre-trained models. We term the method “X-V” or “X-W” to indicate\n",
      "pre-trained model on Pascal VOC or ﬁne-tuned on Willow, respectively. CIE refers to CIE1-H for\n",
      "short. Results in Table 2 suggest that our method is competitive to state-of-the-art.\n",
      "\n",
      "5 CONCLUSION\n",
      "\n",
      "We have presented a novel and effective approach for learning based graph matching. On one hand,\n",
      "the novelty of our method partially lies in the development of the Hungarian attention, which in-\n",
      "trinsically adapts the matching problem. It is further observed from the experiments that Hungarian\n",
      "attention can improve several matching-oriented loss functions, which might bring about potential\n",
      "for a series of combinatorial problems. On the other hand, we also devise the channel independent\n",
      "embedding (CIE) technique for deep graph matching, which decouples the basic merging opera-\n",
      "tions and is shown robust in learning effective graph representation. Extensive experimental results\n",
      "on multiple matching benchmarks show the leading performance of our solver, and highlight the\n",
      "orthogonal contribution of the two proposed components on top of existing techniques.\n",
      "\n",
      "ACKNOWLEDGMENTS\n",
      "\n",
      "Tianshu Yu and Baoxin Li were supported in part by a grant from ONR. Any opinions expressed\n",
      "in this material are those of the authors and do not necessarily reﬂect the views of ONR. Runzhong\n",
      "Wang and Junchi Yan were supported in part by NSFC 61972250 and U19B2035.\n",
      "\n",
      "3The data size is too small to train a deep model. Hence we only evaluate the transfer ability on this dataset.\n",
      "\n",
      "10\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "REFERENCES\n",
      "\n",
      "2011.\n",
      "\n",
      "Kﬁr Aberman, Jing Liao, Mingyi Shi, Dani Lischinski, Baoquan Chen, and Daniel Cohen-Or. Neural\n",
      "\n",
      "best-buddies: Sparse cross-domain correspondence. SIGGRAPH, 2018.\n",
      "\n",
      "Ryan Prescott Adams and Richard S Zemel. Ranking via sinkhorn propagation. arXiv:1106.1925,\n",
      "\n",
      "Florian Bernard, Christian Theobalt, and Michael Moeller. Ds*: Tighter lifting-free convex relax-\n",
      "\n",
      "ations for quadratic matching problems. In CVPR, 2018.\n",
      "\n",
      "L´eon Bottou. Large-scale machine learning with stochastic gradient descent. In COMPSTAT. 2010.\n",
      "\n",
      "Lubomir Bourdev and Jitendra Malik. Poselets: Body part detectors trained using 3d human pose\n",
      "\n",
      "annotations. In ICCV, 2009.\n",
      "\n",
      "Jane Bromley, Isabelle Guyon, Yann LeCun, Eduard S¨ackinger, and Roopak Shah. Signature veriﬁ-\n",
      "\n",
      "cation using a “siamese” time delay neural network. In NIPS, 1994.\n",
      "\n",
      "Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. Spectral networks and locally\n",
      "\n",
      "connected networks on graphs. In ICLR, 2014.\n",
      "\n",
      "Tib´erio S Caetano, Julian J McAuley, Li Cheng, Quoc V Le, and Alex J Smola. Learning graph\n",
      "\n",
      "matching. PAMI, 31(6):1048–1058, 2009.\n",
      "\n",
      "Hongyun Cai, Vincent W Zheng, and Kevin Chen-Chuan Chang. A comprehensive survey of graph\n",
      "\n",
      "embedding: Problems, techniques, and applications. TKDE, 30(9):1616–1637, 2018.\n",
      "\n",
      "Pengfei Chen, Weiwen Liu, Chang-Yu Hsieh, Guangyong Chen, and Shengyu Zhang. Utilizing\n",
      "edge features in graph neural networks via variational information maximization. arXiv preprint\n",
      "arXiv:1906.05488, 2019.\n",
      "\n",
      "Kyunghyun Cho, Bart Van Merri¨enboer, Dzmitry Bahdanau, and Yoshua Bengio. On the properties\n",
      "of neural machine translation: Encoder-decoder approaches. arXiv preprint arXiv:1409.1259,\n",
      "2014.\n",
      "\n",
      "Minsu Cho, Jungmin Lee, and Kyoung Mu Lee. Reweighted random walks for graph matching. In\n",
      "\n",
      "ECCV, 2010.\n",
      "\n",
      "Minsu Cho, Karteek Alahari, and Jean Ponce. Learning graphs to match. In CVPR, 2013.\n",
      "\n",
      "Christopher B Choy, JunYoung Gwak, Silvio Savarese, and Manmohan Chandraker. Universal\n",
      "\n",
      "correspondence network. In NIPS, 2016.\n",
      "\n",
      "Micha¨el Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on\n",
      "\n",
      "graphs with fast localized spectral ﬁltering. In NIPS, 2016.\n",
      "\n",
      "Boris Delaunay et al. Sur la sphere vide.\n",
      "\n",
      "Izv. Akad. Nauk SSSR, Otdelenie Matematicheskii i\n",
      "\n",
      "Estestvennyka Nauk, 7(793-800):1–2, 1934.\n",
      "\n",
      "Alexey Dosovitskiy, Philipp Fischer, Eddy Ilg, Philip Hausser, Caner Hazirbas, Vladimir Golkov,\n",
      "Patrick Van Der Smagt, Daniel Cremers, and Thomas Brox. Flownet: Learning optical ﬂow with\n",
      "convolutional networks. In ICCV, 2015.\n",
      "\n",
      "Patrick Emami and Sanjay Ranka.\n",
      "\n",
      "Learning permutations with sinkhorn policy gradient.\n",
      "\n",
      "arXiv:1805.07010, 2018.\n",
      "\n",
      "Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman.\n",
      "\n",
      "The pascal visual object classes (voc) challenge. IJCV, 88(2):303–338, 2010.\n",
      "\n",
      "Paolo Frasconi, Marco Gori, and Alessandro Sperduti. A general framework for adaptive processing\n",
      "\n",
      "of data structures. TNN, 9(5):768–786, 1998.\n",
      "\n",
      "Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural\n",
      "\n",
      "message passing for quantum chemistry. In ICML, 2017.\n",
      "\n",
      "11\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "Liyu Gong and Qiang Cheng. Exploiting edge features for graph neural networks. In CVPR, 2019.\n",
      "\n",
      "Marco Gori, Gabriele Monfardini, and Franco Scarselli. A new model for learning in graph domains.\n",
      "\n",
      "Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs.\n",
      "\n",
      "Juris Hartmanis. Computers and intractability: a guide to the theory of np-completeness (michael r.\n",
      "\n",
      "garey and david s. johnson). Siam Review, 24(1):90, 1982.\n",
      "\n",
      "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-\n",
      "\n",
      "In IJCNN, 2005.\n",
      "\n",
      "In NIPS, 2017.\n",
      "\n",
      "nition. In CVPR, 2016.\n",
      "\n",
      "works. In ICLR, 2017.\n",
      "\n",
      "Thomas N Kipf and Max Welling. Semi-supervised classiﬁcation with graph convolutional net-\n",
      "\n",
      "Harold W Kuhn. The hungarian method for the assignment problem. Naval research logistics\n",
      "\n",
      "quarterly, 2(1-2):83–97, 1955.\n",
      "\n",
      "Eugene L Lawler. The quadratic assignment problem. Management science, 9(4):586–599, 1963.\n",
      "\n",
      "Marius Leordeanu and Martial Hebert. A spectral technique for correspondence problems using\n",
      "\n",
      "pairwise constraints. In ICCV, 2005.\n",
      "\n",
      "Yijun Li, Chen Fang, Jimei Yang, Zhaowen Wang, Xin Lu, and Ming-Hsuan Yang. Universal style\n",
      "\n",
      "transfer via feature transforms. In NIPS, 2017.\n",
      "\n",
      "Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel. Gated graph sequence neural\n",
      "\n",
      "networks. In ICLR, 2016.\n",
      "\n",
      "detection. In ICCV, 2017.\n",
      "\n",
      "Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll´ar. Focal loss for dense object\n",
      "\n",
      "Eliane Maria Loiola, Nair Maria Maia de Abreu, Paulo Oswaldo Boaventura-Netto, Peter Hahn, and\n",
      "Tania Querido. A survey for the quadratic assignment problem. European journal of operational\n",
      "research, 176(2):657–690, 2007.\n",
      "\n",
      "Gonzalo Mena, David Belanger, Gonzalo Mu˜noz, and Jasper Snoek. Sinkhorn networks: Using\n",
      "optimal transport techniques to learn permutations. NIPS Workshop in Optimal Transport and\n",
      "Machine Learning, 2017.\n",
      "\n",
      "Anton Milan, Seyed Hamid Rezatoﬁghi, Ravi Garg, Anthony R. Dick, and Ian D. Reid. Data-driven\n",
      "\n",
      "approximations to np-hard problems. In AAAI, 2017.\n",
      "\n",
      "Vinod Nair and Geoffrey E Hinton. Rectiﬁed linear units improve restricted boltzmann machines.\n",
      "\n",
      "In ICML, 2010.\n",
      "\n",
      "Giorgio Patrini, Marcello Carioni, Patrick Forre, Samarth Bhargav, Max Welling, Rianne van den\n",
      "\n",
      "Berg, Tim Genewein, and Frank Nielsen. Sinkhorn autoencoders. arXiv:1810.01118, 2018.\n",
      "\n",
      "Zhaofan Qiu, Ting Yao, and Tao Mei. Learning spatio-temporal representation with pseudo-3d\n",
      "\n",
      "residual networks. In ICCV, 2017.\n",
      "\n",
      "Zhe Ren, Junchi Yan, Bingbing Ni, Bin Liu, Xiaokang Yang, and Hongyuan Zha. Unsupervised\n",
      "\n",
      "deep learning for optical ﬂow estimation. In AAAI, 2017.\n",
      "\n",
      "Rodrigo Santa Cruz, Basura Fernando, Anoop Cherian, and Stephen Gould. Visual permutation\n",
      "\n",
      "learning. TPAMI, 2018.\n",
      "\n",
      "Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini.\n",
      "\n",
      "The graph neural network model. TNN, 20(1):61–80, 2008.\n",
      "\n",
      "Kristof T Sch¨utt, Farhad Arbabzadah, Stefan Chmiela, Klaus R M¨uller, and Alexandre Tkatchenko.\n",
      "Quantum-chemical insights from deep tensor neural networks. Nature communications, 8:13890,\n",
      "2017.\n",
      "\n",
      "12\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image\n",
      "\n",
      "recognition. arXiv preprint arXiv:1409.1556, 2014.\n",
      "\n",
      "Richard Sinkhorn. A relationship between arbitrary positive matrices and doubly stochastic matri-\n",
      "\n",
      "ces. AoMS, 1964.\n",
      "\n",
      "Alessandro Sperduti and Antonina Starita. Supervised neural networks for the classiﬁcation of\n",
      "\n",
      "structures. TNN, 8(3):714–735, 1997.\n",
      "\n",
      "Du Tran, Heng Wang, Lorenzo Torresani, Jamie Ray, Yann LeCun, and Manohar Paluri. A closer\n",
      "\n",
      "look at spatiotemporal convolutions for action recognition. In CVPR, 2018.\n",
      "\n",
      "Ioannis Tsochantaridis, Thorsten Joachims, Thomas Hofmann, and Yasemin Altun. Large margin\n",
      "\n",
      "methods for structured and interdependent output variables. JMLR, 6(Sep):1453–1484, 2005.\n",
      "\n",
      "Petar Veliˇckovi´c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li`o, and Yoshua\n",
      "\n",
      "Bengio. Graph attention networks. In ICLR, 2018.\n",
      "\n",
      "Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. Pointer networks. In NIPS, 2015.\n",
      "\n",
      "Runzhong Wang, Junchi Yan, and Xiaokang Yang. Learning combinatorial embedding networks for\n",
      "\n",
      "deep graph matching. In ICCV, 2019.\n",
      "\n",
      "P. Welinder, S. Branson, T. Mita, C. Wah, F. Schroff, S. Belongie, and P. Perona. Caltech-UCSD\n",
      "\n",
      "Birds 200. Technical Report CNS-TR-2010-001, California Institute of Technology, 2010.\n",
      "\n",
      "Hongteng Xu, Dixin Luo, and Lawrence Carin. Gromov-wasserstein learning for graph matching\n",
      "\n",
      "and node embedding. In ICML, 2019.\n",
      "\n",
      "Junchi Yan, Chao Zhang, Hongyuan Zha, Wei Liu, Xiaokang Yang, and Stephen M Chu. Discrete\n",
      "\n",
      "hyper-graph matching. In CVPR, 2015.\n",
      "\n",
      "Tianshu Yu, Junchi Yan, Yilin Wang, Wei Liu, and Baoxin Li. Generalizing graph matching beyond\n",
      "\n",
      "quadratic assignment model. In NIPS, 2018.\n",
      "\n",
      "Andrei Zanﬁr and Cristian Sminchisescu. Deep learning of graph matching. In CVPR, 2018.\n",
      "\n",
      "Zhen Zhang and Wee Sun Lee. Deep graphical feature learning for the feature matching problem.\n",
      "\n",
      "In ICCV, 2019.\n",
      "\n",
      "Feng Zhou and Fernando De la Torre. Factorized graph matching. In CVPR, 2012.\n",
      "\n",
      "A APPENDIX\n",
      "\n",
      "A.1 SYNTHETIC TEST\n",
      "\n",
      "Synthetic graphs are generated for training and testing following the protocol in (Cho et al., 2010).\n",
      "Speciﬁcally, Kpt keypoints are generated for a pair of graphs with a 1024-dimensional random\n",
      "feature for each node, which is sampled from uniform distribution U(−1, 1). Disturbance is also\n",
      "applied to graph pairs including: Gaussian node feature noise from N (0, σ2\n",
      "f t); random afﬁne trans-\n",
      "\n",
      "formation\n",
      "\n",
      "(cid:34)s cos θ −s sin θ\n",
      "s cos θ\n",
      "\n",
      "s sin θ\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "(cid:35)\n",
      "\n",
      "tx\n",
      "ty\n",
      "1\n",
      "\n",
      "with s ∼ U(0.8, 1.2), θ ∼ U(−60, 60), tx, ty ∼ U(−10, 10)\n",
      "\n",
      "followed by Gaussian coordinate position noise N (0, σ2\n",
      "co). By default we assign Kpt = 25, σf t =\n",
      "1.5, σco = 5. Two graphs share the same structure. We generate 10 random distributions for each\n",
      "test. Results are shown in Fig. 6. The performance of PCA and CIE is reported. We see our\n",
      "method signiﬁcantly outperformed PCA. It can further be noticed that Hungarian attention can help\n",
      "to achieve an even higher accuracy. Readers are referred to Wang et al. (2019) for some other results\n",
      "on synthetic test.\n",
      "\n",
      "However, we also notice that the way to generate synthetic graphs is much different from the dis-\n",
      "tribution of real-world data. For real-world data, on one hand, there is strong correlation on the\n",
      "\n",
      "13\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "Figure 6: Results on synthetic test where two different loss functions are compared in ablative study.\n",
      "\n",
      "neighboring node features. This is the reason why the message passing from nearby node features\n",
      "works. However, the features of synthetic data are randomly generated and there is no correlation\n",
      "between neighboring node features. Therefore, message passing mechanism is not very effective to\n",
      "reveal the relation or pattern among local nodes for synthetic data. On the other hand, features of\n",
      "real-world data typically lie on a manifold embedded in high dimensional space, hence is low di-\n",
      "mensional. However, randomly generated features will span the whole space and show no patterns.\n",
      "\n",
      "Taking into account the aforementioned factors, we believe there is a demand for a novel strategy to\n",
      "generate more reasonable synthetic data. This can be one of the future works.\n",
      "\n",
      "A.2 COMPARISON OF PASCAL VOC AND WILLOW\n",
      "\n",
      "As we claim that Willow dataset is biased compared with Pascal VOC dataset, we qualitatively show\n",
      "some randomly selected examples in Fig. 7. We select several images with the same class “car” from\n",
      "both datasets. We also choose images with “bird” from Pascal VOC and “duck” from Willow since\n",
      "they somewhat share similar semantic information. We see in either case, Pascal VOC contains more\n",
      "variation and degradation compared with Willow in terms of pose, scale, appearance, etc. In general,\n",
      "Willow dataset is easier for algorithms to learn. While there is a signiﬁcant performance gap of PCA\n",
      "over these two datasets, the performance of CIE on Willow without ﬁne-tune (Table 2) is consistent\n",
      "to the performance on Pascal VOC (Table 1). As such, we infer the performance degradation of CIE\n",
      "on “duck” in Willow test (Table 2) is due to such bias. The pre-trained CIE on Pascal VOC tends\n",
      "to produce more stable and higher average accuracy on all types of images, rather than focusing on\n",
      "“easy-to-learn” samples by PCA. This is a different learning strategy from PCA.\n",
      "\n",
      "14\n",
      "\n",
      "1.21.251.31.351.41.451.51.551.6Noise0.550.60.650.70.750.80.850.90.951AccuracyPCA-PCIE1-PCIE1-H51015202530354045Node size0.60.650.70.750.80.850.90.951AccuracyPCA-PCIE1-PCIE1-H\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "(a) Car images from Pascal VOC\n",
      "\n",
      "(b) Car images from Willow\n",
      "\n",
      "(c) Bird images from Pascal VOC\n",
      "\n",
      "(d) Duck images from Willow\n",
      "\n",
      "Figure 7: Image examples from Pascal VOC and Willow.\n",
      "\n",
      "15\n",
      "\n",
      "\f",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "ON THE WEAKNESSES OF REINFORCEMENT LEARN-\n",
      "ING FOR NEURAL MACHINE TRANSLATION\n",
      "\n",
      "Leshem Choshen1, Lior Fox2, Zohar Aizenbud1, Omri Abend1,3\n",
      "1 School of Computer Science and Engineering, 2 The Edmond and Lily Safra Center for Brain Sciences\n",
      "3 Department of Cognitive Sciences\n",
      "The Hebrew University of Jerusalem\n",
      "first.last@mail.huji.ac.il, oabend@cs.huji.ac.il\n",
      "\n",
      "ABSTRACT\n",
      "\n",
      "Reinforcement learning (RL) is frequently used to increase performance in text\n",
      "generation tasks, including machine translation (MT), notably through the use\n",
      "of Minimum Risk Training (MRT) and Generative Adversarial Networks (GAN).\n",
      "However, little is known about what and how these methods learn in the context of\n",
      "MT. We prove that one of the most common RL methods for MT does not optimize\n",
      "the expected reward, as well as show that other methods take an infeasibly long\n",
      "time to converge. In fact, our results suggest that RL practices in MT are likely to\n",
      "improve performance only where the pre-trained parameters are already close to\n",
      "yielding the correct translation. Our ﬁndings further suggest that observed gains\n",
      "may be due to effects unrelated to the training signal, concretely, changes in the\n",
      "shape of the distribution curve.\n",
      "\n",
      "1\n",
      "\n",
      "INTRODUCTION\n",
      "\n",
      "Reinforcement learning (RL) is an appealing path for advancement in Machine Translation (MT), as\n",
      "it allows training systems to optimize non-differentiable score functions, common in MT evaluation,\n",
      "as well as tackling the “exposure bias” (Ranzato et al., 2015) in standard training, namely that the\n",
      "model is not exposed during training to incorrectly generated tokens, and is thus unlikely to recover\n",
      "from generating such tokens at test time. These motivations have led to much interest in RL for text\n",
      "generation in general and MT in particular (see §2). Various policy gradient methods have been used,\n",
      "notably REINFORCE (Williams, 1992) and variants thereof (e.g., Ranzato et al., 2015; Edunov et al.,\n",
      "2018) and Minimum Risk Training (MRT; e.g., Och, 2003; Shen et al., 2016). Another popular use\n",
      "of RL is for training GANs (Yang et al., 2018; Tevet et al., 2018). Nevertheless, despite increasing\n",
      "interest and strong results, little is known about what accounts for these performance gains, and the\n",
      "training dynamics involved.\n",
      "\n",
      "We present the following contributions. First, our theoretical analysis shows that commonly used\n",
      "approximation methods are theoretically ill-founded, and may converge to parameter values that do\n",
      "not minimize the risk, nor are local minima thereof (§2.2).\n",
      "\n",
      "Second, using both naturalistic experiments and carefully constructed simulations, we show that\n",
      "performance gains observed in the literature likely stem not from making target tokens the most\n",
      "probable, but from unrelated effects, such as increasing the peakiness of the output distribution (i.e.,\n",
      "the probability mass of the most probable tokens). We do so by comparing a setting where the\n",
      "reward is informative, vs. one where it is constant. In §4 we discuss this peakiness effect (PKE).\n",
      "\n",
      "Third, we show that promoting the target token to be the mode is likely to take a prohibitively long\n",
      "time. The only case we ﬁnd, where improvements are likely, is where the target token is among\n",
      "the ﬁrst 2-3 most probable tokens according to the pretrained model. These ﬁndings suggest that\n",
      "REINFORCE (§5) and CMRT (§6) are likely to improve over the pre-trained model only under the\n",
      "best possible conditions, i.e., where the pre-trained model is “nearly” correct.\n",
      "\n",
      "We conclude by discussing other RL practices in MT which should be avoided for practical and\n",
      "theoretical reasons, and brieﬂy discuss alternative RL approaches that will allow RL to tackle a\n",
      "larger class of errors in pre-trained models (§7).\n",
      "\n",
      "1\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "2 RL IN MACHINE TRANSLATION\n",
      "\n",
      "An MT system generates tokens y = (y1, ..., yn) from a vocabulary V one token at a time. The\n",
      "probability of generating yi given preceding tokens y<i is given by Pθ(·|x, y<i), where x is the\n",
      "source sentence and θ are the model parameters. For each generated token yi, we denote with\n",
      "r(yi; y<i, x, y(ref )) the score, or reward, for generating yi given y<i, x, and the reference sentence\n",
      "y(ref ). For brevity, we omit parameters where they are ﬁxed within context. For simplicity, we\n",
      "assume r does not depend on following tokens y>i.\n",
      "\n",
      "We also assume there is exactly one valid target token, as de facto, training is done against a single\n",
      "reference (Schulz et al., 2018). In practice, either a token-level reward is approximated using Monte-\n",
      "Carlo methods (e.g., Yang et al., 2018), or a sentence-level (sparse) reward is given at the end of the\n",
      "episode (sentence). The latter is equivalent to a uniform token-level reward.\n",
      "\n",
      "r is often the negative log-likelihood, or a standard MT metric, e.g., BLEU (Papineni et al., 2002).\n",
      "RL’s goal is to maximize the expected episode reward (denoted with R); i.e., to ﬁnd\n",
      "\n",
      "θ∗ = arg max\n",
      "\n",
      "R(θ) = arg max\n",
      "\n",
      "Ey∼Pθ [r(y)]\n",
      "\n",
      "θ\n",
      "\n",
      "θ\n",
      "\n",
      "2.1 REINFORCE\n",
      "\n",
      "For a given source sentence, and past predictions y<i, REINFORCE (Williams, 1992) samples k\n",
      "tokens (k is a hyperparameter) S = (cid:0)y(1), ..., y(k)(cid:1) from Pθ and updates θ according to this rule:\n",
      "\n",
      "(1)\n",
      "\n",
      "(2)\n",
      "\n",
      "∆θ ∝\n",
      "\n",
      "r(yi)∇ log(Pθ(yi))\n",
      "\n",
      "1\n",
      "k\n",
      "\n",
      "k\n",
      "\n",
      "(cid:88)\n",
      "\n",
      "i=1\n",
      "\n",
      "The right-hand side of equation 2 is an unbiased estimator of the gradient of the objective function,\n",
      "i.e., E [∆θ] ∝ ∇θR (θ). Therefore, REINFORCE is performing a form of stochastic gradient ascent\n",
      "on R, and has similar formal guarantees. From here follows that if R is constant with respect to θ,\n",
      "then the expected ∆θ prescribed by REINFORCE is zero. We note that r may be shifted by a constant\n",
      "term (called a “baseline”), without affecting the optimal value for θ.\n",
      "\n",
      "REINFORCE is used in MT, text generation, and image-to-text tasks (Liu et al., 2016; Wu et al., 2018;\n",
      "Rennie et al., 2017; Shetty et al., 2017; Hendricks et al., 2016) – in isolation, or as a part of training\n",
      "(Ranzato et al., 2015). Lately, an especially prominent use for REINFORCE is adversarial training\n",
      "with discrete data, where another network predicts the reward (GAN). For some recent work on RL\n",
      "for NMT, see (Zhang et al., 2016; Li et al., 2017; Wu et al., 2017; Yu et al., 2017; Yang et al., 2018).\n",
      "\n",
      "2.2 MINIMUM RISK TRAINING\n",
      "\n",
      "The term Minimum Risk Training (MRT) is used ambiguously in MT to refer either to the appli-\n",
      "cation of REINFORCE to minimizing the risk (equivalently, to maximizing the expected reward, the\n",
      "negative loss), or more commonly to a somewhat different estimation method, which we term Con-\n",
      "trastive MRT (CMRT) and turn now to analyzing. CMRT was proposed by Och (2003), adapted to\n",
      "NMT by Shen et al. (2016), and often used since (Ayana et al., 2016; Neubig, 2016; Shen et al.,\n",
      "2017; Edunov et al., 2018; Makarov & Clematide, 2018; Neubig et al., 2018).\n",
      "\n",
      "The method works as follows: at each iteration, sample k tokens S = {y1, . . . , yk} from Pθ, and\n",
      "update θ according to the gradient of\n",
      "\n",
      "where\n",
      "\n",
      "(cid:101)R(θ, S) =\n",
      "\n",
      "Qθ,S(yi)r(yi) = Ey∼Q\n",
      "\n",
      "(cid:2)r(y)(cid:3)\n",
      "\n",
      "k\n",
      "\n",
      "(cid:88)\n",
      "\n",
      "i=1\n",
      "\n",
      "Qθ,S(yi) =\n",
      "\n",
      "P (yi)α\n",
      "yj ∈S P (yj)α\n",
      "\n",
      "(cid:80)\n",
      "\n",
      "Commonly (but not universally), deduplication is performed, so (cid:101)R sums over a set of unique values\n",
      "(Sennrich et al., 2017). This changes little in our empirical results and theoretical analysis.\n",
      "\n",
      "Despite the resemblance in deﬁnitions of R (equation 1) and (cid:101)R (indeed, (cid:101)R is sometimes presented as\n",
      "an approximation of R), they differ in two important aspects. First, Q’s support is S, so increasing\n",
      "\n",
      "2\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "Q(yi) for some yi necessarily comes at the expense of Q(y) for some y ∈ S. In contrast, increas-\n",
      "ing P (yi), as in REINFORCE, may come at the expense of P (y) for any y ∈ V . Second, α is a\n",
      "smoothness parameter: the closer α is to 0, the closer Q is to be uniform.\n",
      "\n",
      "We show in Appendix A.1 that despite its name, CMRT does not optimize R, nor does it optimize\n",
      "E[ (cid:101)R]. That is, it may well converge to values that are not local maxima of R, making it theoretically\n",
      "ill-founded.1 However, given CMRT popularity, the strong results it yielded and the absence of\n",
      "theory for explaining it, we discuss it here. Given a sample S, the gradient of (cid:101)R is given by\n",
      "\n",
      "∇ (cid:101)R = α\n",
      "\n",
      "Q(yi) · r(yi) · ∇ log P (yi)\n",
      "\n",
      "− EQ[r]∇ log Z(S)\n",
      "\n",
      "(3)\n",
      "\n",
      "(cid:17)\n",
      "\n",
      "k\n",
      "\n",
      "(cid:88)\n",
      "\n",
      "(cid:16)\n",
      "\n",
      "i=1\n",
      "\n",
      "where Z(S) = (cid:80)\n",
      "\n",
      "i P (yi)α. See Appendix A.2.\n",
      "\n",
      "Comparing Equations 2 and 3, the differences between REINFORCE and CMRT are reﬂected again.\n",
      "First, ∇ (cid:101)R has an additional term, proportional to ∇ log Z(S), which yields the contrastive effect.\n",
      "This contrast may improve the rate of convergence since it counters the decrease of probability mass\n",
      "for non-sampled tokens.\n",
      "\n",
      "Second, given S, the relative weighting of the gradients ∇ log P (yi) is proportional to r(yi)Q(yi), or\n",
      "equivalently to r(yi)P (yi)α. CMRT with deduplication sums over distinct values in S (equation 3),\n",
      "while REINFORCE sums over all values. This means that the relative weight of the unique value yi\n",
      "is r(yi)|{yi∈S}|\n",
      "in REINFORCE. For α = 1 the expected value of these relative weights is the same,\n",
      "and so for α < 1 (as is commonly used), more weight is given to improbable tokens, which could\n",
      "also have a positive effect on the convergence rate.2 However, if α is too close to 0, ∇ (cid:101)R vanishes,\n",
      "as it is not affected by θ. This tradeoff explains the importance of tuning α reported in the literature.\n",
      "In §6 we present simulations with CMRT, showing very similar trends as presented by REINFORCE.\n",
      "\n",
      "k\n",
      "\n",
      "3 MOTIVATING DISCUSSION\n",
      "\n",
      "Implementing a stochastic gradient ascent, REINFORCE is guaranteed to converge to a stationary\n",
      "point of R under broad conditions. However, not much is known about its convergence rate under\n",
      "the prevailing conditions in NMT.\n",
      "\n",
      "We begin with a qualitative, motivating analysis of these questions. As work on language generation\n",
      "empirically showed, RNNs quickly learn to output very peaky distributions (Press et al., 2017). This\n",
      "tendency is advantageous for generating ﬂuent sentences with high probability, but may also entail\n",
      "slower convergence rates when using RL to ﬁne-tune the model, because RL methods used in text\n",
      "generation sample from the (pretrained) policy distribution, which means they mostly sample what\n",
      "the pretrained model deems to be likely. Since the pretrained model (or policy) is peaky, exploration\n",
      "of other potentially more rewarding tokens will be limited, hampering convergence.\n",
      "\n",
      "Intuitively, REINFORCE increases the probabilities of successful (positively rewarding) observa-\n",
      "tions, weighing updates by how rewarding they were. When sampling a handful of tokens in each\n",
      "context (source sentence x and generated preﬁx y<i), and where the number of epochs is not large,\n",
      "it is unlikely that more than a few unique tokens will be sampled from Pθ(·|x, y<i). (In practice, k\n",
      "is typically between 1 and 20, and the number of epochs between 1 and 100.) It is thus unlikely that\n",
      "anything but the initially most probable candidates will be observed. Consequently, REINFORCE\n",
      "initially raises their probabilities, even if more rewarding tokens can be found down the list.\n",
      "\n",
      "We thus hypothesize the peakiness of the distribution, i.e., the probability mass allocated to the most\n",
      "probable tokens, will increase, at least in the ﬁrst phase. We call this the peakiness-effect (PKE),\n",
      "and show it occurs both in simulations (§4.1) and in full-scale NMT experiments (§4.2).\n",
      "\n",
      "With more iterations, the most-rewarding tokens will be eventually sampled, and gradually gain\n",
      "probability mass. This discussion suggests that training will be extremely sample-inefﬁcient. We\n",
      "assess the rate of convergence empirically in §5, ﬁnding this to be indeed the case.\n",
      "\n",
      "1Sakaguchi et al. (2017) discuss the relation between CMRT and REINFORCE, claiming that CMRT is a\n",
      "\n",
      "variant . Appendix A.1 shows that CMRT does not in fact optimize the same objective.\n",
      "\n",
      "2Not performing deduplication (e.g. in THUMT (Zhang et al., 2017)) results in assigning higher relative\n",
      "\n",
      "weight to high-probability tokens, which may have an adverse effect on convergence rate.\n",
      "\n",
      "3\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "Figure 1: A histogram of the up-\n",
      "date size (x-axis) to the total pre-\n",
      "dicted probability of the 10 most\n",
      "probable tokens (left) or the most\n",
      "probable token (right) in the Con-\n",
      "stant Reward setting. An update\n",
      "is overwhelmingly more probable\n",
      "to increase this probability than to\n",
      "decrease it.\n",
      "\n",
      "(a) Top 10\n",
      "\n",
      "(b) Mode\n",
      "\n",
      "4 THE PEAKINESS EFFECT\n",
      "\n",
      "We turn to demonstrate that the initially most probable tokens will initially gain probability mass,\n",
      "even if they are not the most rewarding, yielding a PKE.\n",
      "\n",
      "Caccia et al. (2018) recently observed in the context of language modeling using GANs that per-\n",
      "formance gains similar to those GAN yield can be achieved by decreasing the temperature for the\n",
      "prediction softmax (i.e., making it peakier). However, they proposed no causes for this effect. Our\n",
      "ﬁndings propose an underlying mechanism leading to this trend. We return to this point in §7. Fur-\n",
      "thermore, given their ﬁndings, it is reasonable to assume that our results are relevant for RL use in\n",
      "other generation tasks, whose output space too is discrete, high-dimensional and concentrated.\n",
      "\n",
      "4.1 CONTROLLED SIMULATIONS\n",
      "\n",
      "We experiment with a 1-layer softmax model, that predicts a single token i ∈ V with probability\n",
      "eθi\n",
      "j eθj . θ = {θj}j∈V are the model’s parameters. This model simulates the top of any MT decoder\n",
      "(cid:80)\n",
      "that ends with a softmax layer, as essentially all NMT decoders do. To make experiments realistic,\n",
      "we use similar parameters as those reported in the inﬂuential Transformer NMT system (Vaswani\n",
      "et al., 2017). Speciﬁcally, the size of V (distinct BPE tokens) is 30,715, and the initial values for θ\n",
      "were sampled from 1,000 sets of logits taken from decoding the standard newstest2013 development\n",
      "set, using a pretrained Transformer model. The model was pretrained on WMT2015 training data\n",
      "(Bojar et al., 2015). Hyperparameters are reported in Appendix A.3. We deﬁne one of the tokens in\n",
      "V to be the target token and denote it with ybest. We assign deterministic token reward, this makes\n",
      "learning easier than when relying on approximations and our predictions optimistic. We experiment\n",
      "with two reward functions:\n",
      "\n",
      "1. Simulated Reward: r(y) = 2 for y = ybest, r(y) = 1 if y is one of the 10 initially highest\n",
      "scoring tokens, and r(y) = 0 otherwise. This simulates a condition where the pretrained\n",
      "model is of decent but sub-optimal quality. r here is at the scale of popular rewards used in\n",
      "MT, such as GAN-based rewards or BLEU (which are between 0 and 1).\n",
      "\n",
      "2. Constant Reward: r is constantly equal to 1, for all tokens. This setting is aimed to\n",
      "\n",
      "conﬁrm that PKE is not a result of the signal carried by the reward.\n",
      "\n",
      "Experiments with the ﬁrst setting were run 100 times, each time for 50K steps, updating θ after\n",
      "each step. With the second setting, it is sufﬁcient to take a single step at a time, as the expected\n",
      "update after each step is zero, and so any PKE seen in a single step is only accentuated in the next.\n",
      "It is, therefore, more telling to run more repetitions rather than more steps per initialization. We,\n",
      "therefore, sample 10,000 pretrained distributions, and perform a single REINFORCE step.\n",
      "\n",
      "As RL training lasts about 30 epochs before stopping, samples about 100K tokens per epoch, and\n",
      "as the network already predicts ybest in about two thirds of the contexts,3 we estimate the number\n",
      "of steps used in practice to be in the order of magnitude of 1M. For visual clarity, we present\n",
      "ﬁgures for 50K-100K steps. However, full experiments (with 1M steps) exhibit similar trends:\n",
      "where REINFORCE was not close to converging after 50K steps, the same was true after 1M steps.\n",
      "\n",
      "We evaluate the peakiness of a distribution in terms of the probability of the most probable token\n",
      "(the mode), the total probability of the ten most probable tokens, and the entropy of the distribution\n",
      "(lower entropy indicates more peakiness).\n",
      "\n",
      "3Based on our NMT experiments, which we assume to be representative of the error rate of other systems.\n",
      "\n",
      "4\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "(c)\n",
      "\n",
      "Figure 2: Token probabilities through REINFORCE training, in the controlled simulations in the Simulated\n",
      "Reward setting. The left/center/right ﬁgures correspond to simulations where the target token (ybest) was\n",
      "initially the second/third/fourth most probable token. The green line corresponds to the target token, yellow\n",
      "lines to medium-reward tokens and red lines to no-reward tokens.\n",
      "\n",
      "Results. The distributions become peakier in terms of all three measures: on average, the mode’s\n",
      "probability and the 10 most probable tokens increases, and the entropy decreases. Figure 1a presents\n",
      "the histogram of the update size, the difference in the probability of the 10 most probable tokens in\n",
      "the Constant Reward setting, after a single step. Figure 1b depicts similar statistics for the mode.\n",
      "The average entropy in the pretrained model is 2.9 is reduced to 2.85 after one REINFORCE step.\n",
      "\n",
      "Simulated Reward setting shows similar trends. For example, entropy decreases from 3 to about\n",
      "0.001 in 100K steps. This extreme decrease suggests it is effectively a deterministic policy. PKE is\n",
      "achieved in a few hundred steps, usually before other effects become prominent (see Figure 2), and\n",
      "is stronger than for Constant Reward.\n",
      "\n",
      "4.2 NMT EXPERIMENTS\n",
      "\n",
      "We turn to analyzing a real-world application of REINFORCE to\n",
      "NMT. Important differences between this and the previous simula-\n",
      "tions are: (1) it is rare in NMT for REINFORCE to sample from the\n",
      "same conditional distribution more than a handful of times, given\n",
      "the number of source sentences x and sentence preﬁxes y<i (con-\n",
      "texts); and (2) in NMT Pθ(·|x, y<i) shares parameters between con-\n",
      "texts, which means that updating Pθ for one context may inﬂuence\n",
      "Pθ for another.\n",
      "\n",
      "We follow the same pretraining as in §4.1. We then follow Yang\n",
      "et al. (2018) in deﬁning the reward function based on the expected\n",
      "BLEU score. Expected BLEU is computed by sampling sufﬁxes for\n",
      "the sentence, and averaging the BLEU score of the sampled sen-\n",
      "tences against the reference.\n",
      "\n",
      "We use early stopping with a patience of 10 epochs, where each\n",
      "epoch consists of 5,000 sentences sampled from the WMT2015\n",
      "(Bojar et al., 2015) German-English training data. We use k = 1.\n",
      "We retuned the learning-rate, and positive baseline settings against\n",
      "the development set. Other hyper-parameters were an exact repli-\n",
      "cation of the experiments reported in (Yang et al., 2018).\n",
      "\n",
      "Figure 3: The cumulative distri-\n",
      "bution of the probability of the\n",
      "most likely token in the NMT ex-\n",
      "periments. The green distribu-\n",
      "tion corresponds to the pretrained\n",
      "model, and the blue corresponds\n",
      "to the reinforced model. The y-\n",
      "axis is the proportion of condi-\n",
      "tional probabilities with a mode\n",
      "of value ≤ x (the x-axis). Note\n",
      "that a lower cumulative percent-\n",
      "age means a more peaked output\n",
      "distribution. A lower cumulative\n",
      "percentage means a more peaked\n",
      "output distribution.\n",
      "\n",
      "Results. Results indicate an increase in the peakiness of the conditional distributions. Our results\n",
      "are based on a sample of 1,000 contexts from the pretrained model, and another (independent)\n",
      "sample from the reinforced model.\n",
      "\n",
      "The modes of the conditional distributions tend to increase. Figure 3 presents the distribution of the\n",
      "modes’ probability in the reinforced conditional distributions compared with the pretrained model,\n",
      "showing a shift of probability mass towards higher probabilities for the mode, following RL. Another\n",
      "indication of the increased peakiness is the decrease in the average entropy of Pθ, which was reduced\n",
      "from 3.45 in the pretrained model to an average of 2.82 following RL. This more modest reduction\n",
      "in entropy (compared to §4.1) might also suggest that the procedure did not converge to the optimal\n",
      "\n",
      "5\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "value for θ, as then we would have expected the entropy to substantially drop if not to 0 (overﬁt),\n",
      "then to the average entropy of valid next tokens (given the source and a preﬁx of the sentence).\n",
      "\n",
      "5 PERFORMANCE FOLLOWING REINFORCE\n",
      "\n",
      "We now turn to assessing under what conditions it is likely that REINFORCE will lead to an improve-\n",
      "ment in the performance of an NMT system. As in the previous section, we use both controlled\n",
      "simulations and NMT experiments.\n",
      "\n",
      "5.1 CONTROLLED SIMULATIONS\n",
      "\n",
      "We use the same model and experimental setup described in Section 4.1, this time only exploring\n",
      "the Simulated Reward setting, as a Constant Reward is not expected to converge to any meaningful\n",
      "θ. Results are averaged over 100 conditional distributions sampled from the pretrained model.\n",
      "\n",
      "Caution should be exercised when determining the learning rate\n",
      "(LR). Common LRs used in the NMT literature are of the scale of\n",
      "10−4. However, in our simulations, no LR smaller than 0.1 yielded\n",
      "any improvement in R. We thus set the LR to be 0.1. We note\n",
      "that in our simulations, a higher learning rate means faster conver-\n",
      "gence as our reward is noise-free: it is always highest for the best\n",
      "option. In practice, increasing the learning rate may deteriorate re-\n",
      "sults, as it may cause the system to overﬁt to the sampled instances.\n",
      "Indeed, when increasing the learning rate in our NMT experiments\n",
      "(see below) by an order of magnitude, early stopping caused the RL\n",
      "procedure to stop without any parameter updates.\n",
      "\n",
      "Figure 2 shows the change in Pθ over the ﬁrst 50K REINFORCE\n",
      "steps (probabilities are averaged over 100 repetitions), for a case\n",
      "where ybest was initially the second, third and fourth most probable.\n",
      "Although these are the easiest settings, and despite the high learning\n",
      "rate, it fails to make ybest the mode of the distribution within 100K\n",
      "steps, unless ybest was initially the second most probable. In cases where ybest is initially of a lower\n",
      "rank than four, it is hard to see any increase in its probability, even after 1M steps.\n",
      "\n",
      "Figure 4: Cumulative percentage\n",
      "of contexts where the pretrained\n",
      "model ranks ybest in rank x or\n",
      "below and where it does not rank\n",
      "ybest ﬁrst (x = 0). In about half\n",
      "the cases it is ranked fourth or be-\n",
      "low.\n",
      "\n",
      "5.2 NMT EXPERIMENTS\n",
      "\n",
      "We trained an NMT system, using the same procedure as in Section 4.2, and report BLEU scores\n",
      "over the news2014 test set. After training with an expected BLEU reward, we indeed see a minor\n",
      "improvement which is consistent between trials and pretrained models. While the pretrain BLEU\n",
      "score is 30.31, the reinforced one is 30.73.\n",
      "\n",
      "Analyzing what words were inﬂuenced by the RL procedure, we begin by computing the cumulative\n",
      "probability of the target token ybest to be ranked lower than a given rank according to the pretrained\n",
      "model. Results (Figure 4) show that in about half of the cases, ybest is not among the top three\n",
      "choices of the pretrained model, and we thus expect it not to gain substantial probability following\n",
      "REINFORCE, according to our simulations.\n",
      "\n",
      "We next turn to compare the ranks the reinforced model assigns to the target tokens, and their\n",
      "ranks according to the pretrained model. Figure 6 presents the difference in the probability that\n",
      "ybest is ranked at a given rank following RL and the probability it is ranked there initially. Results\n",
      "indicate that indeed more target tokens are ranked ﬁrst, and less second, but little consistent shift of\n",
      "probability mass occurs otherwise across the ten ﬁrst ranks. It is possible that RL has managed to\n",
      "push ybest in some cases between very low ranks (<1,000) to medium-low ranks (between 10 and\n",
      "1,000). However, token probabilities in these ranks are so low that it is unlikely to affect the system\n",
      "outputs in any way. This ﬁts well with the results of our simulations that predicted that only the\n",
      "initially top-ranked tokens are likely to change.\n",
      "\n",
      "In an attempt to explain the improved BLEU score following RL with PKE, we repeat the NMT ex-\n",
      "periment this time using a constant reward of 1. Our results present a nearly identical improvement\n",
      "in BLEU, achieving 30.72, and a similar pattern in the change of the target tokens’ ranks (see Ap-\n",
      "\n",
      "6\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "Figure 5: The probability of dif-\n",
      "ferent tokens following CMRT, in\n",
      "the controlled simulations in the\n",
      "Simulated Reward setting. The\n",
      "left/right ﬁgures correspond to\n",
      "simulations where the target to-\n",
      "ken (ybest) was initially the sec-\n",
      "ond/third most probable token.\n",
      "The green line corresponds to\n",
      "the target token, yellow lines to\n",
      "medium-reward tokens and red\n",
      "lines to tokens with r(y) = 0.\n",
      "\n",
      "pendix 8). Therefore, there is room to suspect that even in cases where RL yields an improvement\n",
      "in BLEU, it may partially result from reward-independent factors, such as PKE.4\n",
      "\n",
      "6 EXPERIMENTS WITH CONTRASTIVE MRT\n",
      "\n",
      "In §2.2 we showed that CMRT does not, in fact, maximize R, and so\n",
      "does not enjoy the same theoretical guarantees as REINFORCE and\n",
      "similar policy gradient methods. However, being the RL procedure\n",
      "of choice in much recent work we repeat the simulations described\n",
      "in §4 and §5, assessing CMRT’s performance in these conditions.\n",
      "We experiment with α = 0.005 and k = 20, common settings in\n",
      "the literature, and average over 100 trials.\n",
      "\n",
      "Figure 5 shows how the distribution Pθ changes over the course\n",
      "of 50K update steps to θ, where ybest is taken to be the second\n",
      "and third initially most probable token (Simulated Reward setting).\n",
      "Results are similar in trends to those obtained with REINFORCE:\n",
      "MRT succeeds in pushing ybest to be the highest ranked token if it\n",
      "was initially second, but struggles where it was initially ranked third\n",
      "or below. We only observe a small PKE in MRT. This is probably\n",
      "due to the contrastive effect, which means that tokens that were not\n",
      "sampled do not lose probability mass.\n",
      "\n",
      "All graphs we present here allow sampling the same token more\n",
      "than once in each batch (i.e., S is a sample with replacements).\n",
      "Simulations with deduplication show similar results.\n",
      "\n",
      "7 DISCUSSION\n",
      "\n",
      "Figure 6: Difference between\n",
      "the ranks of ybest in the rein-\n",
      "forced and the pretrained model.\n",
      "Each column x corresponds to\n",
      "the difference in the probabil-\n",
      "ity that ybest is ranked in rank\n",
      "x in the reinforced model, and\n",
      "the same probability in the pre-\n",
      "trained model.\n",
      "\n",
      "In this paper, we showed that the type of distributions used in NMT entail that promoting the target\n",
      "token to be the mode is likely to take a prohibitively long times for existing RL practices, except\n",
      "under the best conditions (where the pretrained model is “nearly” correct). This leads us to conclude\n",
      "that observed improvements from using RL for NMT are likely due either to ﬁne-tuning the most\n",
      "probable tokens in the pretrained model (an effect which may be more easily achieved using rerank-\n",
      "ing methods, and uses but little of the power of RL methods), or to effects unrelated to the signal\n",
      "carried by the reward, such as PKE. Another contribution of this paper is in showing that CMRT\n",
      "does not optimize the expected reward and is thus theoretically unmotivated.\n",
      "\n",
      "A number of reasons lead us to believe that in our NMT experiments, improvements are not due to\n",
      "the reward function, but to artefacts such as PKE. First, reducing a constant baseline from r, so as to\n",
      "make the expected reward zero, disallows learning. This is surprising, as REINFORCE, generally and\n",
      "in our simulations, converges faster where the reward is centered around zero, and so the fact that\n",
      "this procedure here disallows learning hints that other factors are in play. As PKE can be observed\n",
      "even where the reward is constant (if the expected reward is positive; see §4.1), this suggests PKE\n",
      "\n",
      "4We tried several other reward functions as well, all of which got BLEU scores of 30.73–30.84. This\n",
      "\n",
      "improvement is very stable across metrics, trials and pretrained models.\n",
      "\n",
      "7\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "may play a role here. Second, we observe more peakiness in the reinforced model and in such\n",
      "cases, we expect improvements in BLEU (Caccia et al., 2018). Third, we achieve similar results\n",
      "with a constant reward in our NMT experiments (§5.2). Fourth, our controlled simulations show\n",
      "that asymptotic convergence is not reached in any but the easiest conditions (§5.1).\n",
      "\n",
      "Our analysis further suggests that gradient clipping, sometimes used in NMT (Zhang et al., 2016;\n",
      "Wieting et al., 2019), is expected to hinder convergence further. It should be avoided when using\n",
      "REINFORCE as it violates REINFORCE’s assumptions.\n",
      "\n",
      "The per-token sampling as done in our experiments is more exploratory than beam search (Wu et al.,\n",
      "2018), reducing PKE. Furthermore, the latter does not sample from the behavior policy, but does not\n",
      "properly account for being off-policy in the parameter updates.\n",
      "\n",
      "Adding the reference to the sample S, which some implementations allow (Sennrich et al., 2017)\n",
      "may help reduce the problems of never sampling the target tokens. However, as Edunov et al.\n",
      "(2018) point out, this practice may lower results, as it may destabilize training by leading the model\n",
      "to improve over outputs it cannot generalize over, as they are very different from anything the model\n",
      "assigns a high probability to, at the cost of other outputs.\n",
      "\n",
      "8 CONCLUSION\n",
      "\n",
      "The standard MT scenario poses several uncommon challenges for RL. First, the action space in\n",
      "MT problems is a high-dimensional discrete space (generally in the size of the vocabulary of the\n",
      "target language or the product thereof for sentences). This contrasts with the more common sce-\n",
      "nario studied by contemporary RL methods, which focuses mostly on much smaller discrete action\n",
      "spaces (e.g., video games (Mnih et al., 2015; 2016)), or continuous action spaces of relatively low\n",
      "dimensions (e.g., simulation of robotic control tasks (Lillicrap et al., 2015)). Second, reward for MT\n",
      "is naturally very sparse – almost all possible sentences are “wrong” (hence, not rewarding) in a given\n",
      "context. Finally, it is common in MT to use RL for tuning a pretrained model. Using a pretrained\n",
      "model ameliorates the last problem. But then, these pretrained models are in general quite peaky,\n",
      "and because training is done on-policy – that is, actions are being sampled from the same model\n",
      "being optimized – exploration is inherently limited.\n",
      "\n",
      "Here we argued that, taken together, these challenges result in signiﬁcant weaknesses for current RL\n",
      "practices for NMT, that may ultimately prevent them from being truly useful. At least some of these\n",
      "challenges have been widely studied in the RL literature, with numerous techniques developed to\n",
      "address them, but were not yet adopted in NLP. We turn to discuss some of them.\n",
      "\n",
      "Off-policy methods, in which observations are sampled from a different policy than the one being\n",
      "currently optimized, are prominent in RL (Watkins & Dayan, 1992; Sutton & Barto, 1998), and\n",
      "were also studied in the context of policy gradient methods (Degris et al., 2012; Silver et al., 2014).\n",
      "In principle, such methods allow learning from a more “exploratory” policy. Moreover, a key mo-\n",
      "tivation for using α in CMRT is smoothing; off-policy sampling allows smoothing while keeping\n",
      "convergence guarantees.\n",
      "\n",
      "In its basic form, exploration in REINFORCE relies on stochasticity in the action-selection (in MT,\n",
      "this is due to sampling). More sophisticated exploration methods have been extensively studied,\n",
      "for example using measures for the exploratory usefulness of states or actions (Fox et al., 2018), or\n",
      "relying on parameter-space noise rather than action-space noise (Plappert et al., 2017).\n",
      "\n",
      "For MT, an additional challenge is that even effective exploration (sampling diverse sets of obser-\n",
      "vations), may not be enough, since the state-action space is too large to be effectively covered, with\n",
      "almost all sentences being not rewarding. Recently, diversity-based and multi-goal methods for RL\n",
      "were proposed to tackle similar challenges (Andrychowicz et al., 2017; Ghosh et al., 2018; Eysen-\n",
      "bach et al., 2019). We believe the adoption of such methods is a promising path forward for the\n",
      "application of RL in NLP.\n",
      "\n",
      "8\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "9 ACKNOWLEDGMENTS\n",
      "\n",
      "This work was supported by the Israel Science Foundation (grant no. 929/17) and by the HUJI\n",
      "Cyber Security Research Center in conjunction with the Israel National Cyber Bureau in the Prime\n",
      "Minister’s Ofﬁce.\n",
      "\n",
      "REFERENCES\n",
      "\n",
      "Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob\n",
      "McGrew, Josh Tobin, OpenAI Pieter Abbeel, and Wojciech Zaremba. Hindsight experience re-\n",
      "play. In Advances in Neural Information Processing Systems, pp. 5048–5058, 2017.\n",
      "\n",
      "Shiqi Shen Ayana, Zhiyuan Liu, and Maosong Sun. Neural headline generation with minimum risk\n",
      "\n",
      "training. arXiv preprint arXiv:1604.01904, 2016.\n",
      "\n",
      "Ondrej Bojar, Rajen Chatterjee, Christian Federmann, Barry Haddow, Matthias Huck, Chris\n",
      "Hokamp, Philipp Koehn, Varvara Logacheva, Christof Monz, Matteo Negri, Matt Post, Carolina\n",
      "Scarton, Lucia Specia, and Marco Turchi. Findings of the 2015 workshop on statistical machine\n",
      "translation. In WMT@EMNLP, 2015.\n",
      "\n",
      "Massimo Caccia, Lucas Caccia, William Fedus, Hugo Larochelle, Joelle Pineau, and Laurent Char-\n",
      "lin. Language gans falling short. arXiv preprint arXiv:1811.02549, 2018. URL https:\n",
      "//arxiv.org/pdf/1811.02549.pdf.\n",
      "\n",
      "Thomas Degris, Martha White, and Richard S Sutton. Off-policy actor-critic. arXiv preprint\n",
      "\n",
      "arXiv:1205.4839, 2012.\n",
      "\n",
      "Sergey Edunov, Myle Ott, Michael Auli, David Grangier, and Marc’Aurelio Ranzato. Classical\n",
      "structured prediction losses for sequence to sequence learning. In Proceedings of the 2018 Con-\n",
      "ference of the North American Chapter of the Association for Computational Linguistics: Human\n",
      "Language Technologies, Volume 1 (Long Papers), pp. 355–364. Association for Computational\n",
      "Linguistics, 2018. doi: 10.18653/v1/N18-1033. URL http://aclweb.org/anthology/\n",
      "N18-1033.\n",
      "\n",
      "Benjamin Eysenbach, Abhishek Gupta, Julian Ibarz, and Sergey Levine. Diversity is all you need:\n",
      "Learning skills without a reward function. In International Conference on Learning Representa-\n",
      "tions, 2019. URL https://openreview.net/forum?id=SJx63jRqFm.\n",
      "\n",
      "Lior Fox, Leshem Choshen, and Yonatan Loewenstein. Dora the explorer: Directed outreaching\n",
      "\n",
      "reinforcement action-selection. ICLR, abs/1804.04012, 2018.\n",
      "\n",
      "Dibya Ghosh, Avi Singh, Aravind Rajeswaran, Vikash Kumar, and Sergey Levine. Divide-and-\n",
      "conquer reinforcement learning. In International Conference on Learning Representations, 2018.\n",
      "URL https://openreview.net/forum?id=rJwelMbR-.\n",
      "\n",
      "Lisa Anne Hendricks, Zeynep Akata, Marcus Rohrbach, Jeff Donahue, Bernt Schiele, and Trevor\n",
      "\n",
      "Darrell. Generating visual explanations. In ECCV, 2016.\n",
      "\n",
      "Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. CoRR,\n",
      "\n",
      "abs/1412.6980, 2015.\n",
      "\n",
      "Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola\n",
      "Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar,\n",
      "Alexandra Constantin, and Evan Herbst. Moses: Open source toolkit for statistical machine\n",
      "In Proceedings of the 45th Annual Meeting of the Association for Computational\n",
      "translation.\n",
      "Linguistics Companion Volume Proceedings of the Demo and Poster Sessions, pp. 177–180, 2007.\n",
      "\n",
      "Jiwei Li, Will Monroe, Tianlin Shi, Sébastien Jean, Alan Ritter, and Dan Jurafsky. Adversarial learn-\n",
      "ing for neural dialogue generation. In Proceedings of the 2017 Conference on Empirical Methods\n",
      "in Natural Language Processing, pp. 2157–2169, Copenhagen, Denmark, September 2017. As-\n",
      "sociation for Computational Linguistics. URL https://www.aclweb.org/anthology/\n",
      "D17-1230.\n",
      "\n",
      "9\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,\n",
      "David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv\n",
      "preprint arXiv:1509.02971, 2015.\n",
      "\n",
      "Siqi Liu, Zhenhai Zhu, Ning Ye, Sergio Guadarrama, and Kevin Murphy. Optimization of image\n",
      "\n",
      "description metrics using policy gradient methods. CoRR, abs/1612.00370, 2, 2016.\n",
      "\n",
      "Peter Makarov and Simon Clematide. Neural transition-based string transduction for limited-\n",
      "\n",
      "resource setting in morphology. In COLING, 2018.\n",
      "\n",
      "Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Belle-\n",
      "mare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level\n",
      "control through deep reinforcement learning. Nature, 518(7540):529–533, 2015.\n",
      "\n",
      "Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim\n",
      "Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement\n",
      "learning. In International conference on machine learning, pp. 1928–1937, 2016.\n",
      "\n",
      "Graham Neubig. Lexicons and minimum risk training for neural machine translation: Naist-cmu at\n",
      "\n",
      "wat2016. In WAT@COLING, 2016.\n",
      "\n",
      "Graham Neubig, Matthias Sperber, Xinyi Wang, Matthieu Felix, Austin Matthews, Sarguna Pad-\n",
      "manabhan, Ye Qi, Devendra Singh Sachan, Philip Arthur, Pierre Godard, John Hewitt, Rachid\n",
      "Riad, and Liming Wang. Xnmt: The extensible neural machine translation toolkit. In AMTA,\n",
      "2018.\n",
      "\n",
      "Franz Josef Och. Minimum error rate training in statistical machine translation. In Proceedings of\n",
      "the 41st Annual Meeting on Association for Computational Linguistics-Volume 1, pp. 160–167.\n",
      "Association for Computational Linguistics, 2003.\n",
      "\n",
      "Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic\n",
      "evaluation of machine translation. In Proceedings of the 40th annual meeting on association for\n",
      "computational linguistics, pp. 311–318. Association for Computational Linguistics, 2002. URL\n",
      "https://www.aclweb.org/anthology/P02-1040.pdf.\n",
      "\n",
      "Matthias Plappert, Rein Houthooft, Prafulla Dhariwal, Szymon Sidor, Richard Y Chen, Xi Chen,\n",
      "Tamim Asfour, Pieter Abbeel, and Marcin Andrychowicz. Parameter space noise for exploration.\n",
      "arXiv preprint arXiv:1706.01905, 2017.\n",
      "\n",
      "O. Press, A. Bar, B. Bogin, J. Berant, and L. Wolf. Language generation with recurrent generative\n",
      "In Fist Workshop on Learning to Generate Natural\n",
      "\n",
      "adversarial networks without pre-training.\n",
      "Language@ICML, 2017.\n",
      "\n",
      "Marc’Aurelio Ranzato, Sumit Chopra, Michael Auli, and Wojciech Zaremba. Sequence level train-\n",
      "\n",
      "ing with recurrent neural networks. arXiv preprint arXiv:1511.06732, 2015.\n",
      "\n",
      "Steven J Rennie, Etienne Marcheret, Youssef Mroueh, Jerret Ross, and Vaibhava Goel. Self-critical\n",
      "sequence training for image captioning. In Proceedings of the IEEE Conference on Computer\n",
      "Vision and Pattern Recognition, pp. 7008–7024, 2017.\n",
      "\n",
      "Keisuke Sakaguchi, Matt Post, and Benjamin Van Durme. Grammatical error correction with neural\n",
      "\n",
      "reinforcement learning. arXiv preprint arXiv:1707.00299, 2017.\n",
      "\n",
      "Philip Schulz, Wilker Aziz, and Trevor Cohn. A stochastic decoder for neural machine translation.\n",
      "\n",
      "In ACL, 2018.\n",
      "\n",
      "Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with\n",
      "subword units. In Proceedings of the 54th Annual Meeting of the Association for Computational\n",
      "Linguistics (Volume 1: Long Papers), volume 1, pp. 1715–1725, 2016. URL http://www.\n",
      "aclweb.org/anthology/P16-1162.\n",
      "\n",
      "Rico Sennrich, Orhan Firat, Kyunghyun Cho, Alexandra Birch, Barry Haddow, Julian Hitschler,\n",
      "Marcin Junczys-Dowmunt, Samuel Läubli, Antonio Valerio Miceli Barone, Jozef Mokry, and\n",
      "Maria Nadejde. Nematus: a toolkit for neural machine translation. In EACL, 2017.\n",
      "\n",
      "10\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "Shiqi Shen, Yong Cheng, Zhongjun He, Wei He, Hua Wu, Maosong Sun, and Yang Liu. Minimum\n",
      "risk training for neural machine translation. In Proceedings of the 54th Annual Meeting of the\n",
      "Association for Computational Linguistics (Volume 1: Long Papers), pp. 1683–1692. Association\n",
      "for Computational Linguistics, 2016. doi: 10.18653/v1/P16-1159. URL http://aclweb.\n",
      "org/anthology/P16-1159.\n",
      "\n",
      "Shiqi Shen, Yang Liu, and Maosong Sun. Optimizing non-decomposable evaluation metrics for\n",
      "\n",
      "neural machine translation. Journal of Computer Science and Technology, 32:796–804, 2017.\n",
      "\n",
      "Rakshith Shetty, Marcus Rohrbach, Lisa Anne Hendricks, Mario Fritz, and Bernt Schiele. Speaking\n",
      "the same language: Matching machine to human captions by adversarial training. In 2017 IEEE\n",
      "International Conference on Computer Vision (ICCV), pp. 4155–4164. IEEE, 2017.\n",
      "\n",
      "David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller.\n",
      "\n",
      "Deterministic policy gradient algorithms. In ICML, 2014.\n",
      "\n",
      "Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 1998.\n",
      "\n",
      "G. Tevet, G. Habib, V. Shwartz, and J. Berant. Evaluating text GANs as language models. arXiv\n",
      "\n",
      "preprint arXiv:1810.12686, 2018.\n",
      "\n",
      "Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running\n",
      "average of its recent magnitude. COURSERA: Neural networks for machine learning, 4(2):26–\n",
      "31, 2012.\n",
      "\n",
      "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n",
      "Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Informa-\n",
      "tion Processing Systems, pp. 5998–6008, 2017. URL https://papers.nips.cc/paper/\n",
      "7181-attention-is-all-you-need.pdf.\n",
      "\n",
      "Christopher JCH Watkins and Peter Dayan. Q-learning. Machine learning, 8(3-4):279–292, 1992.\n",
      "\n",
      "John Wieting, Taylor Berg-Kirkpatrick, Kevin Gimpel, and Graham Neubig. Beyond BLEU: Train-\n",
      "In The 57th Annual Meeting of the\n",
      "ing neural machine translation with semantic similarity.\n",
      "Association for Computational Linguistics (ACL), Florence, Italy, July 2019. URL https:\n",
      "//arxiv.org/abs/1909.06694.\n",
      "\n",
      "Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement\n",
      "\n",
      "learning. Machine learning, 8(3-4):229–256, 1992.\n",
      "\n",
      "Lijun Wu, Yingce Xia, Li Zhao, Fei Tian, Tao Qin, Jianhuang Lai, and Tie-Yan Liu. Adversarial\n",
      "\n",
      "neural machine translation. arXiv preprint arXiv:1704.06933, 2017.\n",
      "\n",
      "Lijun Wu, Fei Tian, Tao Qin, Jianhuang Lai, and Tie-Yan Liu. A study of reinforcement learning\n",
      "\n",
      "for neural machine translation. In EMNLP, 2018.\n",
      "\n",
      "Zhen Yang, Wei Chen, Feng Wang, and Bo Xu. Improving neural machine translation with condi-\n",
      "tional sequence generative adversarial nets. In Proceedings of the 2018 Conference of the North\n",
      "American Chapter of the Association for Computational Linguistics: Human Language Technolo-\n",
      "gies, Volume 1 (Long Papers), pp. 1346–1355. Association for Computational Linguistics, 2018.\n",
      "doi: 10.18653/v1/N18-1122. URL http://aclweb.org/anthology/N18-1122.\n",
      "\n",
      "Lantao Yu, Weinan Zhang, Jun Wang, and Yong Yu. Seqgan: Sequence generative adversarial nets\n",
      "\n",
      "with policy gradient. In AAAI, pp. 2852–2858, 2017.\n",
      "\n",
      "Jiac heng Zhang, Yanzhuo Ding, Shiqi Shen, Yong Cheng, Maosong Sun, Huanbo Luan, and\n",
      "Yang Liu. Thumt: An open source toolkit for neural machine translation. arXiv preprint\n",
      "arXiv:1706.06415, 2017.\n",
      "\n",
      "Yizhe Zhang, Zhe Gan, and Lawrence Carin. Generating text via adversarial training.\n",
      "\n",
      "In NIPS\n",
      "\n",
      "workshop on Adversarial Training, volume 21, 2016.\n",
      "\n",
      "11\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "A APPENDIX\n",
      "\n",
      "A.1 CONTRASTIVE MRT DOES NOT MAXIMIZE THE EXPECTED REWARD\n",
      "\n",
      "We hereby detail a simple example where following the Contrastive MRT method (see §2.2) does\n",
      "not converge to the parameter value that maximizes R.\n",
      "\n",
      "Let θ be a real number in [0, 0.5], and let Pθ be a family of distributions over three values a, b, c such\n",
      "that:\n",
      "\n",
      "Let r(a) = 1, r(b) = 0, r(c) = 0.5. The expected reward as a function of θ is:\n",
      "\n",
      "Pθ (x) =\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "x = a\n",
      "θ\n",
      "2θ2\n",
      "x = b\n",
      "1 − θ − 2θ2 x = c\n",
      "\n",
      "R(θ) = θ + 0.5(1 − θ − 2θ2)\n",
      "\n",
      "R(θ) is uniquely maximized by θ∗ = 0.25.\n",
      "\n",
      "Table 1 details the possible samples of size k = 2, their probabilities, the corresponding (cid:101)R and\n",
      "its gradient. Standard numerical methods show that E[∇ (cid:101)R] over possible samples S is positive for\n",
      "θ ∈ (0, γ) and negative for θ ∈ (γ, 0.5], where γ ≈ 0.295. This means that for any initialization of\n",
      "θ ∈ (0, 0.5], Contrastive MRT will converge to γ if the learning rate is sufﬁciently small. For θ = 0,\n",
      "(cid:101)R ≡ 0.5, and there will be no gradient updates, so the method will converge to θ = 0. Neither of\n",
      "these values maximizes R(θ).\n",
      "We note that by using some g (θ) the γ could be arbitrarily far from θ∗. g could also map to\n",
      "(−inf, inf ) more often used in neural networks parameters.\n",
      "\n",
      "We further note that resorting to maximizing E[ (cid:101)R] instead, does not maximize R(θ) either. Indeed,\n",
      "plotting E[ (cid:101)R] as a function of θ for this example, yields a maximum at θ ≈ 0.32.\n",
      "\n",
      "Table 1: The gradients of (cid:101)R for each possible sample S. The batch size is k = 2. Rows correspond to different\n",
      "sampled outcomes. ∇ (cid:101)R is the gradient of (cid:101)R given the corresponding value for S.\n",
      "\n",
      "S\n",
      "\n",
      "{a, b}\n",
      "\n",
      "{a, c}\n",
      "\n",
      "{b, c}\n",
      "a, a\n",
      "b, b\n",
      "c, c\n",
      "\n",
      "P (S)\n",
      "4θ3\n",
      "\n",
      "θ2\n",
      "4θ4\n",
      "\n",
      "(1-θ-2θ2)2\n",
      "\n",
      "2θ(1-θ-2θ2)\n",
      "4θ2(1-θ-2θ2)\n",
      "\n",
      "0.5 +\n",
      "\n",
      "θ\n",
      "\n",
      "2−4θ2\n",
      "\n",
      "1−θ−2θ2\n",
      "\n",
      "2−2θ\n",
      "\n",
      "(cid:101)R\n",
      "1\n",
      "\n",
      "1+2θ\n",
      "\n",
      "1\n",
      "0\n",
      "0.5\n",
      "\n",
      "∇ (cid:101)R\n",
      "−2\n",
      "\n",
      "(1+2θ)2\n",
      "2x2+1\n",
      "\n",
      "2(1−2θ2)2\n",
      "\n",
      "θ2−2θ\n",
      "(1−θ)2\n",
      "\n",
      "0\n",
      "0\n",
      "0\n",
      "\n",
      "A.2 DERIVING THE GRADIENT OF (cid:101)R\n",
      "\n",
      "Given S, recall the deﬁnition of (cid:101)R:\n",
      "\n",
      "Taking the deriviative w.r.t. θ:\n",
      "\n",
      "(cid:101)R(θ, S) =\n",
      "\n",
      "Qθ,S(yi)r(yi)\n",
      "\n",
      "k\n",
      "\n",
      "(cid:88)\n",
      "\n",
      "i=1\n",
      "\n",
      "12\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "k\n",
      "\n",
      "(cid:88)\n",
      "\n",
      "i=1\n",
      "\n",
      "r(yi)\n",
      "\n",
      "∇P (y) · αP (y)α−1 · Z(S) − ∇Z(S) · P (y)α\n",
      "\n",
      "=\n",
      "\n",
      "Z(S)2\n",
      "\n",
      "k\n",
      "\n",
      "(cid:88)\n",
      "\n",
      "i=1\n",
      "\n",
      "k\n",
      "\n",
      "(cid:88)\n",
      "\n",
      "i=1\n",
      "\n",
      "k\n",
      "\n",
      "(cid:88)\n",
      "\n",
      "(cid:16)\n",
      "\n",
      "α\n",
      "\n",
      "i=1\n",
      "\n",
      "r(yi)\n",
      "\n",
      "(cid:16) α∇P (yi)\n",
      "\n",
      "P (yi)\n",
      "\n",
      "Q(yi) −\n",
      "\n",
      "∇Z(S)\n",
      "Z(S)\n",
      "\n",
      "(cid:17)\n",
      "\n",
      "Q(yi)\n",
      "\n",
      "=\n",
      "\n",
      "r(yi)Q(yi)\n",
      "\n",
      "α∇ log P (yi) − ∇ log Z(S)\n",
      "\n",
      "=\n",
      "\n",
      "(cid:16)\n",
      "\n",
      "(cid:17)\n",
      "\n",
      "r(yi)Q(yi)∇ log P (yi)\n",
      "\n",
      "− EQ[r]∇ log Z(S)\n",
      "\n",
      "(cid:17)\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "(c)\n",
      "\n",
      "Figure 7: The probability of different tokens following REINFORCE, in the controlled simulations in the Con-\n",
      "stant Reward setting. The left/center/right ﬁgures correspond to simulations where the target token (ybest) was\n",
      "initially the second/third/fourth most probable token. The green line corresponds to the target token, yellow\n",
      "lines to medium-reward tokens and red lines to tokens with r(y) = 0.\n",
      "\n",
      "A.3 NMT IMPLEMENTATION DETAILS\n",
      "\n",
      "True casing and tokenization were used (Koehn et al., 2007), including escaping html symbols and\n",
      "\"-\" that represents a compound was changed into a separate token of =. Some preprocessing used\n",
      "before us converted the latter to ##AT##-##AT## but standard tokenizers in use process that into 11\n",
      "different tokens, which over-represents the signiﬁcance of that character when BLEU is calculated.\n",
      "BPE (Sennrich et al., 2016) extracted 30,715 tokens. For the MT experiments we used 6 layers in\n",
      "the encoder and the decoder. The size of the embeddings was 512. Gradient clipping was used with\n",
      "size of 5 for pre-training (see Discussion on why not to use it in training). We did not use attention\n",
      "dropout, but 0.1 residual dropout rate was used. In pretraining and training sentences of more than\n",
      "50 tokens were discarded. Pretraining and training were considered ﬁnished when BLEU did not\n",
      "increase in the development set for 10 consecutive evaluations, and evaluation was done every 1,000\n",
      "and 5,000 for batches of size 100 and 256 for pretraining and training respectively. Learning rate\n",
      "used for rmsprop (Tieleman & Hinton, 2012) was 0.01 in pretraining and for adam (Kingma & Ba,\n",
      "2015) with decay was 0.005 for training. 4,000 learning rate warm up steps were used. Pretraining\n",
      "took about 7 days with 4 GPUs, afterwards, training took roughly the same time. Monte Carlo used\n",
      "20 sentence rolls per word.\n",
      "\n",
      "A.4 DETAILED RESULTS FOR CONSTANT REWARD SETTING\n",
      "\n",
      "We present graphs for the constant reward setting in Figures 8 and 7. Trends are similar to the ones\n",
      "obtained for the Simulated Reward setting.\n",
      "\n",
      "13\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "Figure 8: Difference between the ranks of ybest in the reinforced with constant reward and the pretrained model.\n",
      "Each column x corresponds to the difference in the probability that ybest is ranked in rank x in the reinforced\n",
      "model, and the same probability in the pretrained model.\n",
      "\n",
      "14\n",
      "\n",
      "\f",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Under review as a conference paper at ICLR 2020\n",
      "\n",
      "REINFORCEMENT LEARNING BASED\n",
      "GRAPH-TO-SEQUENCE MODEL FOR\n",
      "NATURAL QUESTION GENERATION\n",
      "\n",
      "Anonymous authors\n",
      "Paper under double-blind review\n",
      "\n",
      "ABSTRACT\n",
      "\n",
      "Natural question generation (QG) aims to generate questions from a passage and\n",
      "an answer. Previous works on QG either (i) ignore the rich structure informa-\n",
      "tion hidden in text, (ii) solely rely on cross-entropy loss that leads to issues like\n",
      "exposure bias and inconsistency between train/test measurement, or (iii) fail to\n",
      "fully exploit the answer information. To address these limitations, in this paper,\n",
      "we propose a reinforcement learning (RL) based graph-to-sequence (Graph2Seq)\n",
      "model for QG. Our model consists of a Graph2Seq generator with a novel Bidi-\n",
      "rectional Gated Graph Neural Network based encoder to embed the passage, and\n",
      "a hybrid evaluator with a mixed objective function that combines both the cross-\n",
      "entropy and RL loss to ensure the generation of syntactically and semantically\n",
      "valid text. We also introduce an effective Deep Alignment Network for incorpo-\n",
      "rating the answer information into the passage at both the word and contextual\n",
      "level. Our model is end-to-end trainable and achieves new state-of-the-art scores,\n",
      "outperforming existing methods by a signiﬁcant margin on the standard SQuAD\n",
      "benchmark for QG.\n",
      "\n",
      "1\n",
      "\n",
      "INTRODUCTION\n",
      "\n",
      "Natural question generation (QG) has many useful applications such as improving the question an-\n",
      "swering task (Chen et al., 2017; 2019a) by providing more training data (Tang et al., 2017; Yuan\n",
      "et al., 2017), generating practice exercises and assessments for educational purposes (Heilman &\n",
      "Smith, 2010; Danon & Last, 2017), and helping dialog systems to kick-start and continue a conver-\n",
      "sation with human users (Mostafazadeh et al., 2016). While many existing works focus on QG from\n",
      "images (Fan et al., 2018; Li et al., 2018) or knowledge bases (Serban et al., 2016; Elsahar et al.,\n",
      "2018), in this work, we focus on QG from text.\n",
      "\n",
      "Conventional methods (Mostow & Chen, 2009; Heilman & Smith, 2010; Heilman, 2011) for QG rely\n",
      "on heuristic rules or hand-crafted templates, leading to the issues of low generalizability and scal-\n",
      "ability. Recent attempts have been focused on exploiting Neural Network (NN) based approaches\n",
      "that do not require manually-designed rules and are end-to-end trainable. Encouraged by the huge\n",
      "success of neural machine translation, these approaches formulate the QG task as a sequence-to-\n",
      "sequence (Seq2Seq) learning problem. Speciﬁcally, attention-based Seq2Seq models (Bahdanau\n",
      "et al., 2014; Luong et al., 2015) and their enhanced versions with copy (Vinyals et al., 2015; Gu\n",
      "et al., 2016) and coverage (Tu et al., 2016) mechanisms have been widely applied and show promis-\n",
      "ing results on this task (Du et al., 2017; Zhou et al., 2017; Song et al., 2018a; Kumar et al., 2018a).\n",
      "However, these methods typically ignore the hidden structural information associated with a word\n",
      "sequence such as the syntactic parsing tree. Failing to utilize the rich text structure information\n",
      "beyond the simple word sequence may limit the effectiveness of these models for QG.\n",
      "\n",
      "It has been observed that in general, cross-entropy based sequence training has several limitations\n",
      "like exposure bias and inconsistency between train/test measurement (Ranzato et al., 2015; Wu\n",
      "et al., 2016). As a result, they do not always produce the best results on discrete evaluation metrics\n",
      "on sequence generation tasks such as text summerization (Paulus et al., 2017) or question gener-\n",
      "ation (Song et al., 2017). To cope with these issues, some recent QG approaches (Song et al.,\n",
      "2017; Kumar et al., 2018b) directly optimize evaluation metrics using Reinforcement Learning\n",
      "\n",
      "1\n",
      "\n",
      "\f",
      "Under review as a conference paper at ICLR 2020\n",
      "\n",
      "(RL) (Williams, 1992). However, existing approaches usually only employ evaluation metrics like\n",
      "BLEU and ROUGE-L as rewards for RL optimization. More importantly, they fail to exploit other\n",
      "important metrics such as syntactic and semantic constraints for guiding high-quality text genera-\n",
      "tion.\n",
      "\n",
      "Early works on neural QG did not take into account the answer information when generating a\n",
      "question. Recent works have started to explore various means of utilizing the answer information.\n",
      "When question generation is guided by the semantics of an answer, the resulting questions become\n",
      "more relevant and readable. Conceptually, there are three different ways to incorporate the answer\n",
      "information by simply marking the answer location in the passage (Zhou et al., 2017; Zhao et al.,\n",
      "2018; Liu et al., 2019), or using complex passage-answer matching strategies (Song et al., 2017),\n",
      "or separating answers from passages when applying a Seq2Seq model (Kim et al., 2018; Sun et al.,\n",
      "2018). However, they neglect potential semantic relations between passage words and answer words,\n",
      "and thus fail to explicitly model the global interactions among them in the embedding space.\n",
      "\n",
      "To address these aforementioned issues, in this paper, we present a novel reinforcement learning\n",
      "based generator-evaluator architecture that aims to: i) make full use of rich hidden structure in-\n",
      "formation beyond the simple word sequence; ii) generate syntactically and semantically valid text\n",
      "while maintaining the consistency of train/test measurement; iii) model explicitly the global interac-\n",
      "tions of semantic relationships between passage and answer at both word-level and contextual-level.\n",
      "In particular, to achieve the ﬁrst goal, we ﬁrst explore two different means to either construct a\n",
      "syntax-based static graph or a semantics-aware dynamic graph from the text sequence, as well as its\n",
      "rich hidden structure information. Then, we design a graph-to-sequence (Graph2Seq) model based\n",
      "generator that encodes the graph representation of a text passage and decodes a question sequence\n",
      "using a Recurrent Neural Network (RNN). Our Graph2Seq model is based on a novel bidirectional\n",
      "gated graph neural network, which extends the original gated graph neural network (Li et al., 2015)\n",
      "by considering both incoming and outgoing edges, and fusing them during the graph embedding\n",
      "learning. To achieve the second goal, we design a hybrid evaluator which is trained by optimizing\n",
      "a mixed objective function that combines both cross-entropy and RL loss. We use not only discrete\n",
      "evaluation metrics like BLEU, but also semantic metrics like word mover’s distance (Kusner et al.,\n",
      "2015) to encourage both syntactically and semantically valid text generation. To achieve the third\n",
      "goal, we propose a novel Deep Alignment Network (DAN) for effectively incorporating answer\n",
      "information into the passage at multiple granularity levels.\n",
      "\n",
      "Our main contributions are as follows:\n",
      "\n",
      "• We propose a novel RL-based Graph2Seq model for natural question generation. To the\n",
      "\n",
      "best of our knowledge, we are the ﬁrst to introduce the Graph2Seq architecture for QG.\n",
      "\n",
      "• We explore both static and dynamic ways of constructing graph from text and are the ﬁrst\n",
      "\n",
      "to systematically investigate their performance impacts on a GNN encoder.\n",
      "\n",
      "• The proposed model is end-to-end trainable, achieves new state-of-the-art scores, and out-\n",
      "performs existing methods by a signiﬁcant margin on the standard SQuAD benchmark for\n",
      "QG. Our human evaluation study also corroborates that the questions generated by our\n",
      "model are more natural (semantically and syntactically) compared to other baselines.\n",
      "\n",
      "2 AN RL-BASED GENERATOR-EVALUATOR ARCHITECTURE\n",
      "\n",
      "In this section, we deﬁne the question generation task, and then present our RL-based Graph2Seq\n",
      "model for question generation. We ﬁrst motivate the design, and then present the details of each\n",
      "component as shown in Fig. 1.\n",
      "\n",
      "2.1 PROBLEM FORMULATION\n",
      "\n",
      "The goal of question generation is to generate natural language questions based on a given form of\n",
      "data, such as knowledge base triples or tables (Bao et al., 2018), sentences (Du et al., 2017; Song\n",
      "et al., 2018a), or images (Li et al., 2018), where the generated questions need to be answerable from\n",
      "the input data. In this paper, we focus on QG from a given text passage, along with a target answer.\n",
      "We assume that a text passage is a collection of word tokens X p “ txp\n",
      "answer is also a collection of word tokens X a “ txa\n",
      "\n",
      "N u, and a target\n",
      "Lu. The task of natural question\n",
      "\n",
      "2, ..., xp\n",
      "\n",
      "2, ..., xa\n",
      "\n",
      "1, xp\n",
      "\n",
      "1, xa\n",
      "\n",
      "2\n",
      "\n",
      "\f",
      "Under review as a conference paper at ICLR 2020\n",
      "\n",
      "Figure 1: Overall architecture of the proposed model. Best viewed in color.\n",
      "\n",
      "generation is to generate the best natural language question consisting of a sequence of word tokens\n",
      "ˆY “ ty1, y2, ..., yT u which maximizes the conditional likelihood ˆY “ arg maxY P pY |X p, X aq.\n",
      "Here N , L, and T are the lenghts of the passage, answer and question, respectively. We focus on the\n",
      "problem setting where we have a set of passage (and answers) and target questions pairs, to learn\n",
      "the mapping; existing QG approaches (Du et al., 2017; Song et al., 2018a; Zhao et al., 2018; Kim\n",
      "et al., 2018) make a similar assumption.\n",
      "\n",
      "2.2 DEEP ALIGNMENT NETWORK\n",
      "\n",
      "Answer information is crucial for generating relevant and high quality questions from a passage. Un-\n",
      "like previous methods that neglect potential semantic relations between passage and answer words,\n",
      "we explicitly model the global interactions among them in the embedding space. To this end, we\n",
      "propose a novel Deep Alignment Network (DAN) component for effectively incorporating answer\n",
      "information into the passage with multiple granularity levels. Speciﬁcally, we perform attention-\n",
      "based soft-alignment at the word level, as well as at the contextualized hidden state level, so that\n",
      "multiple levels of alignments can help learn hierarchical representations.\n",
      "\n",
      "Figure 2: The attention-based soft-alignment mechanism.\n",
      "\n",
      "Let Xp P RF ˆN and rXp P R rFpˆN denote two embeddings associated with passage text. Similarly,\n",
      "let Xa P RF ˆL and rXa P R rFaˆL denoted two embeddings associated with answer text. Concep-\n",
      "tually, as shown in Fig. 2, the soft-alignment mechanism consists of three steps: i) compute the\n",
      "attention score βi,j for each pair of passage word xp\n",
      "j : ii) multiply the atten-\n",
      "tion matrix β with the answer embeddings rXa to obtain the aligned answer embeddings Hp for the\n",
      "passage; iii) concatenate the resulting aligned answer embeddings Hp with the passage embeddings\n",
      "rXp to get the ﬁnal passage embeddings rHp P Rp\n",
      "Formally, we deﬁne our soft-alignment function as following:\n",
      "\n",
      "i and answer word xa\n",
      "\n",
      "rFaqˆN .\n",
      "\n",
      "rFp`\n",
      "\n",
      "rHp “ AlignpXp, Xa, rXp, rXaq “ CATp\n",
      "\n",
      "(1)\n",
      "where the matrix rHp is the ﬁnal passage embedding, the function CAT is a simple concatenation\n",
      "operation, and β is a N ˆ L attention score matrix, computed by\n",
      "\n",
      "rXp; Hpq “ CATp\n",
      "\n",
      "rXp; rXaβT q\n",
      "\n",
      "¯\n",
      "´\n",
      "ReLUpWXpqT ReLUpWXaq\n",
      "\n",
      "β 9 exp\n",
      "\n",
      "(2)\n",
      "\n",
      "3\n",
      "\n",
      "\f",
      "Under review as a conference paper at ICLR 2020\n",
      "\n",
      "where W P RdˆF is a trainable weight matrix, with d being the hidden state size and ReLU is the\n",
      "rectiﬁed linear unit (Nair & Hinton, 2010). After introducing the general soft-alignment mechanism,\n",
      "we next introduce how we do soft-alignment at both word-level and contextualized hidden state\n",
      "level.\n",
      "\n",
      "2.2.1 WORD-LEVEL ALIGNMENT\n",
      "\n",
      "In the word-level alignment stage, we ﬁrst perform a soft-alignment between the passage and the\n",
      "answer based only on their pretrained GloVe embeddings and compute the ﬁnal passage embed-\n",
      "dings by rHp “ AlignpGp, Ga, rGp; Bp; Lps, Gaq, where Gp, Bp, and Lp are the corresponding\n",
      "GloVe embedding (Pennington et al., 2014), BERT embedding (Devlin et al., 2018), and linguistic\n",
      "feature (i.e., case, NER and POS) embedding of the passage text, respectively. Then a bidirectional\n",
      "LSTM (Hochreiter & Schmidhuber, 1997) is applied to the ﬁnal passage embeddings rHp “ t\n",
      "to obtain contextualized passage embeddings sHp P R sF ˆN .\n",
      "On the other hand, for the answer text Xa, we simply concatenate its GloVe embedding Ga and\n",
      "its BERT embedding Ba to obtain its word embedding matrix Ha P Rd1ˆL. Another BiLSTM is\n",
      "then applied to the concatenated answer embedding sequence to obtain the contextualized answer\n",
      "embeddings sHa P R sF ˆL.\n",
      "\n",
      "rhp\n",
      "i uN\n",
      "i“1\n",
      "\n",
      "2.2.2 HIDDEN-LEVEL ALIGNMENT\n",
      "\n",
      "In the hidden-level alignment stage, we perform another soft-alignment based on the contextual-\n",
      "ized passage and answer embeddings. Similarly, we compute the aligned answer embedding, and\n",
      "concatenate it with the contextualized passage embedding to obtain the ﬁnal passage embedding ma-\n",
      "trix AlignprGp; Bp; sHps, rGa; Ba; sHas, sHp, sHaq. Finally, we apply another BiLSTM to the above\n",
      "concatenated embedding to get a sF ˆ N passage embedding matrix X.\n",
      "\n",
      "2.3 BIDIECTIONAL GRAPH-TO-SEQUENCE GENERATOR\n",
      "\n",
      "While RNNs are good at capturing local dependencies among consecutive words in text, GNNs\n",
      "have been shown to better utilize the rich hidden text structure information such as syntactic parsing\n",
      "(Xu et al., 2018b) or semantic parsing (Song et al., 2018b), and can model the global interactions\n",
      "(relations) among sequence words to further improve the representations. Therefore, unlike most of\n",
      "the existing methods that rely on RNNs to encode the input passage, we ﬁrst construct a passage\n",
      "graph G from text where each passage word is treated as a graph node, and then employ a novel\n",
      "Graph2Seq model to encode the passage graph (and answer), and to decode the natural language\n",
      "question.\n",
      "\n",
      "2.3.1 PASSAGE GRAPH CONSTRUCTION\n",
      "\n",
      "Existing GNNs assume a graph structured input and directly consume it for computing the corre-\n",
      "sponding node embeddings. However, we need to construct a graph from the text. Although there\n",
      "are early attempts on constructing a graph from a sentence (Xu et al., 2018b), there is no clear an-\n",
      "swer as to the best way of representing text as a graph. We explore both static and dynamic graph\n",
      "construction approaches, and systematically investigate the performance differences between these\n",
      "two methods in the experimental section.\n",
      "\n",
      "Syntax-based static graph construction: We construct a directed and unweighted passage graph\n",
      "based on dependency parsing. For each sentence in a passage, we ﬁrst get its dependency parse\n",
      "tree. We then connect neighboring dependency parse trees by connecting those nodes that are at a\n",
      "sentence boundary and next to each other in text.\n",
      "\n",
      "Semantics-aware dynamic graph construction: We dynamically build a directed and weighted graph\n",
      "to model semantic relationships among passage words. We make the process of building such a\n",
      "graph depend on not only the passage, but also on the answer. The graph construction procedure\n",
      "consists of three steps: i) we compute a dense adjacency matrix A for the passage graph by applying\n",
      "self-attention to the word-level passage embeddings rHp, ii) a kNN-style graph sparsiﬁcation strat-\n",
      "egy (Chen et al., 2019b) is adopted to obtain a sparse adjacency matrix ¯A, where we only keep the\n",
      "\n",
      "4\n",
      "\n",
      "\f",
      "Under review as a conference paper at ICLR 2020\n",
      "\n",
      "K nearest neighbors (including itself) as well as the associated attention scores (i.e., the remaining\n",
      "attentions scores are masked off) for each node; and iii) inspired by BiLSTM over LSTM, we also\n",
      "compute two normalized adjacency matrices A% and A$ according to their incoming and outgo-\n",
      "ing directions, by applying softmax operation on the resulting sparse adjacency matrix ¯A and its\n",
      "transpose, respectively.\n",
      "\n",
      "A “ ReLUpU rHpqT ReLUpU rHpq,\n",
      "\n",
      "¯A “ kNNpAq, A%, A$ “ softmaxpt ¯A, ¯AT uq\n",
      "\n",
      "(3)\n",
      "\n",
      "rFaq trainable weight matrix. Note that the supervision signal is able to\n",
      "where U is a d ˆ p\n",
      "back-propagate through the kNN-style graph sparsiﬁcation operation since the K nearest attention\n",
      "scores are kept.\n",
      "\n",
      "rFp `\n",
      "\n",
      "2.3.2 BIDIRECTIONAL GATED GRAPH NEURAL NETWORKS\n",
      "\n",
      "To effectively learn the graph embeddings from the constructed text graph, we propose a novel Bidi-\n",
      "rectional Gated Graph Neural Network (BiGGNN) which extends Gated Graph Sequence Neural\n",
      "Networks (Li et al., 2015) by learning node embedding from both incoming and outgoing edges\n",
      "in an interleaved fashion when processing the directed passage graph. Similar idea has also been\n",
      "exploited in (Xu et al., 2018a), which extended another popular variant of GNNs - GraphSAGE\n",
      "(Hamilton et al., 2017). However, one of key difference between our BiGGNN and their bidi-\n",
      "rectional GraphSAGE is that we fuse the intermediate node embeddings from both incoming and\n",
      "outgoing edges in every iteration during the training, whereas their model simply trains the node\n",
      "embeddings of each direction independently and concatenates them in the ﬁnal step.\n",
      "\n",
      "In BiGGNN, node embeddings are initialized to the passage embeddings X returned by DAN. The\n",
      "same set of network parameters are shared at every hop of computation. At each computation hop,\n",
      "for every node in the graph, we apply an aggregation function which takes as input a set of incoming\n",
      "(or outgoing) neighboring node vectors and outputs a backward (or forward) aggregation vector. For\n",
      "the syntax-based static graph, we use a mean aggregator for simplicity although other operators such\n",
      "as max or attention (Veliˇckovi´c et al., 2017) could also be employed,\n",
      "\n",
      "hk\n",
      "N\n",
      "hk\n",
      "N\n",
      "\n",
      "%pvq\n",
      "\n",
      "$pvq\n",
      "\n",
      "“ MEANpthk´1\n",
      "“ MEANpthk´1\n",
      "\n",
      "v\n",
      "\n",
      "u\n",
      "\n",
      "u Y thk´1\n",
      "u Y thk´1\n",
      "\n",
      ", @u P N\n",
      ", @u P N\n",
      "\n",
      "u\n",
      "\n",
      "v\n",
      "\n",
      "%pvquq\n",
      "\n",
      "$pvquq\n",
      "\n",
      "For the semantics-aware dynamic graph we compute a weighted average for aggregation where the\n",
      "weights come from the normalized adjacency matrices A% and A$, deﬁned as,\n",
      "\n",
      "hk\n",
      "N\n",
      "\n",
      "%pvq\n",
      "\n",
      "“\n",
      "\n",
      "v,uhk´1\n",
      "a%\n",
      "\n",
      "u\n",
      "\n",
      ", hk\n",
      "N\n",
      "\n",
      "“\n",
      "\n",
      "$pvq\n",
      "\n",
      "v,uhk´1\n",
      "a$\n",
      "\n",
      "u\n",
      "\n",
      "ÿ\n",
      "\n",
      "@uPN\n",
      "\n",
      "%pvq\n",
      "\n",
      "ÿ\n",
      "\n",
      "@uPN\n",
      "\n",
      "$pvq\n",
      "\n",
      "While (Xu et al., 2018a) learn separate node embeddings for both directions independently, we\n",
      "choose to fuse the information aggregated in the two directions at each hop, which we ﬁnd works\n",
      "better in general.\n",
      "\n",
      "hk\n",
      "\n",
      "N “ Fusephk\n",
      "N\n",
      "\n",
      ", hk\n",
      "N\n",
      "\n",
      "q\n",
      "\n",
      "%pvq\n",
      "\n",
      "$pvq\n",
      "\n",
      "We design the fusion function as a gated sum of two information sources,\n",
      "\n",
      "Fusepa, bq “ z d a ` p1 ´ zq d b,\n",
      "\n",
      "z “ σpWzra; b; a d b; a ´ bs ` bzq\n",
      "\n",
      "where d is the component-wise multiplication, σ is a sigmoid function, and z is a gating vector.\n",
      "\n",
      "Finally, a Gated Recurrent Unit (GRU) (Cho et al., 2014) is used to update the node embeddings by\n",
      "incorporating the aggregation information.\n",
      "\n",
      "(4)\n",
      "\n",
      "(5)\n",
      "\n",
      "(6)\n",
      "\n",
      "(7)\n",
      "\n",
      "(8)\n",
      "\n",
      "After n hops of GNN computation, where n is a hyperparameter, we obtain the ﬁnal state embedding\n",
      "hn\n",
      "v for node v. To compute the graph-level embedding, we ﬁrst apply a linear projection to the node\n",
      "embeddings, and then apply max-pooling over all node embeddings to get a d-dim vector hG.\n",
      "\n",
      "hk\n",
      "v “ GRUphk´1\n",
      "\n",
      "v\n",
      "\n",
      ", hk\n",
      "\n",
      "Nq\n",
      "\n",
      "5\n",
      "\n",
      "\f",
      "Under review as a conference paper at ICLR 2020\n",
      "\n",
      "2.3.3 RNN DECODER\n",
      "\n",
      "On the decoder side, we adopt the same model architecture as other state-of-the-art Seq2Seq\n",
      "mdoels where an attention-based (Bahdanau et al., 2014; Luong et al., 2015) LSTM decoder with\n",
      "copy (Vinyals et al., 2015; Gu et al., 2016) and coverage mechanisms (Tu et al., 2016) is empolyed.\n",
      "The decoder takes the graph-level embedding hG followed by two separate fully-connected layers\n",
      "as initial hidden states (i.e., c0 and s0) and the node embeddings thn\n",
      "v , @v P Gu as the attention\n",
      "memory, and generates the output sequence one word at a time. The particular decoder used in this\n",
      "work closely follows (See et al., 2017). We refer the readers to Appendix A for more details.\n",
      "\n",
      "2.4 HYBRID EVALUATOR\n",
      "\n",
      "It has been observed that optimizing such cross-entropy based training objectives for sequence learn-\n",
      "ing does not always produce the best results on discrete evaluation metrics (Ranzato et al., 2015;\n",
      "Wu et al., 2016; Paulus et al., 2017). Major limitations of this strategy include exposure bias and\n",
      "evaluation discrepancy between training and testing. To tackle these issues, some recent QG ap-\n",
      "proaches (Song et al., 2017; Kumar et al., 2018b) directly optimize evaluation metrics using REIN-\n",
      "FORCE. We further use a mixed objective function with both syntactic and semantic constraints for\n",
      "guiding text generation. In particular, we present a hybrid evaluator with a mixed objective function\n",
      "that combines both cross-entropy loss and RL loss in order to ensure the generation of syntactically\n",
      "and semantically valid text.\n",
      "\n",
      "For the RL part, we employ the self-critical sequence training (SCST) algorithm (Rennie et al.,\n",
      "2017) to directly optimize the evaluation metrics. SCST is an efﬁcient REINFORCE algorithm that\n",
      "utilizes the output of its own test-time inference algorithm to normalize the rewards it experiences.\n",
      "In SCST, at each training iteration, the model generates two output sequences: the sampled output\n",
      "Y s, produced by multinomial sampling, that is, each word ys\n",
      "t is sampled according to the likelihood\n",
      "P pyt|X, yătq predicted by the generator, and the baseline output ˆY , obtained by greedy search, that\n",
      "is, by maximizing the output probability distribution at each decoding step. We deﬁne rpY q as the\n",
      "reward of an output sequence Y , computed by comparing it to corresponding ground-truth sequence\n",
      "Y ˚ with some reward metrics. The loss function is deﬁned as:\n",
      "log P pys\n",
      "\n",
      "Lrl “ prp ˆY q ´ rpY sqq\n",
      "\n",
      "(9)\n",
      "\n",
      "ÿ\n",
      "\n",
      "t |X, ys\n",
      "\n",
      "ătq\n",
      "\n",
      "t\n",
      "\n",
      "As we can see, if the sampled output has a higher reward than the baseline one, we maximize its\n",
      "likelihood, and vice versa.\n",
      "\n",
      "One of the key factors for RL is to pick the proper reward function. To take syntactic and semantic\n",
      "constraints into account, we consider the following metrics as our reward functions:\n",
      "\n",
      "Evaluation metric as reward function: We use one of our evaluation metrics, BLEU-4, as our reward\n",
      "function feval, which lets us directly optimize the model towards the evaluation metrics.\n",
      "\n",
      "Semantic metric as reward function: One drawback of some evaluation metrics like BLEU is that\n",
      "they do not measure meaning, but only reward systems for n-grams that have exact matches in\n",
      "the reference system. To make our reward function more effective and robust, we additionally use\n",
      "word movers distance (WMD) as a semantic reward function fsem. WMD is the state-of-the-art\n",
      "approach to measure the dissimilarity between two sentences based on word embeddings (Kusner\n",
      "et al., 2015). Following Gong et al. (2019), we take the negative of the WMD distance between\n",
      "a generated sequence and the ground-truth sequence and divide it by the sequence length as its\n",
      "semantic score.\n",
      "\n",
      "We deﬁne the ﬁnal reward function as rpY q “ fevalpY, Y ˚q ` αfsempY, Y ˚q where α is a scalar.\n",
      "\n",
      "2.5 TRAINING AND TESTING\n",
      "\n",
      "We train our model in two stages. In the ﬁrst state, we train the model using regular cross-entropy\n",
      "loss, deﬁned as,\n",
      "\n",
      "ÿ\n",
      "\n",
      "Llm “\n",
      "\n",
      "´ log P py˚\n",
      "\n",
      "t |X, y˚\n",
      "\n",
      "ătq ` λ covlosst\n",
      "\n",
      "(10)\n",
      "\n",
      "where y˚\n",
      "coverage loss deﬁned as\n",
      "\n",
      "t is the word at the t-th position of the ground-truth output sequence and covlosst is the\n",
      "i being the i-th element of the attention vector over\n",
      "\n",
      "iq, with at\n",
      "\n",
      "i minpat\n",
      "\n",
      "i, ct\n",
      "\n",
      "ř\n",
      "\n",
      "t\n",
      "\n",
      "6\n",
      "\n",
      "\f",
      "Under review as a conference paper at ICLR 2020\n",
      "\n",
      "the input sequence at time step t. Scheduled teacher forcing (Bengio et al., 2015) is adopted to\n",
      "alleviate the exposure bias problem. In the second stage, we ﬁne-tune the model by optimizing a\n",
      "mixed objective function combining both cross-entropy loss and RL loss, deﬁned as,\n",
      "\n",
      "where γ is a scaling factor controling the trade-off between cross-entropy loss and RL loss. During\n",
      "the testing phase, we use beam search to generate ﬁnal predictions.\n",
      "\n",
      "L “ γLrl ` p1 ´ γqLlm\n",
      "\n",
      "(11)\n",
      "\n",
      "3 EXPERIMENTS\n",
      "\n",
      "We evaluate our proposed model against state-of-the-art methods on the SQuAD dataset (Rajpurkar\n",
      "et al., 2016). Our full models have two variants G2Ssta+BERT+RL and G2Sdyn+BERT+RL which\n",
      "adopts static graph construction or dynamic graph construction, respectively. For model settings and\n",
      "sensitivity analysis, please refer to Appendix B and C. 1\n",
      "\n",
      "3.1 BASELINE METHODS\n",
      "\n",
      "We compare against the following baselines in our experiments: i) SeqCopyNet (Zhou et al., 2018),\n",
      "ii) NQG++ (Zhou et al., 2017), iii) MPQG+R (Song et al., 2017), iv) AFPQA (Sun et al., 2018),\n",
      "v) s2sa-at-mp-gsa (Zhao et al., 2018), vi) ASs2s (Kim et al., 2018), and vii) CGC-QG (Liu et al.,\n",
      "2019). Detailed descriptions of the baselines are provided in Appendix D. Experiments on baselines\n",
      "followed by * are conducted using released source codes. Results of other baselines are taken from\n",
      "the corresponding papers, with unreported metrics marked as –.\n",
      "\n",
      "3.2 DATA AND METRICS\n",
      "\n",
      "SQuAD contains more than 100K questions posed by crowd workers on 536 Wikipedia arti-\n",
      "cles. Since the test set of the original SQuAD is not publicly available, the accessible parts\n",
      "(«90%) are used as the entire dataset in our experiments. For fair comparison with previ-\n",
      "ous methods, we evaluated our model on both data split-1 (Song et al., 2018a)2 that contains\n",
      "75,500/17,934/11,805 (train/development/test) examples and data split-2 (Zhou et al., 2017) 3 that\n",
      "contains 86,635/8,965/8,964 examples.\n",
      "\n",
      "Following previous works, we use BLEU-4 (Papineni et al., 2002), METEOR (Banerjee & Lavie,\n",
      "2005), ROUGE-L (Lin, 2004) and Q-BLEU1 (Nema & Khapra, 2018) as our evaluation metrics.\n",
      "Initially, BLEU-4 and METEOR were designed for evaluating machine translation systems and\n",
      "ROUGE-L was designed for evaluating text summarization systems. Recently, Q-BLEU1 was de-\n",
      "signed for better evaluating question generation systems, which was shown to correlate signiﬁcantly\n",
      "better with human judgments compared to existing metrics.\n",
      "\n",
      "Besides automatic evaluation metrics, we also conduct a human evaluation study on split-2. We ask\n",
      "human evaluators to rate generated questions from a set of anonymized competing systems based\n",
      "on whether they are syntactically correct, semantically correct and relevant to the passage. The\n",
      "rating scale is from 1 to 5, on each of the three categories. Evaluation scores from all evaluators\n",
      "were collected and averaged as ﬁnal scores. Further details on human evaluation can be found\n",
      "in Appendix E.\n",
      "\n",
      "3.3 EXPERIMENTAL RESULTS AND HUMAN EVALUATION\n",
      "\n",
      "Table 1 shows the automatic evaluation results comparing our proposed models against other state-\n",
      "of-the-art baseline methods. First of all, we can see that both of our full models G2Ssta+BERT+RL\n",
      "and G2Sdyn+BERT+RL achieve the new state-of-the-art scores on both data splits and consistently\n",
      "outperform previous methods by a signiﬁcant margin. This highlights that our RL-based Graph2Seq\n",
      "model, together with the deep alignment network, successfully addresses the three issues we high-\n",
      "lighted in Sec. 1. Between these two variants, G2Ssta+BERT+RL outperforms G2Sdyn+BERT+RL\n",
      "\n",
      "1The implementation of our model will be made publicly available after the review period.\n",
      "2https://www.cs.rochester.edu/˜lsong10/downloads/nqg_data.tgz\n",
      "3https://res.qyzhou.me/redistribute.zip\n",
      "\n",
      "7\n",
      "\n",
      "\f",
      "Under review as a conference paper at ICLR 2020\n",
      "\n",
      "Table 1: Automatic evaluation results on the SQuAD test set.\n",
      "\n",
      "BLEU-4 METEOR ROUGE-L Q-BLEU1 BLEU-4 METEOR ROUGE-L Q-BLEU1\n",
      "\n",
      "Methods\n",
      "\n",
      "Split-1\n",
      "\n",
      "–\n",
      "–\n",
      "\n",
      "14.39\n",
      "\n",
      "SeqCopyNet\n",
      "NQG++\n",
      "MPQG+R*\n",
      "AFPQA\n",
      "s2sa-at-mp-gsa\n",
      "ASs2s\n",
      "CGC-QG\n",
      "G2Sdyn+BERT+RL 17.55\n",
      "17.94\n",
      "G2Ssta+BERT+RL\n",
      "\n",
      "15.32\n",
      "16.20\n",
      "\n",
      "–\n",
      "\n",
      "–\n",
      "\n",
      "–\n",
      "–\n",
      "\n",
      "–\n",
      "\n",
      "–\n",
      "\n",
      "19.29\n",
      "19.92\n",
      "\n",
      "21.42\n",
      "21.76\n",
      "\n",
      "–\n",
      "–\n",
      "\n",
      "–\n",
      "\n",
      "–\n",
      "\n",
      "43.91\n",
      "43.96\n",
      "\n",
      "45.59\n",
      "46.02\n",
      "\n",
      "–\n",
      "–\n",
      "\n",
      "–\n",
      "–\n",
      "–\n",
      "–\n",
      "\n",
      "55.40\n",
      "55.60\n",
      "\n",
      "Split-2\n",
      "\n",
      "–\n",
      "–\n",
      "\n",
      "–\n",
      "\n",
      "–\n",
      "\n",
      "44.00\n",
      "\n",
      "–\n",
      "\n",
      "–\n",
      "\n",
      "–\n",
      "\n",
      "19.67\n",
      "\n",
      "44.24\n",
      "\n",
      "21.24\n",
      "21.53\n",
      "21.70\n",
      "\n",
      "44.53\n",
      "45.91\n",
      "45.98\n",
      "\n",
      "13.02\n",
      "13.29\n",
      "14.71\n",
      "15.64\n",
      "15.82\n",
      "16.17\n",
      "17.55\n",
      "18.06\n",
      "18.30\n",
      "\n",
      "–\n",
      "–\n",
      "\n",
      "–\n",
      "–\n",
      "–\n",
      "–\n",
      "\n",
      "55.00\n",
      "55.20\n",
      "\n",
      "18.99\n",
      "\n",
      "42.46\n",
      "\n",
      "52.00\n",
      "\n",
      "18.93\n",
      "\n",
      "42.60\n",
      "\n",
      "50.30\n",
      "\n",
      "Table 2: Human evaluation results (˘ standard deviation) on the SQuAD split-2 test set. The rating\n",
      "scale is from 1 to 5 (higher scores indicate better results).\n",
      "\n",
      "Methods\n",
      "MPQG+R*\n",
      "G2Ssta+BERT+RL\n",
      "Ground-truth\n",
      "\n",
      "Syntactically correct\n",
      "4.34 (0.15)\n",
      "4.41 (0.09)\n",
      "4.74 (0.14)\n",
      "\n",
      "Semantically correct Relevant\n",
      "4.01 (0.23)\n",
      "4.31 (0.12)\n",
      "4.74 (0.19)\n",
      "\n",
      "3.21 (0.31)\n",
      "3.79 (0.45)\n",
      "4.25 (0.38)\n",
      "\n",
      "on all the metrics. Also, unlike the baseline methods, our model does not rely on any hand-crafted\n",
      "rules or ad-hoc strategies, and is fully end-to-end trainable.\n",
      "\n",
      "As shown in Table 2, we conducted a human evaluation study to assess the quality of the questions\n",
      "generated by our model, the baseline method MPQG+R, and the ground-truth data in terms of syn-\n",
      "tax, semantics and relevance metrics. We can see that our best performing model achieves good\n",
      "results even compared to the ground-truth, and outperforms the strong baseline method MPQG+R.\n",
      "Our error analysis shows that main syntactic error occurs in repeated/unknown words in generated\n",
      "questions. Further, the slightly lower quality on semantics also impacts the relevance.\n",
      "\n",
      "3.4 ABLATION STUDY\n",
      "\n",
      "Table 3: Ablation study on the SQuAD split-2 test set.\n",
      "\n",
      "Methods\n",
      "G2Sdyn+BERT+RL\n",
      "G2Ssta+BERT+RL\n",
      "G2Ssta+BERT-ﬁxed+RL\n",
      "G2Sdyn+BERT\n",
      "G2Ssta+BERT\n",
      "G2Ssta+BERT-ﬁxed\n",
      "G2Sdyn+RL\n",
      "G2Ssta+RL\n",
      "\n",
      "BLEU-4\n",
      "18.06\n",
      "18.30\n",
      "18.20\n",
      "17.56\n",
      "18.02\n",
      "17.86\n",
      "17.18\n",
      "17.49\n",
      "\n",
      "Methods\n",
      "G2Sdyn\n",
      "G2Ssta\n",
      "G2Sdyn w/o DAN\n",
      "G2Ssta w/o DAN\n",
      "G2Ssta w/o BiGGNN, w/ Seq2Seq\n",
      "G2Ssta w/o BiGGNN, w/ GCN\n",
      "G2Ssta w/ GGNN-forward\n",
      "G2Ssta w/ GGNN-backward\n",
      "\n",
      "BLEU-4\n",
      "16.81\n",
      "16.96\n",
      "12.58\n",
      "12.62\n",
      "16.14\n",
      "14.47\n",
      "16.53\n",
      "16.75\n",
      "\n",
      "As shown in Table 3, we perform an ablation study to systematically assess the impact of differ-\n",
      "ent model components (e.g., BERT, RL, DAN, and BiGGNN) for two proposed full model variants\n",
      "(static vs dynamic) on the SQuAD split-2 test set. It conﬁrms our ﬁnding that syntax-based static\n",
      "graph construction (G2Ssta+BERT+RL) performs better than semantics-aware dynamic graph con-\n",
      "struction (G2Sdyn+BERT+RL) in almost every setting. However, it may be too early to conclude\n",
      "which one is the method of choice for QG. On the one hand, an advantage of static graph construc-\n",
      "tion is that useful domain knowledge can be hard-coded into the graph, which can greatly beneﬁt the\n",
      "downstream task. However, it might suffer if there is a lack of prior knowledge for a speciﬁc domain\n",
      "knowledge. On the other hand, dynamic graph construction does not need any prior knowledge\n",
      "about the hidden structure of text, and only relies on the attention matrix to capture these structured\n",
      "information, which provides an easy way to achieve a decent performance. One interesting direction\n",
      "is to explore effective ways of combining both static and dynamic graphs.\n",
      "\n",
      "8\n",
      "\n",
      "\f",
      "Under review as a conference paper at ICLR 2020\n",
      "\n",
      "By turning off the Deep Alignment Network (DAN), the BLEU-4 score of G2Ssta (similarly for\n",
      "G2Sdyn) dramatically drops from 16.96% to 12.62%, which indicates the importance of answer\n",
      "information for QG and shows the effectiveness of DAN. This can also be veriﬁed by comparing the\n",
      "performance between the DAN-enhanced Seq2Seq model (16.14 BLEU-4 score) and other carefully\n",
      "designed answer-aware Seq2Seq baselines such as NQG++ (13.29 BLEU-4 score), MPQG+R (14.71\n",
      "BLEU-4 score) and AFPQA (15.82 BLEU-4 score). Further experiments demonstrate that both\n",
      "word-level (G2Ssta w/ DAN-word only) and hidden-level (G2Ssta w/ DAN-hidden only) answer\n",
      "alignments in DAN are helpful.\n",
      "\n",
      "We can see the advantages of Graph2Seq learning over Seq2Seq learning on this task by comparing\n",
      "the performance between G2Ssta and Seq2Seq. Compared to Seq2Seq based QG methods that com-\n",
      "pletely ignore hidden structure information in the passage, our Graph2Seq based method is aware of\n",
      "more hidden structure information such as semantic similarity between any pair of words that are not\n",
      "directly connected or syntactic relationships between two words captured in a dependency parsing\n",
      "tree. In our experiments, we also observe that doing both forward and backward message passing\n",
      "in the GNN encoder is beneﬁcial. Surprisingly, using GCN (Kipf & Welling, 2016) as the graph en-\n",
      "coder (and converting the input graph to an undirected graph) does not provide good performance.\n",
      "In addition, ﬁne-tuning the model using REINFORCE can further improve the model performance\n",
      "in all settings (i.e., w/ and w/o BERT), which shows the beneﬁts of directly optimizing the evalu-\n",
      "ation metrics. Besides, we ﬁnd that the pretrained BERT embedding has a considerable impact on\n",
      "the performance and ﬁne-tuning BERT embedding even further improves the performance, which\n",
      "demonstrates the power of large-scale pretrained language models.\n",
      "\n",
      "3.5 CASE STUDY\n",
      "\n",
      "Table 4: Generated questions on SQuAD split-2 test set. Target answers are underlined.\n",
      "\n",
      "Passage: for the successful execution of a project , effective planning is essential .\n",
      "Gold: what is essential for the successful execution of a project ?\n",
      "G2Ssta w/o BiGGNN (Seq2Seq): what type of planning is essential for the project ?\n",
      "G2Ssta w/o DAN.: what type of planning is essential for the successful execution of a project ?\n",
      "G2Ssta: what is essential for the successful execution of a project ?\n",
      "G2Ssta+BERT: what is essential for the successful execution of a project ?\n",
      "G2Ssta+BERT+RL: what is essential for the successful execution of a project ?\n",
      "G2Sdyn+BERT+RL: what is essential for the successful execution of a project ?\n",
      "Passage: the church operates three hundred sixty schools and institutions overseas .\n",
      "Gold: how many schools and institutions does the church operate overseas ?\n",
      "G2Ssta w/o BiGGNN (Seq2Seq): how many schools does the church have ?\n",
      "G2Ssta w/o DAN.: how many schools does the church have ?\n",
      "G2Ssta: how many schools and institutions does the church have ?\n",
      "G2Ssta+BERT: how many schools and institutions does the church have ?\n",
      "G2Ssta+BERT+RL: how many schools and institutions does the church operate ?\n",
      "G2Sdyn+BERT+RL: how many schools does the church operate ?\n",
      "\n",
      "In Table 4, we further show a few examples that illustrate the quality of generated text given a pas-\n",
      "sage under different ablated systems. As we can see, incorporating answer information helps the\n",
      "model identify the answer type of the question to be generated, and thus makes the generated ques-\n",
      "tions more relevant and speciﬁc. Also, we ﬁnd our Graph2Seq model can generate more complete\n",
      "and valid questions compared to the Seq2Seq baseline. We think it is because a Graph2Seq model\n",
      "is able to exploit the rich text structure information better than a Seq2Seq model. Lastly, it shows\n",
      "that ﬁne-tuning the model using REINFORCE can improve the quality of the generated questions.\n",
      "\n",
      "4 RELATED WORK\n",
      "\n",
      "4.1 NATURAL QUESTION GENERATION\n",
      "\n",
      "Early works (Mostow & Chen, 2009; Heilman & Smith, 2010; Heilman, 2011) for QG focused on\n",
      "rule-based approaches that rely on heuristic rules or hand-crafted templates, with low generaliz-\n",
      "ability and scalability. Recent attempts have focused on NN-based approaches that do not require\n",
      "\n",
      "9\n",
      "\n",
      "\f",
      "Under review as a conference paper at ICLR 2020\n",
      "\n",
      "manually-designed rules and are end-to-end trainable. Existing NN-based approaches (Du et al.,\n",
      "2017; Yao et al.; Zhou et al., 2018) rely on the Seq2Seq model with attention, copy or coverage\n",
      "mechanisms. In addition, various ways (Zhou et al., 2017; Song et al., 2017; Zhao et al., 2018; Sun\n",
      "et al., 2018; Kim et al., 2018; Liu et al., 2019) have been proposed to utilize the target answer so as\n",
      "to guide the generation of the question. To address the limitations of cross-entropy based sequence\n",
      "learning, some approaches (Song et al., 2017; Kumar et al., 2018b) aim at directly optimizing eval-\n",
      "uation metrics using REINFORCE.\n",
      "\n",
      "However, the existing approaches for QG suffer from several limitations; they (i) ignore the rich\n",
      "structure information hidden in text, (ii) solely rely on cross-entropy loss that leads to issues like\n",
      "exposure bias and inconsistency between train/test measurement, and (iii) fail to fully exploit the\n",
      "answer information. To address these limitations, we propose a reinforcement learning (RL) based\n",
      "graph-to-sequence (Graph2Seq) model for QG as well as deep alignment networks to effectively\n",
      "cope with the QG task. To the best of our knowledge, we are the ﬁrst to introduce the Graph2Seq\n",
      "architecture to solve the question generation task.\n",
      "\n",
      "4.2 GRAPH NEURAL NETWORKS\n",
      "\n",
      "Over the past few years, graph neural networks (GNNs) (Kipf & Welling, 2016; Gilmer et al., 2017;\n",
      "Hamilton et al., 2017; Li et al., 2015) have attracted increasing attention. Due to more recent ad-\n",
      "vances in graph representation learning, a number of works have extended the widely used Seq2Seq\n",
      "architectures (Sutskever et al., 2014; Cho et al., 2014) to Graph2Seq architectures for machine trans-\n",
      "lation, semantic parsing, and AMR(SQL)-to-text tasks (Bastings et al., 2017; Beck et al., 2018; Xu\n",
      "et al., 2018a;b;c; Song et al., 2018b). While the high-quality graph structure is crucial for the per-\n",
      "formance of GNN-based approaches, most existing works use syntax-based static graph structures\n",
      "when applied to textual data. Very recently, researchers have started exploring methods to automat-\n",
      "ically construct a graph of visual objects (Norcliffe-Brown et al., 2018) or words (Liu et al., 2018;\n",
      "Chen et al., 2019b) when applying GNNs to non-graph structured data.\n",
      "\n",
      "To the best of our knowledge, we are the ﬁrst to investigate systematically the performance difference\n",
      "between syntactic-aware static graph construction and semantics-aware dynamic graph construction\n",
      "in the context of question generation.\n",
      "\n",
      "5 CONCLUSION\n",
      "\n",
      "We proposed a novel RL based Graph2Seq model for QG, where the answer information is utilized\n",
      "by an effective Deep Alignment Network and a novel bidirectional GNN is proposed to process the\n",
      "directed passage graph. Our two-stage training strategy beneﬁts from both cross-entropy based and\n",
      "REINFORCE based sequence training. We also explore both static and dynamic graph construction\n",
      "from text, and systematically investigate and analyze the performance difference between the two.\n",
      "On the benchmark SQuAD dataset, our proposed model outperforms previous state-of-the-art meth-\n",
      "ods by a signiﬁcant margin and achieve new best results. One of the interesting future directions is\n",
      "to investigate more effective ways of automatically learning graph structures from any data source,\n",
      "including texts. It would be also be interesting to study the unpaired problem setting for QG.\n",
      "\n",
      "REFERENCES\n",
      "\n",
      "Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\n",
      "\n",
      "learning to align and translate. arXiv preprint arXiv:1409.0473, 2014.\n",
      "\n",
      "Satanjeev Banerjee and Alon Lavie. Meteor: An automatic metric for mt evaluation with improved\n",
      "correlation with human judgments. In Proceedings of the acl workshop on intrinsic and extrinsic\n",
      "evaluation measures for machine translation and/or summarization, pp. 65–72, 2005.\n",
      "\n",
      "Junwei Bao, Duyu Tang, Nan Duan, Zhao Yan, Yuanhua Lv, Ming Zhou, and Tiejun Zhao. Table-\n",
      "In Thirty-Second AAAI Conference on\n",
      "\n",
      "to-text: Describing table region with natural language.\n",
      "Artiﬁcial Intelligence, 2018.\n",
      "\n",
      "10\n",
      "\n",
      "\f",
      "Under review as a conference paper at ICLR 2020\n",
      "\n",
      "Joost Bastings, Ivan Titov, Wilker Aziz, Diego Marcheggiani, and Khalil Sima’an. Graph convolu-\n",
      "tional encoders for syntax-aware neural machine translation. arXiv preprint arXiv:1704.04675,\n",
      "2017.\n",
      "\n",
      "Daniel Beck, Gholamreza Haffari, and Trevor Cohn. Graph-to-sequence learning using gated graph\n",
      "\n",
      "neural networks. arXiv preprint arXiv:1806.09835, 2018.\n",
      "\n",
      "Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer. Scheduled sampling for sequence\n",
      "prediction with recurrent neural networks. In Advances in Neural Information Processing Sys-\n",
      "tems, pp. 1171–1179, 2015.\n",
      "\n",
      "Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. Reading wikipedia to answer open-\n",
      "\n",
      "domain questions. arXiv preprint arXiv:1704.00051, 2017.\n",
      "\n",
      "Yu Chen, Lingfei Wu, and Mohammed J Zaki. Bidirectional attentive memory networks for question\n",
      "\n",
      "answering over knowledge bases. arXiv preprint arXiv:1903.02188, 2019a.\n",
      "\n",
      "Yu Chen, Lingfei Wu, and Mohammed J Zaki. Graphﬂow: Exploiting conversation ﬂow with graph\n",
      "neural networks for conversational machine comprehension. arXiv preprint arXiv:1908.00059,\n",
      "2019b.\n",
      "\n",
      "Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-\n",
      "ger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder–decoder\n",
      "for statistical machine translation. In EMNLP, pp. 1724–1734, 2014.\n",
      "\n",
      "Guy Danon and Mark Last. A syntactic approach to domain-speciﬁc automatic question generation.\n",
      "\n",
      "arXiv preprint arXiv:1712.09827, 2017.\n",
      "\n",
      "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep\n",
      "\n",
      "bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\n",
      "\n",
      "Xinya Du, Junru Shao, and Claire Cardie. Learning to ask: Neural question generation for reading\n",
      "\n",
      "comprehension. arXiv preprint arXiv:1705.00106, 2017.\n",
      "\n",
      "Hady Elsahar, Christophe Gravier, and Frederique Laforest. Zero-shot question generation from\n",
      "knowledge graphs for unseen predicates and entity types. arXiv preprint arXiv:1802.06842, 2018.\n",
      "\n",
      "Zhihao Fan, Zhongyu Wei, Siyuan Wang, Yang Liu, and Xuanjing Huang. A reinforcement learning\n",
      "framework for natural question generation using bi-discriminators. In Proceedings of the 27th\n",
      "International Conference on Computational Linguistics, pp. 1763–1774, 2018.\n",
      "\n",
      "Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural\n",
      "message passing for quantum chemistry. In Proceedings of the 34th International Conference on\n",
      "Machine Learning-Volume 70, pp. 1263–1272. JMLR. org, 2017.\n",
      "\n",
      "Hongyu Gong, Suma Bhat, Lingfei Wu, Jinjun Xiong, and Wen-mei Hwu. Reinforcement learning\n",
      "based text style transfer without parallel training corpus. arXiv preprint arXiv:1903.10671, 2019.\n",
      "\n",
      "Jiatao Gu, Zhengdong Lu, Hang Li, and Victor OK Li.\n",
      "\n",
      "Incorporating copying mechanism in\n",
      "\n",
      "sequence-to-sequence learning. arXiv preprint arXiv:1603.06393, 2016.\n",
      "\n",
      "Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs.\n",
      "\n",
      "In Advances in Neural Information Processing Systems, pp. 1024–1034, 2017.\n",
      "\n",
      "Michael Heilman. Automatic factual question generation from text. 2011.\n",
      "\n",
      "Michael Heilman and Noah A Smith. Good question! statistical ranking for question generation.\n",
      "In Human Language Technologies: The 2010 Annual Conference of the North American Chapter\n",
      "of the Association for Computational Linguistics, pp. 609–617. Association for Computational\n",
      "Linguistics, 2010.\n",
      "\n",
      "Sepp Hochreiter and J¨urgen Schmidhuber. Long short-term memory. Neural computation, 9(8):\n",
      "\n",
      "1735–1780, 1997.\n",
      "\n",
      "11\n",
      "\n",
      "\f",
      "Under review as a conference paper at ICLR 2020\n",
      "\n",
      "Yanghoon Kim, Hwanhee Lee, Joongbo Shin, and Kyomin Jung. Improving neural question gener-\n",
      "\n",
      "ation using answer separation. arXiv preprint arXiv:1809.02393, 2018.\n",
      "\n",
      "Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\n",
      "\n",
      "arXiv:1412.6980, 2014.\n",
      "\n",
      "Durk P Kingma, Tim Salimans, and Max Welling. Variational dropout and the local reparameteri-\n",
      "\n",
      "zation trick. In Advances in Neural Information Processing Systems, pp. 2575–2583, 2015.\n",
      "\n",
      "Thomas N Kipf and Max Welling. Semi-supervised classiﬁcation with graph convolutional net-\n",
      "\n",
      "works. arXiv preprint arXiv:1609.02907, 2016.\n",
      "\n",
      "Vishwajeet Kumar, Kireeti Boorla, Yogesh Meena, Ganesh Ramakrishnan, and Yuan-Fang Li. Au-\n",
      "tomating reading comprehension by generating question and answer pairs. In Paciﬁc-Asia Con-\n",
      "ference on Knowledge Discovery and Data Mining, pp. 335–348. Springer, 2018a.\n",
      "\n",
      "Vishwajeet Kumar, Ganesh Ramakrishnan, and Yuan-Fang Li. A framework for automatic question\n",
      "generation from text using deep reinforcement learning. arXiv preprint arXiv:1808.04961, 2018b.\n",
      "\n",
      "Matt Kusner, Yu Sun, Nicholas Kolkin, and Kilian Weinberger. From word embeddings to document\n",
      "\n",
      "distances. In International Conference on Machine Learning, pp. 957–966, 2015.\n",
      "\n",
      "Yikang Li, Nan Duan, Bolei Zhou, Xiao Chu, Wanli Ouyang, Xiaogang Wang, and Ming Zhou.\n",
      "Visual question generation as dual task of visual question answering. In Proceedings of the IEEE\n",
      "Conference on Computer Vision and Pattern Recognition, pp. 6116–6124, 2018.\n",
      "\n",
      "Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel. Gated graph sequence neural\n",
      "\n",
      "networks. arXiv preprint arXiv:1511.05493, 2015.\n",
      "\n",
      "Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. Text Summarization\n",
      "\n",
      "Branches Out, 2004.\n",
      "\n",
      "Bang Liu, Mingjun Zhao, Di Niu, Kunfeng Lai, Yancheng He, Haojie Wei, and Yu Xu. Learning to\n",
      "\n",
      "generate questions by learning what not to generate. arXiv preprint arXiv:1902.10418, 2019.\n",
      "\n",
      "Pengfei Liu, Shuaichen Chang, Xuanjing Huang, Jian Tang, and Jackie Chi Kit Cheung. Contextu-\n",
      "alized non-local neural networks for sequence learning. arXiv preprint arXiv:1811.08600, 2018.\n",
      "\n",
      "Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\n",
      "\n",
      "based neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\n",
      "\n",
      "Nasrin Mostafazadeh, Ishan Misra, Jacob Devlin, Margaret Mitchell, Xiaodong He, and Lucy Van-\n",
      "derwende. Generating natural questions about an image. arXiv preprint arXiv:1603.06059, 2016.\n",
      "\n",
      "Jack Mostow and Wei Chen. Generating instruction automatically for the reading strategy of self-\n",
      "\n",
      "questioning. 2009.\n",
      "\n",
      "Vinod Nair and Geoffrey E Hinton. Rectiﬁed linear units improve restricted boltzmann machines. In\n",
      "Proceedings of the 27th international conference on machine learning (ICML-10), pp. 807–814,\n",
      "2010.\n",
      "\n",
      "Preksha Nema and Mitesh M Khapra. Towards a better metric for evaluating question generation\n",
      "\n",
      "systems. arXiv preprint arXiv:1808.10192, 2018.\n",
      "\n",
      "Will Norcliffe-Brown, Stathis Vafeias, and Sarah Parisot. Learning conditioned graph structures for\n",
      "interpretable visual question answering. In Advances in Neural Information Processing Systems,\n",
      "pp. 8344–8353, 2018.\n",
      "\n",
      "Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic\n",
      "evaluation of machine translation. In Proceedings of the 40th annual meeting on association for\n",
      "computational linguistics, pp. 311–318. Association for Computational Linguistics, 2002.\n",
      "\n",
      "Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\n",
      "\n",
      "summarization. arXiv preprint arXiv:1705.04304, 2017.\n",
      "\n",
      "12\n",
      "\n",
      "\f",
      "Under review as a conference paper at ICLR 2020\n",
      "\n",
      "Jeffrey Pennington, Richard Socher, and Christopher Manning. Glove: Global vectors for word\n",
      "representation. In Proceedings of the 2014 conference on empirical methods in natural language\n",
      "processing (EMNLP), pp. 1532–1543, 2014.\n",
      "\n",
      "Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions\n",
      "\n",
      "for machine comprehension of text. arXiv preprint arXiv:1606.05250, 2016.\n",
      "\n",
      "Marc’Aurelio Ranzato, Sumit Chopra, Michael Auli, and Wojciech Zaremba. Sequence level train-\n",
      "\n",
      "ing with recurrent neural networks. arXiv preprint arXiv:1511.06732, 2015.\n",
      "\n",
      "Steven J Rennie, Etienne Marcheret, Youssef Mroueh, Jerret Ross, and Vaibhava Goel. Self-critical\n",
      "sequence training for image captioning. In Proceedings of the IEEE Conference on Computer\n",
      "Vision and Pattern Recognition, pp. 7008–7024, 2017.\n",
      "\n",
      "Abigail See, Peter J Liu, and Christopher D Manning. Get to the point: Summarization with pointer-\n",
      "\n",
      "generator networks. arXiv preprint arXiv:1704.04368, 2017.\n",
      "\n",
      "Iulian Vlad Serban, Alberto Garc´ıa-Dur´an, Caglar Gulcehre, Sungjin Ahn, Sarath Chandar, Aaron\n",
      "Courville, and Yoshua Bengio. Generating factoid questions with recurrent neural networks: The\n",
      "30m factoid question-answer corpus. arXiv preprint arXiv:1603.06807, 2016.\n",
      "\n",
      "Linfeng Song, Zhiguo Wang, and Wael Hamza. A uniﬁed query-based generative model for question\n",
      "\n",
      "generation and question answering. arXiv preprint arXiv:1709.01058, 2017.\n",
      "\n",
      "Linfeng Song, Zhiguo Wang, Wael Hamza, Yue Zhang, and Daniel Gildea. Leveraging context in-\n",
      "formation for natural question generation. In Proceedings of the 2018 Conference of the North\n",
      "American Chapter of the Association for Computational Linguistics: Human Language Technolo-\n",
      "gies, Volume 2 (Short Papers), pp. 569–574, 2018a.\n",
      "\n",
      "Linfeng Song, Yue Zhang, Zhiguo Wang, and Daniel Gildea. A graph-to-sequence model for amr-\n",
      "\n",
      "to-text generation. arXiv preprint arXiv:1805.02473, 2018b.\n",
      "\n",
      "Xingwu Sun, Jing Liu, Yajuan Lyu, Wei He, Yanjun Ma, and Shi Wang. Answer-focused and\n",
      "position-aware neural question generation. In Proceedings of the 2018 Conference on Empirical\n",
      "Methods in Natural Language Processing, pp. 3930–3939, 2018.\n",
      "\n",
      "Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks.\n",
      "\n",
      "In Advances in neural information processing systems, pp. 3104–3112, 2014.\n",
      "\n",
      "Duyu Tang, Nan Duan, Tao Qin, Zhao Yan, and Ming Zhou. Question answering and question\n",
      "\n",
      "generation as dual tasks. arXiv preprint arXiv:1706.02027, 2017.\n",
      "\n",
      "Zhaopeng Tu, Zhengdong Lu, Yang Liu, Xiaohua Liu, and Hang Li. Modeling coverage for neural\n",
      "\n",
      "machine translation. arXiv preprint arXiv:1601.04811, 2016.\n",
      "\n",
      "Petar Veliˇckovi´c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua\n",
      "\n",
      "Bengio. Graph attention networks. arXiv preprint arXiv:1710.10903, 2017.\n",
      "\n",
      "Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. Pointer networks.\n",
      "\n",
      "In Advances in Neural\n",
      "\n",
      "Information Processing Systems, pp. 2692–2700, 2015.\n",
      "\n",
      "Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement\n",
      "\n",
      "learning. Machine learning, 8(3-4):229–256, 1992.\n",
      "\n",
      "Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey,\n",
      "Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine trans-\n",
      "arXiv preprint\n",
      "lation system: Bridging the gap between human and machine translation.\n",
      "arXiv:1609.08144, 2016.\n",
      "\n",
      "Kun Xu, Lingfei Wu, Zhiguo Wang, and Vadim Sheinin. Graph2seq: Graph to sequence learning\n",
      "\n",
      "with attention-based neural networks. arXiv preprint arXiv:1804.00823, 2018a.\n",
      "\n",
      "Kun Xu, Lingfei Wu, Zhiguo Wang, Mo Yu, Liwei Chen, and Vadim Sheinin. Exploiting\n",
      "rich syntactic information for semantic parsing with graph-to-sequence model. arXiv preprint\n",
      "arXiv:1808.07624, 2018b.\n",
      "\n",
      "13\n",
      "\n",
      "\f",
      "Under review as a conference paper at ICLR 2020\n",
      "\n",
      "Kun Xu, Lingfei Wu, Zhiguo Wang, Mo Yu, Liwei Chen, and Vadim Sheinin. Sql-to-text generation\n",
      "\n",
      "with graph-to-sequence model. arXiv preprint arXiv:1809.05255, 2018c.\n",
      "\n",
      "Kaichun Yao, Libo Zhang, Tiejian Luo, Lili Tao, and Yanjun Wu. Teaching machines to ask ques-\n",
      "\n",
      "tions.\n",
      "\n",
      "Xingdi Yuan, Tong Wang, Caglar Gulcehre, Alessandro Sordoni, Philip Bachman, Sandeep Sub-\n",
      "ramanian, Saizheng Zhang, and Adam Trischler. Machine comprehension by text-to-text neural\n",
      "question generation. arXiv preprint arXiv:1705.02012, 2017.\n",
      "\n",
      "Yao Zhao, Xiaochuan Ni, Yuanyuan Ding, and Qifa Ke. Paragraph-level neural question generation\n",
      "with maxout pointer and gated self-attention networks. In Proceedings of the 2018 Conference\n",
      "on Empirical Methods in Natural Language Processing, pp. 3901–3910, 2018.\n",
      "\n",
      "Qingyu Zhou, Nan Yang, Furu Wei, Chuanqi Tan, Hangbo Bao, and Ming Zhou. Neural question\n",
      "generation from text: A preliminary study. In National CCF Conference on Natural Language\n",
      "Processing and Chinese Computing, pp. 662–671. Springer, 2017.\n",
      "\n",
      "Qingyu Zhou, Nan Yang, Furu Wei, and Ming Zhou. Sequential copying networks. In Thirty-Second\n",
      "\n",
      "AAAI Conference on Artiﬁcial Intelligence, 2018.\n",
      "\n",
      "A DETAILS ON THE RNN DECODER\n",
      "\n",
      "At each decoding step t, an attention mechanism learns to attend to the most relevant words in the\n",
      "input sequence, and computes a context vector h˚\n",
      "t based on the current decoding state st, the current\n",
      "coverage vector ct and the attention memory. In addition, the generation probability pgen P r0, 1s is\n",
      "calculated from the context vector h˚\n",
      "t , the decoder state st and the decoder input yt´1. Next, pgen is\n",
      "used as a soft switch to choose between generating a word from the vocabulary, or copying a word\n",
      "from the input sequence. We dynamically maintain an extended vocabulary which is the union of the\n",
      "usual vocabulary and all words appearing in a batch of source examples (i.e., passages and answers).\n",
      "Finally, in order to encourage the decoder to utilize the diverse components of the input sequence, a\n",
      "coverage mechanism is applied. At each step, we maintain a coverage vector ct, which is the sum\n",
      "of attention distributions over all previous decoder time steps. A coverage loss is also computed to\n",
      "penalize repeatedly attending to the same locations of the input sequence.\n",
      "\n",
      "B MODEL SETTINGS\n",
      "\n",
      "We keep and ﬁx the 300-dim GloVe vectors for the most frequent 70,000 words in the training set.\n",
      "We compute the 1024-dim BERT embeddings on the ﬂy for each word in text using a (trainable)\n",
      "weighted sum of all BERT layer outputs. The embedding sizes of case, POS and NER tags are set\n",
      "to 3, 12 and 8, respectively. We set the hidden state size of BiLSTM to 150 so that the concatenated\n",
      "state size for both directions is 300. The size of all other hidden layers is set to 300. We apply\n",
      "a variational dropout (Kingma et al., 2015) rate of 0.4 after word embedding layers and 0.3 after\n",
      "RNN layers. We set the neighborhood size to 10 for dynamic graph construction. The number of\n",
      "GNN hops is set to 3. During training, in each epoch, we set the initial teacher forcing probability\n",
      "to 0.75 and exponentially increase it to 0.75 ˚ 0.9999i where i is the training step. We set α in the\n",
      "reward function to 0.1, γ in the mixed loss function to 0.99, and the coverage loss ratio λ to 0.4.\n",
      "We use Adam (Kingma & Ba, 2014) as the optimizer, and the learning rate is set to 0.001 in the\n",
      "pretraining stage and 0.00001 in the ﬁne-tuning stage. We reduce the learning rate by a factor of\n",
      "0.5 if the validation BLEU-4 score stops improving for three epochs. We stop the training when no\n",
      "improvement is seen for 10 epochs. We clip the gradient at length 10. The batch size is set to 60 and\n",
      "50 on data split-1 and split-2, respectively. The beam search width is set to 5. All hyperparameters\n",
      "are tuned on the development set.\n",
      "\n",
      "C SENSITIVITY ANALYSIS OF HYPERPARAMETERS\n",
      "\n",
      "To study the effect of the number of GNN hops, we conduct experiments on the G2Ssta model on\n",
      "the SQuAD split-2 data. Fig. 3 shows that our model is not very sensitive to the number of GNN\n",
      "hops and can achieve reasonably good results with various number of hops.\n",
      "\n",
      "14\n",
      "\n",
      "\f",
      "Under review as a conference paper at ICLR 2020\n",
      "\n",
      "Figure 3: Effect of the number of GNN hops.\n",
      "\n",
      "D DETAILS ON BASELINE METHODS\n",
      "\n",
      "SeqCopyNet (Zhou et al., 2018) proposed an extension to the copy mechanism which learns to copy\n",
      "not only single words but also sequences from the input sentence.\n",
      "\n",
      "NQG++ (Zhou et al., 2017) proposed an attention-based Seq2Seq model equipped with a copy\n",
      "mechanism and a feature-rich encoder to encode answer position, POS and NER tag information.\n",
      "\n",
      "MPQG+R (Song et al., 2017) proposed an RL-based Seq2Seq model with a multi-perspective\n",
      "matching encoder to incorporate answer information. Copy and coverage mechanisms are applied.\n",
      "\n",
      "AFPQA (Sun et al., 2018) consists of an answer-focused component which generates an interroga-\n",
      "tive word matching the answer type, and a position-aware component which is aware of the position\n",
      "of the context words when generating a question by modeling the relative distance between the\n",
      "context words and the answer.\n",
      "\n",
      "s2sa-at-mp-gsa (Zhao et al., 2018) proposed a model which contains a gated attention encoder and\n",
      "a maxout pointer decoder to tackle the challenges of processing long input sequences. For fair\n",
      "comparison, we report the results of the sentence-level version of their model to match with our\n",
      "settings.\n",
      "\n",
      "ASs2s (Kim et al., 2018) proposed an answer-separated Seq2Seq model which treats the passage\n",
      "and the answer separately.\n",
      "\n",
      "CGC-QG (Liu et al., 2019) proposed a multi-task learning framework to guide the model to learn\n",
      "the accurate boundaries between copying and generation.\n",
      "\n",
      "E DETAILS ON HUMAN EVALUATION\n",
      "\n",
      "We conducted a small-scale (i.e., 50 random examples per system) human evaluation on the split-2\n",
      "data. We asked 5 human evaluators to give feedback on the quality of questions generated by a set\n",
      "of anonymized competing systems. In each example, given a triple containing a source passage, a\n",
      "target answer and an anonymised system output, they were asked to rate the quality of the output\n",
      "by answering the following three questions: i) is this generated question syntactically correct? ii)\n",
      "is this generated question semantically correct? and iii) is this generated question relevant to the\n",
      "passage? For each evaluation question, the rating scale is from 1 to 5 where a higher score means\n",
      "better quality (i.e., 1: Poor, 2: Marginal, 3: Acceptable, 4: Good, 5: Excellent). Responses from all\n",
      "evaluators were collected and averaged.\n",
      "\n",
      "F MORE RESULTS ON ABLATION STUDY\n",
      "\n",
      "We perform the comprehensive ablation study to systematically assess the impact of different model\n",
      "components (e.g., BERT, RL, DAN, BiGGNN, FEAT, DAN-word, and DAN-hidden) for two pro-\n",
      "\n",
      "15\n",
      "\n",
      "\f",
      "Under review as a conference paper at ICLR 2020\n",
      "\n",
      "Table 5: Ablation study on the SQuAD split-2 test set.\n",
      "\n",
      "Methods\n",
      "G2Sdyn+BERT+RL\n",
      "G2Ssta+BERT+RL\n",
      "G2Ssta+BERT-ﬁxed+RL\n",
      "G2Sdyn+BERT\n",
      "G2Ssta+BERT\n",
      "G2Ssta+BERT-ﬁxed\n",
      "G2Sdyn+RL\n",
      "G2Ssta+RL\n",
      "G2Sdyn\n",
      "G2Ssta\n",
      "\n",
      "BLEU-4\n",
      "18.06\n",
      "18.30\n",
      "18.20\n",
      "17.56\n",
      "18.02\n",
      "17.86\n",
      "17.18\n",
      "17.49\n",
      "16.81\n",
      "16.96\n",
      "\n",
      "Methods\n",
      "G2Sdyn w/o feat\n",
      "G2Ssta w/o feat\n",
      "G2Sdyn w/o DAN\n",
      "G2Ssta w/o DAN\n",
      "G2Ssta w/ DAN-word only\n",
      "G2Ssta w/ DAN-hidden only\n",
      "G2Ssta w/ GGNN-forward\n",
      "G2Ssta w/ GGNN-backward\n",
      "G2Ssta w/o BiGGNN, w/ Seq2Seq\n",
      "G2Ssta w/o BiGGNN, w/ GCN\n",
      "\n",
      "BLEU-4\n",
      "16.51\n",
      "16.65\n",
      "12.58\n",
      "12.62\n",
      "15.92\n",
      "16.07\n",
      "16.53\n",
      "16.75\n",
      "16.14\n",
      "14.47\n",
      "\n",
      "posed full model variants (static vs dynamic) on the SQuAD split-2 test set. Our experimental\n",
      "results conﬁrmed that every component in our proposed model makes the contribution to the overall\n",
      "performance.\n",
      "\n",
      "16\n",
      "\n",
      "\f",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "SELF: LEARNING TO FILTER NOISY LABELS WITH\n",
      "SELF-ENSEMBLING\n",
      "\n",
      "Duc Tam Nguyen ∗, Chaithanya Kumar Mummadi ∗†, Thi Phuong Nhung Ngo †,\n",
      "Thi Hoai Phuong Nguyen ‡, Laura Beggel †, Thomas Brox †\n",
      "\n",
      "ABSTRACT\n",
      "\n",
      "Deep neural networks (DNNs) have been shown to over-ﬁt a dataset when be-\n",
      "ing trained with noisy labels for a long enough time. To overcome this problem,\n",
      "we present a simple and effective method self-ensemble label ﬁltering (SELF) to\n",
      "progressively ﬁlter out the wrong labels during training. Our method improves\n",
      "the task performance by gradually allowing supervision only from the potentially\n",
      "non-noisy (clean) labels and stops learning on the ﬁltered noisy labels. For the\n",
      "ﬁltering, we form running averages of predictions over the entire training dataset\n",
      "using the network output at different training epochs. We show that these en-\n",
      "semble estimates yield more accurate identiﬁcation of inconsistent predictions\n",
      "throughout training than the single estimates of the network at the most recent\n",
      "training epoch. While ﬁltered samples are removed entirely from the supervised\n",
      "training loss, we dynamically leverage them via semi-supervised learning in the\n",
      "unsupervised loss. We demonstrate the positive effect of such an approach on var-\n",
      "ious image classiﬁcation tasks under both symmetric and asymmetric label noise\n",
      "and at different noise ratios. It substantially outperforms all previous works on\n",
      "noise-aware learning across different datasets and can be applied to a broad set of\n",
      "network architectures.\n",
      "\n",
      "1\n",
      "\n",
      "INTRODUCTION\n",
      "\n",
      "The acquisition of large quantities of a high-quality human annotation is a frequent bottleneck in\n",
      "applying DNNs. There are two cheap but imperfect alternatives to collect annotation at large scale:\n",
      "crowdsourcing from non-experts and web annotations, particularly for image data where the tags and\n",
      "online query keywords are treated as valid labels. Both these alternatives typically introduce noisy\n",
      "(wrong) labels. While Rolnick et al. (2017) empirically demonstrated that DNNs can be surprisingly\n",
      "robust to label noise under certain conditions, Zhang et al. (2017) has shown that DNNs have the\n",
      "capacity to memorize the data and will do so eventually when being confronted with too many noisy\n",
      "labels. Consequently, training DNNs with traditional learning procedures on noisy data strongly\n",
      "deteriorates their ability to generalize – a severe problem. Hence, limiting the inﬂuence of label\n",
      "noise is of great practical importance.\n",
      "\n",
      "A common approach to mitigate the negative inﬂuence of noisy labels is to eliminate them from the\n",
      "training data and train deep learning models just with the clean labels (Fr´enay & Verleysen, 2013).\n",
      "Employing semi-supervised learning can even counteract the noisy labels (Laine & Aila, 2016; Luo\n",
      "et al., 2018). However, the decision which labels are noisy and which are not is decisive for learning\n",
      "robust models. Otherwise, unﬁltered noisy labels still inﬂuence the (supervised) loss and affect the\n",
      "task performance as in these previous works. They use the entire label set to compute the loss and\n",
      "severely lack a mechanism to identify and ﬁlter out the erroneous labels from the labels set.\n",
      "\n",
      "In this paper, we propose a self-ensemble label ﬁltering (SELF) framework that identiﬁes potentially\n",
      "noisy labels during training and keeps the network from receiving supervision from the ﬁltered noisy\n",
      "labels. This allows DNNs to gradually focus on learning from undoubtedly correct samples even\n",
      "with an extreme level of noise in the labels (e.g., 80% noise ratio) and leads to improved performance\n",
      "as the supervision become less noisy. The key contribution of our work is progressive ﬁltering, i.e.,\n",
      "\n",
      "∗Computer Vision Group, University of Freiburg, Germany\n",
      "†Bosch Center for AI, Bosch GmbH, Germany\n",
      "‡Karlsruhe Institute of Technology, Germany\n",
      "\n",
      "1\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "(a) Evaluation on CIFAR-10\n",
      "\n",
      "(b) Evaluation on CIFAR-100\n",
      "\n",
      "Figure 1: Comparing the performance of SELF with previous works for learning under different\n",
      "(symmetric) label noise ratios on the (a) CIFAR-10 & (b) CIFAR-100 datasets. SELF retains higher\n",
      "robust classiﬁcation accuracy at all noise levels.\n",
      "\n",
      "leverage the knowledge provided in the network’s output over different training iterations to form a\n",
      "consensus of predictions (self-ensemble predictions) to progressively identify and ﬁlter out the noisy\n",
      "labels from the labeled data.\n",
      "\n",
      "When learning under label noise, the network receives noisy updates and hence ﬂuctuates strongly.\n",
      "Such conduct of training would impede to learn stable neural representations and further mislead the\n",
      "consensus of the predictions. Therefore, it is essential to incorporate a model with stable training\n",
      "behavior to obtain better estimates from the consensus. Concretely, we employ the semi-supervised\n",
      "technique as a backbone to our framework to stabilize the learning process of the model. Correctly,\n",
      "we maintain the running average model, such as proposed by Tarvainen & Valpola (2017), a.k.a.\n",
      "the Mean-Teacher model. This model ensemble learning provides a more stable supervisory signal\n",
      "than the noisy model snapshots and provides a stable ground for progressive ﬁltering to ﬁlter out\n",
      "potential noisy labels. Note that this is different from just a mere combination of semi-supervised\n",
      "techniques with a noisy label ﬁltering method.\n",
      "\n",
      "We call our approach self-ensemble label ﬁltering (SELF) - that establishes model ensemble learning\n",
      "as a backbone to form a solid consensus of the self-ensemble predictions to ﬁlter out the noisy labels\n",
      "progressively. Our framework allows to compute supervised loss on cleaner subsets rather than the\n",
      "entire noisy labeled data as in previous works. It further leverages the entire dataset, including the\n",
      "ﬁltered out erroneous samples in the unsupervised loss. To best of our knowledge, we are the ﬁrst\n",
      "to identify and propose self-ensemble as a principled technique against learning under noisy labels.\n",
      "\n",
      "Our motivation stems from the observation that DNNs start to learn from easy samples in initial\n",
      "phases and gradually adapt to hard ones during training. When trained on wrongly labeled data,\n",
      "DNNs learn from clean labels at ease and receive inconsistent error signals from the noisy labels\n",
      "before over-ﬁtting to the dataset. The network’s prediction is likely to be consistent on clean samples\n",
      "and inconsistent or oscillates strongly on wrongly labeled samples over different training iterations.\n",
      "Based on this observation, we record the outputs of a single network made on different training\n",
      "epochs and treat them as an ensemble of predictions obtained from different individual networks. We\n",
      "call these ensembles that are evolved from a single network self-ensemble predictions. Subsequently,\n",
      "we identify the correctly labeled samples via the agreement between the provided label set and our\n",
      "running average of self-ensemble predictions. The samples of ensemble predictions that agree with\n",
      "the provided labels are likely to be consistent and treated as clean samples.\n",
      "\n",
      "In summary, our SELF framework stabilizes the training process and improves the generalization\n",
      "ability of DNNs. We evaluate the proposed technique on image classiﬁcation tasks using CI-\n",
      "FAR10, CIFAR100 & ImageNet. We demonstrate that SELF consistently outperforms the existing\n",
      "approaches on asymmetric and symmetric noise at all noise levels, as shown in Fig. 1. Besides,\n",
      "SELF remains robust towards the choice of the network architecture. Our work is transferable to\n",
      "other tasks without the need to modify the architecture or the primary learning objective.\n",
      "\n",
      "2\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "Figure 2: Overview of the self-ensemble label ﬁltering (SELF) framework. The model starts in\n",
      "iteration 0 with training from the noisy label set. During training, the model maintains a self-\n",
      "ensemble, a running average of itself (Tarvainen & Valpola, 2017) to provide a stable learning signal.\n",
      "Also, the model collects a self-ensemble prediction (moving-average) for the subsequent ﬁltering.\n",
      "Once the best model is found, these predictions identify and ﬁlter out noisy labels using the original\n",
      "label set L0. The model performs this progressive ﬁltering until there is no more better model. For\n",
      "details see Algorithm 1.\n",
      "\n",
      "2 SELF-ENSEMBLE LABEL FILTERING\n",
      "\n",
      "2.1 OVERVIEW\n",
      "\n",
      "Fig. 2 shows an overview of our proposed approach. In the beginning, we assume that the labels\n",
      "of the training set are noisy. The model attempts to identify correct labels progressively using self-\n",
      "forming ensembles of models and predictions. Since wrong labels cause strong ﬂuctuations in the\n",
      "model’s predictions, using ensembles is a natural way to counteract noisy labels.\n",
      "\n",
      "Concretely, in each iteration, the model learns from a detected set of potentially correct labels and\n",
      "maintains a running average of model snapshots (realized by the Mean Teacher model Tarvainen &\n",
      "Valpola (2017)). This ensemble model is evaluated on the entire dataset and provides an additional\n",
      "learning signal for training the single models. Additionally, our framework maintains the running-\n",
      "average of the model’s predictions for the ﬁltering process. The model is trained until we ﬁnd the\n",
      "best model w.r.t. the performance on the validation set (e.g., by early-stopping). The set of correct\n",
      "labels is detected based on the strategy deﬁned in Sec. 2.2. In the next iteration, we again use all data\n",
      "and the new ﬁltered label set as input for the model training. The iterative training procedure stops\n",
      "when no better model can be found. In the following, we give more details about the combination\n",
      "of this training and ﬁltering procedure.\n",
      "\n",
      "2.2 PROGRESSIVE LABEL FILTERING\n",
      "\n",
      "Progressive detection of correctly labeled samples Our framework Self-Ensemble Label Filter-\n",
      "ing (Algorithm 1) focuses on the detection of certainly correct labels from the provided label set L0.\n",
      "In each iteration i, the model is trained using the label set of potentially correct labels Li. At the end\n",
      "of each iteration, the model determines the next correct label set Li+1 using the ﬁltering strategy\n",
      "described in 2.2 The model stops learning when no improvement was achieved after training on the\n",
      "reﬁned label set Li+1.\n",
      "\n",
      "In other words, in each iteration, the model attempts to learn from the easy, in some sense, obviously\n",
      "correct labels. However, learning from easy samples also affects similar but harder samples from the\n",
      "same classes. Therefore, by learning from these easy samples, the network can gradually distinguish\n",
      "between hard and wrongly-labeled samples.\n",
      "\n",
      "3\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "Algorithm 1 SELF: Self-Ensemble Label Filtering pseudocode\n",
      "Require: Dtrain = noisy labeled training set\n",
      "Require: Dval = noisy labeled validation set\n",
      "Require: (x, y) = training stimuli and label\n",
      "Require: α = ensembling momentum, 0 ≤ α ≤ 1\n",
      "\n",
      "while acc(Mi, Dval) ≥ acc(Mbest, Dval) do\n",
      "\n",
      "i ← 0\n",
      "Mi ← train(Dtrain, Dval)\n",
      "Mbest ← Mi\n",
      "zi ← 0\n",
      "\n",
      "Mbest ← Mi\n",
      "Df ilter ← Dtrain\n",
      "i ← i + 1\n",
      "for (x, y) in Df ilter do\n",
      "\n",
      "ˆzi ← Mbest(x)\n",
      "zi ← αzi−1 + (1 − α)ˆzi\n",
      "if y (cid:54)= argmax(zi) then\n",
      "\n",
      "y ← ∅ in Df ilter\n",
      "\n",
      "end if\n",
      "end for\n",
      "Mi ← train(Df ilter, Dval)\n",
      "\n",
      "end while\n",
      "return Mbest\n",
      "\n",
      "(cid:46) counter to track iterations\n",
      "(cid:46) initial Mean-Teacher ensemble model training\n",
      "(cid:46) set initial model as best model\n",
      "(cid:46) initialize ensemble predictions of all samples\n",
      "(ignored sample index for simplicity)\n",
      "(cid:46) iterate until no best model is found on Dval\n",
      "(cid:46) save the best model\n",
      "(cid:46) set ﬁltered dataset as initial label set\n",
      "\n",
      "(cid:46) evaluate model output ˆzi\n",
      "(cid:46) accumulate ensemble predictions zi\n",
      "(cid:46) verify agreement of ensemble predictions & label\n",
      "(cid:46) identify it as noisy label & remove from label set\n",
      "\n",
      "(cid:46) train Mean-Teacher model on ﬁltered label set\n",
      "\n",
      "Our framework does not focus on repairing all noisy labels. Although the detection of wrong labels is\n",
      "sometimes easy, ﬁnding their correct hidden label might be extremely challenging in case of having\n",
      "many classes. If the noise is sufﬁciently random, the set of correct labels will be representative to\n",
      "achieve high model performance. Further, in our framework, the label ﬁltering is performed on the\n",
      "original label set L0 from iteration 0. Clean labels erroneously removed in an earlier iteration (e.g.,\n",
      "labels of hard to classify samples) can be reconsidered for model training again in later iterations.\n",
      "\n",
      "Filtering strategy The model can determine the set of potentially correct labels Li based on agree-\n",
      "ment between the label y and its maximal likelihood prediction ˆy|x with Li = {(y, x) | ˆyx =\n",
      "y; ∀(y, x) ∈ L0}. L0 is the label set provided in the beginning, (y, x) are the samples and their\n",
      "respective noisy labels in the iteration i. In other words, the labels are only used for supervised\n",
      "training if in the current epoch, the model predicts the respective label to be the correct class with\n",
      "the highest likelihood. In practice, our framework does not use ˆy(x) of model snapshots for ﬁltering\n",
      "but a moving-average of the ensemble models and predictions to improve the ﬁltering decision.\n",
      "\n",
      "2.3 SELF-ENSEMBLE LEARNING\n",
      "\n",
      "The model’s predictions for noisy samples tend to ﬂuctuate. For example, take a cat wrongly labeled\n",
      "as a tiger. Other cat samples would encourage the model to predict the given cat image as a cat.\n",
      "Contrary, the wrong label tiger regularly pulls the model back to predict the cat as a tiger. Hence,\n",
      "using the model’s predictions gathered in one single training epoch for ﬁltering is sub-optimal.\n",
      "Therefore, in our framework SELF, our model relies on ensembles of models and predictions.\n",
      "\n",
      "Model ensemble with Mean Teacher A natural way to form a model ensemble is by using an\n",
      "exponential running average of model snapshots (Fig. 3a). This idea was proposed in Tarvainen\n",
      "& Valpola (2017) for semi-supervised learning and is known as the Mean Teacher model. In our\n",
      "framework, both the mean teacher model and the normal model are evaluated on all data to preserve\n",
      "the consistency between both models. The consistency loss between student and teacher output\n",
      "distribution can be realized with Mean-Square-Error loss or Kullback-Leibler-divergence. More\n",
      "details for training with the model ensemble can be found in Appendix A.1\n",
      "\n",
      "Prediction ensemble Additionally, we propose to collect the sample predictions over multiple\n",
      "training epochs: zj = αzj−1 + (1 − α)ˆzj , whereby zj depicts the moving-average prediction\n",
      "of sample k at epoch j, α is a momentum, ˆzj is the model prediction for sample k in epoch j.\n",
      "This scheme is displayed in Fig. 3b. For each sample, we store the moving-average predictions,\n",
      "accumulated over the past iterations. Besides having a more stable basis for the ﬁltering step, our\n",
      "proposed procedure also leads to negligible memory and computation overhead.\n",
      "\n",
      "4\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "(a) Model ensemble (Mean teacher)\n",
      "\n",
      "(b) Predictions ensemble\n",
      "\n",
      "Figure 3: Maintaining the (a) model and (b) predictions ensembles is very effective against noisy\n",
      "model updates. These ensembles are self-forming during the training process as a moving-average\n",
      "of (a) model snapshots or (b) class predictions from previous training steps.\n",
      "\n",
      "Further, due to continuous training of the best model from the previous model, computation time can\n",
      "be signiﬁcantly reduced, compared to re-training the model from scratch. On the new ﬁltered dataset,\n",
      "the model must only slowly adapt to the new noise ratio contained in the training set. Depending on\n",
      "the computation budget, a maximal number of iterations for ﬁltering can be set to save time.\n",
      "\n",
      "3 RELATED WORKS\n",
      "\n",
      "Reed et al. (2014); Azadi et al. (2015) performed early works on learning robustly under label\n",
      "noise for deep neural networks. Recently, Rolnick et al. (2017) have shown for classiﬁcation that\n",
      "deep neural networks come with natural robustness to label noise following a particular random\n",
      "distribution. No modiﬁcation of the network or the training procedure is required to achieve this\n",
      "robustness. Following this insight, our framework SELF relies on this natural robustness to kickstart\n",
      "the self-ensemble ﬁltering process to extend the robust behavior to more challenging scenarios.\n",
      "\n",
      "Laine & Aila (2016); Luo et al. (2018) proposed to apply semi-supervised techniques on the data to\n",
      "counteract noise. These and other semi-supervised learning techniques learn from a static, initial set\n",
      "of noisy labels and have no mechanisms to repair labels. Therefore, the supervised losses in their\n",
      "learning objective are typically high until the model strongly overﬁts to the label noise. Compared\n",
      "to these works, our framework performs a variant of self-supervised label corrections. The network\n",
      "learns from a dynamic, variable set of labels, which is determined by the network itself. Progressive\n",
      "ﬁltering allows the network to (1) focus on a label set with a signiﬁcantly lower noise ratio and (2)\n",
      "repair wrong decisions made by itself in an earlier iteration.\n",
      "\n",
      "Other works assign weights to potentially wrong labels to reduce the learning signal (Jiang et al.,\n",
      "2017; Ren et al., 2018; Jenni & Favaro, 2018). These approaches tend to assign less extreme weights\n",
      "or hyperparameters that are hard to set. Since the typical classiﬁcation loss is highly non-linear, a\n",
      "lower weight might still lead to learning from wrong labels. Compared to these works, the samples\n",
      "in SELF only receive extreme weights: either they are zero or one. Further, SELF focuses only on\n",
      "self-detecting the correct samples, instead of repairing the wrong labels. Typically, the set of correct\n",
      "samples are much easier to detect and are sufﬁciently representative to achieve high performance.\n",
      "\n",
      "Han et al. (2018b); Jiang et al. (2017) employ two collaborating and simultaneously learning net-\n",
      "works to determine which samples to learn from and which not. However, the second network is\n",
      "free in its predictions and hence hard to tune. Compared to these works, we use ensemble learning as\n",
      "a principled approach to counteract model ﬂuctuations. In SELF, the second network is extremely\n",
      "restricted and is only composed of running averages of the ﬁrst network. To realize the second\n",
      "network, we use the mean-teacher model (Tarvainen & Valpola, 2017) as a backbone. Compared\n",
      "to their work, our self-ensemble label ﬁltering gradually detects the correct labels and learns from\n",
      "them, so the label set is variable. Further, we do use not only model ensembles but also an ensemble\n",
      "of predictions to detect correct labels.\n",
      "\n",
      "Other works modify the primary loss function of the classiﬁcation tasks. Patrini et al. (2017) es-\n",
      "timates the noise transition matrix to correct the loss, Han et al. (2018a) uses human-in-the-loop,\n",
      "Zhang & Sabuncu (2018); Thulasidasan et al. (2019) propose other forms of cross-entropy losses.\n",
      "The loss modiﬁcation impedes the transfer of these ideas to other tasks than classiﬁcation. Compared\n",
      "to these works, our framework SELF does not modify the primary loss. However, many tasks rely on\n",
      "the presence of clean labels such as anomaly detection (Nguyen et al., 2019a) or self-supervised and\n",
      "unsupervised learning (Nguyen et al., 2019b). The progressive ﬁltering procedure and self-ensemble\n",
      "learning proposed are also applicable in these tasks to counteract noise effectively.\n",
      "\n",
      "5\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "Table 1: Comparison of classiﬁcation accuracy when learning under uniform label noise on CIFAR-\n",
      "10 and CIFAR-100. Following previous works, we compare two evaluation scenarios: with a noisy\n",
      "validation set (top) and with 1000 clean validation samples (bottom). The best model is marked in\n",
      "bold. Having a small clean validation set improves the model but is not necessary.\n",
      "\n",
      "NOISE RATIO\n",
      "\n",
      "CIFAR-10\n",
      "\n",
      "CIFAR-100\n",
      "\n",
      "40% 60% 80% 40% 60% 80 %\n",
      "\n",
      "USING NOISY VALIDATION SET\n",
      "\n",
      "69.66\n",
      "70.64\n",
      "78.15\n",
      "86.06\n",
      "89.00\n",
      "89.00\n",
      "87.13\n",
      "87.62\n",
      "83.25\n",
      "81.85\n",
      "83.36\n",
      "85.34\n",
      "83.27\n",
      "93.70\n",
      "\n",
      "90.93\n",
      "78.00\n",
      "86.55\n",
      "86.92\n",
      "95.10\n",
      "\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "82.54\n",
      "82.70\n",
      "74.96\n",
      "74.04\n",
      "72.84\n",
      "80.07\n",
      "74.39\n",
      "93.15\n",
      "\n",
      "87.58\n",
      "-\n",
      "-\n",
      "-\n",
      "93.77\n",
      "\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "20.00\n",
      "49.00\n",
      "64.07\n",
      "67.92\n",
      "54.64\n",
      "29.22\n",
      "-\n",
      "53.81\n",
      "40.09\n",
      "69.91\n",
      "\n",
      "70.80\n",
      "-\n",
      "-\n",
      "-\n",
      "79.93\n",
      "\n",
      "51.34\n",
      "49.10\n",
      "-\n",
      "58.01\n",
      "61.00\n",
      "68.00\n",
      "61.77\n",
      "62.64\n",
      "31.05\n",
      "55.95\n",
      "52.01\n",
      "53.69\n",
      "52.88\n",
      "71.98\n",
      "\n",
      "68.20\n",
      "59.00\n",
      "58.34\n",
      "61.31\n",
      "74.76\n",
      "\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "53.16\n",
      "54.04\n",
      "19.12\n",
      "47.98\n",
      "42.27\n",
      "41.47\n",
      "42.64\n",
      "66.21\n",
      "\n",
      "59.44\n",
      "-\n",
      "-\n",
      "-\n",
      "68.35\n",
      "\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "13.00\n",
      "35.00\n",
      "29.16\n",
      "29.60\n",
      "08.90\n",
      "23.22\n",
      "\n",
      "15.00\n",
      "18.46\n",
      "42.09\n",
      "\n",
      "34.06\n",
      "-\n",
      "-\n",
      "-\n",
      "46.43\n",
      "\n",
      "REED-HARD (REED ET AL., 2014)\n",
      "S-MODEL (GOLDBERGER & BEN-REUVEN, 2016)\n",
      "OPEN-SET WANG ET AL. (2018)\n",
      "RAND. WEIGHTS (REN ET AL., 2018)\n",
      "BI-LEVEL-MODEL (JENNI & FAVARO, 2018)\n",
      "MENTORNET (JIANG ET AL., 2017)\n",
      "Lq (ZHANG & SABUNCU, 2018)\n",
      "TRUNC Lq (ZHANG & SABUNCU, 2018)\n",
      "FORWARD ˆT (PATRINI ET AL., 2017)\n",
      "CO-TEACHING (HAN ET AL., 2018B)\n",
      "D2L (MA ET AL., 2018)\n",
      "SL (WANG ET AL., 2019)\n",
      "JOINTOPT (TANAKA ET AL., 2018)\n",
      "SELF (OURS)\n",
      "\n",
      "DAC (THULASIDASAN ET AL., 2019)\n",
      "MENTORNET (JIANG ET AL., 2017)\n",
      "RAND. WEIGHTS (REN ET AL., 2018)\n",
      "REN ET AL (REN ET AL., 2018)\n",
      "SELF* (OURS)\n",
      "\n",
      "4 EVALUATION\n",
      "\n",
      "4.1 EXPERIMENTS DESCRIPTIONS\n",
      "\n",
      "4.1.1 STRUCTURE OF THE ANALYSIS\n",
      "\n",
      "USING CLEAN VALIDATION SET (1000 IMAGES)\n",
      "\n",
      "We evaluate our approach on CIFAR-10, CIFAR-100, an ImageNet-ILSVRC on different noise sce-\n",
      "narios. For CIFAR-10, CIFAR-100, and ImageNet, we consider the typical situation with symmetric\n",
      "and asymmetric label noise. In the case of the symmetric noise, a label is randomly ﬂipped to another\n",
      "class with probability p. Following previous works, we also consider label ﬂips of semantically sim-\n",
      "ilar classes on CIFAR-10, and pair-wise label ﬂips on CIFAR-100. Finally, we perform studies on\n",
      "the choice of the network architecture and the ablation of the components in our framework. Tab. 6\n",
      "(Appendix) shows the in-deep analysis of semi-supervised learning strategies combined with recent\n",
      "works. Overall, the proposed framework SELF outperforms all these combinations.\n",
      "\n",
      "4.1.2 COMPARISONS TO PREVIOUS WORKS\n",
      "\n",
      "We compare our work to previous methods from Reed-Hard (Reed et al., 2014), S-model (Gold-\n",
      "berger & Ben-Reuven, 2016), Wang et al. (2018), Rand. weights (Ren et al., 2018), Bi-level-model\n",
      "(Jenni & Favaro, 2018), D2L (Ma et al., 2018), SL (Wang et al., 2019), Lq (Zhang & Sabuncu,\n",
      "2018), Trunc Lq (Zhang & Sabuncu, 2018), Forward ˆT (Patrini et al., 2017), DAC (Thulasidasan\n",
      "et al., 2019), Random reweighting (Ren et al., 2018), and Learning to reweight (Ren et al., 2018).\n",
      "For co-teaching (Han et al., 2018b), MentorNet (Jiang et al., 2017), JointOpt (Tanaka et al., 2018),\n",
      "the source codes are available and hence used for evaluation.\n",
      "\n",
      "(Ren et al., 2018) and DAC (Thulasidasan et al., 2019) considered the setting of having a small clean\n",
      "validation set of 1000 and 5000 images respectively. For comparison purposes, we also experiment\n",
      "with a small clean set of 1000 images additionally. Further, we abandon oracle experiments or\n",
      "methods using additional information to keep the evaluation comparable. For instance, Forward T\n",
      "(Patrini et al., 2017) uses the true underlying confusion matrix to correct the loss. This information\n",
      "is neither known in typical scenarios nor used by other methods.\n",
      "\n",
      "6\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "Table 2: Asymmetric noise on CIFAR-10, CIFAR-100. All methods use Resnet34. CIFAR-10: ﬂip\n",
      "TRUCK → AUTOMOBILE, BIRD → AIRPLANE, DEER → HORSE, CAT↔DOG with prob. p.\n",
      "CIFAR-100: ﬂip class i to (i + 1)%100 with prob. p. SELF retains high performances across all\n",
      "noise ratios and outperforms all previous works.\n",
      "\n",
      "CIFAR-10\n",
      "\n",
      "NOISE RATIO\n",
      "\n",
      "10%\n",
      "\n",
      "20%\n",
      "\n",
      "30%\n",
      "\n",
      "40%\n",
      "\n",
      "CCE\n",
      "MAE\n",
      "FORWARD ˆT\n",
      "Lq\n",
      "TRUNC Lq\n",
      "SL\n",
      "JOINTOPT\n",
      "SELF (OURS)\n",
      "\n",
      "90.69\n",
      "82.61\n",
      "90.52\n",
      "90.91\n",
      "90.43\n",
      "88.24\n",
      "90.12\n",
      "93.75\n",
      "\n",
      "88.59\n",
      "52.93\n",
      "89.09\n",
      "89.33\n",
      "89.45\n",
      "85.36\n",
      "89.45\n",
      "92.76\n",
      "\n",
      "86.14\n",
      "50.36\n",
      "86.79\n",
      "85.45\n",
      "87.10\n",
      "80.64\n",
      "87.18\n",
      "92.42\n",
      "\n",
      "80.11\n",
      "45.52\n",
      "83.55\n",
      "76.74\n",
      "82.28\n",
      "\n",
      "-\n",
      "\n",
      "87.97\n",
      "89.07\n",
      "\n",
      "10%\n",
      "\n",
      "66.54\n",
      "13.38\n",
      "45.96\n",
      "68.36\n",
      "68.86\n",
      "65.58\n",
      "69.61\n",
      "72.45\n",
      "\n",
      "CIFAR-100\n",
      "20%\n",
      "\n",
      "30%\n",
      "\n",
      "59.20\n",
      "11.50\n",
      "42.46\n",
      "66.59\n",
      "66.59\n",
      "65.14\n",
      "68.94\n",
      "70.53\n",
      "\n",
      "51.40\n",
      "08.91\n",
      "38.13\n",
      "61.45\n",
      "61.87\n",
      "63.10\n",
      "63.99\n",
      "65.09\n",
      "\n",
      "40%\n",
      "\n",
      "42.74\n",
      "08.20\n",
      "34.44\n",
      "47.22\n",
      "47.66\n",
      "\n",
      "-\n",
      "\n",
      "53.71\n",
      "53.83\n",
      "\n",
      "Whenever possible, we adopt the reported performance from the corresponding publications. The\n",
      "testing scenarios are kept as similar as possible to enable a fair comparison. All tested scenarios use\n",
      "a noisy validation set with the same noise distribution as the training set unless stated otherwise. All\n",
      "model performances are reported on the clean test set.\n",
      "\n",
      "Table 3: Effect of the choice of network architecture on classiﬁcation accuracy on CIFAR-10 &\n",
      "-100 with uniform label noise. SELF is compatible with all tested architectures. Here * represents\n",
      "baseline accuracy of the architectures that are trained on fully supervised setting at 0% label noise.\n",
      "\n",
      "CIFAR-10\n",
      "\n",
      "CIFAR-100\n",
      "\n",
      "CIFAR-10\n",
      "\n",
      "CIFAR-100\n",
      "\n",
      "RESNET101\n",
      "\n",
      "93.89*\n",
      "\n",
      "81.14*\n",
      "\n",
      "RESNET34\n",
      "\n",
      "93.5*\n",
      "\n",
      "76.76*\n",
      "\n",
      "NOISE\n",
      "\n",
      "40% 80% 40% 80%\n",
      "\n",
      "NOISE\n",
      "\n",
      "40% 80% 40% 80%\n",
      "\n",
      "MENTORNET\n",
      "CO-T.\n",
      "SELF\n",
      "\n",
      "89.00\n",
      "62.58\n",
      "92.77\n",
      "\n",
      "49.00\n",
      "21.79\n",
      "64.52\n",
      "\n",
      "68.00\n",
      "39.58\n",
      "69.00\n",
      "\n",
      "35.00\n",
      "16.79\n",
      "39.73\n",
      "\n",
      "Lq\n",
      "TRUNC Lq\n",
      "FORWARD ˆT\n",
      "SELF\n",
      "\n",
      "87.13\n",
      "87.62\n",
      "83.25\n",
      "91.13\n",
      "\n",
      "64.07\n",
      "67.92\n",
      "54.64\n",
      "63.59\n",
      "\n",
      "61.77\n",
      "62.64\n",
      "31.05\n",
      "66.71\n",
      "\n",
      "29.16\n",
      "29.60\n",
      "8.90\n",
      "35.56\n",
      "\n",
      "WRN 28-10\n",
      "\n",
      "96.21*\n",
      "\n",
      "81.02*\n",
      "\n",
      "NOISE\n",
      "\n",
      "40% 80% 40% 80%\n",
      "\n",
      "MENTORNET\n",
      "REWEIGHT\n",
      "SELF\n",
      "\n",
      "88.7\n",
      "86.02\n",
      "93.34\n",
      "\n",
      "46.30\n",
      "-\n",
      "67.41\n",
      "\n",
      "67.50\n",
      "58.01\n",
      "72.48\n",
      "\n",
      "30.10\n",
      "-\n",
      "42.06\n",
      "\n",
      "RESNET26\n",
      "\n",
      "96.37*\n",
      "\n",
      "81.20*\n",
      "\n",
      "NOISE\n",
      "\n",
      "CO-T.\n",
      "SELF\n",
      "\n",
      "40% 80% 40% 80%\n",
      "\n",
      "81.85\n",
      "93.70\n",
      "\n",
      "29.22\n",
      "69.91\n",
      "\n",
      "55.95\n",
      "71.98\n",
      "\n",
      "23.22\n",
      "42.09\n",
      "\n",
      "4.1.3 NETWORKS CONFIGURATION AND TRAINING\n",
      "\n",
      "For the basic training of self-ensemble model, we use the Mean Teacher model (Tarvainen &\n",
      "Valpola, 2017) available on GitHub 1 . The students and teacher networks are residual networks (He\n",
      "et al., 2016) with 26 layers with Shake-Shake-regularization (Gastaldi, 2017). We use the Py-\n",
      "Torch (Paszke et al., 2017) implementation of the network and keep the training settings close\n",
      "to (Tarvainen & Valpola, 2017). The network is trained with Stochastic Gradient Descent. In each\n",
      "ﬁltering iteration, the model is trained for a maximum of 300 epochs, with patience of 50 epochs.\n",
      "For more training details, see the appendix.\n",
      "\n",
      "4.2 EXPERIMENTS RESULTS\n",
      "\n",
      "4.2.1 SYMMETRIC LABEL NOISE\n",
      "\n",
      "CIFAR-10 and 100 Results for typical uniform noise scenarios with noise ratios on CIFAR-10\n",
      "and CIFAR-100 are shown in Tab. 1. More results are visualized in Fig. 1a (CIFAR-10) and Fig. 1b\n",
      "(CIFAR-100). Our approach SELF performs robustly in the case of lower noise ratios with up to 60%\n",
      "and outperforms previous works. Although a strong performance loss occurs at 80% label noise,\n",
      "\n",
      "1https://github.com/CuriousAI/mean-teacher\n",
      "\n",
      "7\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "Table 4: Classiﬁcation accuracy on clean\n",
      "ImageNet validation dataset. The mod-\n",
      "els are trained at 40% label noise and the\n",
      "best model is picked based on the evalu-\n",
      "ation on noisy validation data. Mentornet\n",
      "shows the best previously reported results.\n",
      "Mentornet* is based on Resnet-101. We\n",
      "chose the smaller Resnext50 model to re-\n",
      "duce the run-time.\n",
      "\n",
      "Accurracy\n",
      "\n",
      "Mentornet*\n",
      "ResNext\n",
      "Mean-T.\n",
      "SELF (Ours)\n",
      "\n",
      "Resnext18 Resnext50\n",
      "P@1 P@5 P@1 P@5\n",
      "\n",
      "-\n",
      "\n",
      "65.10 85.90\n",
      "-\n",
      "50.6 75.99 56.25 80.90\n",
      "58.04 81.82 62.96 85.72\n",
      "66.92 86.65 71.31 89.92\n",
      "\n",
      "Table 5: Ablation study on CIFAR-10 and CIFAR-\n",
      "100. The Resnet baseline was trained on the full\n",
      "noisy label set. Adding progressive ﬁltering im-\n",
      "proves over this baseline. The Mean Teacher main-\n",
      "tains an ensemble of model snapshots, which helps\n",
      "counteract noise. Having progressive ﬁltering and\n",
      "model ensembles (-MVA-pred.) makes the model\n",
      "more robust but still fails at 80% noise. The full\n",
      "SELF framework additionally uses the prediction\n",
      "ensemble for detection of correct labels.\n",
      "\n",
      "NOISE RATIO\n",
      "\n",
      "RESNET26\n",
      "FILTERING\n",
      "MEAN-T.\n",
      "- MVA-PRED.\n",
      "SELF (OURS)\n",
      "\n",
      "CIFAR-10\n",
      "\n",
      "CIFAR-100\n",
      "40% 80% 40% 80%\n",
      "\n",
      "83.20\n",
      "87.35\n",
      "93.70\n",
      "93.77\n",
      "93.70\n",
      "\n",
      "41.37\n",
      "49.58\n",
      "52.50\n",
      "57.40\n",
      "69.91\n",
      "\n",
      "53.18\n",
      "61.40\n",
      "65.85\n",
      "71.69\n",
      "71.98\n",
      "\n",
      "19.92\n",
      "23.42\n",
      "26.31\n",
      "38,61\n",
      "42.09\n",
      "\n",
      "SELF still outperforms most of the previous approaches. The experiment SELF* using a 1000 clean\n",
      "validation images shows that the performance loss mostly originates from the progressive ﬁltering\n",
      "relying too strongly on the extremely noisy validation set.\n",
      "\n",
      "ImageNet-ILSVRC Tab. 4 shows the precision@1 and @5 of various models, given 40% label\n",
      "noise in the training set. Our networks are based on ResNext18 and Resnext50. Note that MentorNet\n",
      "(Jiang et al., 2017) uses Resnet101 (P@1: 78.25) (Goyal et al., 2017), which has higher performance\n",
      "compared to Resnext50 (P@1: 77.8) (Xie et al., 2017) on the standard ImageNet validation set.\n",
      "\n",
      "Despite the weaker model, SELF (ResNext50) surpasses the best previously reported results by\n",
      "more than 5% absolute improvement. Even the signiﬁcantly weaker model ResNext18 outperforms\n",
      "MentorNet, which is based on a more powerful ResNet101 network.\n",
      "\n",
      "4.2.2 ASYMMETRIC LABEL NOISE\n",
      "Tab. 2 shows more challenging noise scenarios when the noise is not class-symmetric and uniform.\n",
      "Concretely, labels are ﬂipped among semantically similar classes such as CAT and DOG on CIFAR-\n",
      "10. On CIFAR-100, each label is ﬂipped to the next class with a probability p. In these scenarios, our\n",
      "framework SELF also retains high performance and only shows a small performance drop at 40%\n",
      "noise. The high label noise resistance of our framework indicates that the proposed self-ensemble\n",
      "ﬁltering process helps the network identify correct samples, even under extreme noise ratios.\n",
      "\n",
      "4.2.3 EFFECTS OF DIFFERENT ARCHITECTURES\n",
      "Previous works utilize a various set of different architectures, which hinders a fair comparison.\n",
      "Tab. 3 shows the performance of our framework SELF compared to previous approaches. SELF\n",
      "outperforms other works in all scenarios except for CIFAR-10 with 80% noise. Typical robust\n",
      "learning approaches lead to signiﬁcant accuracy losses at 40% noise, while SELF still retains high\n",
      "performance. Further, note that SELF allows the network’s performance to remain consistent across\n",
      "the different underlying architectures.\n",
      "\n",
      "4.2.4 ABLATION STUDY\n",
      "Tab. 5 shows the importance of each component in our framework. See Fig. 4a, Fig. 4b for experi-\n",
      "ments on more noise ratios. As expected, the Resnet-baseline rapidly breaks down with increasing\n",
      "noise ratios. Adding self-supervised ﬁltering increases the performance slightly in lower noise ratios.\n",
      "However, the model has to rely on extremely noisy snapshots. Contrary, using a model ensemble\n",
      "alone such as in Mean-Teacher can counteract noise on the noisy dataset CIFAR-10. On the more\n",
      "challenging CIFAR-100, however, the performance decreases strongly. With self-supervised ﬁlter-\n",
      "ing and model ensembles, SELF (without MVA-pred) is more robust and only impairs performance\n",
      "at 80% noise. The last performance boost is given by using moving-average predictions so that the\n",
      "network can reliably detect correctly labeled samples gradually.\n",
      "\n",
      "Fig. 4 shows the ablation experiments on more noise ratios. The analyses shows that each component\n",
      "in SELF is essential for the model to learn robustly.\n",
      "\n",
      "8\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "(a) Ablation exps. on CIFAR-10\n",
      "\n",
      "(b) Ablation exps. on CIFAR-100\n",
      "\n",
      "Figure 4: Ablation study on the importance of the components in our framework SELF, evaluated\n",
      "on (a) Cifar-10 and (b) Cifar-100 with uniform noise. Please refer Tab. 5 for details of components.\n",
      "\n",
      "Table 6: Analysis of semi-supervised learning (SSL) strategies: entropy learning, mean-teacher\n",
      "combined with recent works. Our progressive ﬁltering strategy is shown to be effective and per-\n",
      "forms well regardless of the choice of the semi-supervised learning backbone. Overall, the proposed\n",
      "method SELF outperforms all these combinations. Best model in each SSL-category is marked\n",
      "in bold. Running mean-teacher+ co-teaching using the same conﬁguration is not possible due to\n",
      "memory constraints.\n",
      "\n",
      "NOISE RATIO\n",
      "\n",
      "CIFAR-10\n",
      "\n",
      "CIFAR-100\n",
      "\n",
      "40%\n",
      "\n",
      "60%\n",
      "\n",
      "80%\n",
      "\n",
      "40%\n",
      "\n",
      "60%\n",
      "\n",
      "80%\n",
      "\n",
      "BASELINE MODELS\n",
      "\n",
      "RESNET26 (GASTALDI, 2017)\n",
      "CO-TEACHING (HAN ET AL., 2018B)\n",
      "JOINTOPT (TANAKA ET AL., 2018)\n",
      "PROGRESSIVE FILTERING (OURS)\n",
      "\n",
      "ENTROPY\n",
      "ENTROPY + CO-TEACHING\n",
      "ENTROPY + JOINT-OPT\n",
      "ENTROPY+FILTERING (OURS)\n",
      "\n",
      "83.20\n",
      "81.85\n",
      "83.27\n",
      "87.35\n",
      "\n",
      "79.13\n",
      "84.94\n",
      "84.44\n",
      "90.04\n",
      "\n",
      "72.35\n",
      "74.04\n",
      "74.39\n",
      "75.47\n",
      "\n",
      "85.98\n",
      "74.28\n",
      "75.86\n",
      "83.88\n",
      "\n",
      "41.37\n",
      "29.22\n",
      "40.09\n",
      "49.58\n",
      "\n",
      "46.93\n",
      "35.16\n",
      "39.16\n",
      "52.46\n",
      "\n",
      "53.18\n",
      "55.95\n",
      "52.88\n",
      "61.40\n",
      "\n",
      "54.65\n",
      "55.68\n",
      "56.73\n",
      "59.97\n",
      "\n",
      "44.31\n",
      "47.98\n",
      "42.64\n",
      "50.60\n",
      "\n",
      "41.34\n",
      "43.52\n",
      "43.27\n",
      "46.45\n",
      "\n",
      "19.92\n",
      "23.22\n",
      "18.46\n",
      "23.42\n",
      "\n",
      "21.29\n",
      "20.5\n",
      "17.24\n",
      "23.53\n",
      "\n",
      "SEMI-SUPERVISED LEARNING WITH ENTROPY LEARNING\n",
      "\n",
      "SEMI-SUPERVISED LEARNING WITH MEAN-TEACHER\n",
      "\n",
      "MEAN TEACHER\n",
      "MEAN-TEACHER + JOINTOPT\n",
      "MEAN-TEACHER + FILTERING - SELF (OURS)\n",
      "\n",
      "93.70\n",
      "91.40\n",
      "93.70\n",
      "\n",
      "90.40\n",
      "83.62\n",
      "92.85\n",
      "\n",
      "52.5\n",
      "45.12\n",
      "69.91\n",
      "\n",
      "65.85\n",
      "60.09\n",
      "71.98\n",
      "\n",
      "54.48\n",
      "45.92\n",
      "66.21\n",
      "\n",
      "26.31\n",
      "23.54\n",
      "42.58\n",
      "\n",
      "4.2.5 SEMI-SUPERVISED LEARNING FOR PROGRESSIVE FILTERING\n",
      "\n",
      "Tab. 6 shows different semi-supervised learning strategies: entropy learning, mean-teacher com-\n",
      "bined with recent works. Note that Co-Teaching+Mean-Teacher cannot be implemented and run in\n",
      "the same conﬁguration as other experiments, due to memory constraints.\n",
      "\n",
      "The analysis indicates the semi-supervised losses mostly stabilize the baselines, compared to the\n",
      "model without semi-supervised learning. However, Co-teaching and JointOpt sometimes perform\n",
      "worse than the purely semi-supervised model. This result indicates that their proposed frameworks\n",
      "are not always compatible with semi-supervised losses.\n",
      "\n",
      "The progressive ﬁltering technique is seamlessly compatible with different semi-supervised losses.\n",
      "The ﬁltering outperforms its counterparts when combined with Entropy Learning or Mean-teacher\n",
      "model. Overall, SELF outperforms all considered combinations.\n",
      "\n",
      "9\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "5 CONCLUSION\n",
      "\n",
      "We propose a simple and easy to implement a framework to train robust deep learning models under\n",
      "incorrect or noisy labels. We ﬁlter out the training samples that are hard to learn (possibly noisy\n",
      "labeled samples) by leveraging ensemble of predictions of the single network’s output over different\n",
      "training epochs. Subsequently, we allow clean supervision from the non-hard samples and further\n",
      "leverage additional unsupervised loss from the entire dataset. We show that our framework results in\n",
      "DNN models with superior generalization performance on CIFAR-10, CIFAR-100 & ImageNet and\n",
      "outperforms all previous works under symmetric (uniform) and asymmetric noises. Furthermore,\n",
      "our models remain robust despite the increasing noise ratio and change in network architectures.\n",
      "\n",
      "REFERENCES\n",
      "\n",
      "Samaneh Azadi, Jiashi Feng, Stefanie Jegelka, and Trevor Darrell. Auxiliary image regularization\n",
      "\n",
      "for deep cnns with noisy labels. arXiv preprint arXiv:1511.07069, 2015.\n",
      "\n",
      "Benoˆıt Fr´enay and Michel Verleysen. Classiﬁcation in the presence of label noise: a survey. IEEE\n",
      "\n",
      "transactions on neural networks and learning systems, 25(5):845–869, 2013.\n",
      "\n",
      "Xavier Gastaldi. Shake-shake regularization. arXiv preprint arXiv:1705.07485, 2017.\n",
      "\n",
      "Jacob Goldberger and Ehud Ben-Reuven. Training deep neural-networks using a noise adaptation\n",
      "\n",
      "layer. 2016.\n",
      "\n",
      "Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,\n",
      "\n",
      "Aaron Courville, and Yoshua Bengio. Generative Adversarial Nets. pp. 9.\n",
      "\n",
      "Priya Goyal, Piotr Doll´ar, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, An-\n",
      "drew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet\n",
      "in 1 hour. arXiv preprint arXiv:1706.02677, 2017.\n",
      "\n",
      "Bo Han, Jiangchao Yao, Gang Niu, Mingyuan Zhou, Ivor Tsang, Ya Zhang, and Masashi Sugiyama.\n",
      "Masking: A new perspective of noisy supervision. In Advances in Neural Information Processing\n",
      "Systems, pp. 5836–5846, 2018a.\n",
      "\n",
      "Bo Han, Quanming Yao, Xingrui Yu, Gang Niu, Miao Xu, Weihua Hu, Ivor Tsang, and Masashi\n",
      "Sugiyama. Co-teaching: Robust training of deep neural networks with extremely noisy labels. In\n",
      "NeurIPS, pp. 8535–8545, 2018b.\n",
      "\n",
      "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-\n",
      "nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.\n",
      "770–778, 2016.\n",
      "\n",
      "Simon Jenni and Paolo Favaro. Deep bilevel learning. In ECCV, 2018.\n",
      "\n",
      "Lu Jiang, Zhengyuan Zhou, Thomas Leung, Li-Jia Li, and Li Fei-Fei. MentorNet: Learning Data-\n",
      "Driven Curriculum for Very Deep Neural Networks on Corrupted Labels. arXiv:1712.05055 [cs],\n",
      "December 2017. URL http://arxiv.org/abs/1712.05055. arXiv: 1712.05055.\n",
      "\n",
      "Samuli Laine and Timo Aila. Temporal ensembling for semi-supervised learning. arXiv preprint\n",
      "\n",
      "arXiv:1610.02242, 2016.\n",
      "\n",
      "Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. arXiv\n",
      "\n",
      "preprint arXiv:1608.03983, 2016.\n",
      "\n",
      "Yucen Luo, Jun Zhu, Mengxi Li, Yong Ren, and Bo Zhang. Smooth neighbors on teacher graphs\n",
      "for semi-supervised learning. In Proceedings of the IEEE Conference on Computer Vision and\n",
      "Pattern Recognition, pp. 8896–8905, 2018.\n",
      "\n",
      "Xingjun Ma, Yisen Wang, Michael E Houle, Shuo Zhou, Sarah M Erfani, Shu-Tao Xia, Sudan-\n",
      "thi Wijewickrema, and James Bailey. Dimensionality-driven learning with noisy labels. arXiv\n",
      "preprint arXiv:1806.02612, 2018.\n",
      "\n",
      "10\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "Duc Tam Nguyen, Zhongyu Lou, Michael Klar, and Thomas Brox. Anomaly detection with\n",
      "multiple-hypotheses predictions. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Pro-\n",
      "ceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings\n",
      "of Machine Learning Research, pp. 4800–4809, Long Beach, California, USA, 09–15 Jun 2019a.\n",
      "PMLR. URL http://proceedings.mlr.press/v97/nguyen19b.html.\n",
      "\n",
      "Tam Nguyen, Maximilian Dax, Chaithanya Kumar Mummadi, Nhung Ngo, Thi Hoai Phuong\n",
      "Nguyen, Zhongyu Lou, and Thomas Brox. Deepusps: Deep robust unsupervised saliency predic-\n",
      "tion via self-supervision. In Advances in Neural Information Processing Systems, pp. 204–214,\n",
      "2019b.\n",
      "\n",
      "Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito,\n",
      "Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in\n",
      "pytorch. 2017.\n",
      "\n",
      "Giorgio Patrini, Alessandro Rozza, Aditya Krishna Menon, Richard Nock, and Lizhen Qu. Making\n",
      "deep neural networks robust to label noise: A loss correction approach. In Proceedings of the\n",
      "IEEE Conference on Computer Vision and Pattern Recognition, pp. 1944–1952, 2017.\n",
      "\n",
      "Scott Reed, Honglak Lee, Dragomir Anguelov, Christian Szegedy, Dumitru Erhan, and Andrew\n",
      "Rabinovich. Training deep neural networks on noisy labels with bootstrapping. arXiv preprint\n",
      "arXiv:1412.6596, 2014.\n",
      "\n",
      "Mengye Ren, Wenyuan Zeng, Bin Yang, and Raquel Urtasun. Learning to Reweight Examples for\n",
      "Robust Deep Learning. arXiv:1803.09050 [cs, stat], March 2018. URL http://arxiv.org/\n",
      "abs/1803.09050. arXiv: 1803.09050.\n",
      "\n",
      "David Rolnick, Andreas Veit, Serge Belongie, and Nir Shavit. Deep learning is robust to massive\n",
      "\n",
      "label noise. arXiv preprint arXiv:1705.10694, 2017.\n",
      "\n",
      "Ilya Sutskever, James Martens, George E Dahl, and Geoffrey E Hinton. On the importance of\n",
      "\n",
      "initialization and momentum in deep learning. ICML (3), 28(1139-1147):5, 2013.\n",
      "\n",
      "Daiki Tanaka, Daiki Ikami, Toshihiko Yamasaki, and Kiyoharu Aizawa. Joint optimization frame-\n",
      "work for learning with noisy labels. In Proceedings of the IEEE Conference on Computer Vision\n",
      "and Pattern Recognition, pp. 5552–5560, 2018.\n",
      "\n",
      "Antti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged consis-\n",
      "tency targets improve semi-supervised deep learning results. In Advances in neural information\n",
      "processing systems, pp. 1195–1204, 2017.\n",
      "\n",
      "Sunil Thulasidasan, Tanmoy Bhattacharya, Jeff Bilmes, Gopinath Chennupati, and Jamal Mohd-\n",
      "Yusof. Combating label noise in deep learning using abstention. arXiv preprint arXiv:1905.10964,\n",
      "2019.\n",
      "\n",
      "Yisen Wang, Weiyang Liu, Xingjun Ma, James Bailey, Hongyuan Zha, Le Song, and Shu-Tao Xia.\n",
      "\n",
      "Iterative learning with open-set noisy labels. arXiv preprint arXiv:1804.00092, 2018.\n",
      "\n",
      "Yisen Wang, Xingjun Ma, Zaiyi Chen, Yuan Luo, Jinfeng Yi, and James Bailey. Symmetric cross en-\n",
      "tropy for robust learning with noisy labels. In Proceedings of the IEEE International Conference\n",
      "on Computer Vision, pp. 322–330, 2019.\n",
      "\n",
      "Saining Xie, Ross Girshick, Piotr Doll´ar, Zhuowen Tu, and Kaiming He. Aggregated residual trans-\n",
      "formations for deep neural networks. In Proceedings of the IEEE Conference on Computer Vision\n",
      "and Pattern Recognition, pp. 1492–1500, 2017.\n",
      "\n",
      "Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding\n",
      "deep learning requires rethinking generalization. In International Conference on Learning Rep-\n",
      "resentations, 2017. URL https://openreview.net/forum?id=Sy8gdB9xx.\n",
      "\n",
      "Zhilu Zhang and Mert Sabuncu. Generalized cross entropy loss for training deep neural networks\n",
      "with noisy labels. In Advances in Neural Information Processing Systems, pp. 8778–8788, 2018.\n",
      "\n",
      "11\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "A APPENDIX\n",
      "\n",
      "A.1 MEAN TEACHER MODEL FOR ITERATIVE FILTERING\n",
      "\n",
      "We apply the Mean Teacher algorithm in each iteration i in the train(Df ilter, Dval) procedure as\n",
      "follows.\n",
      "\n",
      "• Input: examples with potentially clean labels Df ilter from the ﬁltering procedure. In the\n",
      "\n",
      "beginning (i = 0), here Df ilter refers to entire labeled dataset.\n",
      "• Initialize a supervised neural network as the student model M s\n",
      "i .\n",
      "• Initialize the Mean Teacher model M t\n",
      "\n",
      "i as a copy of the student model with all weights\n",
      "\n",
      "detached.\n",
      "\n",
      "• Let the loss function be the sum of normal classiﬁcation loss of M s\n",
      "\n",
      "i and the consistency\n",
      "\n",
      "loss between the outputs of M t\n",
      "\n",
      "i and M t\n",
      "i\n",
      "\n",
      "• Select an optimizer\n",
      "• In each training iteration:\n",
      "\n",
      "– Update the weights of M s\n",
      "– Update the weights of M t\n",
      "– Evaluate performance of M s\n",
      "\n",
      "i using the selected optimizer\n",
      "i as an exponential moving-average of the student weights\n",
      "i over Dval to verify the early stopping criteria.\n",
      "\n",
      "i and M t\n",
      "\n",
      "• Return the best M t\n",
      "i\n",
      "\n",
      "A.2 ASSUMPTIONS DICUSSIONS\n",
      "\n",
      "Our method performs best when the following assumptions are hold.\n",
      "\n",
      "Natural robustness assumption of deep networks\n",
      "(Rolnick et al., 2017): The networks attempt\n",
      "to learn the easiest way to explain most of the data. SELF uses this assumption to kickstart the\n",
      "learning process.\n",
      "\n",
      "Correct samples dominate over wrongly labeled samples At 80% noise on CIFAR-10, the cor-\n",
      "rectly labeled cats (20% out of all cat images) still dominates over samples wrongly labeled as cat\n",
      "(8.8% for each class).\n",
      "\n",
      "Independence results in less overﬁtting SELF performs best if the noises on the validation set\n",
      "and training set are i.i.d. . SELF uses the validation data for early stopping. Hence, a high correlation\n",
      "of label noise between train and valid increases the chance of model overﬁtting.\n",
      "\n",
      "Sufﬁcient label randomness assumption The subset of all correctly labeled samples capture all\n",
      "samples clusters. In fact, many works from the active learning literature show that less than 100\n",
      "% of the labeled samples are required to achieve the highest model performance. SELF performs\n",
      "progressive expansion of the correct labels sets. At larger noise ratios, not all clusters are covered\n",
      "by the identiﬁed samples. Therefore on task containing many classes, e.g., CIFAR-100, the model\n",
      "performance decreases faster than on CIFAR-10.\n",
      "\n",
      "The model performance reduces when these assumptions are strongly violated. Each assumption\n",
      "should have its own ”critical” threshold for violation. A future in-depth analysis to challenge the\n",
      "assumptions is an interesting future research direction.\n",
      "\n",
      "A.3 TRAINING DETAILS\n",
      "\n",
      "A.3.1 CIFAR-10 AND CIFAR-100\n",
      "\n",
      "Dataset Tab. 7 shows the details of CIFAR-10 and 100 datasets in our evaluation pipeline. The\n",
      "validation set is contaminated with the same noise ratio as the training data unless stated otherwise.\n",
      "\n",
      "12\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "Network training For the training our model SELF, we use the standard conﬁguration provided\n",
      "by Tarvainen & Valpola (2017) 2. Concretely, we use the SGD-optimizer with Nesterov Sutskever\n",
      "et al. (2013) momentum, a learning rate of 0.05 with cosine learning rate annealing Loshchilov &\n",
      "Hutter (2016), a weight decay of 2e-4, max iteration per ﬁltering step of 300, patience of 50 epochs,\n",
      "total epochs count of 600.\n",
      "\n",
      "Table 7: Dataset description. Classiﬁcation tasks on CIFAR-10 and CIFAR-100 with uniform noise.\n",
      "Note that the noise on the training and validation set is not correlated. Hence, maximizing the\n",
      "accuracy on the noisy set provides a useful (but noisy) estimate for the generalization ability on\n",
      "unseen test data.\n",
      "\n",
      "TASK\n",
      "RESOLUTION\n",
      "\n",
      "DATA\n",
      "\n",
      "TYPE\n",
      "\n",
      "CIFAR-10\n",
      "\n",
      "CIFAR-100\n",
      "\n",
      "CLASSIFICATION\n",
      "\n",
      "100-WAY\n",
      "\n",
      "TRAIN (NOISY)\n",
      "VALID (NOISY)\n",
      "TEST (CLEAN)\n",
      "\n",
      "10-WAY\n",
      "\n",
      "32X32\n",
      "\n",
      "45000\n",
      "5000\n",
      "10000\n",
      "\n",
      "45000\n",
      "5000\n",
      "10000\n",
      "\n",
      "For basic training of baselines models without semi-supervised learning, we had to set the learning\n",
      "rate to 0.01. In the case of higher learning rates, the loss typically explodes. Every other option is\n",
      "kept the same.\n",
      "\n",
      "Semi-supervised learning For the mean teacher training, additional hyperparameters are required.\n",
      "In both cases of CIFAR-10 and CIFAR-100, we again take the standard conﬁguration with the con-\n",
      "sistency loss to mean-squared-error and a consistency weight: 100.0, logit distance cost: 0.01,\n",
      "consistency-ramp-up:5. The total batch-size is 512, with 124 samples being reserved for labeled\n",
      "samples, 388 for unlabeled data. Each epoch is deﬁned as a complete processing of all unlabeled\n",
      "data. When training without semi-supervised-learning, the entire batch is used for labeled data.\n",
      "\n",
      "Data augmentation The data are normalized to zero-mean and standard-variance of one. Further,\n",
      "we use real-time data augmentation with random translation and reﬂection, subsequently random\n",
      "horizontal ﬂip. The standard PyTorch-library provides these transformations.\n",
      "\n",
      "A.3.2\n",
      "\n",
      "IMAGENET-ILSVRC-2015\n",
      "\n",
      "Network Training The network used for evaluation were ResNet He et al. (2016) and Resnext Xie\n",
      "et al. (2017) for training. All ResNext variants use a cardinality of 32 and base width of 4 (32x4d).\n",
      "ResNext models follow the same structure as their Resnet counterparts, except for the cardinality\n",
      "and base width.\n",
      "\n",
      "All other conﬁgurations are kept as close as possible to Tarvainen & Valpola (2017). The initial\n",
      "learning rate to handle large batches Goyal et al. (2017) is set to 0.1; the base learning rate is 0.025\n",
      "with a single cycle of cosine annealing.\n",
      "\n",
      "Semi-supervised learning Due to the large images, the batch size is set to 40 in total with 20/20\n",
      "for labeled and unlabeled samples, respectively. We found the Kullback-divergence leads to no\n",
      "meaningful network training. Hence, we set the consistency loss to mean-squared-error, with a\n",
      "weight of 1000. We use consistency ramp up of 5 epochs to give the mean teacher more time in\n",
      "the beginning. Weight decay is set to 5e-5; patience is four epochs to stop training in the current\n",
      "ﬁltering iteration.\n",
      "\n",
      "Filtering We ﬁlter noisy samples with the topk=5 strategy, instead of taking the maximum-\n",
      "likelihood (ML) prediction as on CIFAR-10 and CIFAR-100. That means the samples are kept for\n",
      "supervised training if their provided label lies within the top 5 predictions of the model. The main\n",
      "\n",
      "2https://github.com/CuriousAI/mean-teacher\n",
      "\n",
      "13\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 5: Simple training losses to counter label noise. (a) shows the prediction of a sample given\n",
      "a model. The red bar indicates the noisy label, blue the correct one. Arrows depict the magnitude\n",
      "of the gradients (b) Typical losses reweighting schemes are not wrong but suffer from the gradient\n",
      "vanishing problem. Non-linear losses such as Negative-log-likelihood are not designed for gradient\n",
      "ascent.\n",
      "\n",
      "reason is that each image of ImageNet might contain multiple objects. Filtering with ML-predictions\n",
      "is too strict and would lead to a small recall of the detection of the correct sample.\n",
      "\n",
      "Data Augmentation For all data, we normalize the RGB-images by the mean: (0.485, 0.456,\n",
      "0.406) and the standard variance (0.229, 0.224, 0.225). For training data, we perform a random\n",
      "rotation of up to 10 degrees, randomly resize images to 224x224, apply random horizontal ﬂip\n",
      "and random color jittering. This noise is needed in regular mean-teacher training. The jittering\n",
      "setting are: brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1. The validation data are resized\n",
      "to 256x256 and randomly cropped to 224x224\n",
      "\n",
      "A.3.3 SEMI-SUPERVISED LOSSES\n",
      "\n",
      "For the learning of wrongly labeled samples, Fig. 6 shows the relationship between the typical\n",
      "reweighting scheme and our baseline push-away-loss. Typically, reweighting is applied directly to\n",
      "the losses with samples weights w(k) for each sample k as shown in Eq. 4\n",
      "\n",
      "min w(k)\n",
      "\n",
      "i N LL(y(k)\n",
      "\n",
      "label|x(k), D)\n",
      "\n",
      "D is the dataset, x(k) and y(k)\n",
      "the sample k at step i. Negative samples weights w(k)\n",
      "from the wrong labels. Let w(k)\n",
      "\n",
      "i with c(k)\n",
      "\n",
      "i = −c(k)\n",
      "\n",
      "i\n",
      "\n",
      "label are the samples k and its noisy label. w(k)\n",
      "\n",
      "i > 0, then we have:\n",
      "\n",
      "is the samples weight for\n",
      "are often assigned to push the network away\n",
      "\n",
      "i\n",
      "\n",
      "Which results in:\n",
      "\n",
      "min −c(k)\n",
      "\n",
      "i N LL(y(k)\n",
      "\n",
      "label|x(k), D)\n",
      "\n",
      "max c(k)\n",
      "\n",
      "i N LL(y(k)\n",
      "\n",
      "label|x(k), D)\n",
      "\n",
      "In other words, we perform gradient ascent for wrongly labeled samples. However, the Negative-\n",
      "log-likelihood is not designed for gradient ascent. Hence the gradients of wrongly labeled samples\n",
      "vanish if the prediction is too close to the noisy label. This effect is similar to the training of\n",
      "Generative Adversarial Network (GAN) Goodfellow et al.. In the GAN-framework, the generator\n",
      "loss is not simply set to the negated version of the discriminator’s loss for the same reason.\n",
      "\n",
      "Therefore, to provide a fair comparison with our framework, we suggest the push-away-loss\n",
      "LP ush−away(y(k)\n",
      "\n",
      "label, x(k), D) with improved gradients as follows:\n",
      "\n",
      "(1)\n",
      "\n",
      "(2)\n",
      "\n",
      "(3)\n",
      "\n",
      "(4)\n",
      "\n",
      "1\n",
      "\n",
      "(cid:88)\n",
      "\n",
      "min\n",
      "\n",
      "|Y |−1\n",
      "\n",
      "y,y(cid:54)=y(k)\n",
      "\n",
      "label\n",
      "\n",
      "c(k)\n",
      "i N LL(y|x(k), D)\n",
      "\n",
      "Whereby Y is the set of all classes in the training set. This loss has improved gradients to push the\n",
      "model away from the potentially wrong labels.\n",
      "\n",
      "14\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "(a) Extreme predictions such as [0, 1]\n",
      "Figure 6: The entropy loss for semi-supervised learning.\n",
      "are encouraged by minimizing the entropy on each prediction. (b) Additionally, maximizing the\n",
      "entropy of the mean prediction on the entire dataset or a large batch forces the model to balance its\n",
      "predictions over multiple samples.\n",
      "\n",
      "Table 8: Accuracy of the complete removal of samples during iterative ﬁltering on CIFAR-10 and\n",
      "CIFAR-100. The underlying model is the MeanTeacher based on Resnet26. When samples are com-\n",
      "pletely removed from the training set, they are no longer used for either supervised-or-unsupervised\n",
      "learning. This common strategy from previous works leads to rapid performance breakdown.\n",
      "\n",
      "NOISE RATIO\n",
      "\n",
      "CIFAR-10\n",
      "\n",
      "40%\n",
      "\n",
      "80 %\n",
      "\n",
      "CIFAR-100\n",
      "40%\n",
      "80 %\n",
      "\n",
      "USING NOISY DATA ONLY\n",
      "\n",
      "DATA REMOVAL\n",
      "SELF (OURS)\n",
      "\n",
      "93.4\n",
      "93.7\n",
      "\n",
      "59.98\n",
      "69.91\n",
      "\n",
      "68.99\n",
      "71.98\n",
      "\n",
      "35.53\n",
      "42.09\n",
      "\n",
      "WITH CLEAN VALIDATION SET\n",
      "\n",
      "COMPL. REMOVAL\n",
      "SELF (OURS)\n",
      "\n",
      "94.39\n",
      "95.1\n",
      "\n",
      "70.93\n",
      "79.93\n",
      "\n",
      "71.86\n",
      "74.76\n",
      "\n",
      "36.61\n",
      "46.43\n",
      "\n",
      "Entropy minimization The typical entropy loss for semi-supervised learning is shown in Fig. 6.\n",
      "It encourages the model to provide extreme predictions (such as 0 or 1) for each sample. Over a\n",
      "large number of samples, the model should balance its predictions over all classes.\n",
      "\n",
      "The entropy loss can easily be applied to all samples to express the uncertainty about the provided\n",
      "labels. Alternatively, the loss can be combined with a strict ﬁltering strategy, as in our work, which\n",
      "removes the labels of potentially wrongly labeled samples.\n",
      "\n",
      "For a large noise ratio, predictions of wrongly labeled samples ﬂuctuate strongly over previous\n",
      "training iterations. Amplifying these network decisions could lead to even noisier models model.\n",
      "Combined with iterative ﬁltering, the framework will have to rely on a single noisy model snapshot.\n",
      "In the case of an unsuitable snapshot, the ﬁltering step will make many wrong decisions.\n",
      "\n",
      "A.4 MORE EXPERIMENTS RESULTS\n",
      "\n",
      "A.4.1 COMPLETE REMOVAL OF SAMPLES\n",
      "\n",
      "Tab. 8 shows the results of deleting samples from the training set. It leads to signiﬁcant performances\n",
      "gaps compared to our strategy (SELF), which considers the removed samples as unlabeled data. In\n",
      "case of a considerable label noise of 80%, the gap is close to 9%.\n",
      "\n",
      "Continuously using the ﬁltered samples lead to signiﬁcantly better results. The unsupervised-loss\n",
      "provides meaningful learning signals, which should be used for better model training.\n",
      "\n",
      "15\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 7: Sample training curves of our approach SELF on CIFAR-100 with (a) 60% and (b) 80%\n",
      "noise, using noisy validation data. Note that with our approach, the training loss remains close to 0.\n",
      "Further, note that the mean-teacher continously outperforms the noisy student models. This shows\n",
      "the positive effect of temporal emsembling to counter label noise.\n",
      "\n",
      "A.4.2 SAMPLE TRAINING PROCESS\n",
      "\n",
      "Fig. 7 shows the sample training processes of SELF under 60% and 80% noise on CIFAR-100. The\n",
      "mean-teacher always outperform the student models. Further, note that regular training leads to\n",
      "rapid over-ﬁtting to label noise.\n",
      "\n",
      "Contrary, with our effective ﬁltering strategy, both models slowly increase their performance while\n",
      "the training accuracy approaches 100%. Hence, by using progressive ﬁltering, our model could\n",
      "erase the inconsistency in the provided labels set.\n",
      "\n",
      "16\n",
      "\n",
      "\f",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "SHARING KNOWLEDGE IN MULTI-TASK\n",
      "DEEP REINFORCEMENT LEARNING\n",
      "\n",
      "Carlo D’Eramo & Davide Tateo\n",
      "Department of Computer Science\n",
      "TU Darmstadt, IAS\n",
      "Hochschulstraße 10, 64289, Darmstadt, Germany\n",
      "{carlo.deramo,davide.tateo}@tu-darmstadt.de\n",
      "\n",
      "Andrea Bonarini & Marcello Restelli\n",
      "Politecnico di Milano, DEIB\n",
      "Piazza Leonardo da Vinci 32, 20133, Milano\n",
      "{andrea.bonarini,marcello.restelli}@polimi.it\n",
      "\n",
      "Jan Peters\n",
      "TU Darmstadt, IAS\n",
      "Hochschulstraße 10, 64289, Darmstadt, Germany\n",
      "Max Planck Institute for Intelligent Systems\n",
      "Max-Planck-Ring 4, 72076, Tübingen, Germany\n",
      "jan.peters@tu-darmstadt.de\n",
      "\n",
      "ABSTRACT\n",
      "\n",
      "We study the beneﬁt of sharing representations among tasks to enable the effective\n",
      "use of deep neural networks in Multi-Task Reinforcement Learning. We leverage\n",
      "the assumption that learning from different tasks, sharing common properties, is\n",
      "helpful to generalize the knowledge of them resulting in a more effective feature ex-\n",
      "traction compared to learning a single task. Intuitively, the resulting set of features\n",
      "offers performance beneﬁts when used by Reinforcement Learning algorithms.\n",
      "We prove this by providing theoretical guarantees that highlight the conditions\n",
      "for which is convenient to share representations among tasks, extending the well-\n",
      "known ﬁnite-time bounds of Approximate Value-Iteration to the multi-task setting.\n",
      "In addition, we complement our analysis by proposing multi-task extensions of\n",
      "three Reinforcement Learning algorithms that we empirically evaluate on widely\n",
      "used Reinforcement Learning benchmarks showing signiﬁcant improvements over\n",
      "the single-task counterparts in terms of sample efﬁciency and performance.\n",
      "\n",
      "1\n",
      "\n",
      "INTRODUCTION\n",
      "\n",
      "Multi-Task Learning (MTL) ambitiously aims to learn multiple tasks jointly instead of learning them\n",
      "separately, leveraging the assumption that the considered tasks have common properties which can be\n",
      "exploited by Machine Learning (ML) models to generalize the learning of each of them. For instance,\n",
      "the features extracted in the hidden layers of a neural network trained on multiple tasks have the\n",
      "advantage of being a general representation of structures common to each other. This translates into\n",
      "an effective way of learning multiple tasks at the same time, but it can also improve the learning\n",
      "of each individual task compared to learning them separately (Caruana, 1997). Furthermore, the\n",
      "learned representation can be used to perform Transfer Learning (TL), i.e. using it as a preliminary\n",
      "knowledge to learn a new similar task resulting in a more effective and faster learning than learning\n",
      "the new task from scratch (Baxter, 2000; Thrun & Pratt, 2012).\n",
      "\n",
      "The same beneﬁts of extraction and exploitation of common features among the tasks achieved\n",
      "in MTL, can be obtained in Multi-Task Reinforcement Learning (MTRL) when training a single\n",
      "agent on multiple Reinforcement Learning (RL) problems with common structures (Taylor & Stone,\n",
      "2009; Lazaric, 2012). In particular, in MTRL an agent can be trained on multiple tasks in the same\n",
      "\n",
      "1\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "domain, e.g. riding a bicycle or cycling while going towards a goal, or on different but similar\n",
      "domains, e.g. balancing a pendulum or balancing a double pendulum1. Considering recent advances\n",
      "in Deep Reinforcement Learning (DRL) and the resulting increase in the complexity of experimental\n",
      "benchmarks, the use of Deep Learning (DL) models, e.g. deep neural networks, has become a popular\n",
      "and effective way to extract common features among tasks in MTRL algorithms (Rusu et al., 2015;\n",
      "Liu et al., 2016; Higgins et al., 2017). However, despite the high representational capacity of DL\n",
      "models, the extraction of good features remains challenging. For instance, the performance of the\n",
      "learning process can degrade when unrelated tasks are used together (Caruana, 1997; Baxter, 2000);\n",
      "another detrimental issue may occur when the training of a single model is not balanced properly\n",
      "among multiple tasks (Hessel et al., 2018).\n",
      "\n",
      "Recent developments in MTRL achieve signiﬁcant results in feature extraction by means of algorithms\n",
      "speciﬁcally developed to address these issues. While some of these works rely on a single deep\n",
      "neural network to model the multi-task agent (Liu et al., 2016; Yang et al., 2017; Hessel et al., 2018;\n",
      "Wulfmeier et al., 2019), others use multiple deep neural networks, e.g. one for each task and another\n",
      "for the multi-task agent (Rusu et al., 2015; Parisotto et al., 2015; Higgins et al., 2017; Teh et al., 2017).\n",
      "Intuitively, achieving good results in MTRL with a single deep neural network is more desirable\n",
      "than using many of them, since the training time is likely much less and the whole architecture is\n",
      "easier to implement. In this paper we study the beneﬁts of shared representations among tasks. We\n",
      "theoretically motivate the intuitive effectiveness of our method, deriving theoretical guarantees that\n",
      "exploit the theoretical framework provided by Maurer et al. (2016), in which the authors present\n",
      "upper bounds on the quality of learning in MTL when extracting features for multiple tasks in a\n",
      "single shared representation. The signiﬁcancy of this result is that the cost of learning the shared\n",
      "representation decreases with a factor O(1/\n",
      "T ), where T is the number of tasks for many function\n",
      "approximator hypothesis classes. The main contribution of this work is twofold.\n",
      "\n",
      "√\n",
      "\n",
      "1. We derive upper conﬁdence bounds for Approximate Value-Iteration (AVI) and Approximate\n",
      "Policy-Iteration (API)2 (Farahmand, 2011) in the MTRL setting, and we extend the approx-\n",
      "imation error bounds in Maurer et al. (2016) to the case of multiple tasks with different\n",
      "dimensionalities. Then, we show how to combine these results resulting in, to the best\n",
      "of our knowledge, the ﬁrst proposed extension of the ﬁnite-time bounds of AVI/API to\n",
      "MTRL. Despite being an extension of previous works, we derive these results to justify\n",
      "our approach showing how the error propagation in AVI/API can theoretically beneﬁt from\n",
      "learning multiple tasks jointly.\n",
      "\n",
      "2. We leverage these results proposing a neural network architecture, for which these bounds\n",
      "hold with minor assumptions, that allow us to learn multiple tasks with a single regressor\n",
      "extracting a common representation. We show an empirical evidence of the consequence of\n",
      "our bounds by means of a variant of Fitted Q-Iteration (FQI) (Ernst et al., 2005), based on our\n",
      "shared network and for which our bounds apply, that we call Multi Fitted Q-Iteration (MFQI).\n",
      "Then, we perform an empirical evaluation in challenging RL problems proposing multi-\n",
      "task variants of the Deep Q-Network (DQN) (Mnih et al., 2015) and Deep Deterministic\n",
      "Policy Gradient (DDPG) (Lillicrap et al., 2015) algorithms. These algorithms are practical\n",
      "implementations of the more general AVI/API framework, designed to solve complex\n",
      "problems. In this case, the bounds apply to these algorithms only with some assumptions,\n",
      "e.g. stationary sampling distribution. The outcome of the empirical analysis joins the\n",
      "theoretical results, showing signiﬁcant performance improvements compared to the single-\n",
      "task version of the algorithms in various RL problems, including several MuJoCo (Todorov\n",
      "et al., 2012) domains.\n",
      "\n",
      "2 PRELIMINARIES\n",
      "\n",
      "Let B(X ) be the space of bounded measurable functions w.r.t. the σ-algebra σX , and similarly\n",
      "B(X , L) be the same bounded by L < ∞.\n",
      "\n",
      "A Markov Decision Process (MDP) is deﬁned as a 5-tuple M =< S, A, P, R, γ >, where S is the\n",
      "state space, A is the action space, P : S × A → S is the transition distribution where P(s(cid:48)|s, a)\n",
      "\n",
      "1For simplicity, in this paper we refer to the concepts of task and domain interchangeably.\n",
      "2All proofs and the theorem for API are in Appendix A.2.\n",
      "\n",
      "2\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "is the probability of reaching state s(cid:48) when performing action a in state s, R : S × A × S →\n",
      "R is the reward function, and γ ∈ (0, 1] is the discount factor. A deterministic policy π maps,\n",
      "for each state, the action to perform: π : S → A. Given a policy π, the value of an action\n",
      "a in a state s represents the expected discounted cumulative reward obtained by performing a\n",
      "in s and following π thereafter: Qπ(s, a) (cid:44) E[(cid:80)∞\n",
      "k=0 γkri+k+1|si = s, ai = a, π], where ri+1\n",
      "is the reward obtained after the i-th transition. The expected discounted cumulative reward is\n",
      "maximized by following the optimal policy π∗ which is the one that determines the optimal action\n",
      "values, i.e., the ones that satisfy the Bellman optimality equation (Bellman, 1954): Q∗(s, a) (cid:44)\n",
      "(cid:82)\n",
      "S P(s(cid:48)|s, a) [R(s, a, s(cid:48)) + γ maxa(cid:48) Q∗(s(cid:48), a(cid:48))] ds(cid:48). The solution of the Bellman optimality equation\n",
      "is the ﬁxed point of the optimal Bellman operator T ∗ : B(S × A) → B(S × A) deﬁned as\n",
      "(T ∗Q)(s, a) (cid:44) (cid:82)\n",
      "S P(s(cid:48)|s, a)[R(s, a, s(cid:48)) + γ maxa(cid:48) Q(s(cid:48), a(cid:48))]ds(cid:48). In the MTRL setting, there are\n",
      "multiple MDPs M(t) =< S (t), A(t), P (t), R(t), γ(t) > where t ∈ {1, . . . , T } and T is the number\n",
      "of MDPs. For each MDP M(t), a deterministic policy πt : S (t) → A(t) induces an action-value\n",
      "function Qπt\n",
      "i+k+1|si = s(t), ai = a(t), πt]. In this setting, the goal is to\n",
      "maximize the sum of the expected cumulative discounted reward of each task.\n",
      "\n",
      "t (s(t), a(t)) = E[(cid:80)∞\n",
      "\n",
      "k=0 γkr(t)\n",
      "\n",
      "In our theoretical analysis of the MTRL problem, the complexity of representation plays a central role.\n",
      "As done in Maurer et al. (2016), we consider the Gaussian complexity, a variant of the well-known\n",
      "Rademacher complexity, to measure the complexity of the representation. Given a set ¯X ∈ X T n of n\n",
      "input samples for each task t ∈ {1, . . . , T }, and a class H composed of k ∈ {1, . . . , K} functions,\n",
      "the Gaussian complexity of a random set H( ¯X) = {(hk(Xti)) : h ∈ H} ⊆ RKT n is deﬁned as\n",
      "follows:\n",
      "\n",
      "G(H( ¯X)) = E\n",
      "\n",
      "γtkihk(Xti)\n",
      "\n",
      "(cid:34)\n",
      "\n",
      "sup\n",
      "h∈H\n",
      "\n",
      "(cid:88)\n",
      "\n",
      "tki\n",
      "\n",
      "(cid:35)\n",
      "\n",
      ",\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "Xti\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "where γtki are independent standard normal variables. We also need to deﬁne the following quantity,\n",
      "taken from Maurer (2016): let γ be a vector of m random standard normal variables, and f ∈ F :\n",
      "Y → Rm, with Y ⊆ Rn, we deﬁne\n",
      "\n",
      "O(F) =\n",
      "\n",
      "sup\n",
      "\n",
      "y,y(cid:48)∈Y,y(cid:54)=y(cid:48)\n",
      "\n",
      "(cid:34)\n",
      "\n",
      "E\n",
      "\n",
      "sup\n",
      "f ∈F\n",
      "\n",
      "(cid:104)γ, f (y) − f (y(cid:48))(cid:105)\n",
      "\n",
      "(cid:107)y − y(cid:48)(cid:107)\n",
      "\n",
      "(cid:35)\n",
      "\n",
      ".\n",
      "\n",
      "(1)\n",
      "\n",
      "(2)\n",
      "\n",
      "Equation 2 can be viewed as a Gaussian average of Lipschitz quotients, and appears in the bounds\n",
      "provided in this work. Finally, we deﬁne L(F) as the upper bound of the Lipschitz constant of all the\n",
      "functions f in the function class F.\n",
      "\n",
      "3 THEORETICAL ANALYSIS\n",
      "\n",
      "The following theoretical study starts from the derivation of theoretical guarantees for MTRL in the\n",
      "AVI framework, extending the results of Farahmand (2011) in the MTRL scenario. Then, to bound\n",
      "the approximation error term in the AVI bound, we extend the result described in Maurer (2006)\n",
      "to MTRL. As we discuss, the resulting bounds described in this section clearly show the beneﬁt of\n",
      "sharing representation in MTRL. To the best of our knowledge, this is the ﬁrst general result for\n",
      "MTRL; previous works have focused on ﬁnite MDPs (Brunskill & Li, 2013) or linear models (Lazaric\n",
      "& Restelli, 2011).\n",
      "\n",
      "3.1 MULTI-TASK REPRESENTATION LEARNING\n",
      "\n",
      "The multi-task representation learning problem consists in learning simultaneously a set of T tasks\n",
      "µt, modeled as probability measures over the space of the possible input-output pairs (x, y), with\n",
      "x ∈ X and y ∈ R, being X the input space. Let w ∈ W : X → RJ , h ∈ H : RJ → RK and\n",
      "f ∈ F : RK → R be functions chosen from their respective hypothesis classes. The functions\n",
      "in the hypothesis classes must be Lipschitz continuous functions. Let ¯Z = (Z1, . . . , ZT ) be the\n",
      "multi-sample over the set of tasks µ = (µ1, . . . , µT ), where Zt = (Zt1, . . . , Ztn) ∼ µn\n",
      "t and\n",
      "Zti = (Xti, Yti) ∼ µt. We can formalize our regression problem as the following minimization\n",
      "\n",
      "3\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "problem:\n",
      "\n",
      "min\n",
      "\n",
      "(cid:40)\n",
      "\n",
      "1\n",
      "nT\n",
      "\n",
      "T\n",
      "(cid:88)\n",
      "\n",
      "N\n",
      "(cid:88)\n",
      "\n",
      "t=1\n",
      "\n",
      "i=1\n",
      "\n",
      "(cid:96)(ft(h(wt(Xti))), Yti) : f ∈ F T , h ∈ H, w ∈ W T\n",
      "\n",
      ",\n",
      "\n",
      "(3)\n",
      "\n",
      "(cid:41)\n",
      "\n",
      "where we use f = (f1, . . . , fT ), w = (w1, . . . , wT ), and deﬁne the minimizers of Equation (3) as ˆw,\n",
      "ˆh, and ˆf . We assume that the loss function (cid:96) : R × R → [0, 1] is 1-Lipschitz in the ﬁrst argument for\n",
      "every value of the second argument. While this assumption may seem restrictive, the result obtained\n",
      "can be easily scaled to the general case. To use the principal result of this section, for a generic loss\n",
      "function (cid:96)(cid:48), it is possible to use (cid:96)(·) = (cid:96)(cid:48)(·)/(cid:15)max, where (cid:15)max is the maximum value of (cid:96)(cid:48). The expected\n",
      "loss over the tasks, given w, h and f is the task-averaged risk:\n",
      "\n",
      "εavg(w, h, f ) =\n",
      "\n",
      "E [(cid:96)(ft(h(wt(X))), Y )]\n",
      "\n",
      "(4)\n",
      "\n",
      "1\n",
      "T\n",
      "\n",
      "T\n",
      "(cid:88)\n",
      "\n",
      "t=1\n",
      "\n",
      "The minimum task-averaged risk, given the set of tasks µ and the hypothesis classes W, H and F is\n",
      "ε∗\n",
      "avg, and the corresponding minimizers are w∗, h∗ and f ∗.\n",
      "\n",
      "3.2 MULTI-TASK APPROXIMATE VALUE ITERATION BOUND\n",
      "\n",
      "We start by considering the bound for the AVI framework which applies for the single-task scenario.\n",
      "Theorem 1. (Theorem 3.4 of Farahmand (2011)) Let K be a positive integer, and Qmax ≤ Rmax\n",
      "1−γ . Then\n",
      "k=0 ⊂ B(S × A, Qmax) and the corresponding sequence (εk)K−1\n",
      "for any sequence (Qk)K\n",
      "k=0 , where\n",
      "εk = (cid:107)Qk+1 − T ∗Qk(cid:107)2\n",
      "ν, we have:\n",
      "\n",
      "(cid:107)Q∗ − QπK (cid:107)1,ρ ≤\n",
      "\n",
      "2γ\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "(1 − γ)2\n",
      "\n",
      "1\n",
      "2\n",
      "\n",
      "inf\n",
      "\n",
      "r∈[0,1]\n",
      "\n",
      "C\n",
      "\n",
      "VI,ρ,ν(K; r)E\n",
      "\n",
      "2 (ε0, . . . , εK−1; r) +\n",
      "\n",
      "(cid:21)\n",
      "\n",
      "γKRmax\n",
      "\n",
      ",\n",
      "\n",
      "(5)\n",
      "\n",
      "2\n",
      "\n",
      "1 − γ\n",
      "\n",
      "where\n",
      "\n",
      "CVI,ρ,ν(K; r) =\n",
      "\n",
      "(cid:18) 1 − γ\n",
      "\n",
      "(cid:19)2\n",
      "\n",
      "2\n",
      "\n",
      "sup\n",
      "1,...,π(cid:48)\n",
      "π(cid:48)\n",
      "K\n",
      "\n",
      "K−1\n",
      "(cid:88)\n",
      "\n",
      "k=0\n",
      "\n",
      "a2(1−r)\n",
      "k\n",
      "\n",
      "(cid:88)\n",
      "\n",
      "γm(cid:16)\n",
      "\n",
      "m≥0\n",
      "\n",
      "cVI1,ρ,ν(m, K − k; π(cid:48)\n",
      "\n",
      "K)\n",
      "\n",
      "1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "+cVI2,ρ,ν(m + 1; π(cid:48)\n",
      "\n",
      "k+1, . . . , π(cid:48)\n",
      "\n",
      "K)\n",
      "\n",
      "\n",
      "\n",
      ",\n",
      "\n",
      "(6)\n",
      "\n",
      "\n",
      "2\n",
      "\n",
      "(cid:17)\n",
      "\n",
      "with E(ε0, . . . , εK−1; r) = (cid:80)K−1\n",
      "and ν, and the series αk are deﬁned as in Farahmand (2011).\n",
      "\n",
      "k=0 α2r\n",
      "\n",
      "k εk, the two coefﬁcients cVI1,ρ,ν, cVI2,ρ,ν, the distributions ρ\n",
      "\n",
      "In the multi-task scenario, let the average approximation error across tasks be:\n",
      "\n",
      "εavg,k( ˆwk, ˆhk, ˆfk) =\n",
      "\n",
      "(cid:107)Qt,k+1 − T ∗\n",
      "\n",
      "t Qt,k(cid:107)2\n",
      "ν,\n",
      "\n",
      "(7)\n",
      "\n",
      "1\n",
      "T\n",
      "\n",
      "T\n",
      "(cid:88)\n",
      "\n",
      "t=1\n",
      "\n",
      "is the optimal Bellman operator of task t.\n",
      "\n",
      "where Qt,k+1 = ˆft,k ◦ ˆhk ◦ ˆwt,k, and T ∗\n",
      "t\n",
      "In the following, we extend the AVI bound of Theorem 1 to the multi-task scenario, by computing\n",
      "the average loss across tasks and pushing inside the average using Jensen’s inequality.\n",
      "Theorem 2. Let K be a positive integer, and Qmax ≤ Rmax\n",
      "1−γ . Then for any sequence (Qk)K\n",
      "A, Qmax) and the corresponding sequence (εavg,k)K−1\n",
      "we have:\n",
      "\n",
      "k=0 ⊂ B(S ×\n",
      "t Qt,k(cid:107)2\n",
      "ν,\n",
      "\n",
      "k=0 , where εavg,k =\n",
      "\n",
      "t=1(cid:107)Qt,k+1−T ∗\n",
      "\n",
      "(cid:80)T\n",
      "\n",
      "1\n",
      "T\n",
      "\n",
      "1\n",
      "T\n",
      "\n",
      "T\n",
      "(cid:88)\n",
      "\n",
      "t=1\n",
      "\n",
      "(cid:107)Q∗\n",
      "\n",
      "t − QπK\n",
      "\n",
      "t (cid:107)1,ρ ≤\n",
      "\n",
      "2γ\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "(1 − γ)2\n",
      "\n",
      "inf\n",
      "\n",
      "r∈[0,1]\n",
      "\n",
      "1\n",
      "2\n",
      "\n",
      "1\n",
      "2\n",
      "\n",
      "C\n",
      "\n",
      "VI(K; r)E\n",
      "\n",
      "avg(εavg,0, . . . , εavg,K−1; r) +\n",
      "\n",
      "2γKRmax,avg\n",
      "\n",
      "(cid:21)\n",
      "\n",
      "1 − γ\n",
      "\n",
      "(8)\n",
      "\n",
      "with Eavg = (cid:80)K−1\n",
      "\n",
      "k=0 α2r\n",
      "\n",
      "k εavg,k, γ = max\n",
      "(cid:40) (1−γ)γK−k−1\n",
      "\n",
      "t∈{1,...,T }\n",
      "\n",
      "1\n",
      "T\n",
      "\n",
      "(cid:80)T\n",
      "\n",
      "t=1 Rmax,t and αk =\n",
      "\n",
      "1−γK+1\n",
      "\n",
      "(1−γ)γK\n",
      "1−γK+1\n",
      "\n",
      "0 ≤ k < K,\n",
      "\n",
      ".\n",
      "\n",
      "k = K\n",
      "\n",
      "γt, C\n",
      "\n",
      "VI(K; r) = max\n",
      "\n",
      "C\n",
      "\n",
      "VI,ρ,ν(K; t, r), Rmax,avg =\n",
      "\n",
      "t∈{1,...,T }\n",
      "\n",
      "1\n",
      "2\n",
      "\n",
      "1\n",
      "2\n",
      "\n",
      "4\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "Remarks Theorem 2 retains most of the properties of Theorem 3.4 of Farahmand (2011), except\n",
      "that the regression error in the bound is now task-averaged. Interestingly, the second term of the\n",
      "sum in Equation (8) depends on the average maximum reward for each task. In order to obtain this\n",
      "result we use an overly pessimistic bound on γ and the concentrability coefﬁcients, however this\n",
      "approximation is not too loose if the MDPs are sufﬁciently similar.\n",
      "\n",
      "3.3 MULTI-TASK APPROXIMATION ERROR BOUND\n",
      "\n",
      "We bound the task-averaged approximation error εavg at each AVI iteration k involved in (8) following\n",
      "a derivation similar to the one proposed by Maurer et al. (2016), obtaining:\n",
      "Theorem 3. Let µ, W, H and F be deﬁned as above and assume 0 ∈ H and f (0) = 0, ∀f ∈ F.\n",
      "Then for δ > 0 with probability at least 1 − δ in the draw of ¯Z ∼ (cid:81)T\n",
      "\n",
      "t=1 µn\n",
      "\n",
      "t we have that\n",
      "\n",
      "εavg( ˆw, ˆh, ˆf ) ≤ L(F)\n",
      "\n",
      "c1\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "L(H) supl∈{1,...,T } G(W(Xl))\n",
      "\n",
      "supw(cid:107)w( ¯X)(cid:107)O(H)\n",
      "\n",
      "minp∈P G(H(p))\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "suph,w(cid:107)h(w( ¯X))(cid:107)O(F)\n",
      "\n",
      "+c3\n",
      "\n",
      "nT\n",
      "\n",
      "n\n",
      "\n",
      "+ c4\n",
      "\n",
      "+ c2\n",
      "\n",
      "√\n",
      "\n",
      "n\n",
      "\n",
      "T\n",
      "\n",
      "nT\n",
      "\n",
      "(cid:115)\n",
      "\n",
      "+\n",
      "\n",
      "8 ln( 3\n",
      "δ )\n",
      "nT\n",
      "\n",
      "+ ε∗\n",
      "\n",
      "avg.\n",
      "\n",
      "(9)\n",
      "\n",
      "√\n",
      "\n",
      "√\n",
      "\n",
      "Remarks The assumptions 0 ∈ H and f (0) = 0 for all f ∈ F are not essential for the proof and\n",
      "are only needed to simplify the result. For reasonable function classes, the Gaussian complexity\n",
      "n). If supw(cid:107)w( ¯X)(cid:107) and suph,w(cid:107)h(w( ¯X))(cid:107) can be uniformly bounded, then\n",
      "G(W(Xl)) is O(\n",
      "they are O(\n",
      "nT ). For some function classes, the Gaussian average of Lipschitz quotients O(·) can\n",
      "be bounded independently from the number of samples. Given these assumptions, the ﬁrst and the\n",
      "fourth term of the right hand side of Equation (9), which represent respectively the cost of learning the\n",
      "meta-state space w and the task-speciﬁc f mappings, are both O(1/√\n",
      "n). The second term represents\n",
      "the cost of learning the multi-task representation h and is O(1/\n",
      "nT ), thus vanishing in the multi-task\n",
      "limit T → ∞. The third term can be removed if ∀h ∈ H, ∃p0 ∈ P : h(p) = 0; even when this\n",
      "assumption does not hold, this term can be ignored for many classes of interest, e.g. neural networks,\n",
      "as it can be arbitrarily small.\n",
      "The last term to be bounded in (9) is the minimum average approximation error ε∗\n",
      "avg at each AVI\n",
      "iteration k. Recalling that the task-averaged approximation error is deﬁned as in (7), applying\n",
      "Theorem 5.3 by Farahmand (2011) we obtain:\n",
      "Lemma 4. Let Q∗\n",
      "T ∗\n",
      "t Qt,k(cid:107)2\n",
      "\n",
      "t,k, ∀t ∈ {1, . . . , T } be the minimizers of ε∗\n",
      "\n",
      "avg,k, ˇtk = arg maxt∈{1,...,T }(cid:107)Q∗\n",
      "\n",
      "ν, and bk,i = (cid:107)Qˇtk,i+1 − T ∗\n",
      "\n",
      "ˇt Qˇtk,i(cid:107)ν, then:\n",
      "\n",
      "t,k+1 −\n",
      "\n",
      "√\n",
      "\n",
      "(cid:32)\n",
      "\n",
      "(cid:33)2\n",
      "\n",
      "ε∗\n",
      "avg,k ≤\n",
      "\n",
      "(cid:107)Q∗\n",
      "\n",
      "ˇtk,k+1 − (T ∗\n",
      "\n",
      "ˇt )k+1Qˇtk,0(cid:107)ν +\n",
      "\n",
      "(γˇtk CAE(ν; ˇtk, P ))i+1bk,k−1−i\n",
      "\n",
      ",\n",
      "\n",
      "(10)\n",
      "\n",
      "k−1\n",
      "(cid:88)\n",
      "\n",
      "i=0\n",
      "\n",
      "with CAE deﬁned as in Farahmand (2011).\n",
      "\n",
      "Final remarks The bound for MTRL is derived by composing the results in Theorems 2 and 3, and\n",
      "Lemma 4. The results above highlight the advantage of learning a shared representation. The bound\n",
      "in Theorem 2 shows that a small approximation error is critical to improve the convergence towards\n",
      "the optimal action-value function, and the bound in Theorem 3 shows that the cost of learning the\n",
      "shared representation at each AVI iteration is mitigated by using multiple tasks. This is particularly\n",
      "beneﬁcial when the feature representation is complex, e.g. deep neural networks.\n",
      "\n",
      "3.4 DISCUSSION\n",
      "\n",
      "As stated in the remarks of Equation (9), the beneﬁt of MTRL is evinced by the second component\n",
      "of the bound, i.e. the cost of learning h, which vanishes with the increase of the number of tasks.\n",
      "Obviously, adding more tasks require the shared representation to be large enough to include all\n",
      "of them, undesirably causing the term suph,w(cid:107)h(w( ¯X))(cid:107) in the fourth component of the bound to\n",
      "increase. This introduces a tradeoff between the number of features and number of tasks; however, for\n",
      "\n",
      "5\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "(a) Shared network\n",
      "\n",
      "(b) FQI vs MFQI\n",
      "\n",
      "(c) #Task analysis\n",
      "\n",
      "Figure 1: (a) The architecture of the neural network we propose to learn T tasks simultaneously.\n",
      "The wt block maps each input xt from task µt to a shared set of layers h which extracts a common\n",
      "representation of the tasks. Eventually, the shared representation is specialized in block ft and the\n",
      "output yt of the network is computed. Note that each block can be composed of arbitrarily many\n",
      "layers. (b) Results of FQI and MFQI averaged over 4 tasks in Car-On-Hill, showing (cid:107)Q∗ − QπK (cid:107) on\n",
      "the left, and the discounted cumulative reward on the right. (c) Results of MFQI showing (cid:107)Q∗ −QπK (cid:107)\n",
      "for increasing number of tasks. Both results in (b) and (c) are averaged over 100 experiments, and\n",
      "show the 95% conﬁdence intervals.\n",
      "\n",
      "a reasonable number of tasks the number of features used in the single-task case is enough to handle\n",
      "them, as we show in some experiments in Section 5. Notably, since the AVI/API framework provided\n",
      "by Farahmand (2011) provides an easy way to include the approximation error of a generic function\n",
      "approximator, it is easy to show the beneﬁt in MTRL of the bound in Equation (9). Despite being just\n",
      "multi-task extensions of previous works, our results are the ﬁrst one to theoretically show the beneﬁt\n",
      "of sharing representation in MTRL. Moreover, they serve as a signiﬁcant theoretical motivation,\n",
      "besides to the intuitive ones, of the practical algorithms that we describe in the following sections.\n",
      "\n",
      "4 SHARING REPRESENTATIONS\n",
      "\n",
      "We want to empirically evaluate the beneﬁt of our theoretical study in the problem of jointly learning\n",
      "T different tasks µt, introducing a neural network architecture for which our bounds hold. Following\n",
      "our theoretical framework, the network we propose extracts representations wt from inputs xt for each\n",
      "task µt, mapping them to common features in a set of shared layers h, specializing the learning of\n",
      "each task in respective separated layers ft, and ﬁnally computing the output yt = (ft ◦ h ◦ wt)(xt) =\n",
      "ft(h(wt(xt))) (Figure 1(a)). The idea behind this architecture is not new in the literature. For\n",
      "instance, similar ideas have already been used in DQN variants to improve exploration on the same\n",
      "task via bootstrapping (Osband et al., 2016) and to perform MTRL (Liu et al., 2016).\n",
      "\n",
      "The intuitive and desirable property of this architecture is the exploitation of the regularization effect\n",
      "introduced by the shared representation of the jointly learned tasks. Indeed, unlike learning a single\n",
      "task that may end up in overﬁtting, forcing the model to compute a shared representation of the tasks\n",
      "helps the regression process to extract more general features, with a consequent reduction in the\n",
      "variance of the learned function. This intuitive justiﬁcation for our approach, joins the theoretical\n",
      "beneﬁt proven in Section 3. Note that our architecture can be used in any MTRL problem involving a\n",
      "regression process; indeed, it can be easily used in value-based methods as a Q-function regressor,\n",
      "or in policy search as a policy regressor. In both cases, the targets are learned for each task µt\n",
      "in its respective output block ft. Remarkably, as we show in the experimental Section 5, it is\n",
      "straightforward to extend RL algorithms to their multi-task variants only through the use of the\n",
      "proposed network architecture, without major changes to the algorithms themselves.\n",
      "\n",
      "5 EXPERIMENTAL RESULTS\n",
      "\n",
      "To empirically evince the effect described by our bounds, we propose an extension of FQI (Ernst\n",
      "et al., 2005; Riedmiller, 2005), that we call MFQI, for which our AVI bounds apply. Then, to\n",
      "empirically evaluate our approach in challenging RL problems, we introduce multi-task variants\n",
      "of two well-known DRL algorithms: DQN (Mnih et al., 2015) and DDPG (Lillicrap et al., 2015),\n",
      "which we call Multi Deep Q-Network (MDQN) and Multi Deep Deterministic Policy Gradient\n",
      "(MDDPG) respectively. Note that for these methodologies, our AVI and API bounds hold only with\n",
      "\n",
      "6\n",
      "\n",
      "hhw1w1w2w2wTwTf1f1f2f2fTfTInputOutputx1x2xTy1y2yT........02550# Iterations0.150.200.250.300.350.400.450.50Q*QKFQIMULTI02550# Iterations0.050.000.050.100.15Performance02550# Iterations0.150.200.250.300.350.400.450.50Q*QK1248\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "(a) Multi-task\n",
      "\n",
      "(b) Transfer\n",
      "\n",
      "Figure 2: Discounted cumulative reward averaged over 100 experiments of DQN and MDQN for\n",
      "each task and for transfer learning in the Acrobot problem. An epoch consists of 1, 000 steps, after\n",
      "which the greedy policy is evaluated for 2, 000 steps. The 95% conﬁdence intervals are shown.\n",
      "\n",
      "the simplifying assumption that the samples are i.i.d.; nevertheless they are useful to show the beneﬁt\n",
      "of our method also in complex scenarios, e.g. MuJoCo (Todorov et al., 2012). We remark that in\n",
      "these experiments we are only interested in showing the beneﬁt of learning multiple tasks with a\n",
      "shared representation w.r.t. learning a single task; therefore, we only compare our methods with\n",
      "the single task counterparts, ignoring other works on MTRL in literature. Experiments have been\n",
      "developed using the MushroomRL library (D’Eramo et al., 2020), and run on an NVIDIA R(cid:13) DGX\n",
      "StationTM and Intel R(cid:13) AI DevCloud. Refer to Appendix B for all the details and our motivations\n",
      "about the experimental settings.\n",
      "\n",
      "5.1 MULTI FITTED Q-ITERATION\n",
      "\n",
      "As a ﬁrst empirical evaluation, we consider FQI, as an example of an AVI algorithm, to show the\n",
      "effect described by our theoretical AVI bounds in experiments. We consider the Car-On-Hill problem\n",
      "as described in Ernst et al. (2005), and select four different tasks from it changing the mass of the\n",
      "car and the value of the actions (details in Appendix B). Then, we run separate instances of FQI\n",
      "with a single task network for each task respectively, and one of MFQI considering all the tasks\n",
      "simultaneously. Figure 1(b) shows the L1-norm of the difference between Q∗ and QπK averaged\n",
      "over all the tasks. It is clear how MFQI is able to get much closer to the optimal Q-function, thus\n",
      "giving an empirical evidence of the AVI bounds in Theorem 2. For completeness, we also show the\n",
      "advantage of MFQI w.r.t. FQI in performance. Then, in Figure 1(c) we provide an empirical evidence\n",
      "of the beneﬁt of increasing the number of tasks in MFQI in terms of both quality and stability.\n",
      "\n",
      "5.2 MULTI DEEP Q-NETWORK\n",
      "\n",
      "As in Liu et al. (2016), our MDQN uses separate replay memories for each task and the batch\n",
      "used in each training step is built picking the same number of samples from each replay memory.\n",
      "Furthermore, a step of the algorithm consists of exactly one step in each task. These are the only\n",
      "minor changes to the vanilla DQN algorithm we introduce, while all other aspects, such as the use of\n",
      "the target network, are not modiﬁed. Thus, the time complexity of MDQN is considerably lower than\n",
      "vanilla DQN thanks to the learning of T tasks with a single model, but at the cost of a higher memory\n",
      "complexity for the collection of samples for each task. We consider ﬁve problems with similar\n",
      "state spaces, sparse rewards and discrete actions: Cart-Pole, Acrobot, Mountain-Car, Car-On-Hill,\n",
      "and Inverted-Pendulum. The implementation of the ﬁrst three problems is the one provided by the\n",
      "OpenAI Gym library Brockman et al. (2016), while Car-On-Hill is described in Ernst et al. (2005)\n",
      "and Inverted-Pendulum in Lagoudakis & Parr (2003).\n",
      "\n",
      "Figure 2(a) shows the performance of MDQN w.r.t. to vanilla DQN that uses a single-task network\n",
      "structured as the multi-task one in the case with T = 1. The ﬁrst three plots from the left show good\n",
      "performance of MDQN, which is both higher and more stable than DQN. In Car-On-Hill, MDQN is\n",
      "slightly slower than DQN to reach the best performance, but eventually manages to be more stable.\n",
      "Finally, the Inverted-Pendulum experiment is clearly too easy to solve for both approaches, but it is\n",
      "still useful for the shared feature extraction in MDQN. The described results provide important hints\n",
      "about the better quality of the features extracted by MDQN w.r.t. DQN. To further demonstrate this,\n",
      "we evaluate the performance of DQN on Acrobot, arguably the hardest of the ﬁve problems, using\n",
      "a single-task network with the shared parameters in h initialized with the weights of a multi-task\n",
      "\n",
      "7\n",
      "\n",
      "02550#Epochs20406080PerformanceCart-Pole02550#Epochs10090807060Acrobot02550#Epochs10095908580757065Mountain-Car02550#Epochs0.00.10.20.30.4Car-On-Hill02550#Epochs0.60.40.20.0Inverted-PendulumDQNMULTI02550#Epochs10090807060PerformanceAcrobotNo initializationUnfreeze-0Unfreeze-10No unfreeze\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "(a) Multi-task for pendulums\n",
      "\n",
      "(b) Transfer for pendulums\n",
      "\n",
      "(c) Multi-task for walkers\n",
      "\n",
      "(d) Transfer for walkers\n",
      "\n",
      "Figure 3: Discounted cumulative reward averaged over 40 experiments of DDPG and MDDPG for\n",
      "each task and for transfer learning in the Inverted-Double-Pendulum and Hopper problems. An\n",
      "epoch consists of 10, 000 steps, after which the greedy policy is evaluated for 5, 000 steps. The 95%\n",
      "conﬁdence intervals are shown.\n",
      "\n",
      "network trained with MDQN on the other four problems. Arbitrarily, the pre-trained weights can be\n",
      "adjusted during the learning of the new task or can be kept ﬁxed and only the remaining randomly\n",
      "initialized parameters in w and f are trained. From Figure 2(b), the advantages of initializing the\n",
      "weights are clear. In particular, we compare the performance of DQN without initialization w.r.t.\n",
      "DQN with initialization in three settings: in Unfreeze-0 the initialized weights are adjusted, in No-\n",
      "Unfreeze they are kept ﬁxed, and in Unfreeze-10 they are kept ﬁxed until epoch 10 after which they\n",
      "start to be optimized. Interestingly, keeping the shared weights ﬁxed shows a signiﬁcant performance\n",
      "improvement in the earliest epochs, but ceases to improve soon. On the other hand, the adjustment of\n",
      "weights from the earliest epochs shows improvements only compared to the uninitialized network\n",
      "in the intermediate stages of learning. The best results are achieved by starting to adjust the shared\n",
      "weights after epoch 10, which is approximately the point at which the improvement given by the\n",
      "ﬁxed initialization starts to lessen.\n",
      "\n",
      "5.3 MULTI DEEP DETERMINISTIC POLICY GRADIENT\n",
      "\n",
      "In order to show how the ﬂexibility of our approach easily allows to perform MTRL in policy search\n",
      "algorithms, we propose MDDPG as a multi-task variant of DDPG. As an actor-critic method, DDPG\n",
      "requires an actor network and a critic network. Intuitively, to obtain MDDPG both the actor and critic\n",
      "networks should be built following our proposed structure. We perform separate experiments on two\n",
      "sets of MuJoCo Todorov et al. (2012) problems with similar continuous state and action spaces: the\n",
      "ﬁrst set includes Inverted-Pendulum, Inverted-Double-Pendulum, and Inverted-Pendulum-Swingup as\n",
      "implemented in the pybullet library, whereas the second set includes Hopper-Stand, Walker-Walk,\n",
      "and Half-Cheetah-Run as implemented in the DeepMind Control SuiteTassa et al. (2018). Figure 3(a)\n",
      "shows a relevant improvement of MDDPG w.r.t. DDPG in the pendulum tasks. Indeed, while in\n",
      "Inverted-Pendulum, which is the easiest problem among the three, the performance of MDDPG is\n",
      "only slightly better than DDPG, the difference in the other two problems is signiﬁcant. The advantage\n",
      "of MDDPG is conﬁrmed in Figure 3(c) where it performs better than DDPG in Hopper and equally\n",
      "good in the other two tasks. Again, we perform a TL evaluation of DDPG in the problems where\n",
      "it suffers the most, by initializing the shared weights of a single-task network with the ones of a\n",
      "multi-task network trained with MDDPG on the other problems. Figures 3(b) and 3(d) show evident\n",
      "advantages of pre-training the shared weights and a signiﬁcant difference between keeping them ﬁxed\n",
      "or not.\n",
      "\n",
      "8\n",
      "\n",
      "050100#Epochs2030405060708090100PerformanceInverted-PendulumDDPGMULTI050100#Epochs100200300400500600700800Inverted-Double-Pendulum050100#Epochs100806040200Inverted-Pendulum-Swingup050100#Epochs200400600800PerformanceInverted-Double-PendulumNo initializationUnfreeze-0No unfreeze050100#Epochs05101520253035PerformanceHopper050100#Epochs010203040506070Walker050100#Epochs0510152025303540Half-CheetahDDPGMULTI050100#Epochs010203040PerformanceHopperNo initializationUnfreeze-0No unfreeze\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "6 RELATED WORKS\n",
      "\n",
      "Our work is inspired from both theoretical and empirical studies in MTL and MTRL literature. In\n",
      "particular, the theoretical analysis we provide follows previous results about the theoretical properties\n",
      "of multi-task algorithms. For instance, Cavallanti et al. (2010) and Maurer (2006) prove the theoretical\n",
      "advantages of MTL based on linear approximation. More in detail, Maurer (2006) derives bounds on\n",
      "MTL when a linear approximator is used to extract a shared representation among tasks. Then, Maurer\n",
      "et al. (2016), which we considered in this work, describes similar results that extend to the use of\n",
      "non-linear approximators. Similar studies have been conducted in the context of MTRL. Among the\n",
      "others, Lazaric & Restelli (2011) and Brunskill & Li (2013) give theoretical proofs of the advantage\n",
      "of learning from multiple MDPs and introduces new algorithms to empirically support their claims,\n",
      "as done in this work.\n",
      "\n",
      "Generally, contributions in MTRL assume that properties of different tasks, e.g. dynamics and reward\n",
      "function, are generated from a common generative model. About this, interesting analyses consider\n",
      "Bayesian approaches; for instance Wilson et al. (2007) assumes that the tasks are generated from a\n",
      "hierarchical Bayesian model, and likewise Lazaric & Ghavamzadeh (2010) considers the case when\n",
      "the value functions are generated from a common prior distribution. Similar considerations, which\n",
      "however does not use a Bayesian approach, are implicitly made in Taylor et al. (2007), Lazaric et al.\n",
      "(2008), and also in this work.\n",
      "\n",
      "In recent years, the advantages of MTRL have been empirically evinced also in DRL, especially\n",
      "exploiting the powerful representational capacity of deep neural networks. For instance, Parisotto\n",
      "et al. (2015) and Rusu et al. (2015) propose to derive a multi-task policy from the policies learned by\n",
      "DQN experts trained separately on different tasks. Rusu et al. (2015) compares to a therein introduced\n",
      "variant of DQN, which is very similar to our MDQN and the one in Liu et al. (2016), showing how\n",
      "their method overcomes it in the Atari benchmark Bellemare et al. (2013). Further developments,\n",
      "extend the analysis to policy search (Yang et al., 2017; Teh et al., 2017), and to multi-goal RL (Schaul\n",
      "et al., 2015; Andrychowicz et al., 2017). Finally, Hessel et al. (2018) addresses the problem of\n",
      "balancing the learning of multiple tasks with a single deep neural network proposing a method that\n",
      "uniformly adapts the impact of each task on the training updates of the agent.\n",
      "\n",
      "7 CONCLUSION\n",
      "\n",
      "We have theoretically proved the advantage in RL of using a shared representation to learn multiple\n",
      "tasks w.r.t. learning a single task. We have derived our results extending the AVI/API bounds (Farah-\n",
      "mand, 2011) to MTRL, leveraging the upper bounds on the approximation error in MTL provided\n",
      "in Maurer et al. (2016). The results of this analysis show that the error propagation during the\n",
      "AVI/API iterations is reduced according to the number of tasks. Then, we proposed a practical way of\n",
      "exploiting this theoretical beneﬁt which consists in an effective way of extracting shared representa-\n",
      "tions of multiple tasks by means of deep neural networks. To empirically show the advantages of our\n",
      "method, we carried out experiments on challenging RL problems with the introduction of multi-task\n",
      "extensions of FQI, DQN, and DDPG based on the neural network structure we proposed. As desired,\n",
      "the favorable empirical results conﬁrm the theoretical beneﬁt we described.\n",
      "\n",
      "9\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "ACKNOWLEDGMENTS\n",
      "\n",
      "This project has received funding from the European Union’s Horizon 2020 research and innovation\n",
      "programme under grant agreement No. #640554 (SKILLS4ROBOTS) and No. #713010 (GOAL-\n",
      "Robots). This project has also been supported by grants from NVIDIA, the NVIDIA DGX Station,\n",
      "and the Intel R(cid:13) AI DevCloud. The authors thank Alberto Maria Metelli, Andrea Tirinzoni and Matteo\n",
      "Papini for their helpful insights during the development of the project.\n",
      "\n",
      "REFERENCES\n",
      "\n",
      "Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob\n",
      "McGrew, Josh Tobin, OpenAI Pieter Abbeel, and Wojciech Zaremba. Hindsight experience replay.\n",
      "In Advances in Neural Information Processing Systems, pp. 5048–5058, 2017.\n",
      "\n",
      "Jonathan Baxter. A model of inductive bias learning. Journal of Artiﬁcial Intelligence Research, 12:\n",
      "\n",
      "149–198, 2000.\n",
      "\n",
      "Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environ-\n",
      "ment: An evaluation platform for general agents. Journal of Artiﬁcial Intelligence Research, 47:\n",
      "253–279, 2013.\n",
      "\n",
      "Richard Bellman. The theory of dynamic programming. Technical report, RAND Corp Santa Monica\n",
      "\n",
      "CA, 1954.\n",
      "\n",
      "Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and\n",
      "\n",
      "Wojciech Zaremba. Openai gym, 2016.\n",
      "\n",
      "Emma Brunskill and Lihong Li. Sample complexity of multi-task reinforcement learning.\n",
      "Proceedings of the Twenty-Ninth Conference on Uncertainty in Artiﬁcial Intelligence, 2013.\n",
      "\n",
      "In\n",
      "\n",
      "Rich Caruana. Multitask learning. Machine learning, 28(1):41–75, 1997.\n",
      "\n",
      "Giovanni Cavallanti, Nicolo Cesa-Bianchi, and Claudio Gentile. Linear algorithms for online\n",
      "\n",
      "multitask classiﬁcation. Journal of Machine Learning Research, 11(Oct):2901–2934, 2010.\n",
      "\n",
      "Carlo D’Eramo, Davide Tateo, Andrea Bonarini, Marcello Restelli, and Jan Peters. Mushroomrl:\n",
      "\n",
      "Simplifying reinforcement learning research. arXiv:2001.01102, 2020.\n",
      "\n",
      "Damien Ernst, Pierre Geurts, and Louis Wehenkel. Tree-based batch mode reinforcement learning.\n",
      "\n",
      "Journal of Machine Learning Research, 6(Apr):503–556, 2005.\n",
      "\n",
      "Amir-massoud Farahmand. Regularization in reinforcement learning. 2011.\n",
      "\n",
      "Matteo Hessel, Hubert Soyer, Lasse Espeholt, Wojciech Czarnecki, Simon Schmitt, and Hado van\n",
      "\n",
      "Hasselt. Multi-task deep reinforcement learning with popart. arXiv:1809.04474, 2018.\n",
      "\n",
      "Irina Higgins, Arka Pal, Andrei Rusu, Loic Matthey, Christopher Burgess, Alexander Pritzel, Matthew\n",
      "Botvinick, Charles Blundell, and Alexander Lerchner. Darla: Improving zero-shot transfer in\n",
      "reinforcement learning. In International Conference on Machine Learning, pp. 1480–1490, 2017.\n",
      "\n",
      "Michail G Lagoudakis and Ronald Parr. Least-squares policy iteration. Journal of machine learning\n",
      "\n",
      "research, 4(Dec):1107–1149, 2003.\n",
      "\n",
      "Alessandro Lazaric. Transfer in reinforcement learning: a framework and a survey. In Reinforcement\n",
      "\n",
      "Learning, pp. 143–173. Springer, 2012.\n",
      "\n",
      "Alessandro Lazaric and Mohammad Ghavamzadeh. Bayesian multi-task reinforcement learning. In\n",
      "\n",
      "ICML-27th International Conference on Machine Learning, pp. 599–606. Omnipress, 2010.\n",
      "\n",
      "Alessandro Lazaric and Marcello Restelli. Transfer from multiple mdps. In Advances in Neural\n",
      "\n",
      "Information Processing Systems, pp. 1746–1754, 2011.\n",
      "\n",
      "Alessandro Lazaric, Marcello Restelli, and Andrea Bonarini. Transfer of samples in batch rein-\n",
      "forcement learning. In Proceedings of the 25th international conference on Machine learning, pp.\n",
      "544–551. ACM, 2008.\n",
      "\n",
      "10\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,\n",
      "David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv\n",
      "preprint arXiv:1509.02971, 2015.\n",
      "\n",
      "Lydia Liu, Urun Dogan, and Katja Hofmann. Decoding multitask dqn in the world of minecraft. In\n",
      "\n",
      "European Workshop on Reinforcement Learning, 2016.\n",
      "\n",
      "Andreas Maurer. Bounds for linear multi-task learning. Journal of Machine Learning Research, 7\n",
      "\n",
      "(Jan):117–139, 2006.\n",
      "\n",
      "Science, 650:109–122, 2016.\n",
      "\n",
      "Andreas Maurer. A chain rule for the expected suprema of gaussian processes. Theoretical Computer\n",
      "\n",
      "Andreas Maurer, Massimiliano Pontil, and Bernardino Romera-Paredes. The beneﬁt of multitask\n",
      "\n",
      "representation learning. The Journal of Machine Learning Research, 17(1):2853–2884, 2016.\n",
      "\n",
      "Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare,\n",
      "Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control\n",
      "through deep reinforcement learning. Nature, 518(7540):529, 2015.\n",
      "\n",
      "Ian Osband, Charles Blundell, Alexander Pritzel, and Benjamin Van Roy. Deep exploration via\n",
      "bootstrapped dqn. In Advances in neural information processing systems, pp. 4026–4034, 2016.\n",
      "\n",
      "Emilio Parisotto, Jimmy Lei Ba, and Ruslan Salakhutdinov. Actor-mimic: Deep multitask and\n",
      "\n",
      "transfer reinforcement learning. arXiv preprint arXiv:1511.06342, 2015.\n",
      "\n",
      "Martin Riedmiller. Neural ﬁtted q iteration–ﬁrst experiences with a data efﬁcient neural reinforcement\n",
      "\n",
      "learning method. In European Conference on Machine Learning, pp. 317–328. Springer, 2005.\n",
      "\n",
      "Andrei A Rusu, Sergio Gomez Colmenarejo, Caglar Gulcehre, Guillaume Desjardins, James Kirk-\n",
      "patrick, Razvan Pascanu, Volodymyr Mnih, Koray Kavukcuoglu, and Raia Hadsell. Policy\n",
      "distillation. arXiv preprint arXiv:1511.06295, 2015.\n",
      "\n",
      "Tom Schaul, Daniel Horgan, Karol Gregor, and David Silver. Universal value function approximators.\n",
      "\n",
      "In International Conference on Machine Learning, pp. 1312–1320, 2015.\n",
      "\n",
      "Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de Las Casas, David Budden,\n",
      "Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq, Timothy P. Lillicrap, and Martin A. Riedmiller.\n",
      "Deepmind control suite. CoRR, abs/1801.00690, 2018.\n",
      "\n",
      "Matthew E Taylor and Peter Stone. Transfer learning for reinforcement learning domains: A survey.\n",
      "\n",
      "Journal of Machine Learning Research, 10(Jul):1633–1685, 2009.\n",
      "\n",
      "Matthew E Taylor, Peter Stone, and Yaxin Liu. Transfer learning via inter-task mappings for temporal\n",
      "\n",
      "difference learning. Journal of Machine Learning Research, 8(Sep):2125–2167, 2007.\n",
      "\n",
      "Yee Teh, Victor Bapst, Wojciech M Czarnecki, John Quan, James Kirkpatrick, Raia Hadsell, Nicolas\n",
      "Heess, and Razvan Pascanu. Distral: Robust multitask reinforcement learning. In Advances in\n",
      "Neural Information Processing Systems, pp. 4496–4506, 2017.\n",
      "\n",
      "Sebastian Thrun and Lorien Pratt. Learning to learn. Springer Science & Business Media, 2012.\n",
      "\n",
      "Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.\n",
      "\n",
      "In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems. IEEE, 2012.\n",
      "\n",
      "Aaron Wilson, Alan Fern, Soumya Ray, and Prasad Tadepalli. Multi-task reinforcement learning: a\n",
      "hierarchical bayesian approach. In Proceedings of the 24th international conference on Machine\n",
      "learning, pp. 1015–1022. ACM, 2007.\n",
      "\n",
      "Markus Wulfmeier, Abbas Abdolmaleki, Roland Hafner, Jost Tobias Springenberg, Michael Neunert,\n",
      "Tim Hertweck, Thomas Lampe, Noah Siegel, Nicolas Heess, and Martin Riedmiller. Regularized\n",
      "hierarchical policies for compositional transfer in robotics. arXiv:1906.11228, 2019.\n",
      "\n",
      "Zhaoyang Yang, Kathryn E Merrick, Hussein A Abbass, and Lianwen Jin. Multi-task deep reinforce-\n",
      "\n",
      "ment learning for continuous action control. In IJCAI, pp. 3301–3307, 2017.\n",
      "\n",
      "11\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "A PROOFS\n",
      "\n",
      "A.1 APPROXIMATED VALUE-ITERATION BOUNDS\n",
      "\n",
      "Proof of Theorem 2. We compute the average expected loss across tasks:\n",
      "\n",
      "1\n",
      "T\n",
      "\n",
      "T\n",
      "(cid:88)\n",
      "\n",
      "t=1\n",
      "\n",
      "(cid:107)Q∗\n",
      "\n",
      "t − QπK\n",
      "\n",
      "t (cid:107)1,ρ\n",
      "\n",
      "1\n",
      "T\n",
      "\n",
      "T\n",
      "(cid:88)\n",
      "\n",
      "t=1\n",
      "\n",
      "2γ\n",
      "\n",
      "(1 − γ)2\n",
      "\n",
      "(1 − γ)2\n",
      "\n",
      "(1 − γ)2\n",
      "\n",
      "2γ\n",
      "\n",
      "2γ\n",
      "\n",
      "2γ\n",
      "\n",
      "≤\n",
      "\n",
      "≤\n",
      "\n",
      "≤\n",
      "\n",
      "≤\n",
      "\n",
      "≤\n",
      "\n",
      "2γt\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "(1 − γt)2\n",
      "\n",
      "inf\n",
      "\n",
      "r∈[0,1]\n",
      "\n",
      "T\n",
      "(cid:88)\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "t=1\n",
      "\n",
      "inf\n",
      "\n",
      "r∈[0,1]\n",
      "\n",
      "1\n",
      "T\n",
      "\n",
      "T\n",
      "(cid:88)\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "t=1\n",
      "\n",
      "inf\n",
      "\n",
      "r∈[0,1]\n",
      "\n",
      "1\n",
      "T\n",
      "\n",
      "inf\n",
      "\n",
      "r∈[0,1]\n",
      "\n",
      "T\n",
      "(cid:88)\n",
      "\n",
      "(cid:16)\n",
      "\n",
      "t=1\n",
      "\n",
      "1\n",
      "T\n",
      "\n",
      "(cid:34)\n",
      "\n",
      "(cid:34)\n",
      "\n",
      "(cid:34)\n",
      "\n",
      "1\n",
      "2\n",
      "\n",
      "1\n",
      "2\n",
      "\n",
      "1\n",
      "2\n",
      "\n",
      "1\n",
      "2\n",
      "\n",
      "1\n",
      "T\n",
      "\n",
      "C\n",
      "\n",
      "VI,ρ,ν(K; t, r)E\n",
      "\n",
      "2 (εt,0, . . . , εt,K−1; t, r) +\n",
      "\n",
      "C\n",
      "\n",
      "VI,ρ,ν(K; t, r)E\n",
      "\n",
      "2 (εt,0, . . . , εt,K−1; t, r) +\n",
      "\n",
      "C\n",
      "\n",
      "VI,ρ,ν(K; t, r)E\n",
      "\n",
      "2 (εt,0, . . . , εt,K−1; t, r)\n",
      "\n",
      "+\n",
      "\n",
      "C\n",
      "\n",
      "VI,ρ,ν(K; t, r)E\n",
      "\n",
      "2 (εt,0, . . . , εt,K−1; t, r)\n",
      "\n",
      "+\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "1 − γt\n",
      "\n",
      "γK\n",
      "t Rmax,t\n",
      "\n",
      "2\n",
      "\n",
      "1 − γt\n",
      "\n",
      "γK\n",
      "t Rmax,t\n",
      "\n",
      "(cid:21)\n",
      "\n",
      "(cid:21)\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "2\n",
      "\n",
      "1 − γ\n",
      "\n",
      "γKRmax,avg\n",
      "\n",
      "(cid:17)\n",
      "\n",
      "2\n",
      "\n",
      "1 − γ\n",
      "\n",
      "γKRmax,avg\n",
      "\n",
      "(cid:35)\n",
      "\n",
      "(cid:35)\n",
      "\n",
      "(cid:35)\n",
      "\n",
      "(1 − γ)2\n",
      "\n",
      "inf\n",
      "\n",
      "r∈[0,1]\n",
      "\n",
      "1\n",
      "2\n",
      "\n",
      "C\n",
      "\n",
      "VI(K; r)\n",
      "\n",
      "T\n",
      "(cid:88)\n",
      "\n",
      "(cid:16)\n",
      "\n",
      "1\n",
      "\n",
      "t=1\n",
      "\n",
      "E\n",
      "\n",
      "2 (εt,0, . . . , εt,K−1; t, r)\n",
      "\n",
      "+\n",
      "\n",
      "γKRmax,avg\n",
      "\n",
      "(11)\n",
      "\n",
      "(cid:17)\n",
      "\n",
      "2\n",
      "\n",
      "1 − γ\n",
      "\n",
      "with γ = max\n",
      "\n",
      "γt, C\n",
      "\n",
      "VI(K; r) = max\n",
      "\n",
      "t∈{1,...,T }\n",
      "\n",
      "t∈{1,...,T }\n",
      "\n",
      "1\n",
      "2\n",
      "\n",
      "1\n",
      "2\n",
      "\n",
      "VI,ρ,ν(K; t, r), and Rmax,avg = 1/T (cid:80)T\n",
      "C\n",
      "\n",
      "t=1 Rmax,t.\n",
      "\n",
      "Considering the term 1/T (cid:80)T\n",
      "\n",
      "(cid:104)\n",
      "\n",
      "E 1\n",
      "\n",
      "t=1\n",
      "\n",
      "2 (εt,0, . . . , εt,K−1; t, r)\n",
      "\n",
      "(cid:105)\n",
      "\n",
      "= 1/T (cid:80)T\n",
      "\n",
      "t=1\n",
      "\n",
      "(cid:16)(cid:80)K−1\n",
      "\n",
      "k=0 α2r\n",
      "\n",
      "t,kεt,k\n",
      "\n",
      "(cid:17) 1\n",
      "\n",
      "2\n",
      "\n",
      "let\n",
      "\n",
      "αk =\n",
      "\n",
      "(cid:40) (1−γ)γK−k−1\n",
      "\n",
      "1−γK+1\n",
      "\n",
      "(1−γ)γK\n",
      "1−γK+1\n",
      "\n",
      "0 ≤ k < K,\n",
      "\n",
      ",\n",
      "\n",
      "k = K\n",
      "\n",
      "T\n",
      "(cid:88)\n",
      "\n",
      "(cid:32)K−1\n",
      "(cid:88)\n",
      "\n",
      "t=1\n",
      "\n",
      "k=0\n",
      "\n",
      "α2r\n",
      "\n",
      "t,kεt,k\n",
      "\n",
      "(cid:33) 1\n",
      "\n",
      "2\n",
      "\n",
      "≤\n",
      "\n",
      "1\n",
      "T\n",
      "\n",
      "T\n",
      "(cid:88)\n",
      "\n",
      "(cid:32)K−1\n",
      "(cid:88)\n",
      "\n",
      "t=1\n",
      "\n",
      "k=0\n",
      "\n",
      "(cid:33) 1\n",
      "\n",
      "2\n",
      "\n",
      "α2r\n",
      "\n",
      "k εt,k\n",
      "\n",
      ".\n",
      "\n",
      "T\n",
      "(cid:88)\n",
      "\n",
      "(cid:32)K−1\n",
      "(cid:88)\n",
      "\n",
      "t=1\n",
      "\n",
      "k=0\n",
      "\n",
      "(cid:33) 1\n",
      "\n",
      "2\n",
      "\n",
      "α2r\n",
      "\n",
      "k εt,k\n",
      "\n",
      "≤\n",
      "\n",
      "(cid:32)K−1\n",
      "(cid:88)\n",
      "\n",
      "k=0\n",
      "\n",
      "α2r\n",
      "k\n",
      "\n",
      "1\n",
      "T\n",
      "\n",
      "T\n",
      "(cid:88)\n",
      "\n",
      "t=1\n",
      "\n",
      "(cid:33) 1\n",
      "\n",
      "2\n",
      "\n",
      "εt,k\n",
      "\n",
      ".\n",
      "\n",
      "then we bound\n",
      "\n",
      "Using Jensen’s inequality:\n",
      "\n",
      "1\n",
      "T\n",
      "\n",
      "1\n",
      "T\n",
      "\n",
      "So, now we can write (11) as\n",
      "\n",
      "1\n",
      "T\n",
      "\n",
      "T\n",
      "(cid:88)\n",
      "\n",
      "t=1\n",
      "\n",
      "(cid:107)Q∗\n",
      "\n",
      "t − QπK\n",
      "\n",
      "t (cid:107)1,ρ ≤\n",
      "\n",
      "2γ\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "(1 − γ)2\n",
      "\n",
      "1\n",
      "2\n",
      "\n",
      "inf\n",
      "\n",
      "r∈[0,1]\n",
      "\n",
      "C\n",
      "\n",
      "VI(K; r)E\n",
      "\n",
      "avg(εavg,0, . . . , εavg,K−1; r)\n",
      "\n",
      "1\n",
      "2\n",
      "\n",
      "(cid:21)\n",
      "\n",
      "2\n",
      "\n",
      "+\n",
      "\n",
      "1 − γ\n",
      "\n",
      "γKRmax,avg\n",
      "\n",
      ",\n",
      "\n",
      "with εavg,k = 1/T (cid:80)T\n",
      "\n",
      "t=1 εt,k and Eavg(εavg,0, . . . , εavg,K−1; r) = (cid:80)K−1\n",
      "\n",
      "k=0 α2r\n",
      "\n",
      "k εavg,k.\n",
      "\n",
      "Proof of Lemma 4. Let us start from the deﬁnition of optimal task-averaged risk:\n",
      "\n",
      "ε∗\n",
      "avg,k =\n",
      "\n",
      "(cid:107)Q∗\n",
      "\n",
      "t,k+1 − T ∗\n",
      "\n",
      "t Qt,k(cid:107)2\n",
      "ν,\n",
      "\n",
      "1\n",
      "T\n",
      "\n",
      "T\n",
      "(cid:88)\n",
      "\n",
      "t=1\n",
      "\n",
      "12\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "where Q∗\n",
      "\n",
      "t,k, with t ∈ [1, T ], are the minimizers of εavg,k.\n",
      "\n",
      "Consider the task ˇt such that\n",
      "\n",
      "we can write the following inequality:\n",
      "\n",
      "ˇtk = arg max\n",
      "\n",
      "(cid:107)Q∗\n",
      "\n",
      "t,k+1 − T ∗\n",
      "\n",
      "t Qt,k(cid:107)2\n",
      "ν,\n",
      "\n",
      "t∈{1,...,T }\n",
      "\n",
      "(cid:113)\n",
      "\n",
      "avg,k ≤ (cid:107)Q∗\n",
      "ε∗\n",
      "\n",
      "ˇtk,k+1 − T ∗\n",
      "\n",
      "ˇt Qˇtk,k(cid:107)ν.\n",
      "\n",
      "By the application of Theorem 5.3 by Farahmand (2011) to the right hand side, and deﬁning\n",
      "bk,i = (cid:107)Qˇtk,i+1 − T ∗\n",
      "\n",
      "ˇt Qˇtk,i(cid:107)ν, we obtain:\n",
      "\n",
      "(cid:113)\n",
      "\n",
      "avg,k ≤ (cid:107)Q∗\n",
      "ε∗\n",
      "\n",
      "ˇtk,k+1 − (T ∗\n",
      "\n",
      "ˇt )k+1Qˇtk,0(cid:107)ν +\n",
      "\n",
      "(γˇtk CAE(ν; ˇtk, P ))i+1bk,k−1−i.\n",
      "\n",
      "Squaring both sides yields the result:\n",
      "\n",
      "(cid:32)\n",
      "\n",
      "ε∗\n",
      "avg,k ≤\n",
      "\n",
      "(cid:107)Q∗\n",
      "\n",
      "ˇtk,k+1 − (T ∗\n",
      "\n",
      "ˇt )k+1Qˇtk,0(cid:107)ν +\n",
      "\n",
      "(γˇtk CAE(ν; ˇtk, P ))i+1bk,k−1−i\n",
      "\n",
      ".\n",
      "\n",
      "(cid:33)2\n",
      "\n",
      "k−1\n",
      "(cid:88)\n",
      "\n",
      "i=0\n",
      "\n",
      "k−1\n",
      "(cid:88)\n",
      "\n",
      "i=0\n",
      "\n",
      "A.2 APPROXIMATED POLICY-ITERATION BOUNDS\n",
      "\n",
      "We start by considering the bound for the API framework:\n",
      "Theorem 5. (Theorem 3.2 of Farahmand (2011)) Let K be a positive integer, and Qmax ≤ Rmax\n",
      "for any sequence (Qk)K−1\n",
      "k=0 ⊂ B(S × A, Qmax) and the corresponding sequence (εk)K−1\n",
      "εk = (cid:107)Qk − Qπk (cid:107)2\n",
      "\n",
      "1−γ . Then\n",
      "k=0 , where\n",
      "\n",
      "1\n",
      "2\n",
      "\n",
      "C\n",
      "\n",
      "PI,ρ,ν(K; r)E\n",
      "\n",
      "1\n",
      "\n",
      "2 (ε0, . . . , εK−1; r) + γK−1Rmax\n",
      "\n",
      ",\n",
      "\n",
      "(12)\n",
      "\n",
      "(cid:21)\n",
      "\n",
      "(cid:107)Q∗ − QπK (cid:107)1,ρ ≤\n",
      "\n",
      "ν, we have:\n",
      "2γ\n",
      "\n",
      "(1 − γ)2\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "inf\n",
      "\n",
      "r∈[0,1]\n",
      "\n",
      "where\n",
      "\n",
      "CPI,ρ,ν(K; r) =\n",
      "\n",
      "(cid:18) 1 − γ\n",
      "\n",
      "(cid:19)2\n",
      "\n",
      "2\n",
      "\n",
      "sup\n",
      "0,...,π(cid:48)\n",
      "π(cid:48)\n",
      "K\n",
      "\n",
      "K−1\n",
      "(cid:88)\n",
      "\n",
      "k=0\n",
      "\n",
      "a2(1−r)\n",
      "k\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "(cid:88)\n",
      "\n",
      "m≥0\n",
      "\n",
      "(cid:88)\n",
      "\n",
      "m≥1\n",
      "\n",
      "γmcPI1,ρ,ν(K − k − 1, m + 1; π(cid:48)\n",
      "\n",
      "k+1)+\n",
      "\n",
      "γmcPI2,ρ,ν(K − k − 1, m; π(cid:48)\n",
      "\n",
      "k+1, π(cid:48)\n",
      "\n",
      "k) + cPI3,ρ,ν\n",
      "\n",
      "\n",
      "\n",
      ";\n",
      "\n",
      "\n",
      "2\n",
      "\n",
      "(13)\n",
      "\n",
      "with E(ε0, . . . , εK−1; r) = (cid:80)K−1\n",
      "butions ρ and ν, and the series αk are deﬁned as in Farahmand (2011).\n",
      "\n",
      "k=0 α2r\n",
      "\n",
      "k εk, the three coefﬁcients cPI1,ρ,ν, cPI2,ρ,ν, cPI3,ρ,ν, the distri-\n",
      "\n",
      "From Theorem 5, by computing the average loss across tasks and pushing inside the average using\n",
      "Jensen’s inequality, we derive the API bounds averaged on multiple tasks.\n",
      "Theorem 6. Let K be a positive integer, and Qmax ≤ Rmax\n",
      "A, Qmax) and the corresponding sequence (εavg,k)K−1\n",
      "have:\n",
      "\n",
      "1−γ . Then for any sequence (Qk)K−1\n",
      "\n",
      "k=0 , where εavg,k =\n",
      "\n",
      "t=1(cid:107)Qt,k − Qπk\n",
      "\n",
      "k=0 ⊂ B(S ×\n",
      "\n",
      "ν, we\n",
      "\n",
      "t (cid:107)2\n",
      "\n",
      "(cid:80)T\n",
      "\n",
      "1\n",
      "T\n",
      "\n",
      "1\n",
      "T\n",
      "\n",
      "T\n",
      "(cid:88)\n",
      "\n",
      "t=1\n",
      "\n",
      "(cid:107)Q∗\n",
      "\n",
      "t − QπK\n",
      "\n",
      "t (cid:107)1,ρ ≤\n",
      "\n",
      "2γ\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "(1 − γ)2\n",
      "\n",
      "inf\n",
      "\n",
      "r∈[0,1]\n",
      "\n",
      "1\n",
      "2\n",
      "\n",
      "1\n",
      "2\n",
      "\n",
      "C\n",
      "\n",
      "PI(K; r)E\n",
      "\n",
      "avg(εavg,0, . . . , εavg,K−1; r)\n",
      "\n",
      "+γK−1Rmax,avg\n",
      "\n",
      "(cid:3) ,\n",
      "\n",
      "(14)\n",
      "\n",
      "13\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "with Eavg = (cid:80)K−1\n",
      "\n",
      "k=0 α2r\n",
      "\n",
      "k εavg,k, γ = max\n",
      "(cid:40) (1−γ)γK−k−1\n",
      "\n",
      "t∈{1,...,T }\n",
      "\n",
      "1\n",
      "T\n",
      "\n",
      "(cid:80)T\n",
      "\n",
      "t=1 Rmax,t and αk =\n",
      "\n",
      "1−γK+1\n",
      "\n",
      "(1−γ)γK\n",
      "1−γK+1\n",
      "\n",
      "0 ≤ k < K,\n",
      "\n",
      ".\n",
      "\n",
      "k = K\n",
      "\n",
      "1\n",
      "2\n",
      "\n",
      "1\n",
      "2\n",
      "\n",
      "γt, C\n",
      "\n",
      "PI(K; r) = max\n",
      "\n",
      "C\n",
      "\n",
      "PI,ρ,ν(K; t, r), Rmax,avg =\n",
      "\n",
      "t∈{1,...,T }\n",
      "\n",
      "Proof of Theorem 6. The proof is very similar to the one for AVI. We compute the average expected\n",
      "loss across tasks:\n",
      "\n",
      "1\n",
      "T\n",
      "\n",
      "T\n",
      "(cid:88)\n",
      "\n",
      "t=1\n",
      "\n",
      "(cid:107)Q∗\n",
      "\n",
      "t − QπK\n",
      "\n",
      "t (cid:107)1,ρ\n",
      "\n",
      "1\n",
      "T\n",
      "\n",
      "T\n",
      "(cid:88)\n",
      "\n",
      "t=1\n",
      "\n",
      "2γ\n",
      "\n",
      "(1 − γ)2\n",
      "\n",
      "(1 − γ)2\n",
      "\n",
      "(1 − γ)2\n",
      "\n",
      "2γ\n",
      "\n",
      "2γ\n",
      "\n",
      "2γ\n",
      "\n",
      "≤\n",
      "\n",
      "≤\n",
      "\n",
      "≤\n",
      "\n",
      "≤\n",
      "\n",
      "≤\n",
      "\n",
      "2γt\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "(1 − γt)2\n",
      "\n",
      "inf\n",
      "\n",
      "r∈[0,1]\n",
      "\n",
      "T\n",
      "(cid:88)\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "t=1\n",
      "\n",
      "inf\n",
      "\n",
      "r∈[0,1]\n",
      "\n",
      "1\n",
      "T\n",
      "\n",
      "T\n",
      "(cid:88)\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "t=1\n",
      "\n",
      "inf\n",
      "\n",
      "r∈[0,1]\n",
      "\n",
      "1\n",
      "T\n",
      "\n",
      "inf\n",
      "\n",
      "r∈[0,1]\n",
      "\n",
      "T\n",
      "(cid:88)\n",
      "\n",
      "(cid:16)\n",
      "\n",
      "t=1\n",
      "\n",
      "1\n",
      "T\n",
      "\n",
      "(cid:34)\n",
      "\n",
      "(cid:34)\n",
      "\n",
      "(cid:34)\n",
      "\n",
      "1\n",
      "2\n",
      "\n",
      "1\n",
      "2\n",
      "\n",
      "1\n",
      "T\n",
      "\n",
      "1\n",
      "2\n",
      "\n",
      "C\n",
      "\n",
      "PI,ρ,ν(K; t, r)E\n",
      "\n",
      "1\n",
      "\n",
      "2 (εt,0, . . . , εt,K−1; t, r) + γK−1\n",
      "\n",
      "Rmax,t\n",
      "\n",
      "t\n",
      "\n",
      "1\n",
      "2\n",
      "\n",
      "C\n",
      "\n",
      "PI,ρ,ν(K; t, r)E\n",
      "\n",
      "1\n",
      "\n",
      "2 (εt,0, . . . , εt,K−1; t, r) + γK−1\n",
      "\n",
      "Rmax,t\n",
      "\n",
      "t\n",
      "\n",
      "C\n",
      "\n",
      "PI,ρ,ν(K; t, r)E\n",
      "\n",
      "2 (εt,0, . . . , εt,K−1; t, r)\n",
      "\n",
      "+ γK−1Rmax,avg\n",
      "\n",
      "C\n",
      "\n",
      "PI,ρ,ν(K; t, r)E\n",
      "\n",
      "2 (εt,0, . . . , εt,K−1; t, r)\n",
      "\n",
      "+ γK−1Rmax,avg\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "(cid:21)\n",
      "\n",
      "(cid:21)\n",
      "\n",
      "(cid:35)\n",
      "\n",
      "(cid:35)\n",
      "\n",
      "(cid:35)\n",
      "\n",
      "(1 − γ)2\n",
      "\n",
      "inf\n",
      "\n",
      "r∈[0,1]\n",
      "\n",
      "1\n",
      "2\n",
      "\n",
      "C\n",
      "\n",
      "PI(K; r)\n",
      "\n",
      "T\n",
      "(cid:88)\n",
      "\n",
      "(cid:16)\n",
      "\n",
      "1\n",
      "\n",
      "t=1\n",
      "\n",
      "E\n",
      "\n",
      "2 (εt,0, . . . , εt,K−1; t, r)\n",
      "\n",
      "+ γK−1Rmax,avg\n",
      "\n",
      ".\n",
      "\n",
      "(15)\n",
      "\n",
      "(cid:17)\n",
      "\n",
      "Using Jensen’s inequality as in the AVI scenario, we can write (15) as:\n",
      "\n",
      "1\n",
      "T\n",
      "\n",
      "T\n",
      "(cid:88)\n",
      "\n",
      "t=1\n",
      "\n",
      "(cid:107)Q∗\n",
      "\n",
      "t − QπK\n",
      "\n",
      "t (cid:107)1,ρ ≤\n",
      "\n",
      "2γ\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "(1 − γ)2\n",
      "\n",
      "inf\n",
      "\n",
      "r∈[0,1]\n",
      "\n",
      "1\n",
      "2\n",
      "\n",
      "1\n",
      "2\n",
      "\n",
      "C\n",
      "\n",
      "PI(K; r)E\n",
      "\n",
      "avg(εavg,0, . . . , εavg,K−1; r)\n",
      "\n",
      "+γK−1Rmax,avg\n",
      "\n",
      "(cid:3) ,\n",
      "\n",
      "(16)\n",
      "\n",
      "with εavg,k = 1/T (cid:80)T\n",
      "\n",
      "t=1 εt,k and Eavg(εavg,0, . . . , εavg,K−1; r) = (cid:80)K−1\n",
      "\n",
      "k=0 α2r\n",
      "\n",
      "k εavg,k.\n",
      "\n",
      "A.3 APPROXIMATION BOUNDS\n",
      "\n",
      "1, . . . , w∗\n",
      "Proof of Theorem 3. Let w∗\n",
      "(cid:32)\n",
      "\n",
      "T , h∗ and f ∗\n",
      "\n",
      "1 , . . . , f ∗\n",
      "\n",
      "T be the minimizers of ε∗\n",
      "\n",
      "avg, then:\n",
      "\n",
      "εavg( ˆw, ˆh, ˆf ) − ε∗\n",
      "\n",
      "avg =\n",
      "\n",
      "εavg( ˆw, ˆh, ˆf ) −\n",
      "\n",
      "(cid:96)( ˆft(ˆh( ˆwt(Xti))), Yti)\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "(cid:17)\n",
      "\n",
      "(cid:33)\n",
      "\n",
      "(cid:125)\n",
      "\n",
      "(cid:96)( ˆft(ˆh( ˆwt(Xti))), Yti) −\n",
      "\n",
      "(cid:96)(f ∗\n",
      "\n",
      "t (h∗(w∗\n",
      "\n",
      "t (Xti))), Yti)\n",
      "\n",
      "(cid:124)\n",
      "(cid:32)\n",
      "\n",
      "(cid:124)\n",
      "(cid:32)\n",
      "\n",
      "(cid:124)\n",
      "\n",
      "+\n",
      "\n",
      "+\n",
      "\n",
      "1\n",
      "nT\n",
      "\n",
      "(cid:88)\n",
      "\n",
      "ti\n",
      "\n",
      "1\n",
      "nT\n",
      "\n",
      "(cid:88)\n",
      "\n",
      "ti\n",
      "\n",
      "(cid:88)\n",
      "\n",
      "1\n",
      "nT\n",
      "\n",
      "(cid:123)(cid:122)\n",
      "B\n",
      "\n",
      "ti\n",
      "\n",
      "(cid:33)\n",
      "\n",
      "(cid:125)\n",
      "\n",
      "(cid:96)(f ∗\n",
      "\n",
      "t (h∗(w∗\n",
      "\n",
      "t (Xti))), Yti) − ε∗\n",
      "avg\n",
      "\n",
      ".\n",
      "\n",
      "(cid:33)\n",
      "\n",
      "(cid:125)\n",
      "\n",
      "(17)\n",
      "\n",
      "We proceed to bound the three components individually:\n",
      "\n",
      "• C can be bounded using Hoeffding’s inequality, with probability 1 − δ/2 by (cid:112)ln(2/δ)/(2nT ),\n",
      "\n",
      "as it contains only nT random variables bounded in the interval [0, 1];\n",
      "\n",
      "(cid:88)\n",
      "\n",
      "1\n",
      "nT\n",
      "\n",
      "ti\n",
      "(cid:123)(cid:122)\n",
      "A\n",
      "\n",
      "(cid:123)(cid:122)\n",
      "C\n",
      "\n",
      "14\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "• B can be bounded by 0, by deﬁnition of ˆw, ˆh and ˆf , as they are the minimizers of Equa-\n",
      "\n",
      "tion (3);\n",
      "\n",
      "• the bounding of A is less straightforward and is described in the following.\n",
      "\n",
      "We deﬁne the following auxiliary function spaces:\n",
      "\n",
      "• W (cid:48) = {x ∈ X → (wt(xti)) : (w1, . . . , wT ) ∈ W T },\n",
      "• F (cid:48) = (cid:8)y ∈ RKT n → (ft(yti)) : (f1, . . . , fT ) ∈ F T (cid:9),\n",
      "\n",
      "and the following auxiliary sets:\n",
      "\n",
      "• S = (cid:8)((cid:96)(ft(h(wt(Xti))), Yti)) : f ∈ F T , h ∈ H, w ∈ W T (cid:9) ⊆ RT n,\n",
      "• S(cid:48) = F (cid:48)(H(W (cid:48)( ¯X))) = (cid:8)(ft(h(wt(Xti)))) : f ∈ F T , h ∈ H, w ∈ W T (cid:9) ⊆ RT n,\n",
      "• S(cid:48)(cid:48) = H(W (cid:48)( ¯X)) = (cid:8)(h(wt(Xti))) : h ∈ H, w ∈ W T (cid:9) ⊆ RKT n,\n",
      "\n",
      "which will be useful in our proof.\n",
      "\n",
      "Using Theorem 9 by Maurer et al. (2016), we can write:\n",
      "\n",
      "εavg( ˆw, ˆh, ˆf ) −\n",
      "\n",
      "(cid:96)( ˆft(ˆh( ˆwt(Xti))), Yti)\n",
      "\n",
      "1\n",
      "nT\n",
      "\n",
      "(cid:88)\n",
      "\n",
      "ti\n",
      "\n",
      "sup\n",
      "\n",
      "εavg(w, h, f ) −\n",
      "\n",
      "(cid:96)(ft(h(wt(Xti))), Yti)\n",
      "\n",
      "1\n",
      "nT\n",
      "\n",
      "(cid:88)\n",
      "\n",
      "ti\n",
      "\n",
      "(cid:32)\n",
      "\n",
      "≤\n",
      "\n",
      "≤\n",
      "\n",
      "w∈W T ,h∈H,f ∈F T\n",
      "(cid:115)\n",
      "√\n",
      "\n",
      "2πG(S)\n",
      "\n",
      "+\n",
      "\n",
      "nT\n",
      "\n",
      "9 ln( 2\n",
      "δ )\n",
      "2nT\n",
      "\n",
      ",\n",
      "\n",
      "then by Lipschitz property of the loss function (cid:96) and the contraction lemma Corollary 11 Maurer et al.\n",
      "1 and c(cid:48)\n",
      "(2016): G(S) ≤ G(S(cid:48)). By Theorem 12 by Maurer et al. (2016), for universal constants c(cid:48)\n",
      "2:\n",
      "\n",
      "G(S(cid:48)) ≤ c(cid:48)\n",
      "\n",
      "1L(F (cid:48))G(S(cid:48)(cid:48)) + c(cid:48)\n",
      "\n",
      "2D(S(cid:48)(cid:48))O(F (cid:48)) + min\n",
      "y∈Y\n",
      "\n",
      "G(F(y)),\n",
      "\n",
      "where L(F (cid:48)) is the largest value for the Lipschitz constants in the function space F (cid:48), and D(S(cid:48)(cid:48)) is\n",
      "the Euclidean diameter of the set S(cid:48)(cid:48).\n",
      "Using Theorem 12 by Maurer et al. (2016) again, for universal constants c(cid:48)(cid:48)\n",
      "\n",
      "1 and c(cid:48)(cid:48)\n",
      "2 :\n",
      "\n",
      "G(S(cid:48)(cid:48)) ≤ c(cid:48)(cid:48)\n",
      "\n",
      "1 L(H)G(W (cid:48)( ¯X)) + c(cid:48)(cid:48)\n",
      "\n",
      "2 D(W (cid:48)( ¯X))O(H) + min\n",
      "\n",
      "G(H(p)).\n",
      "\n",
      "(20)\n",
      "\n",
      "Putting (19) and (20) together:\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "G(S(cid:48)) ≤ c(cid:48)\n",
      "\n",
      "1L(F (cid:48))\n",
      "\n",
      "1 L(H)G(W (cid:48)( ¯X)) + c(cid:48)(cid:48)\n",
      "c(cid:48)(cid:48)\n",
      "\n",
      "2 D(W (cid:48)( ¯X))O(H) + min\n",
      "\n",
      "G(H(p))\n",
      "\n",
      "p∈P\n",
      "\n",
      "p∈P\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "+ c(cid:48)\n",
      "\n",
      "= c(cid:48)\n",
      "\n",
      "1c(cid:48)(cid:48)\n",
      "\n",
      "+ c(cid:48)\n",
      "\n",
      "G(F(y))\n",
      "\n",
      "2D(S(cid:48)(cid:48))O(F (cid:48)) + min\n",
      "y∈Y\n",
      "1 L(F (cid:48))L(H)G(W (cid:48)( ¯X)) + c(cid:48)\n",
      "2D(S(cid:48)(cid:48))O(F (cid:48)) + min\n",
      "y∈Y\n",
      "\n",
      "G(F(y)).\n",
      "\n",
      "1c(cid:48)(cid:48)\n",
      "\n",
      "2 L(F (cid:48))D(W (cid:48)( ¯X))O(H) + c(cid:48)\n",
      "\n",
      "1L(F (cid:48)) min\n",
      "p∈P\n",
      "\n",
      "G(H(p))\n",
      "\n",
      "At this point, we have to bound the individual terms in the right hand side of (21), following the same\n",
      "procedure proposed by Maurer et al. (2016).\n",
      "\n",
      "15\n",
      "\n",
      "(cid:33)\n",
      "\n",
      "(18)\n",
      "\n",
      "(19)\n",
      "\n",
      "(21)\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "Firstly, to bound L(F (cid:48)), let y, y(cid:48) ∈ RKT n, where y = (yti) with yti ∈ RK and y(cid:48) = (y(cid:48)\n",
      "ti ∈ RK. We can write the following:\n",
      "y(cid:48)\n",
      "\n",
      "ti) with\n",
      "\n",
      "(cid:107)f (y) − f (y(cid:48))(cid:107)2 =\n",
      "\n",
      "(ft(yti) − ft(y(cid:48)\n",
      "\n",
      "ti))2\n",
      "\n",
      "(cid:88)\n",
      "\n",
      "ti\n",
      "\n",
      "≤ L(F)2 (cid:88)\n",
      "\n",
      "(cid:107)yti − y(cid:48)\n",
      "\n",
      "ti(cid:107)2\n",
      "\n",
      "ti\n",
      "\n",
      "= L(F)2(cid:107)y − y(cid:48)(cid:107)2,\n",
      "\n",
      "(22)\n",
      "\n",
      "(23)\n",
      "\n",
      "(24)\n",
      "\n",
      "whence L(F (cid:48)) ≤ L(F).\n",
      "\n",
      "Then, we bound:\n",
      "\n",
      "G(W (cid:48)( ¯X)) = E\n",
      "\n",
      "(cid:34)\n",
      "\n",
      "sup\n",
      "w∈W T\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "γktiwtk(Xti)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "(cid:88)\n",
      "\n",
      "kti\n",
      "\n",
      "(cid:35)\n",
      "\n",
      "(cid:88)\n",
      "\n",
      "Xti\n",
      "\n",
      "≤\n",
      "\n",
      "(cid:34)\n",
      "\n",
      "E\n",
      "\n",
      "(cid:88)\n",
      "\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "γkliwk(Xli)\n",
      "(cid:12)\n",
      "(cid:12)\n",
      "\n",
      "(cid:35)\n",
      "\n",
      "Xli\n",
      "\n",
      "sup\n",
      "\n",
      "l∈{1,...,T }\n",
      "\n",
      "sup\n",
      "w∈W\n",
      "\n",
      "t\n",
      "= T\n",
      "\n",
      "sup\n",
      "\n",
      "l∈{1,...,T }\n",
      "\n",
      "ki\n",
      "G(W(Xl)).\n",
      "\n",
      "Then, since it is possible to bound the Euclidean diameter using the norm of the supremum value in\n",
      "the set, we bound D(S(cid:48)(cid:48)) ≤ 2 suph,w(cid:107)h(w( ¯X))(cid:107) and D(W (cid:48)( ¯X)) ≤ 2 supw∈W T (cid:107)w( ¯X)(cid:107).\n",
      "Also, we bound O(F (cid:48)):\n",
      "\n",
      "(cid:21)\n",
      "(cid:104)γ, g(y) − g(y(cid:48))(cid:105)\n",
      "\n",
      "= E\n",
      "\n",
      "(cid:34)\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "E\n",
      "\n",
      "sup\n",
      "g∈F (cid:48)\n",
      "\n",
      "γti (ft(yti) − ft(y(cid:48)\n",
      "\n",
      "ti))\n",
      "\n",
      "γi (f (yti) − f (y(cid:48)\n",
      "\n",
      "(cid:35)\n",
      "\n",
      "(cid:35)\n",
      "ti))\n",
      "\n",
      "(cid:88)\n",
      "\n",
      "ti\n",
      "\n",
      "sup\n",
      "f ∈F T\n",
      "\n",
      "(cid:34)\n",
      "\n",
      "sup\n",
      "f ∈F\n",
      "\n",
      "(cid:88)\n",
      "\n",
      "i\n",
      "\n",
      "(cid:34)\n",
      "\n",
      "(cid:88)\n",
      "\n",
      "E\n",
      "\n",
      "t\n",
      "\n",
      "sup\n",
      "f ∈F\n",
      "\n",
      "(cid:88)\n",
      "\n",
      "i\n",
      "\n",
      "γi (f (yti) − f (y(cid:48)\n",
      "\n",
      "ti))\n",
      "\n",
      "(cid:88)\n",
      "\n",
      "O(F)2 (cid:88)\n",
      "\n",
      "(cid:107)yti − y(cid:48)\n",
      "\n",
      "ti(cid:107)2\n",
      "\n",
      "(cid:33) 1\n",
      "\n",
      "2\n",
      "\n",
      "(cid:88)\n",
      "\n",
      "E\n",
      "\n",
      "=\n",
      "\n",
      "t\n",
      "\n",
      "√\n",
      "\n",
      "\n",
      "\n",
      "≤\n",
      "\n",
      "T\n",
      "\n",
      "\n",
      "\n",
      "(cid:32)\n",
      "\n",
      "√\n",
      "\n",
      "≤\n",
      "\n",
      "T\n",
      "\n",
      "√\n",
      "\n",
      "t\n",
      "\n",
      "i\n",
      "T O(F)(cid:107)y − y(cid:48)(cid:107),\n",
      "\n",
      "=\n",
      "\n",
      "1\n",
      "2\n",
      "\n",
      "(cid:35)2\n",
      "\n",
      "\n",
      "whence O(F (cid:48)) ≤\n",
      "\n",
      "T O(F).\n",
      "\n",
      "√\n",
      "\n",
      "To minimize the last term, it is possible to choose y0 = 0, as f (0) = 0, ∀f ∈ F, resulting in\n",
      "miny∈Y G(F(y)) = G(F(0)) = 0.\n",
      "Then, substituting in (21), and recalling that G(S) ≤ G(S(cid:48)):\n",
      "\n",
      "G(S) ≤ c(cid:48)\n",
      "\n",
      "1c(cid:48)(cid:48)\n",
      "\n",
      "1 L(F)L(H)T\n",
      "\n",
      "sup\n",
      "\n",
      "G(W(Xl)) + 2c(cid:48)\n",
      "\n",
      "l∈{1,...,T }\n",
      "\n",
      "1c(cid:48)(cid:48)\n",
      "2 L(F) sup\n",
      "w∈W T\n",
      "√\n",
      "\n",
      "(cid:107)w( ¯X)(cid:107)O(H)\n",
      "\n",
      "+ c(cid:48)\n",
      "\n",
      "1L(F) min\n",
      "p∈P\n",
      "\n",
      "G(H(p)) + 2c(cid:48)\n",
      "\n",
      "(cid:107)h(w( ¯X))(cid:107)\n",
      "\n",
      "T O(F).\n",
      "\n",
      "(25)\n",
      "\n",
      "2 sup\n",
      "h,w\n",
      "\n",
      "16\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "Now, the ﬁrst term A of (17) can be bounded substituting (25) in (18):\n",
      "\n",
      "εavg( ˆw, ˆh, ˆf ) −\n",
      "√\n",
      "\n",
      "1\n",
      "nT\n",
      "\n",
      "(cid:88)\n",
      "\n",
      "ti\n",
      "\n",
      "(cid:96)( ˆft(ˆh( ˆwt(Xti))), Yti)\n",
      "\n",
      "≤\n",
      "\n",
      "(cid:16)\n",
      "\n",
      "2π\n",
      "nT\n",
      "\n",
      "c(cid:48)\n",
      "1c(cid:48)(cid:48)\n",
      "\n",
      "1 L(F)L(H)T\n",
      "\n",
      "sup\n",
      "\n",
      "G(W(Xl)) + 2c(cid:48)\n",
      "\n",
      "1c(cid:48)(cid:48)\n",
      "\n",
      "l∈{1,...,T }\n",
      "\n",
      "(cid:107)w( ¯X)(cid:107)O(H)\n",
      "\n",
      "+ c(cid:48)\n",
      "\n",
      "1L(F) min\n",
      "p∈P\n",
      "\n",
      "G(H(p)) + 2c(cid:48)\n",
      "\n",
      "(cid:107)h(w( ¯X))(cid:107)\n",
      "\n",
      "2 sup\n",
      "h,w\n",
      "\n",
      "L(F)L(H) supl∈{1,...,T } G(W(Xl))\n",
      "\n",
      "= c1\n",
      "\n",
      "n\n",
      "\n",
      "L(F) minp∈P G(H(p))\n",
      "\n",
      "+ c3\n",
      "\n",
      "+ c4\n",
      "\n",
      "nT\n",
      "\n",
      "2 L(F) sup\n",
      "w∈W T\n",
      "(cid:115)\n",
      "\n",
      "(cid:17)\n",
      "\n",
      "√\n",
      "\n",
      "T O(F)\n",
      "\n",
      "9 ln( 2\n",
      "δ )\n",
      "2nT\n",
      "supw(cid:107)w( ¯X)(cid:107)L(F)O(H)\n",
      "\n",
      "+\n",
      "\n",
      "+ c2\n",
      "\n",
      "nT\n",
      "suph,w(cid:107)h(w( ¯X))(cid:107)O(F)\n",
      "\n",
      "(cid:115)\n",
      "\n",
      "+\n",
      "\n",
      "9 ln( 2\n",
      "δ )\n",
      "2nT\n",
      "\n",
      ".\n",
      "\n",
      "A union bound between A, B and C of (17) completes the proof:\n",
      "\n",
      "εavg( ˆw, ˆh, ˆf ) − ε∗\n",
      "\n",
      "avg ≤ c1\n",
      "\n",
      "L(F)L(H) supl∈{1,...,T } G(W(Xl))\n",
      "\n",
      "√\n",
      "\n",
      "n\n",
      "\n",
      "T\n",
      "\n",
      "n\n",
      "\n",
      "+ c2\n",
      "\n",
      "+ c3\n",
      "\n",
      "+ c4\n",
      "\n",
      "(cid:115)\n",
      "\n",
      "+\n",
      "\n",
      "supw(cid:107)w( ¯X)(cid:107)L(F)O(H)\n",
      "\n",
      "L(F) minp∈P G(H(p))\n",
      "\n",
      "suph,w(cid:107)h(w( ¯X))(cid:107)O(F)\n",
      "\n",
      "nT\n",
      "\n",
      "nT\n",
      "\n",
      "√\n",
      "\n",
      "n\n",
      "\n",
      "T\n",
      "\n",
      "8 ln( 3\n",
      "δ )\n",
      "nT\n",
      "\n",
      ".\n",
      "\n",
      "B ADDITIONAL DETAILS OF EMPIRICAL EVALUATION\n",
      "\n",
      "B.1 MULTI FITTED Q-ITERATION\n",
      "\n",
      "We consider Car-On-Hill problem with discount factor 0.95 and horizon 100. Running Adam\n",
      "optimizer with learning rate 0.001 and using a mean squared loss, we train a neural network composed\n",
      "of 2 shared layers of 30 neurons each, with sigmoidal activation function, as described in Riedmiller\n",
      "(2005). We select 8 tasks for the problem changing the mass of the car m and the value of the\n",
      "discrete actions a (Table 1). Figure 1(b) is computed considering the ﬁrst four tasks, while Figure 1(c)\n",
      "considers task 1 in the result with 1 task, tasks 1 and 2 for the result with 2 tasks, tasks 1, 2, 3, and 4\n",
      "for the result with 4 tasks, and all the tasks for the result with 8 tasks. To run FQI and MFQI, for each\n",
      "task we collect transitions running an extra-tree trained following the procedure and setting in Ernst\n",
      "et al. (2005), using an (cid:15)-greedy policy with (cid:15) = 0.1, to obtain a small, but representative dataset. The\n",
      "optimal Q-function for each task is computed by tree-search3 for 100 states uniformly picked from\n",
      "the state space, and the 2 discrete actions, for a total of 200 state-action tuples.\n",
      "\n",
      "B.2 MULTI DEEP Q-NETWORK\n",
      "\n",
      "The ﬁve problems we consider for this experiment are: Cart-Pole, Acrobot, Mountain-Car, Car-On-\n",
      "Hill, and Inverted-Pendulum4. The discount factors are respectively 0.99, 0.99, 0.99, 0.95, and 0.95.\n",
      "The horizons are respectively 500, 1, 000, 1, 000, 100, and 3, 000. The network we use consists of 80\n",
      "ReLu units for each wt, t ∈ {1, . . . , T } block, with T = 5. Then, the shared block h consists of one\n",
      "\n",
      "3We follow the method described in Ernst et al. (2005).\n",
      "4The IDs of the problems in the OpenAI Gym library are: CartPole-v0, Acrobot-v1, and MountainCar-v0.\n",
      "\n",
      "17\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "Task Mass\n",
      "1.0\n",
      "0.8\n",
      "1.0\n",
      "1.2\n",
      "1.0\n",
      "1.0\n",
      "0.8\n",
      "0.85\n",
      "\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "\n",
      "Action set\n",
      "{−4.0; 4.0}\n",
      "{−4.0; 4.0}\n",
      "{−4.5; 4.5}\n",
      "{−4.5; 4.5}\n",
      "\n",
      "{−4.125; 4.125}\n",
      "\n",
      "{−4.25; 4.25}\n",
      "\n",
      "{−4.375; 4.375}\n",
      "\n",
      "{−4.0; 4.0}\n",
      "\n",
      "Table 1: Different values of the mass of the car and available actions chosen for the Car-On-Hill tasks\n",
      "in the MFQI empirical evaluation.\n",
      "\n",
      "i ) = yt(s, a(t)\n",
      "\n",
      "i ) = ft(h(wt(s)), a(t)\n",
      "\n",
      "layer with 80 ReLu units and another one with 80 sigmoid units. Eventually, each ft has a number of\n",
      "linear units equal to the number of discrete actions a(t)\n",
      ", i ∈ {1, . . . , #A(t)} of task µt which outputs\n",
      "i\n",
      "the action-value Qt(s, a(t)\n",
      "i ), ∀s ∈ S (t). The use of sigmoid units\n",
      "in the second layer of h is due to our choice to extract meaningful shared features bounded between 0\n",
      "and 1 to be used as input of the last linear layer, as in most RL approaches. In practice, we have also\n",
      "found that sigmoid units help to reduce task interference in multi-task networks, where instead the\n",
      "linear response of ReLu units cause a problematic increase in the feature values. Furthermore, the use\n",
      "of a bounded feature space reduces the suph,w(cid:107)h(w( ¯X))(cid:107) term in the upper bound of Theorem 3,\n",
      "corresponding to the upper bound of the diameter of the feature space, as shown in Appendix A.\n",
      "The initial replay memory size for each task is 100 and the maximum size is 5, 000. We use Huber\n",
      "loss with Adam optimizer using learning rate 10−3 and batch size of 100 samples for each task. The\n",
      "target network is updated every 100 steps. The exploration is ε-greedy with ε linearly decaying from\n",
      "1 to 0.01 in the ﬁrst 5, 000 steps.\n",
      "\n",
      "B.3 MULTI DEEP DETERMINISTIC POLICY GRADIENT\n",
      "\n",
      "The two set of problems we consider for this experiment are: one including Inverted-Pendulum,\n",
      "Inverted-Double-Pendulum, and Inverted-Pendulum-Swingup, and another one including Hopper-\n",
      "Stand, Walker-Walk, and Half-Cheetah-Run5. The discount factors are 0.99 and the horizons are\n",
      "1, 000 for all problems. The actor network is composed of 600 ReLu units for each wt, t ∈ {1, . . . , T }\n",
      "block, with T = 3. The shared block h has 500 units with ReLu activation function as for MDQN.\n",
      "Finally, each ft has a number of tanh units equal to the number of dimensions of the continuous\n",
      "actions a(t) ∈ A(t) of task µt which outputs the policy πt(s) = yt(s) = ft(h(wt(s))), ∀s ∈ S (t).\n",
      "On the other hand, the critic network consists of the same wt units of the actor, except for the use of\n",
      "sigmoidal units in the h layer, as in MDQN. In addition to this, the actions a(t) are given as input to\n",
      "h. Finally, each ft has a single linear unit Qt(s, a(t)) = yt(s, a(t)) = ft(h(wt(s), a(t))), ∀s ∈ S (t).\n",
      "The initial replay memory size for each task is 64 and the maximum size is 50, 000. We use Huber\n",
      "loss to update the critic network and the policy gradient to update the actor network. In both cases\n",
      "the optimization is performed with Adam optimizer and batch size of 64 samples for each task. The\n",
      "learning rate of the actor is 10−4 and the learning rate of the critic is 10−3. Moreover, we apply\n",
      "(cid:96)2-penalization to the critic network using a regularization coefﬁcient of 0.01. The target networks are\n",
      "updated with soft-updates using τ = 10−3. The exploration is performed using the action computed\n",
      "by the actor network adding a noise generated with an Ornstein-Uhlenbeck process with θ = 0.15\n",
      "and σ = 0.2. Note that most of these values are taken from the original DDPG paper Lillicrap et al.\n",
      "(2015), which optimizes them for the single-task scenario.\n",
      "\n",
      "5The\n",
      "\n",
      "IDs of\n",
      "\n",
      "the problems\n",
      "\n",
      "InvertedPendulumBulletEnv-v0,\n",
      "InvertedDoublePendulumBulletEnv-v0, and InvertedPendulumSwingupBulletEnv-v0. The names of the\n",
      "domain and the task of the problems in the DeepMind Control Suite are: hopper-stand, walker-walk, and\n",
      "cheetah-run.\n",
      "\n",
      "in the pybullet\n",
      "\n",
      "library are:\n",
      "\n",
      "18\n",
      "\n",
      "\f",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "SQIL: IMITATION LEARNING VIA REINFORCEMENT\n",
      "LEARNING WITH SPARSE REWARDS\n",
      "\n",
      "Siddharth Reddy, Anca D. Dragan, Sergey Levine\n",
      "Department of Electrical Engineering and Computer Science\n",
      "University of California, Berkeley\n",
      "sgr,anca,svlevine\n",
      "{\n",
      "\n",
      "@berkeley.edu\n",
      "}\n",
      "\n",
      "ABSTRACT\n",
      "\n",
      "Learning to imitate expert behavior from demonstrations can be challenging, es-\n",
      "pecially in environments with high-dimensional, continuous observations and un-\n",
      "known dynamics. Supervised learning methods based on behavioral cloning (BC)\n",
      "suffer from distribution shift: because the agent greedily imitates demonstrated\n",
      "actions, it can drift away from demonstrated states due to error accumulation. Re-\n",
      "cent methods based on reinforcement learning (RL), such as inverse RL and gen-\n",
      "erative adversarial imitation learning (GAIL), overcome this issue by training an\n",
      "RL agent to match the demonstrations over a long horizon. Since the true reward\n",
      "function for the task is unknown, these methods learn a reward function from the\n",
      "demonstrations, often using complex and brittle approximation techniques that in-\n",
      "volve adversarial training. We propose a simple alternative that still uses RL, but\n",
      "does not require learning a reward function. The key idea is to provide the agent\n",
      "with an incentive to match the demonstrations over a long horizon, by encourag-\n",
      "ing it to return to demonstrated states upon encountering new, out-of-distribution\n",
      "states. We accomplish this by giving the agent a constant reward of r = +1 for\n",
      "matching the demonstrated action in a demonstrated state, and a constant reward\n",
      "of r = 0 for all other behavior. Our method, which we call soft Q imitation learn-\n",
      "ing (SQIL), can be implemented with a handful of minor modiﬁcations to any\n",
      "standard Q-learning or off-policy actor-critic algorithm. Theoretically, we show\n",
      "that SQIL can be interpreted as a regularized variant of BC that uses a sparsity\n",
      "prior to encourage long-horizon imitation. Empirically, we show that SQIL out-\n",
      "performs BC and achieves competitive results compared to GAIL, on a variety of\n",
      "image-based and low-dimensional tasks in Box2D, Atari, and MuJoCo. This pa-\n",
      "per is a proof of concept that illustrates how a simple imitation method based on\n",
      "RL with constant rewards can be as effective as more complex methods that use\n",
      "learned rewards.\n",
      "\n",
      "1\n",
      "\n",
      "INTRODUCTION\n",
      "\n",
      "Many sequential decision-making problems can be tackled by imitation learning: an expert demon-\n",
      "strates near-optimal behavior to an agent, and the agent attempts to replicate that behavior in novel\n",
      "situations (Argall et al., 2009). This paper considers the problem of training an agent to imitate\n",
      "an expert, given expert action demonstrations and the ability to interact with the environment. The\n",
      "agent does not observe a reward signal or query the expert, and does not know the state transition\n",
      "dynamics.\n",
      "\n",
      "Standard approaches based on behavioral cloning (BC) use supervised learning to greedily imitate\n",
      "demonstrated actions, without reasoning about the consequences of actions (Pomerleau, 1991). As\n",
      "a result, compounding errors cause the agent to drift away from the demonstrated states (Ross et al.,\n",
      "2011). The problem with BC is that, when the agent drifts and encounters out-of-distribution states,\n",
      "the agent does not know how to return to the demonstrated states. Recent methods based on in-\n",
      "verse reinforcement learning (IRL) overcome this issue by training an RL agent not only to imitate\n",
      "demonstrated actions, but also to visit demonstrated states (Ng et al., 2000; Wulfmeier et al., 2015;\n",
      "Finn et al., 2016b; Fu et al., 2017). This is also the core idea behind generative adversarial imi-\n",
      "tation learning (GAIL) (Ho & Ermon, 2016), which implements IRL using generative adversarial\n",
      "\n",
      "1\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "networks (Goodfellow et al., 2014; Finn et al., 2016a). Since the true reward function for the task\n",
      "is unknown, these methods construct a reward signal from the demonstrations through adversarial\n",
      "training, making them difﬁcult to implement and use in practice (Kurach et al., 2018).\n",
      "\n",
      "The main idea in this paper is that the effectiveness of adversarial imitation methods can be achieved\n",
      "by a much simpler approach that does not require adversarial training, or indeed learning a reward\n",
      "function at all. Intuitively, adversarial methods encourage long-horizon imitation by providing the\n",
      "agent with (1) an incentive to imitate the demonstrated actions in demonstrated states, and (2) an\n",
      "incentive to take actions that lead it back to demonstrated states when it encounters new, out-of-\n",
      "distribution states. One of the reasons why adversarial methods outperform greedy methods, such as\n",
      "BC, is that greedy methods only do (1), while adversarial methods do both (1) and (2). Our approach\n",
      "is intended to do both (1) and (2) without adversarial training, by using constant rewards instead of\n",
      "learned rewards. The key idea is that, instead of using a learned reward function to provide a reward\n",
      "signal to the agent, we can simply give the agent a constant reward of r = +1 for matching the\n",
      "demonstrated action in a demonstrated state, and a constant reward of r = 0 for all other behavior.\n",
      "\n",
      "We motivate this approach theoretically, by showing that it implements a regularized variant of BC\n",
      "that learns long-horizon imitation by (a) imposing a sparsity prior on the reward function implied\n",
      "by the imitation policy, and (b) incorporating information about the state transition dynamics into\n",
      "the imitation policy. Intuitively, our method accomplishes (a) by training the agent using an ex-\n",
      "tremely sparse reward function – +1 for demonstrations, 0 everywhere else – and accomplishes (b)\n",
      "by training the agent with RL instead of supervised learning.\n",
      "\n",
      "We instantiate our approach with soft Q-learning (Haarnoja et al., 2017) by initializing the agent’s\n",
      "experience replay buffer with expert demonstrations, setting the rewards to a constant r = +1 in\n",
      "the demonstration experiences, and setting rewards to a constant r = 0 in all of the new experiences\n",
      "the agent collects while interacting with the environment. Since soft Q-learning is an off-policy\n",
      "algorithm, the agent does not necessarily have to visit the demonstrated states in order to experience\n",
      "positive rewards. Instead, the agent replays the demonstrations that were initially added to its buffer.\n",
      "Thus, our method can be applied in environments with stochastic dynamics and continuous states,\n",
      "where the demonstrated states are not necessarily reachable by the agent. We call this method soft\n",
      "Q imitation learning (SQIL).\n",
      "\n",
      "The main contribution of this paper is SQIL: a simple and general imitation learning algorithm that is\n",
      "effective in MDPs with high-dimensional, continuous observations and unknown dynamics. We run\n",
      "experiments in four image-based environments – Car Racing, Pong, Breakout, and Space Invaders –\n",
      "and three low-dimensional environments – Humanoid, HalfCheetah, and Lunar Lander – to compare\n",
      "SQIL to two prior methods: BC and GAIL. We ﬁnd that SQIL outperforms BC and achieves com-\n",
      "petitive results compared to GAIL. Our experiments illustrate two key beneﬁts of SQIL: (1) that it\n",
      "can overcome the state distribution shift problem of BC without adversarial training or learning a re-\n",
      "ward function, which makes it easier to use, e.g., with images, and (2) that it is simple to implement\n",
      "using existing Q-learning or off-policy actor-critic algorithms.\n",
      "\n",
      "2 SOFT Q IMITATION LEARNING\n",
      "\n",
      "SQIL performs soft Q-learning (Haarnoja et al., 2017) with three small, but important, modiﬁcations:\n",
      "(1) it initially ﬁlls the agent’s experience replay buffer with demonstrations, where the rewards are\n",
      "set to a constant r = +1; (2) as the agent interacts with the world and accumulates new experiences,\n",
      "it adds them to the replay buffer, and sets the rewards for these new experiences to a constant r = 0;\n",
      "and (3) it balances the number of demonstration experiences and new experiences (50% each) in each\n",
      "sample from the replay buffer.1 These three modiﬁcations are motivated theoretically in Section 3,\n",
      "via an equivalence to a regularized variant of BC. Intuitively, these modiﬁcations create a simple\n",
      "reward structure that gives the agent an incentive to imitate the expert in demonstrated states, and to\n",
      "take actions that lead it back to demonstrated states when it strays from the demonstrations.\n",
      "\n",
      "1 SQIL resembles the Deep Q-learning from Demonstrations (DQfD) (Hester et al., 2017) and Normalized\n",
      "Actor-Critic (NAC) algorithms (Gao et al., 2018), in that it initially ﬁlls the agent’s experience replay buffer\n",
      "with demonstrations. The key difference is that DQfD and NAC are RL algorithms that assume access to a\n",
      "reward signal, while SQIL is an imitation learning algorithm that does not require an extrinsic reward signal\n",
      "from the environment. Instead, SQIL automatically constructs a reward signal from the demonstrations.\n",
      "\n",
      "2\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "Algorithm 1 Soft Q Imitation Learning (SQIL)\n",
      "1: Require λsamp ∈ R≥0, Ddemo\n",
      "2: Initialize Dsamp ← ∅\n",
      "3: while Qθ not converged do\n",
      "4:\n",
      "5:\n",
      "6:\n",
      "7: end while\n",
      "\n",
      "θ ← θ − η∇θ(δ2(Ddemo, 1) + λsampδ2(Dsamp, 0)) {See Equation 1}\n",
      "Sample transition (s, a, s(cid:48)) with imitation policy π(a|s) ∝ exp (Qθ(s, a))\n",
      "Dsamp ← Dsamp ∪ {(s, a, s(cid:48))}\n",
      "\n",
      "Crucially, since soft Q-learning is an off-policy algorithm, the agent does not necessarily have to\n",
      "visit the demonstrated states in order to experience positive rewards.\n",
      "Instead, the agent replays\n",
      "the demonstrations that were initially added to its buffer. Thus, SQIL can be used in stochastic\n",
      "environments with high-dimensional, continuous states, where the demonstrated states may never\n",
      "actually be encountered by the agent.\n",
      "\n",
      "SQIL is summarized in Algorithm 1, where Qθ is the soft Q function,\n",
      "is the squared soft Bellman error,\n",
      "\n",
      "D\n",
      "\n",
      "demo are demonstrations, δ2\n",
      "\n",
      "δ2(\n",
      "\n",
      ", r) (cid:44) 1\n",
      "|D|\n",
      "\n",
      "D\n",
      "\n",
      "(cid:32)\n",
      "\n",
      "(cid:88)\n",
      "\n",
      "(s,a,s(cid:48))∈D\n",
      "\n",
      "(cid:32)\n",
      "\n",
      "−\n",
      "\n",
      "(cid:32)\n",
      "\n",
      "(cid:88)\n",
      "\n",
      "a(cid:48)∈A\n",
      "\n",
      "Qθ(s, a)\n",
      "\n",
      "r + γ log\n",
      "\n",
      "exp (Qθ(s(cid:48), a(cid:48)))\n",
      "\n",
      ",\n",
      "\n",
      "(1)\n",
      "\n",
      "(cid:33)(cid:33)(cid:33)2\n",
      "\n",
      "∈ {\n",
      "\n",
      "0, 1\n",
      "}\n",
      "\n",
      "is a constant reward.2 The experiments in Section 4 use a convolutional neural\n",
      "and r\n",
      "network or multi-layer perceptron to model Qθ, where θ are the weights of the neural network.\n",
      "Section A.3 in the appendix contains additional implementation details, including values for the\n",
      "hyperparameter λsamp; note that the simple default value of λsamp = 1 works well across a variety of\n",
      "environments.\n",
      "\n",
      "As the imitation policy in line 5 of Algorithm 1 learns to behave more like the expert, a growing\n",
      "number of expert-like transitions get added to the buffer\n",
      "samp with an assigned reward of zero. This\n",
      "causes the effective reward for mimicking the expert to decay over time. Balancing the number of\n",
      "demonstration experiences and new experiences (50% each) sampled for the gradient step in line\n",
      "4 ensures that this effective reward remains at least 1/(1 + λsamp), instead of decaying to zero. In\n",
      "practice, we ﬁnd that this reward decay does not degrade performance if SQIL is halted once the\n",
      "squared soft Bellman error loss converges to a minimum (e.g., see Figure 8 in the appendix). Note\n",
      "that prior methods also require similar techniques: both GAIL and adversarial IRL (AIRL) (Fu et al.,\n",
      "2017) balance the number of positive and negative examples in the training set of the discriminator,\n",
      "and AIRL tends to require early stopping to avoid overﬁtting.\n",
      "\n",
      "D\n",
      "\n",
      "3\n",
      "\n",
      "INTERPRETING SQIL AS REGULARIZED BEHAVIORAL CLONING\n",
      "\n",
      "To understand why SQIL works, we sketch a surprising theoretical result: SQIL is equivalent to a\n",
      "variant of behavioral cloning (BC) that uses regularization to overcome state distribution shift.\n",
      "\n",
      "BC is a simple approach that seeks to imitate the expert’s actions using supervised learning – in\n",
      "particular, greedily maximizing the conditional likelihood of the demonstrated actions given the\n",
      "demonstrated states, without reasoning about the consequences of actions. Thus, when the agent\n",
      "makes small mistakes and enters states that are slightly different from those in the demonstrations,\n",
      "the distribution mismatch between the states in the demonstrations and those actually encountered\n",
      "by the agent leads to compounding errors (Ross et al., 2011). We show that SQIL is equivalent to\n",
      "augmenting BC with a regularization term that incorporates information about the state transition\n",
      "dynamics into the imitation policy, and thus enables long-horizon imitation.\n",
      "\n",
      "3.1 PRELIMINARIES\n",
      "\n",
      "Maximum entropy model of expert behavior. SQIL is built on soft Q-learning, which assumes\n",
      "that expert behavior follows the maximum entropy model (Ziebart et al., 2010; Levine, 2018). In\n",
      "\n",
      "2Equation 1 assumes discrete actions, but SQIL can also be used with continuous actions, as shown in\n",
      "\n",
      "Section 4.3.\n",
      "\n",
      "3\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "(2)\n",
      "\n",
      "(3)\n",
      "\n",
      "(4)\n",
      "\n",
      "(5)\n",
      "\n",
      "(6)\n",
      "\n",
      "an inﬁnite-horizon Markov Decision Process (MDP) with a continuous state space\n",
      "action space\n",
      "policy π forms a Boltzmann distribution over actions,\n",
      "\n",
      "and discrete\n",
      ",3 the expert is assumed to follow a policy π that maximizes reward R(s, a). The\n",
      "\n",
      "A\n",
      "\n",
      "S\n",
      "\n",
      "s) (cid:44)\n",
      "\n",
      "π(a\n",
      "|\n",
      "\n",
      "(cid:80)\n",
      "\n",
      "exp (Q(s, a))\n",
      "\n",
      "a(cid:48)∈A exp (Q(s, a(cid:48)))\n",
      "\n",
      ",\n",
      "\n",
      "where Q is the soft Q function. The soft Q values are a function of the rewards and dynamics, given\n",
      "by the soft Bellman equation,\n",
      "\n",
      "Q(s, a) (cid:44) R(s, a) + γEs(cid:48)\n",
      "\n",
      "log\n",
      "\n",
      "exp (Q(s(cid:48), a(cid:48)))\n",
      "\n",
      ".\n",
      "\n",
      "(cid:34)\n",
      "\n",
      "(cid:32)\n",
      "\n",
      "(cid:33)(cid:35)\n",
      "\n",
      "(cid:88)\n",
      "\n",
      "a(cid:48)∈A\n",
      "\n",
      "In our imitation setting, the rewards and dynamics are unknown. The expert generates a ﬁxed set of\n",
      "demo, by rolling out their policy π in the environment and generating state transi-\n",
      "demonstrations\n",
      "tions (s, a, s(cid:48))\n",
      "demo.\n",
      "\n",
      "D\n",
      "∈ D\n",
      "\n",
      "Behavioral cloning (BC). Training an imitation policy with standard BC corresponds to ﬁtting a\n",
      "parametric model πθ that minimizes the negative log-likelihood loss,\n",
      "\n",
      "In our setting, instead of explicitly modeling the policy πθ, we can represent the policy π in terms\n",
      "of a soft Q function Qθ via Equation 2:\n",
      "\n",
      "(cid:96)BC(θ) (cid:44) (cid:88)\n",
      "\n",
      "(s,a)∈Ddemo\n",
      "\n",
      "log πθ(a\n",
      "\n",
      "s).\n",
      "|\n",
      "\n",
      "−\n",
      "\n",
      "s) (cid:44)\n",
      "\n",
      "π(a\n",
      "|\n",
      "\n",
      "(cid:80)\n",
      "\n",
      "exp (Qθ(s, a))\n",
      "\n",
      "a(cid:48)∈A exp (Qθ(s, a(cid:48)))\n",
      "\n",
      ".\n",
      "\n",
      "Using this representation of the policy, we can train Qθ via the maximum-likelihood objective in\n",
      "Equation 4:\n",
      "\n",
      "(cid:32)\n",
      "\n",
      "(cid:96)BC(θ) (cid:44) (cid:88)\n",
      "\n",
      "(s,a)∈Ddemo\n",
      "\n",
      "Qθ(s, a)\n",
      "\n",
      "log\n",
      "\n",
      "−\n",
      "\n",
      "−\n",
      "\n",
      "(cid:33)(cid:33)\n",
      "\n",
      "exp (Qθ(s, a(cid:48)))\n",
      "\n",
      ".\n",
      "\n",
      "(cid:32)\n",
      "\n",
      "(cid:88)\n",
      "\n",
      "a(cid:48)∈A\n",
      "\n",
      "However, optimizing the BC loss in Equation 6 does not in general yield a valid soft Q function\n",
      "Qθ – i.e., a soft Q function that satisﬁes the soft Bellman equation (Equation 3) with respect to\n",
      "the dynamics and some reward function. The problem is that the BC loss does not incorporate any\n",
      "information about the dynamics into the learning objective, so Qθ learns to greedily assign high\n",
      "values to demonstrated actions, without considering the state transitions that occur as a consequence\n",
      "of actions. As a result, Qθ may output arbitrary values in states that are off-distribution from the\n",
      "demonstrations\n",
      "\n",
      "demo.\n",
      "\n",
      "In Section 3.2, we describe a regularized BC algorithm that adds constraints to ensure that Qθ is\n",
      "a valid soft Q function with respect to some implicitly-represented reward function, and further\n",
      "regularizes the implicit rewards with a sparsity prior. In Section 3.3, we show that this approach\n",
      "recovers an algorithm similar to SQIL.\n",
      "\n",
      "D\n",
      "\n",
      "3.2 REGULARIZED BEHAVIORAL CLONING\n",
      "\n",
      "Under the maximum entropy model described in Section 3.1, expert behavior is driven by a reward\n",
      "function, a soft Q function that computes expected future returns, and a policy that takes actions with\n",
      "high soft Q values. In the previous section, we used these assumptions to represent the imitation\n",
      "policy in terms of a model of the soft Q function Qθ (Equation 5). In this section, we represent the\n",
      "reward function implicitly in terms of Qθ, as shown in Equation 7. This allows us to derive SQIL as\n",
      "a variant of BC that imposes a sparsity prior on the implicitly-represented rewards.\n",
      "\n",
      "Sparsity regularization. The issue with BC is that, when the agent encounters states that are out-\n",
      "demo, Qθ may output arbitrary values. One solution from prior work\n",
      "of-distribution with respect to\n",
      "\n",
      "D\n",
      "\n",
      "3Assuming a discrete action space simpliﬁes our analysis. SQIL can be applied to continuous control tasks\n",
      "\n",
      "using existing sampling methods (Haarnoja et al., 2017; 2018), as illustrated in Section 4.3.\n",
      "\n",
      "4\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "(Piot et al., 2014) is to regularize Qθ with a sparsity prior on the implied rewards – in particular, a\n",
      "penalty on the magnitude of the rewards (cid:80)\n",
      "implied by Qθ via the soft Bellman\n",
      "equation (Equation 3), where\n",
      "\n",
      "Rq(s, a)\n",
      "|\n",
      "\n",
      "s∈S,a∈A |\n",
      "\n",
      "Rq(s, a) (cid:44) Qθ(s, a)\n",
      "\n",
      "γEs(cid:48)\n",
      "\n",
      "log\n",
      "\n",
      "exp (Qθ(s(cid:48), a(cid:48)))\n",
      "\n",
      ".\n",
      "\n",
      "(7)\n",
      "\n",
      "(cid:34)\n",
      "\n",
      "(cid:32)\n",
      "\n",
      "(cid:88)\n",
      "\n",
      "a(cid:48)∈A\n",
      "\n",
      "−\n",
      "\n",
      "(cid:33)(cid:35)\n",
      "\n",
      ".\n",
      "\n",
      "Note that the reward function Rq is not explicitly modeled in this method. Instead, we directly\n",
      "minimize the magnitude of the right-hand side of Equation 7, which is equivalent to minimizing\n",
      "Rq(s, a)\n",
      "|\n",
      "|\n",
      "The purpose of the penalty on\n",
      "is two-fold: (1) it imposes a sparsity prior motivated by\n",
      "prior work (Piot et al., 2013), and (2) it incorporates information about the state transition dynamics\n",
      "into the imitation learning objective, since Rq(s, a) is a function of an expectation over next state s(cid:48).\n",
      "(2) is critical for learning long-horizon behavior that imitates the demonstrations, instead of greedy\n",
      "maximization of the action likelihoods in standard BC. For details, see Piot et al. (2014).\n",
      "\n",
      "Rq(s, a)\n",
      "\n",
      "|\n",
      "\n",
      "|\n",
      "\n",
      "Approximations for continuous states. Unlike the discrete environments tested in Piot et al.\n",
      "(2014), we assume the continuous state space\n",
      "cannot be enumerated. Hence, we approximate\n",
      "the penalty (cid:80)\n",
      "by estimating it from samples: transitions (s, a, s(cid:48)) observed in\n",
      "the demonstrations\n",
      "samp periodically sampled during training\n",
      "using the latest imitation policy. This approximation, which follows the standard approach to con-\n",
      "straint sampling (Calaﬁore & Dabbene, 2006), ensures that the penalty covers the state distribution\n",
      "actually encountered by the agent, instead of only the demonstrations.\n",
      "\n",
      "demo, as well as additional rollouts\n",
      "\n",
      "Rq(s, a)\n",
      "|\n",
      "\n",
      "s∈S,a∈A |\n",
      "\n",
      "D\n",
      "\n",
      "D\n",
      "\n",
      "S\n",
      "\n",
      "To make the penalty continuously differentiable, we introduce an additional approximation: instead\n",
      ", we penalize the squared value (Rq(s, a))2. Note that\n",
      "of penalizing the absolute value\n",
      "since the reward function Rq is not explicitly modeled, but instead deﬁned via Qθ in Equation 7,\n",
      "the squared penalty (Rq(s, a))2 is equivalent to the squared soft Bellman error δ2(\n",
      "samp, 0)\n",
      "from Equation 1.\n",
      "\n",
      "Rq(s, a)\n",
      "|\n",
      "|\n",
      "\n",
      "∪ D\n",
      "\n",
      "demo\n",
      "\n",
      "D\n",
      "\n",
      "Regularized BC algorithm. Formally, we deﬁne the regularized BC loss function adapted from\n",
      "Piot et al. (2014) as\n",
      "\n",
      "(cid:96)RBC(θ) (cid:44) (cid:96)BC(θ) + λδ2(\n",
      "\n",
      "demo\n",
      "\n",
      "D\n",
      "\n",
      "∪ D\n",
      "\n",
      "samp, 0),\n",
      "\n",
      "(8)\n",
      "\n",
      "∈\n",
      "\n",
      "R≥0 is a constant hyperparameter, and δ2 denotes the squared soft Bellman error de-\n",
      "where λ\n",
      "ﬁned in Equation 1. The BC loss encourages Qθ to output high values for demonstrated actions at\n",
      "demonstrated states, and the penalty term propagates those high values to nearby states. In other\n",
      "words, Qθ outputs high values for actions that lead to states from which the demonstrated states are\n",
      "reachable. Hence, when the agent ﬁnds itself far from the demonstrated states, it takes actions that\n",
      "lead it back to the demonstrated states.\n",
      "\n",
      "The RBC algorithm follows the same procedure as Algorithm 1, except that in line 4, RBC takes a\n",
      "gradient step on the RBC loss from Equation 8 instead of the SQIL loss.\n",
      "\n",
      "3.3 CONNECTION BETWEEN SQIL AND REGULARIZED BEHAVIORAL CLONING\n",
      "\n",
      "The gradient of the RBC loss in Equation 8 is proportional to the gradient of the SQIL loss in line\n",
      "4 of Algorithm 1, plus an additional term that penalizes the soft value of the initial state s0 (full\n",
      "derivation in Section A.1 of the appendix):\n",
      "\n",
      "θ(cid:96)RBC(θ)\n",
      "\n",
      "∇\n",
      "\n",
      "θ\n",
      "∝ ∇\n",
      "\n",
      "(cid:0)δ2(\n",
      "\n",
      "D\n",
      "\n",
      "demo, 1) + λsampδ2(\n",
      "\n",
      "samp, 0) + V (s0)(cid:1) .\n",
      "\n",
      "(9)\n",
      "\n",
      "D\n",
      "\n",
      "In other words, SQIL solves a similar optimization problem to RBC. The reward function in SQIL\n",
      "also has a clear connection to the sparsity prior in RBC: SQIL imposes the sparsity prior from RBC,\n",
      "by training the agent with an extremely sparse reward function – r = +1 at the demonstrations, and\n",
      "r = 0 everywhere else. Thus, SQIL can be motivated as a practical way to implement the ideas for\n",
      "regularizing BC proposed in Piot et al. (2014).\n",
      "\n",
      "The main beneﬁt of using SQIL instead of RBC is that SQIL is trivial to implement, since it only\n",
      "requires a few small changes to any standard Q-learning implementation (see Section 2). Extending\n",
      "SQIL to MDPs with a continuous action space is also easy, since we can simply replace Q-learning\n",
      "\n",
      "5\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "with an off-policy actor-critic method (Haarnoja et al., 2018) (see Section 4.3). Given the difﬁculty\n",
      "of implementing deep RL algorithms correctly (Henderson et al., 2018), this ﬂexibility makes SQIL\n",
      "more practical to use, since it can be built on top of existing implementations of deep RL algorithms.\n",
      "Furthermore, the ablation study in Section 4.4 suggests that SQIL actually performs better than RBC.\n",
      "\n",
      "4 EXPERIMENTAL EVALUATION\n",
      "\n",
      "Our experiments aim to compare SQIL to existing imitation learning methods on a variety of tasks\n",
      "with high-dimensional, continuous observations, such as images, and unknown dynamics. We\n",
      "benchmark SQIL against BC and GAIL4 on four image-based games – Car Racing, Pong, Break-\n",
      "out, and Space Invaders – and three state-based tasks – Humanoid, HalfCheetah, and Lunar Lander\n",
      "(Brockman et al., 2016; Bellemare et al., 2013; Todorov et al., 2012). We also investigate which\n",
      "components of SQIL contribute most to its performance via an ablation study on the Lunar Lander\n",
      "game. Section A.3 in the appendix contains additional experimental details.\n",
      "\n",
      "4.1 TESTING GENERALIZATION IN IMAGE-BASED CAR RACING\n",
      "\n",
      "train\n",
      "0\n",
      "S\n",
      "\n",
      "than that of the expert demonstrations\n",
      "\n",
      "The goal of this experiment is to study not only how well each method can mimic the expert demon-\n",
      "strations, but also how well they can acquire policies that generalize to new states that are not seen\n",
      "in the demonstrations. To do so, we train the imitation agents in an environment with a different\n",
      "initial state distribution\n",
      ", allowing us to system-\n",
      "atically control the mismatch between the distribution of states in the demonstrations and the states\n",
      "actually encountered by the agent. We run experiments on the Car Racing game from OpenAI Gym.\n",
      ", the car is rotated 90 degrees so that it begins perpendicular to the track, instead of\n",
      "To create\n",
      "parallel to the track as in\n",
      ". This intervention presents a signiﬁcant generalization challenge to\n",
      "the imitation learner, since the expert demonstrations do not contain any examples of states where\n",
      "the car is perpendicular to the road, or even signiﬁcantly off the road axis. The agent must learn to\n",
      "make a tight turn to get back on the road, then stabilize its orientation so that it is parallel to the road,\n",
      "and only then proceed forward to mimic the expert demonstrations.\n",
      "\n",
      "demo\n",
      "0\n",
      "S\n",
      "\n",
      "demo\n",
      "0\n",
      "S\n",
      "\n",
      "train\n",
      "0\n",
      "S\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      ")\n",
      "\n",
      ") No Shift (S demo\n",
      "\n",
      "Domain Shift (S train\n",
      "\n",
      "−21 ± 56\n",
      "−45 ± 18\n",
      "−97 ± 3\n",
      "375 ± 19\n",
      "480 ± 11\n",
      "\n",
      "Random\n",
      "BC\n",
      "GAIL-DQL\n",
      "SQIL (Ours)\n",
      "Expert\n",
      "\n",
      "The results in Figure 1 show that\n",
      "SQIL and BC perform equally well\n",
      "when there is no variation in the ini-\n",
      "tial state. The task is easy enough\n",
      "that even BC achieves a high reward.\n",
      "Note that, in the unperturbed condi-\n",
      "tion (right column), BC substantially\n",
      "outperforms GAIL, despite the well-\n",
      "known shortcomings of BC. This in-\n",
      "dicates that the adversarial optimiza-\n",
      "tion in GAIL can substantially hinder\n",
      "learning, even in settings where standard BC is sufﬁcient. SQIL performs much better than BC when\n",
      "starting from\n",
      ", showing that SQIL is capable of generalizing to a new initial state distribution,\n",
      "while BC is not. SQIL learns to make a tight turn that takes the car through the grass and back\n",
      "onto the road, then stabilizes the car’s orientation so that it is parallel to the track, and then proceeds\n",
      "forward like the expert does in the demonstrations. BC tends to drive straight ahead into the grass\n",
      "instead of turning back onto the road.\n",
      "\n",
      "Figure 1: Average reward on 100 episodes after training.\n",
      "Standard error on three random seeds.\n",
      "\n",
      "−68 ± 4\n",
      "698 ± 10\n",
      "−66 ± 8\n",
      "704 ± 6\n",
      "704 ± 79\n",
      "\n",
      "train\n",
      "0\n",
      "S\n",
      "\n",
      "4 For all the image-based tasks, we implement a version of GAIL that uses deep Q-learning (GAIL-DQL)\n",
      "instead of TRPO as in the original GAIL paper (Ho & Ermon, 2016), since Q-learning performs better than\n",
      "TRPO in these environments, and because this allows for a head-to-head comparison of SQIL and GAIL:\n",
      "both algorithms use the same underlying RL algorithm, but provide the agent with different rewards – SQIL\n",
      "provides constant rewards, while GAIL provides learned rewards. We use the standard GAIL-TRPO method as\n",
      "a baseline for all the low-dimensional tasks, since TRPO performs better than Q-learning in these environments.\n",
      "The original GAIL method implicitly encodes prior knowledge – namely, that terminating an episode is either\n",
      "always desirable or always undesirable. As pointed out in Kostrikov et al. (2019), this makes comparisons to\n",
      "alternative methods unfair. We implement the unbiased version of GAIL proposed by Kostrikov et al. (2019),\n",
      "and use this in all of the experiments. Comparisons to the biased version with implicit termination knowledge\n",
      "are included in Section A.2 in the appendix.\n",
      "\n",
      "6\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "Figure 2: Image-based Atari. Smoothed with a rolling window of 100 episodes. Standard error\n",
      "on three random seeds. X-axis represents amount of interaction with the environment (not expert\n",
      "demonstrations).\n",
      "\n",
      "SQIL outperforms GAIL in both conditions. Since SQIL and GAIL both use deep Q-learning for RL\n",
      "in this experiment, the gap between them may be attributed to the difference in the reward functions\n",
      "they use to train the agent. SQIL beneﬁts from providing a constant reward that does not require\n",
      "ﬁtting a discriminator, while GAIL struggles to train a discriminator to provide learned rewards\n",
      "directly from images.\n",
      "\n",
      "4.2\n",
      "\n",
      "IMAGE-BASED EXPERIMENTS ON ATARI\n",
      "\n",
      "The results in Figure 2 show that SQIL outperforms BC on Pong, Breakout, and Space Invaders\n",
      "– additional evidence that BC suffers from compounding errors, while SQIL does not. SQIL also\n",
      "outperforms GAIL on all three games, illustrating the difﬁculty of using GAIL to train an image-\n",
      "based discriminator, as in Section 4.1.\n",
      "\n",
      "4.3\n",
      "\n",
      "INSTANTIATING SQIL FOR CONTINUOUS CONTROL IN LOW-DIMENSIONAL MUJOCO\n",
      "\n",
      "The experiments in the previous sections evaluate SQIL on MDPs\n",
      "with a discrete action space. This section illustrates how SQIL can\n",
      "be adapted to continuous actions. We instantiate SQIL using soft\n",
      "actor-critic (SAC) – an off-policy RL algorithm that can solve con-\n",
      "tinuous control tasks (Haarnoja et al., 2018).\n",
      "In particular, SAC\n",
      "is modiﬁed in the following ways: (1) the agent’s experience re-\n",
      "play buffer is initially ﬁlled with expert demonstrations, where re-\n",
      "wards are set to r = +1, (2) when taking gradient steps to ﬁt the\n",
      "agent’s soft Q function, a balanced number of demonstration ex-\n",
      "periences and new experiences (50% each) are sampled from the\n",
      "replay buffer, and (3) the agent observes rewards of r = 0 during\n",
      "its interactions with the environment, instead of an extrinsic reward\n",
      "signal that speciﬁes the desired task. This instantiation of SQIL is\n",
      "compared to GAIL on the Humanoid (17 DoF) and HalfCheetah (6\n",
      "DoF) tasks from MuJoCo.\n",
      "\n",
      "The results show that SQIL outperforms BC and performs compa-\n",
      "rably to GAIL on both tasks, demonstrating that SQIL can be suc-\n",
      "cessfully deployed on problems with continuous actions, and that\n",
      "SQIL can perform well even with a small number of demonstra-\n",
      "tions. This experiment also illustrates how SQIL can be run on top\n",
      "of SAC or any other off-policy value-based RL algorithm.\n",
      "\n",
      "Figure 3: SQIL: best per-\n",
      "formance on 10 consecutive\n",
      "training episodes. BC, GAIL:\n",
      "results from Dhariwal et al.\n",
      "(2017).\n",
      "\n",
      "4.4 ABLATION STUDY ON LOW-DIMENSIONAL LUNAR LANDER\n",
      "\n",
      "We hypothesize that SQIL works well because it combines information about the expert’s policy\n",
      "from demonstrations with information about the environment dynamics from rollouts of the imi-\n",
      "tation policy periodically sampled during training. We also expect RBC to perform comparably to\n",
      "SQIL, since their objectives are similar. To test these hypotheses, we conduct an ablation study using\n",
      "the Lunar Lander game from OpenAI Gym. As in Section 4.1, we control the mismatch between the\n",
      "\n",
      "7\n",
      "\n",
      "02000400060008000NumberofOn-PolicyRollouts0100200300400RewardBreakoutSQIL(Ours)GAIL-DQLBC(P’91)Expert0500100015002000NumberofOn-PolicyRollouts−20−1001020RewardPong01000200030004000NumberofOn-PolicyRollouts200400600RewardSpaceInvaders02040NumberofDemonstrationRollouts05001000RewardHalfCheetah-v1ExpertGAIL-TRPO(HE’16)BC(P’91)SQIL(Ours)02040NumberofDemonstrationRollouts0200400600800RewardHumanoid-v1\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "distribution of states in the demonstrations and the states encountered by the agent by manipulating\n",
      "the initial state distribution. To create\n",
      ", the agent is placed in a starting position never visited in\n",
      "the demonstrations.\n",
      "\n",
      "train\n",
      "0\n",
      "S\n",
      "\n",
      "In the ﬁrst variant of SQIL, λsamp is set to zero, to prevent SQIL from using additional samples drawn\n",
      "from the environment (see line 4 of Algorithm 1). This comparison tests if SQIL really needs to\n",
      "interact with the environment, or if it can rely solely on the demonstrations. In the second condition,\n",
      "γ is set to zero to prevent SQIL from accessing information about state transitions (see Equation 1\n",
      "and line 4 of Algorithm 1). This comparison tests if SQIL is actually extracting information about\n",
      "the dynamics from the samples, or if it can perform just as well with a na¨ıve regularizer (setting\n",
      "γ to zero effectively imposes a penalty on the L2-norm of the soft Q values instead of the squared\n",
      "soft Bellman error). In the third condition, a uniform random policy is used to sample additional\n",
      "rollouts, instead of the imitation policy πθ (see line 6 of Algorithm 1). This comparison tests how\n",
      "important it is that the samples cover the states encountered by the agent during training. In the\n",
      "fourth condition, we use RBC to optimize the loss in Equation 8. instead of using SQIL to optimize\n",
      "the loss in line 4 of Algorithm 1. This comparison tests the effect of the additional V (s0) term in\n",
      "RBC vs. SQIL (see Equation 9).\n",
      "\n",
      ")\n",
      "\n",
      "Domain Shift (S train\n",
      "\n",
      ") No Shift (S demo\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "0.10 ± 0.30\n",
      "Random\n",
      "0.07 ± 0.03\n",
      "BC\n",
      "GAIL-TRPO 0.67 ± 0.04\n",
      "SQIL (Ours)\n",
      "\n",
      "The results in Figure 4 show that all\n",
      "methods perform well when there is\n",
      "no variation in the initial state. When\n",
      "the initial state is varied, SQIL per-\n",
      "forms signiﬁcantly better than BC,\n",
      "GAIL, and the ablated variants of\n",
      "SQIL. This conﬁrms our hypothesis\n",
      "that SQIL needs to sample from the\n",
      "environment using the imitation pol-\n",
      "icy, and relies on information about\n",
      "the dynamics encoded in the samples.\n",
      "Surprisingly, SQIL outperforms RBC\n",
      "by a large margin, suggesting that the\n",
      "penalty on the soft value of the initial\n",
      "state V (s0), which is present in RBC but not in SQIL (see Equation 9), degrades performance.\n",
      "\n",
      "0.04 ± 0.02\n",
      "0.93 ± 0.03\n",
      "0.93 ± 0.03\n",
      "0.88 ± 0.03\n",
      "0.87 ± 0.02\n",
      "0.84 ± 0.02\n",
      "0.82 ± 0.02\n",
      "0.89 ± 0.01\n",
      "0.89 ± 0.31\n",
      "\n",
      "Figure 4: Best success rate on 100 consecutive episodes dur-\n",
      "ing training. Standard error on ﬁve random seeds. Perfor-\n",
      "mance bolded if at least within one standard error of expert.\n",
      "\n",
      "0.89 ± 0.02\n",
      "0.12 ± 0.02\n",
      "0.41 ± 0.02\n",
      "0.47 ± 0.02\n",
      "0.66 ± 0.02\n",
      "0.93 ± 0.03\n",
      "\n",
      "γ = 0\n",
      "π = Unif\n",
      "RBC\n",
      "Expert\n",
      "\n",
      "n λsamp = 0\n",
      "\n",
      "o\n",
      "i\n",
      "t\n",
      "a\n",
      "l\n",
      "b\n",
      "A\n",
      "\n",
      "5 DISCUSSION AND RELATED WORK\n",
      "\n",
      "Related work. Concurrently with SQIL, two other imitation learning algorithms that use constant\n",
      "rewards instead of a learned reward function were developed (Sasaki et al., 2019; Wang et al., 2019).\n",
      "We see our paper as contributing additional evidence to support this core idea, rather than proposing\n",
      "a competing method. First, SQIL is derived from sparsity-regularized BC, while the prior meth-\n",
      "ods are derived from an alternative formulation of the IRL objective (Sasaki et al., 2019) and from\n",
      "support estimation methods (Wang et al., 2019), showing that different theoretical approaches inde-\n",
      "pendently lead to using RL with constant rewards as an alternative to adversarial training – a sign\n",
      "that this idea may be a promising direction for future work. Second, SQIL is shown to outperform\n",
      "BC and GAIL in domains that were not evaluated in Sasaki et al. (2019) or Wang et al. (2019) – in\n",
      "particular, tasks with image observations and signiﬁcant shift in the state distribution between the\n",
      "demonstrations and the training environment.\n",
      "\n",
      "Summary. We contribute the SQIL algorithm: a general method for learning to imitate an expert\n",
      "given action demonstrations and access to the environment. Simulation experiments on tasks with\n",
      "high-dimensional, continuous observations and unknown dynamics show that our method outper-\n",
      "forms BC and achieves competitive results compared to GAIL, while being simple to implement on\n",
      "top of existing off-policy RL algorithms.\n",
      "\n",
      "Limitations and future work. We have not yet proven that SQIL matches the expert’s state occu-\n",
      "pancy measure in the limit of inﬁnite demonstrations. One direction for future work would be to\n",
      "rigorously show whether or not SQIL has this property. Another direction would be to extend SQIL\n",
      "to recover not just the expert’s policy, but also their reward function; e.g., by using a parameterized\n",
      "reward function to model rewards in the soft Bellman error terms, instead of using constant rewards.\n",
      "This could provide a simpler alternative to existing adversarial IRL algorithms.\n",
      "\n",
      "8\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "REFERENCES\n",
      "\n",
      "Brenna D Argall, Sonia Chernova, Manuela Veloso, and Brett Browning. A survey of robot learning from\n",
      "\n",
      "demonstration. Robotics and autonomous systems, 57(5):469–483, 2009.\n",
      "\n",
      "Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environment: An\n",
      "\n",
      "evaluation platform for general agents. Journal of Artiﬁcial Intelligence Research, 47:253–279, 2013.\n",
      "\n",
      "Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech\n",
      "\n",
      "Zaremba. Openai gym, 2016.\n",
      "\n",
      "Springer, 2006.\n",
      "\n",
      "Giuseppe Calaﬁore and Fabrizio Dabbene. Probabilistic and randomized methods for design under uncertainty.\n",
      "\n",
      "Prafulla Dhariwal, Christopher Hesse, Oleg Klimov, Alex Nichol, Matthias Plappert, Alec Radford, John\n",
      "Schulman, Szymon Sidor, Yuhuai Wu, and Peter Zhokhov. Openai baselines. https://github.com/\n",
      "openai/baselines, 2017.\n",
      "\n",
      "Chelsea Finn, Paul Christiano, Pieter Abbeel, and Sergey Levine. A connection between generative adversar-\n",
      "ial networks, inverse reinforcement learning, and energy-based models. arXiv preprint arXiv:1611.03852,\n",
      "2016a.\n",
      "\n",
      "Chelsea Finn, Sergey Levine, and Pieter Abbeel. Guided cost learning: Deep inverse optimal control via policy\n",
      "\n",
      "optimization. In International Conference on Machine Learning, pp. 49–58, 2016b.\n",
      "\n",
      "Justin Fu, Katie Luo, and Sergey Levine. Learning robust rewards with adversarial inverse reinforcement\n",
      "\n",
      "learning. arXiv preprint arXiv:1710.11248, 2017.\n",
      "\n",
      "Yang Gao, Ji Lin, Fisher Yu, Sergey Levine, Trevor Darrell, et al. Reinforcement learning from imperfect\n",
      "\n",
      "demonstrations. arXiv preprint arXiv:1802.05313, 2018.\n",
      "\n",
      "Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron\n",
      "In Advances in neural information process-\n",
      "\n",
      "Courville, and Yoshua Bengio. Generative adversarial nets.\n",
      "ing systems, pp. 2672–2680, 2014.\n",
      "\n",
      "David Ha and J¨urgen Schmidhuber. Recurrent world models facilitate policy evolution. arXiv preprint\n",
      "\n",
      "arXiv:1809.01999, 2018.\n",
      "\n",
      "Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement learning with deep energy-\n",
      "\n",
      "based policies. arXiv preprint arXiv:1702.08165, 2017.\n",
      "\n",
      "Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum\n",
      "\n",
      "entropy deep reinforcement learning with a stochastic actor. arXiv preprint arXiv:1801.01290, 2018.\n",
      "\n",
      "Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, and David Meger. Deep rein-\n",
      "\n",
      "forcement learning that matters. In Thirty-Second AAAI Conference on Artiﬁcial Intelligence, 2018.\n",
      "\n",
      "Todd Hester, Matej Vecerik, Olivier Pietquin, Marc Lanctot, Tom Schaul, Bilal Piot, Dan Horgan, John Quan,\n",
      "Andrew Sendonaris, Gabriel Dulac-Arnold, et al. Deep q-learning from demonstrations. arXiv preprint\n",
      "arXiv:1704.03732, 2017.\n",
      "\n",
      "Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. In Advances in Neural Information\n",
      "\n",
      "Processing Systems, pp. 4565–4573, 2016.\n",
      "\n",
      "Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization.\n",
      "\n",
      "arXiv preprint\n",
      "\n",
      "arXiv:1412.6980, 2014.\n",
      "\n",
      "Ilya Kostrikov, Kumar Krishna Agrawal, Debidatta Dwibedi, Sergey Levine, and Jonathan Tompson.\n",
      "Discriminator-actor-critic: Addressing sample inefﬁciency and reward bias in adversarial imitation learn-\n",
      "In International Conference on Learning Representations, 2019. URL https://openreview.\n",
      "ing.\n",
      "net/forum?id=Hk4fpoA5Km.\n",
      "\n",
      "Karol Kurach, Mario Lucic, Xiaohua Zhai, Marcin Michalski, and Sylvain Gelly. The gan landscape: Losses,\n",
      "\n",
      "architectures, regularization, and normalization. arXiv preprint arXiv:1807.04720, 2018.\n",
      "\n",
      "Sergey Levine. Reinforcement learning and control as probabilistic inference: Tutorial and review. arXiv\n",
      "\n",
      "preprint arXiv:1805.00909, 2018.\n",
      "\n",
      "9\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex\n",
      "Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep\n",
      "reinforcement learning. Nature, 518(7540):529, 2015.\n",
      "\n",
      "Andrew Y Ng, Stuart J Russell, et al. Algorithms for inverse reinforcement learning. In Icml, pp. 663–670,\n",
      "\n",
      "2000.\n",
      "\n",
      "Bilal Piot, Matthieu Geist, and Olivier Pietquin. Learning from demonstrations: Is it worth estimating a reward\n",
      "function? In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pp.\n",
      "17–32. Springer, 2013.\n",
      "\n",
      "Bilal Piot, Matthieu Geist, and Olivier Pietquin. Boosted and reward-regularized classiﬁcation for apprentice-\n",
      "ship learning. In Proceedings of the 2014 international conference on Autonomous agents and multi-agent\n",
      "systems, pp. 1249–1256. International Foundation for Autonomous Agents and Multiagent Systems, 2014.\n",
      "\n",
      "Dean A Pomerleau. Efﬁcient training of artiﬁcial neural networks for autonomous navigation. Neural Compu-\n",
      "\n",
      "tation, 3(1):88–97, 1991.\n",
      "\n",
      "St´ephane Ross, Geoffrey Gordon, and Drew Bagnell. A reduction of imitation learning and structured pre-\n",
      "diction to no-regret online learning. In Proceedings of the fourteenth international conference on artiﬁcial\n",
      "intelligence and statistics, pp. 627–635, 2011.\n",
      "\n",
      "Fumihiro Sasaki, Tetsuya Yohira, and Atsuo Kawaguchi. Sample efﬁcient imitation learning for continuous\n",
      "control. In International Conference on Learning Representations, 2019. URL https://openreview.\n",
      "net/forum?id=BkN5UoAqF7.\n",
      "\n",
      "Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In In-\n",
      "telligent Robots and Systems (IROS), 2012 IEEE/RSJ International Conference on, pp. 5026–5033. IEEE,\n",
      "2012.\n",
      "\n",
      "Ruohan Wang, Carlo Ciliberto, Pierluigi Amadori, and Yiannis Demiris. Random expert distillation: Imitation\n",
      "\n",
      "learning via expert policy support estimation. arXiv preprint arXiv:1905.06750, 2019.\n",
      "\n",
      "Markus Wulfmeier, Peter Ondruska, and Ingmar Posner. Maximum entropy deep inverse reinforcement learn-\n",
      "\n",
      "ing. arXiv preprint arXiv:1507.04888, 2015.\n",
      "\n",
      "Brian D Ziebart, J Andrew Bagnell, and Anind K Dey. Modeling interaction via the principle of maximum\n",
      "causal entropy. In Proceedings of the 27th International Conference on International Conference on Machine\n",
      "Learning, pp. 1255–1262. Omnipress, 2010.\n",
      "\n",
      "10\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "A APPENDIX\n",
      "\n",
      "A.1 DERIVATION OF RBC GRADIENT\n",
      "\n",
      "Let τ = (s0, a0, s1, ..., sT ) denote a rollout, where sT is an absorbing state. Let V denote the soft\n",
      "value function,\n",
      "\n",
      "V (s) (cid:44) log\n",
      "\n",
      "exp (Qθ(s, a))\n",
      "\n",
      "(cid:33)\n",
      ".\n",
      "\n",
      "(cid:32)\n",
      "\n",
      "(cid:88)\n",
      "\n",
      "a∈A\n",
      "\n",
      "Splitting up the squared soft Bellman error terms for\n",
      "\n",
      "demo and\n",
      "\n",
      "samp in Equation 8,\n",
      "\n",
      "D\n",
      "\n",
      "D\n",
      "\n",
      "(cid:96)RBC(θ) =\n",
      "\n",
      "∇\n",
      "\n",
      "Qθ(st, at)\n",
      "\n",
      "V (st))\n",
      "\n",
      "− ∇\n",
      "\n",
      "(cid:88)\n",
      "\n",
      "τ ∈Ddemo\n",
      "\n",
      "T −1\n",
      "(cid:88)\n",
      "t=0 −\n",
      "\n",
      "(\n",
      "\n",
      "∇\n",
      "\n",
      "+ λdemo\n",
      "\n",
      "(cid:88)\n",
      "\n",
      "T −1\n",
      "(cid:88)\n",
      "t=0 ∇\n",
      "\n",
      "(cid:88)\n",
      "\n",
      "=\n",
      "\n",
      "τ ∈Ddemo\n",
      "\n",
      "+ λdemo\n",
      "\n",
      "τ ∈Ddemo\n",
      "\n",
      "T −1\n",
      "(cid:88)\n",
      "t=0 ∇\n",
      "(cid:18)\n",
      "\n",
      "δ2\n",
      "\n",
      "∇\n",
      "\n",
      "(Qθ(st, at)\n",
      "\n",
      "γV (st+1))2 + λsamp\n",
      "\n",
      "δ2(\n",
      "\n",
      "samp, 0)\n",
      "\n",
      "−\n",
      "\n",
      "∇\n",
      "\n",
      "D\n",
      "\n",
      "V (st)\n",
      "\n",
      "γ\n",
      "\n",
      "V (st+1)\n",
      "\n",
      "−\n",
      "\n",
      "∇\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "1\n",
      "\n",
      "demo,\n",
      "\n",
      "D\n",
      "\n",
      "2λdemo\n",
      "\n",
      "+ λsamp\n",
      "\n",
      "δ2(\n",
      "\n",
      "∇\n",
      "\n",
      "D\n",
      "\n",
      "samp, 0).\n",
      "\n",
      "(10)\n",
      "\n",
      "(11)\n",
      "\n",
      "Setting γ (cid:44) 1 turns the inner sum in the ﬁrst term into a telescoping sum:\n",
      "\n",
      "(11) =\n",
      "\n",
      "(cid:88)\n",
      "\n",
      "τ ∈Ddemo\n",
      "\n",
      "V (s0)\n",
      "\n",
      "(\n",
      "\n",
      "∇\n",
      "\n",
      "− ∇\n",
      "\n",
      "V (sT )) + λdemo\n",
      "\n",
      "δ2\n",
      "\n",
      "∇\n",
      "\n",
      "demo,\n",
      "\n",
      "D\n",
      "\n",
      "2λdemo\n",
      "\n",
      "+ λsamp\n",
      "\n",
      "δ2(\n",
      "\n",
      "∇\n",
      "\n",
      "D\n",
      "\n",
      "samp, 0).\n",
      "\n",
      "(12)\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "1\n",
      "\n",
      "Since sT is assumed to be absorbing, V (sT ) is zero. Thus,\n",
      "\n",
      "(12) =\n",
      "\n",
      "(cid:88)\n",
      "\n",
      "∇\n",
      "\n",
      "s0∈Ddemo\n",
      "\n",
      "V (s0) + λdemo\n",
      "\n",
      "δ2\n",
      "\n",
      "∇\n",
      "\n",
      "demo,\n",
      "\n",
      "D\n",
      "\n",
      "2λdemo\n",
      "\n",
      "+ λsamp\n",
      "\n",
      "δ2(\n",
      "\n",
      "∇\n",
      "\n",
      "D\n",
      "\n",
      "samp, 0),\n",
      "\n",
      "(13)\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "1\n",
      "\n",
      "In our experiments, we have that all the demonstration rollouts start at the same initial state s0.5\n",
      "Thus,\n",
      "\n",
      "(13)\n",
      "\n",
      "(cid:0)δ2(\n",
      "\n",
      "∝ ∇\n",
      "\n",
      "D\n",
      "\n",
      "demo, 1) + λsampδ2(\n",
      "\n",
      "samp, 0) + V (s0)(cid:1) ,\n",
      "\n",
      "D\n",
      "\n",
      "(14)\n",
      "\n",
      "where λsamp\n",
      "\n",
      "R≥0 is a constant hyperparameter.\n",
      "\n",
      "∈\n",
      "\n",
      "A.2 COMPARING THE BIASED AND UNBIASED VARIANTS OF GAIL\n",
      "\n",
      "As discussed in Section 4, to correct the original GAIL method’s biased handling of rewards at\n",
      "absorbing states, we implement the suggested changes to GAIL in Section 4.2 of Kostrikov et al.\n",
      "(2019): adding a transition to an absorbing state and a self-loop at the absorbing state to the end of\n",
      "each rollout sampled from the environment, and adding a binary feature to the observations indicat-\n",
      "ing whether or not a state is absorbing. This enables GAIL to learn a non-zero reward for absorbing\n",
      "states. We refer to the original, biased GAIL method as GAIL-DQL-B and GAIL-TRPO-B, and the\n",
      "unbiased version as GAIL-DQL-U and GAIL-TRPO-U.\n",
      "\n",
      "The mechanism for learning terminal rewards proposed in Kostrikov et al. (2019) does not apply to\n",
      "SQIL, since SQIL does not learn a reward function. SQIL implicitly assumes a reward of zero at\n",
      "absorbing states in demonstrations. This is the case in all our experiments, which include some envi-\n",
      "ronments where terminating the episode is always undesirable (e.g., walking without falling down)\n",
      "and other environments where success requires terminating the episode (e.g., landing at a target),\n",
      "suggesting that SQIL is not sensitive to the choice of termination reward, and neither signiﬁcantly\n",
      "beneﬁts nor is signiﬁcantly harmed by setting the termination reward to zero.\n",
      "\n",
      "5Demonstration rollouts may still vary due to the stochasticity of the expert policy.\n",
      "\n",
      "11\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "Domain Shift (S train\n",
      "\n",
      ") No Shift (S demo\n",
      "\n",
      ")\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "−21 ± 56\n",
      "−45 ± 18\n",
      "\n",
      "Random\n",
      "BC\n",
      "GAIL-DQL-B −91 ± 4\n",
      "GAIL-DQL-U −97 ± 3\n",
      "SQIL (Ours)\n",
      "Expert\n",
      "\n",
      "375 ± 19\n",
      "480 ± 11\n",
      "\n",
      "−68 ± 4\n",
      "698 ± 10\n",
      "−34 ± 21\n",
      "−66 ± 8\n",
      "704 ± 6\n",
      "704 ± 79\n",
      "\n",
      "Figure 5: Image-based Car Racing. Average reward on 100 episodes after training. Standard error\n",
      "on three random seeds.\n",
      "\n",
      "Figure 6: Image-based Atari. Smoothed with a rolling window of 100 episodes. Standard error\n",
      "on three random seeds. X-axis represents amount of interaction with the environment (not expert\n",
      "demonstrations).\n",
      "\n",
      "Car Racing. The results in Figure 5 show that both the biased (GAIL-DQL-B) and unbiased (GAIL-\n",
      "DQL-U) versions of GAIL perform equally poorly. The problem of training an image-based discrim-\n",
      "inator for this task may be difﬁcult enough that even with an unfair bias toward avoiding crashes that\n",
      "terminate the episode, GAIL-DQL-B does not perform better than GAIL-DQL-U.\n",
      "\n",
      "Atari. The results in Figure 6 show that SQIL outperforms both variants of GAIL on Pong and the\n",
      "unbiased version of GAIL (GAIL-DQL-U) on Breakout and Space Invaders, but performs compara-\n",
      "bly to the biased version of GAIL (GAIL-DQL-B) on Space Invaders and worse than it on Breakout.\n",
      "This may be due to the fact that in Breakout and Space Invaders, the agent has multiple lives –\n",
      "ﬁve in Breakout, and three in Space Invaders – and receives a termination signal that the episode\n",
      "has ended after losing each life. Thus, the agent experiences many more episode terminations than\n",
      "in Pong, exacerbating the bias in the way the original GAIL method handles rewards at absorb-\n",
      "ing states. Our implementation of GAIL-DQL-B in this experiment provides a learned reward of\n",
      "r(s, a) =\n",
      "D(s, a)), where D is the discriminator (see Section A.3 in the appendix for\n",
      "details). The learned reward is always positive, while the implicit reward at an absorbing state is\n",
      "zero. Thus, the agent is inadvertently encouraged to avoid terminating the episode. For Breakout\n",
      "and Space Invaders, this just happens to be the right incentive, since the objective is to stay alive as\n",
      "long as possible. GAIL-DQL-B outperforms SQIL in Breakout and performs comparably to SQIL\n",
      "in Space Invaders because GAIL-DQL-B is accidentally biased in the right way.\n",
      "\n",
      "log (1\n",
      "\n",
      "−\n",
      "\n",
      "−\n",
      "\n",
      "Lunar Lander. The results in Figure 7 show that when the initial state is varied, SQIL outperforms\n",
      "the unbiased variant of GAIL (GAIL-TRPO-U), but underperforms against the biased version of\n",
      "GAIL (GAIL-TRPO-B). The latter result is likely due to the fact that the implementation of GAIL-\n",
      "TRPO-B we used in this experiment provides a learned reward of r(s, a) = log (D(s, a)), where\n",
      "D is the discriminator (see Section A.3 in the appendix for details). The learned reward is always\n",
      "negative, while the implicit reward at an absorbing state is zero. Thus, the agent is inadvertently\n",
      "encouraged to terminate the episode quickly. For the Lunar Lander game, this just happens to be\n",
      "the right incentive, since the objective is to land on the ground and thereby terminate the episode.\n",
      "As in the Atari experiments, GAIL-TRPO-B performs better than SQIL in this experiment because\n",
      "GAIL-TRPO-B is accidentally biased in the right way.\n",
      "\n",
      "12\n",
      "\n",
      "02000400060008000NumberofOn-PolicyRollouts0200400RewardBreakoutSQIL(Ours)GAIL-DQL-BGAIL-DQL-UBC(P’91)Expert0500100015002000NumberofOn-PolicyRollouts−20−1001020RewardPong0200040006000NumberofOn-PolicyRollouts200400600RewardSpaceInvaders\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "Random\n",
      "BC\n",
      "GAIL-TRPO-B (HE’16)\n",
      "GAIL-TRPO-U\n",
      "SQIL (Ours)\n",
      "\n",
      "n λsamp = 0\n",
      "\n",
      "o\n",
      "i\n",
      "t\n",
      "a\n",
      "l\n",
      "b\n",
      "A\n",
      "\n",
      "γ = 0\n",
      "π = Unif\n",
      "RBC\n",
      "Expert\n",
      "\n",
      "Domain Shift (S train\n",
      "\n",
      ") No Shift (S demo\n",
      "\n",
      ")\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "0.10 ± 0.30\n",
      "0.07 ± 0.03\n",
      "0.98 ± 0.01\n",
      "0.67 ± 0.04\n",
      "0.89 ± 0.02\n",
      "0.12 ± 0.02\n",
      "0.41 ± 0.02\n",
      "0.47 ± 0.02\n",
      "0.66 ± 0.02\n",
      "0.93 ± 0.03\n",
      "\n",
      "0.04 ± 0.02\n",
      "0.93 ± 0.03\n",
      "0.95 ± 0.02\n",
      "0.93 ± 0.03\n",
      "0.88 ± 0.03\n",
      "0.87 ± 0.02\n",
      "0.84 ± 0.02\n",
      "0.82 ± 0.02\n",
      "0.89 ± 0.01\n",
      "0.89 ± 0.31\n",
      "\n",
      "Figure 7: Low-dimensional Lunar Lander. Best success rate on 100 consecutive episodes during\n",
      "training. Standard error on ﬁve random seeds. Performance bolded if at least within one standard\n",
      "error of expert.\n",
      "\n",
      "Figure 8: Standard error over two random seeds. No smoothing across training steps.\n",
      "\n",
      "A.3\n",
      "\n",
      "IMPLEMENTATION DETAILS\n",
      "\n",
      "To ensure fair comparisons, the same network architectures were used to evaluate SQIL, GAIL, and\n",
      "BC. For Lunar Lander, we used a network architecture with two fully-connected layers containing\n",
      "128 hidden units each to represent the Q network in SQIL, the policy and discriminator networks in\n",
      "GAIL, and the policy network in BC. For Car Racing, we used four convolutional layers (following\n",
      "(Ha & Schmidhuber, 2018)) and two fully-connected layers containing 256 hidden units each. For\n",
      "Humanoid and HalfCheetah, we used two fully-connected layers containing 256 hidden units each.\n",
      "For Atari, we used the convolutional neural network described in (Mnih et al., 2015) to represent the\n",
      "Q network in SQIL, as well as the Q network and discriminator network in GAIL.\n",
      "\n",
      "To ensure fair comparisons, the same demonstration data were used to train SQIL, GAIL, and BC.\n",
      "For Lunar Lander, we collected 100 demonstration rollouts. For Car Racing, Pong, Breakout, and\n",
      "Space Invaders, we collected 20 demonstration rollouts. Expert demonstrations were generated\n",
      "from scratch for Lunar Lander using DQN (Mnih et al., 2015), and collected from open-source pre-\n",
      "trained policies for Car Racing (Ha & Schmidhuber, 2018) as well as Humanoid and HalfCheetah\n",
      "(Dhariwal et al., 2017). The Humanoid demonstrations were generated by a stochastic expert policy,\n",
      "while the HalfCheetah demonstrations were generated by a deterministic expert policy; both experts\n",
      "were trained using TRPO.6 We used two open-source implementations of GAIL: (Fu et al., 2017)\n",
      "for Lunar Lander, and (Dhariwal et al., 2017) for MuJoCo. We adapted the OpenAI Baselines\n",
      "implementation of GAIL to use soft Q-learning for Car Racing and Atari. Expert demonstrations\n",
      "were generated from scratch for Atari using DQN.\n",
      "For Lunar Lander, we set λsamp = 10−6. For Car Racing, we set λsamp = 0.01. For all other\n",
      "environments, we set λsamp = 1.\n",
      "\n",
      "6https://drive.google.com/drive/folders/1h3H4AY_ZBx08hz-Ct0Nxxus-V1melu1U\n",
      "\n",
      "13\n",
      "\n",
      "050000100000150000NumberofTrainingSteps50100150SquaredBellmanError0200400600RewardHumanoid-v1\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "SQIL was not pre-trained in any of the experiments. GAIL was pre-trained using BC for HalfChee-\n",
      "tah, but was not pre-trained in any other experiments.\n",
      "\n",
      "In standard implementations of soft Q-learning and SAC, the agent’s experience replay buffer typ-\n",
      "ically has a ﬁxed size, and once the buffer is full, old experiences are deleted to make room for\n",
      "new experiences. In SQIL, we never delete demonstration experiences from the replay buffer, but\n",
      "otherwise follow the standard implementation.\n",
      "\n",
      "We use Adam (Kingma & Ba, 2014) to take the gradient step in line 4 of Algorithm 1.\n",
      "\n",
      "The BC and GAIL performance metrics in Section 4.3 are taken from (Dhariwal et al., 2017).7\n",
      "\n",
      "The GAIL and SQIL policies in Section 4.3 are set to be deterministic during the evaluation rollouts\n",
      "used to measure performance.\n",
      "\n",
      "7https://github.com/openai/baselines/blob/master/baselines/gail/result/\n",
      "\n",
      "gail-result.md\n",
      "\n",
      "14\n",
      "\n",
      "\f",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "STRUCTPOOL: STRUCTURED GRAPH POOLING VIA\n",
      "CONDITIONAL RANDOM FIELDS\n",
      "\n",
      "Hao Yuan\n",
      "Department of Computer Science & Engineering\n",
      "Texas A&M University\n",
      "College Station, TX 77843, USA\n",
      "hao.yuan@tamu.edu\n",
      "\n",
      "Shuiwang Ji\n",
      "Department of Computer Science & Engineering\n",
      "Texas A&M University\n",
      "College Station, TX 77843, USA\n",
      "sji@tamu.edu\n",
      "\n",
      "ABSTRACT\n",
      "\n",
      "Learning high-level representations for graphs is of great importance for graph\n",
      "analysis tasks. In addition to graph convolution, graph pooling is an important\n",
      "but less explored research area.\n",
      "In particular, most of existing graph pooling\n",
      "techniques do not consider the graph structural information explicitly. We argue\n",
      "that such information is important and develop a novel graph pooling technique,\n",
      "know as the STRUCTPOOL, in this work. We consider the graph pooling as a\n",
      "node clustering problem, which requires the learning of a cluster assignment ma-\n",
      "trix. We propose to formulate it as a structured prediction problem and employ\n",
      "conditional random ﬁelds to capture the relationships among the assignments of\n",
      "different nodes. We also generalize our method to incorporate graph topologi-\n",
      "cal information in designing the Gibbs energy function. Experimental results on\n",
      "multiple datasets demonstrate the effectiveness of our proposed STRUCTPOOL.\n",
      "\n",
      "1\n",
      "\n",
      "INTRODUCTION\n",
      "\n",
      "Graph neural networks have achieved the state-of-the-art results for multiple graph tasks, such as\n",
      "node classiﬁcation (Veliˇckovi´c et al., 2018; Gao & Ji, 2019b; Gao et al., 2018) and link predic-\n",
      "tion (Zhang & Chen, 2018; Cai & Ji, 2020). These results demonstrate the effectiveness of graph\n",
      "neural networks to learn node representations. However, graph classiﬁcation tasks also require learn-\n",
      "ing good graph-level representations. Since pooling operations are shown to be effective in many\n",
      "image and NLP tasks, it is natural to investigate pooling techniques for graph data (Yu & Koltun,\n",
      "2016; Springenberg et al., 2014). Recent work extends the global sum/average pooling operations\n",
      "to graph models by simply summing or averaging all node features (Atwood & Towsley, 2016;\n",
      "Simonovsky & Komodakis, 2017). However, these trivial global pooling operations may lose im-\n",
      "portant features and ignore structural information. Furthermore, global pooling are not hierarchical\n",
      "so that we cannot apply them where multiple pooling operations are required, such as Graph U-\n",
      "Net (Gao & Ji, 2019a). Several advanced graph pooling methods, such as SORTPOOL (Zhang\n",
      "et al., 2018), TOPKPOOL (Gao & Ji, 2019a), DIFFPOOL (Ying et al., 2018), and SAGPOOL (Lee\n",
      "et al., 2019) , are recently proposed and achieve promising performance on graph classiﬁcation tasks.\n",
      "However, none of them explicitly models the relationships among different nodes and thus may ig-\n",
      "nore important structural information. We argue that such information is important and should be\n",
      "explicitly captured in graph pooling.\n",
      "\n",
      "In this work, we propose a novel graph pooling technique, known as the STRUCTPOOL, that formu-\n",
      "lates graph pooling as a structured prediction problem. Following DIFFPOOL (Ying et al., 2018),\n",
      "we consider graph pooling as a node clustering problem, and each cluster corresponds to a node\n",
      "in the new graph after pooling. Intuitively, two nodes with similar features should have a higher\n",
      "probability of being assigned to the same cluster. Hence, the assignment of a given node should\n",
      "depend on both the input node features and the assignments of other nodes. We formulate this as a\n",
      "structured prediction problem and employ conditional random ﬁelds (CRFs) (Lafferty et al., 2001)\n",
      "to capture such high-order structural relationships among the assignments of different nodes. In\n",
      "addition, we generalize our method by incorporating the graph topological information so that our\n",
      "method can control the clique set in our CRFs. We employ the mean ﬁeld approximation to compute\n",
      "the assignments and describe how to incorporate it in graph networks. Then the networks can be\n",
      "\n",
      "1\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "trained in an end-to-end fashion. Experiments show that our proposed STRUCTPOOL outperforms\n",
      "existing methods signiﬁcantly and consistently. We also show that STRUCTPOOL incurs acceptable\n",
      "computational cost given its superior performance.\n",
      "\n",
      "2 BACKGROUND AND RELATED WORK\n",
      "\n",
      "2.1 GRAPH CONVOLUTIONAL NETWORKS\n",
      "\n",
      "A graph can be represented by its adjacency matrix and node features. Formally, for a graph\n",
      "G consisting of n nodes, its topology information can be represented by an adjacency matrix\n",
      "A ∈ {0, 1}n×n, and the node features can be represented as X ∈ Rn×c assuming each node\n",
      "has a c-dimensional feature vector. Deep graph neural networks (GNNs) learn feature representa-\n",
      "tions for different nodes using these matrices (Gilmer et al., 2017). Several approaches are pro-\n",
      "posed to investigate deep GNNs, and they generally follow a neighborhood information aggregation\n",
      "scheme (Gilmer et al., 2017; Xu et al., 2019; Hamilton et al., 2017; Kipf & Welling, 2017; Veliˇckovi´c\n",
      "et al., 2018). In each step, the representation of a node is updated by aggregating the representations\n",
      "of its neighbors. Graph Convolutional Networks (GCNs) are popular variants of GNNs and inspired\n",
      "by the ﬁrst order graph Laplacian methods (Kipf & Welling, 2017). The graph convolution operation\n",
      "is formally deﬁned as:\n",
      "\n",
      "Xi+1 = f (D− 1\n",
      "\n",
      "2 ˆAD− 1\n",
      "\n",
      "(1)\n",
      "where ˆA = A + I is used to add self-loops to the adjacency matrix, D denotes the diagonal node\n",
      "degree matrix to normalize ˆA, Xi ∈ Rn×ci are the node features after ith graph convolution layer,\n",
      "Pi ∈ Rci×ci+1 is a trainable matrix to perform feature transformation, and f (·) denotes a non-linear\n",
      "activation function. Then Xi ∈ Rn×ci is transformed to Xi+1 ∈ Rn×ci+1 where the number of\n",
      "nodes remains the same. A similar form of GCNs proposed in (Zhang et al., 2018) can be expressed\n",
      "as:\n",
      "\n",
      "2 XiPi),\n",
      "\n",
      "(2)\n",
      "It differs from the GCNs in Equation (1) by performing different normalization and is a theoretically\n",
      "closer approximation to the Weisfeiler-Lehman algorithm (Weisfeiler & Lehman, 1968). Hence, in\n",
      "our models, we use the latter version of GCNs in Equation (2).\n",
      "\n",
      "Xi+1 = f (D−1 ˆAXiPi).\n",
      "\n",
      "2.2 GRAPH POOLING\n",
      "\n",
      "Several advanced pooling techniques are proposed recently for graph models, such as SORTPOOL,\n",
      "TOPKPOOL, DIFFPOOL, and SAGPOOL, and achieve great performance on multiple benchmark\n",
      "datasets. All of SORTPOOL (Zhang et al., 2018), TOPKPOOL (Gao & Ji, 2019a), and SAG-\n",
      "POOL (Lee et al., 2019) learn to select important nodes from the original graph and use these nodes\n",
      "to build a new graph. They share the similar idea to learn a sorting vector based on node representa-\n",
      "tions using GCNs, which indicates the importance of different nodes. Then only the top k important\n",
      "nodes are selected to form a new graph while the other nodes are ignored. However, the ignored\n",
      "nodes may contain important features and this information is lost during pooling. DIFFPOOL (Ying\n",
      "et al., 2018) treats the graph pooling as a node clustering problem. A cluster of nodes from the orig-\n",
      "inal graph are merged to form a new node in the new graph. DIFFPOOL proposes to perform GCNs\n",
      "on node features to obtain node clustering assignment matrix. Intuitively, the cluster assignment\n",
      "of a given node should depend on the cluster assignments of other nodes. However, DIFFPOOL\n",
      "does not explicitly consider such high-order structural relationships, which we believe are important\n",
      "for graph pooling. In this work, we propose a novel structured graph pooling technique, known as\n",
      "the STRUCTPOOL, for effectively learning high-level graph representations. Different from exist-\n",
      "ing methods, our method explicitly captures high-order structural relationships between different\n",
      "nodes via conditional random ﬁelds. In addition, our method is generalized by incorporating graph\n",
      "topological information A to control which node pairs are included in our CRFs.\n",
      "\n",
      "2.3\n",
      "\n",
      "INTEGRATING CRFS WITH GNNS\n",
      "\n",
      "Recent work (Gao et al., 2019; Qu et al., 2019; Ma et al., 2019) investigates how to combine CRFs\n",
      "with GNNs. The CGNF (Ma et al., 2019) is a GNN architecture for graph node classiﬁcation which\n",
      "explicitly models a joint probability of the entire set of node labels via CRFs and performs inference\n",
      "\n",
      "2\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "via dynamic programming. In addition, the GMNN (Qu et al., 2019) focuses on semi-supervised\n",
      "object classiﬁcation tasks and models the joint distribution of object labels conditioned on object\n",
      "attributes using CRFs. It proposes a pseudolikelihood variational EM framework for model learning\n",
      "and inference. Recent work (Gao et al., 2019) integrates CRFs with GNNs by proposing a CRF\n",
      "layer to encourage similar nodes to have similar hidden features so that similarity information can\n",
      "be preserved explicitly. All these methods are proposed for node classiﬁcation tasks and the CRFs\n",
      "are incorporated in different ways. Different from existing work, our STRUCTPOOL is proposed for\n",
      "graph pooling operation and the energy is optimized via mean ﬁeld approximation. All operations\n",
      "in our STRUCTPOOL can be realized by GNN operations so that our STRUCTPOOL can be easily\n",
      "used in any GNNs and trained in an end-to-end fashion.\n",
      "\n",
      "3 STRUCTURED GRAPH POOLING\n",
      "\n",
      "3.1 GRAPH POOLING VIA NODE CLUSTERING\n",
      "\n",
      "Even though pooling techniques are shown to facilitate the training of deep models and improve\n",
      "their performance signiﬁcantly in many image and NLP tasks (Yu & Koltun, 2016; Springenberg\n",
      "et al., 2014), local pooling operations cannot be directly applied to graph tasks. The reason is there\n",
      "is no spatial locality information among graph nodes. Global max/average pooling operations can be\n",
      "employed for graph tasks but they may lead to information loss, due to largely reducing the size of\n",
      "representations trivially. A graph G with n nodes can be represented by a feature matrix X ∈ Rn×c\n",
      "and an adjacent matrix A ∈ {0, 1}n×n. Graph pooling operations aim at reducing the number of\n",
      "graph nodes and learning new representations. Suppose that graph pooling generates a new graph\n",
      "˜G with k nodes. The representation matrices of ˜G are denoted as ˜X ∈ Rk×˜c and ˜A ∈ {0, 1}k×k.\n",
      "The goal of graph pooling is to learn relationships between X, A and ˜X, ˜A.\n",
      "In this work, we\n",
      "consider graph pooling via node clustering. In particular, the nodes of the original graph G are\n",
      "assigned to k different clusters. Then each cluster is transformed to a new node in the new graph\n",
      "˜G. The clustering assignments can be represented as an assignment matrix M ∈ Rn×k. For hard\n",
      "assignments, mi,j ∈ {0, 1} denotes if node i in graph G belongs to cluster j. For soft assignments,\n",
      "mi,j ∈ [0, 1] denotes the probability that node i in graph G belongs to cluster j and (cid:80)\n",
      "j mi,j = 1.\n",
      "Then the new graph ˜G can be computed as\n",
      "\n",
      "˜X = M T X, ˜A = g(M T AM ),\n",
      "\n",
      "(3)\n",
      "\n",
      "where g(·) is a function that g(˜ai,j) = 1 if ˜ai,j > 0 and g(˜ai,j) = 0 otherwise.\n",
      "\n",
      "3.2 LEARNING CLUSTERING ASSIGNMENTS VIA CONDITIONAL RANDOM FIELDS\n",
      "\n",
      "Intuitively, node features describe the properties of different nodes. Then nodes with similar features\n",
      "should have a higher chance to be assigned to the same cluster. That is, for any node in the original\n",
      "graph G, its cluster assignment should not only depend on node feature matrix X but also condition\n",
      "on the cluster assignments of the other nodes. We believe such high-order structural information is\n",
      "useful for graph pooling and should be explicitly captured while learning clustering assignments. To\n",
      "this end, we propose a novel structured graph pooling technique, known as STRUCTPOOL, which\n",
      "generates the assignment matrix by considering the feature matrix X and the relationships between\n",
      "the assignments of different nodes. We propose to formulate this as a conditional random ﬁeld\n",
      "(CRF) problem. The CRFs model a set of random variables with a Markov Random Field (MRF),\n",
      "conditioned on a global observation (Lafferty et al., 2001). We formally deﬁne Y = {Y1, · · · , Yn}\n",
      "as a random ﬁeld where Yi ∈ {1, · · · , k} is a random variable. Each Yi indicates to which cluster\n",
      "the node i is assigned. Here the feature representation X is treated as global observation. We build\n",
      "a graphical model on Y , which is deﬁned as G(cid:48). Then the pair (Y, X) can be deﬁned as a CRF,\n",
      "characterized by the Gibbs distribution as\n",
      "\n",
      "P (Y |X) =\n",
      "\n",
      "exp\n",
      "\n",
      "−\n",
      "\n",
      "ψc(Yc|X)\n",
      "\n",
      " ,\n",
      "\n",
      "(4)\n",
      "\n",
      "1\n",
      "\n",
      "Z(X)\n",
      "\n",
      "(cid:88)\n",
      "\n",
      "c∈CG(cid:48)\n",
      "\n",
      "\n",
      "\n",
      "where c denotes a clique, CG(cid:48) is a set of cliques in G(cid:48), Z(X) is the partition function, and ψc(·) is a\n",
      "potential function induced by c (Kr¨ahenb¨uhl & Koltun, 2011; Lafferty et al., 2001). Then the Gibbs\n",
      "\n",
      "\n",
      "\n",
      "3\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "Figure 1: Illustrations of our proposed STRUCTPOOL. Given a graph with 6 nodes, the color of each\n",
      "node represents its features. We perform graph pooling to obtain a new graph with k = 4 nodes.\n",
      "The unary energy matrix can be obtained by multiple GCN layers using X and A. The pairwise\n",
      "energy is measured by attention matrix using node feature X and topology information A. Then by\n",
      "performing iterative updating, the mean ﬁeld approximation yields the most probable assignment\n",
      "matrix. Finally, we obtain the new graph with 4 nodes, represented by ˜X and ˜A.\n",
      "\n",
      "(5)\n",
      "\n",
      "(6)\n",
      "\n",
      "energy function for an assignment y = {y1, · · · , yn} for all variables can be written as\n",
      "\n",
      "E(y|X) =\n",
      "\n",
      "ψc(yc|X).\n",
      "\n",
      "(cid:88)\n",
      "\n",
      "c∈CG(cid:48)\n",
      "\n",
      "Finding the optimal assignment is equivalent to maximizing P (Y |X), which can also be interpreted\n",
      "as minimizing the Gibbs energy.\n",
      "\n",
      "3.3 GIBBS ENERGY WITH TOPOLOGY INFORMATION\n",
      "\n",
      "Now we deﬁne the clique set CG(cid:48) in G(cid:48). Similar to the existing CRF model (Kr¨ahenb¨uhl & Koltun,\n",
      "2011), we include all unary cliques in CG(cid:48) since we need to measure the energy for assigning\n",
      "each node. For pairwise cliques, we generalize our method to control the pairwise clique set by\n",
      "incorporating the graph topological information A. We consider (cid:96)-hop connectivity based on A\n",
      "to deﬁne the pairwise cliques, which builds pairwise relationships between different nodes. Let\n",
      "A(cid:96) ∈ {0, 1}n×n represent the (cid:96)-hop connectivity of graph G where a(cid:96)\n",
      "i,j = 1 indicates node i and\n",
      "node j are reachable in G within (cid:96) hops. Then we include all pairwise cliques (i, j) in CG(cid:48) if\n",
      "a(cid:96)\n",
      "i,j = 1. Altogether, the Gibbs energy for a cluster assignment y can be written as\n",
      "\n",
      "E(y) =\n",
      "\n",
      "ψu(yi) +\n",
      "\n",
      "ψp(yi, yj)a(cid:96)\n",
      "\n",
      "i,j,\n",
      "\n",
      "(cid:88)\n",
      "\n",
      "i\n",
      "\n",
      "(cid:88)\n",
      "\n",
      "i(cid:54)=j\n",
      "\n",
      "where ψu(yi) represents the unary energy for node i to be assigned to cluster yi.\n",
      "In addition,\n",
      "ψp(yi, yj) is the pairwise energy, which indicates the energy of assigning node i, j to cluster yi, yj\n",
      "respectively. Note that we drop the condition information in Equation (6) for simplicity. If (cid:96) is\n",
      "large enough, our CRF is equivalent to the dense CRFs. If (cid:96) is equal to 1, we have A(cid:96) = A so\n",
      "that only 1-hop information in the adjacent matrix is considered. These two types of energy can be\n",
      "obtained directly by neural networks (Zheng et al., 2015). Given the global observations X and the\n",
      "topology information A, we employ multiple graph convolution layers to obtain the unary energy\n",
      "Ψu ∈ Rn×k. Existing work on image tasks (Kr¨ahenb¨uhl & Koltun, 2011) proposes to employ Gaus-\n",
      "sian kernels to measure the pairwise energy. However, due to computational inefﬁciency, we cannot\n",
      "directly apply it to our CRF model. The pairwise energy proposed in (Kr¨ahenb¨uhl & Koltun, 2011)\n",
      "can be written as\n",
      "\n",
      "ψp(yi, yj) = µ(yi, yj)\n",
      "\n",
      "w(m)k(m)(xi, xj),\n",
      "\n",
      "(7)\n",
      "\n",
      "where k(m)(·, ·) represents the mth Gaussian kernel, xi is the feature vector for node i in X, w(m)\n",
      "denotes learnable weights, and µ(yi, yj) is a compatibility function that models the compatibility\n",
      "\n",
      "K\n",
      "(cid:88)\n",
      "\n",
      "m=1\n",
      "\n",
      "4\n",
      "\n",
      "1234561234Original GraphNew Graph  GCNsAttention Iteratively UpdateUnary EnergyAssignment MatrixSoftmaxPairwise Energy    \f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "Algorithm 1 STRUCTPOOL\n",
      "1: Given a graph G with n nodes represented by X ∈ Rn×c and A ∈ {0, 1}n×n, the goal is to\n",
      "obtain ˜G with k nodes that ˜X ∈ Rk×˜c and ˜A ∈ {0, 1}k×k. The (cid:96)-hop connectivity matrix A(cid:96)\n",
      "can be easily obtained from A.\n",
      "\n",
      "2: Perform GCNs to obtain unary energy matrix Ψu ∈ Rn×k.\n",
      "3: Initialize that Q(i, j) = 1\n",
      "Zi\n",
      "4: while not converged do\n",
      "\n",
      "exp (Ψu(i, j)) for all 0 ≤ i ≤ n and 0 ≤ j ≤ k.\n",
      "\n",
      "xT\n",
      "\n",
      "i xj\n",
      "m(cid:54)=i xT\n",
      "\n",
      "(cid:80)\n",
      "\n",
      "6:\n",
      "\n",
      "5:\n",
      "\n",
      "i xm\n",
      "\n",
      "m(cid:54)=i wi,mQ(m, j).\n",
      "\n",
      "Calculate attention map W that wi,j =\n",
      "Message passing that ˜Q(i, j) = (cid:80)\n",
      "Compatibility transform that ˆQ(i, j) = (cid:80)\n",
      "Local update that ¯Q(i, j) = Ψu(i, j) − ˆQ(i, j).\n",
      "Perform normalization that Q(i, j) = 1\n",
      "Zi\n",
      "\n",
      "7:\n",
      "8:\n",
      "9:\n",
      "10: end while\n",
      "11: For soft assignments, the assignment matrix is M = softmax(Q).\n",
      "12: For hard assignments, the assignment matrix is M = argmax(Q) for each row.\n",
      "13: Obtain new graph ˜Q that ˜X = M T X, ˜A = g(M T AM ).\n",
      "\n",
      "exp (cid:0) ¯Q(i, j)(cid:1) for all i and j.\n",
      "\n",
      "m µ(m, j) ˜Q(i, m).\n",
      "\n",
      "a(cid:96)\n",
      "i,j for all i (cid:54)= j and 0 ≤ i, j ≤ n.\n",
      "\n",
      "between different assignment pairs. However, it is computationally inefﬁcient to accurately com-\n",
      "pute the outputs of Gaussian kernels, especially for graph data when the feature vectors are high-\n",
      "dimensional. Hence, in this work, we propose to employ the attention matrix as the measurement\n",
      "of pairwise energy. Intuitively, Gaussian kernels indicate how strongly different feature vectors are\n",
      "connected with each other. Similarly, the attention matrix reﬂects similarities between different fea-\n",
      "ture vectors but with a signiﬁcantly less computational cost. Speciﬁcally, each feature vector xi is\n",
      "attended to any other feature vector xj if the pair (i, j) is existing in clique set CG(cid:48). Hence, the\n",
      "pairwise energy can be obtained by\n",
      "\n",
      "ψp(yi, yj) = µ(yi, yj)\n",
      "\n",
      "xT\n",
      "i xj\n",
      "k(cid:54)=i xT\n",
      "\n",
      "i xk\n",
      "\n",
      ",\n",
      "\n",
      "(cid:80)\n",
      "\n",
      "(8)\n",
      "\n",
      "It can be efﬁciently computed by matrix multiplication and normalization. Minimizing the Gibbs en-\n",
      "ergy in Equation (6) results in the most probable cluster assignments for a given graph G. However,\n",
      "such minimization is intractable, and hence a mean ﬁeld approximation is proposed (Kr¨ahenb¨uhl &\n",
      "Koltun, 2011), which is an iterative updating algorithm. We follow the mean-ﬁeld approximation\n",
      "to obtain the most probable cluster assignments. Altogether, the steps of our proposed STRUCT-\n",
      "POOL are shown in Algorithm 1. All operations in our proposed STRUCTPOOL can be implemented\n",
      "as GNN operations, and hence the STRUCTPOOL can be employed in any deep graph model and\n",
      "trained in an end-to-end fashion. The unary energy matrix can be obtained by stacking several\n",
      "GCN layers, and the normalization operations (step 3&9 in Algorithm 1) are equivalent to softmax\n",
      "operations. All other steps can be computed by matrix computations. It is noteworthy that the com-\n",
      "patibility function µ(yi, yj) can be implemented as a trainable matrix N ∈ Rk×k, and automatically\n",
      "learned during training. Hence, no prior domain knowledge is required for designing the compatibil-\n",
      "ity function. We illustrate our proposed STRUCTPOOL in Figure 1 where we perform STRUCTPOOL\n",
      "on a graph G with 6 nodes, and obtain a new graph ˜G with 4 nodes.\n",
      "\n",
      "3.4 COMPUTATIONAL COMPLEXITY ANALYSIS\n",
      "\n",
      "We theoretically analyze the computational efﬁciency of our proposed STRUCTPOOL. Since\n",
      "computational efﬁciency is especially important for large-scale graph datasets, we assume that\n",
      "n > k, c, ˜c. The computational complexity of one GCN layer is O(n3 + n2c + nc˜c) ≈ O(n3).\n",
      "Assuming we employ i layers of GCNs to obtain the unary energy, its computational cost is\n",
      "O(in3). Assuming there are m iterations in our updating algorithm, the computational com-\n",
      "plexity is O(m(n2c + n2k + nk2)) ≈ O(mn3). The ﬁnal step for computing ˜A and ˜X takes\n",
      "O(nkc + n2k + nk2) ≈ O(n3) computational complexity. Altogether, the complexity STRUCT-\n",
      "POOL is O((m + i)n3), which is close to the complexity of stacking m + i layers of GCNs.\n",
      "\n",
      "5\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "Table 1: Classiﬁcation results for six benchmark datasets. Note that none of these deep methods\n",
      "can outperform the traditional method WL on COLLAB. We believe the reason is the graphs in\n",
      "COLLAB only have single-layer structures while deep models are too complex to capture them.\n",
      "\n",
      "Method\n",
      "\n",
      "Dataset\n",
      "\n",
      "ENZYMES D&D COLLAB\n",
      "\n",
      "PROTEINS\n",
      "\n",
      "IMDB-B IMDB-M\n",
      "\n",
      "GRAPHLET\n",
      "SHORTEST-PATH\n",
      "WL\n",
      "\n",
      "PATCHYSAN\n",
      "DCNN\n",
      "DGK\n",
      "ECC\n",
      "GRAPHSAGE\n",
      "SET2SET\n",
      "DGCNN\n",
      "DIFFPOOL\n",
      "\n",
      "STRUCTPOOL\n",
      "\n",
      "41.03\n",
      "42.32\n",
      "53.43\n",
      "\n",
      "-\n",
      "-\n",
      "-\n",
      "\n",
      "53.50\n",
      "54.25\n",
      "60.15\n",
      "57.12\n",
      "62.53\n",
      "\n",
      "63.83\n",
      "\n",
      "74.85\n",
      "78.86\n",
      "78.34\n",
      "\n",
      "76.27\n",
      "58.09\n",
      "\n",
      "-\n",
      "\n",
      "72.54\n",
      "75.42\n",
      "78.12\n",
      "79.37\n",
      "80.64\n",
      "\n",
      "84.19\n",
      "\n",
      "64.66\n",
      "59.10\n",
      "78.61\n",
      "\n",
      "72.60\n",
      "52.11\n",
      "73.09\n",
      "67.79\n",
      "68.25\n",
      "71.75\n",
      "73.76\n",
      "75.48\n",
      "\n",
      "74.22\n",
      "\n",
      "72.91\n",
      "76.43\n",
      "74.68\n",
      "\n",
      "75.00\n",
      "61.29\n",
      "71.68\n",
      "72.65\n",
      "70.48\n",
      "74.29\n",
      "75.54\n",
      "76.25\n",
      "\n",
      "80.36\n",
      "\n",
      "-\n",
      "-\n",
      "-\n",
      "\n",
      "-\n",
      "-\n",
      "-\n",
      "\n",
      "-\n",
      "\n",
      "71.00\n",
      "49.06\n",
      "66.96\n",
      "\n",
      "45.23\n",
      "33.49\n",
      "44.55\n",
      "\n",
      "70.03\n",
      "\n",
      "47.83\n",
      "\n",
      "74.70\n",
      "\n",
      "52.47\n",
      "\n",
      "-\n",
      "-\n",
      "-\n",
      "\n",
      "-\n",
      "-\n",
      "-\n",
      "\n",
      "-\n",
      "\n",
      "3.5 DEEP GRAPH NETWORKS FOR GRAPH CLASSIFICATION\n",
      "\n",
      "In this section, we investigate graph classiﬁcation tasks which require both good node-level and\n",
      "graph-level representations. For most state-of-the-art deep graph classiﬁcation models, they share\n",
      "a similar pipeline that ﬁrst produces node representations using GNNs, then performs pooling op-\n",
      "erations to obtain high-level representations, and ﬁnally employs fully-connected layers to perform\n",
      "classiﬁcation. Note that the high-level representations can be either a vector or a group of k vectors.\n",
      "For a set of graphs with different node numbers, with a pre-deﬁned k, our proposed STRUCTPOOL\n",
      "can produce k vectors for each graphs. Hence, our method can be easily generalized and coupled\n",
      "to any deep graph classiﬁcation model. Specially, our model for graph classiﬁcation is developed\n",
      "based on DGCNN (Zhang et al., 2018). Given any input graph, our model ﬁrst employs several\n",
      "layers of GCNs (Equation (2)) to aggregate features from neighbors and learn representations for\n",
      "nodes. Next, we perform one STRUCTPOOL layer to obtain k vectors for each graph. Finally, 1D\n",
      "convolutional layers and fully-connected layers are used to classify the graph.\n",
      "\n",
      "4 EXPERIMENTAL STUDIES\n",
      "\n",
      "4.1 DATASETS AND EXPERIMENTAL SETTINGS\n",
      "\n",
      "We evaluate our proposed STRUCTPOOL on eight benchmark datasets, including ﬁve bioinformatics\n",
      "protein datasets: ENZYMES, PTC, MUTAG, PROTEINS (Borgwardt et al., 2005), D&D (Dobson\n",
      "& Doig, 2003), and three social network datasets: COLLAB (Yanardag & Vishwanathan, 2015b),\n",
      "IMDB-B, IMDB-M (Yanardag & Vishwanathan, 2015a). Most of them are relatively large-scale and\n",
      "hence suitable for evaluating deep graph models. We report the statistics and properties of them in\n",
      "Supplementary Table 6. Please see the Supplementary Section A for experimental settings.\n",
      "\n",
      "We compare our method with several state-of-the-art deep GNN methods. PATCHYSAN (Niepert\n",
      "et al., 2016) learns node representations and a canonical node ordering to perform classiﬁcation.\n",
      "DCNN (Atwood & Towsley, 2016) learns multi-scale substructure features by diffusion graph con-\n",
      "volutions and performs global sum pooling. DGK (Yanardag & Vishwanathan, 2015a) models latent\n",
      "representations for sub-structures in graphs, which is similar to learn word embeddings. ECC (Si-\n",
      "monovsky & Komodakis, 2017) performs GCNs conditioning on both node features and edge in-\n",
      "formation and uses global sum pooling before the ﬁnal classiﬁer. GRAPHSAGE (Hamilton et al.,\n",
      "2017) is an inductive framework which generates node embeddings by sampling and aggregating\n",
      "features from local neighbors, and it employs global mean pooling. SET2SET (Vinyals et al., 2015)\n",
      "proposes an aggregation method to replace the global pooling operations in deep graph networks.\n",
      "DGCNN (Zhang et al., 2018) proposes a pooling strategy named SORTPOOL which sorts all nodes\n",
      "\n",
      "6\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "Table 2: Comparisons between different pooling techniques under the same framework.\n",
      "\n",
      "Method\n",
      "\n",
      "Dataset\n",
      "\n",
      "ENZYMES D&D COLLAB\n",
      "\n",
      "PROTEINS\n",
      "\n",
      "IMDB-B IMDB-M\n",
      "\n",
      "SUM POOL\n",
      "SORTPOOL\n",
      "TOPK POOL\n",
      "DIFFPOOL\n",
      "SAGPOOL\n",
      "\n",
      "STRUCTPOOL\n",
      "\n",
      "47.33\n",
      "52.83\n",
      "53.67\n",
      "60.33\n",
      "64.17\n",
      "\n",
      "63.83\n",
      "\n",
      "78.72\n",
      "80.60\n",
      "81.71\n",
      "80.94\n",
      "81.03\n",
      "\n",
      "84.19\n",
      "\n",
      "69.45\n",
      "73.92\n",
      "73.34\n",
      "71.78\n",
      "73.28\n",
      "\n",
      "74.22\n",
      "\n",
      "76.26\n",
      "76.83\n",
      "77.47\n",
      "77.74\n",
      "78.82\n",
      "\n",
      "80.36\n",
      "\n",
      "51.69\n",
      "70.00\n",
      "72.80\n",
      "72.40\n",
      "73.40\n",
      "\n",
      "74.70\n",
      "\n",
      "42.76\n",
      "46.26\n",
      "49.00\n",
      "50.13\n",
      "51.13\n",
      "\n",
      "52.47\n",
      "\n",
      "by learning and selects the ﬁrst k nodes to form a new graph. DIFFPOOL (Ying et al., 2018) is\n",
      "built based on GRAPHSAGE architecture but with their proposed differentiable pooling. Note that\n",
      "for most of these methods, pooling operations are employed to obtain graph-level representations\n",
      "before the ﬁnal classiﬁer. In addition, we compare our STRUCTPOOL with three graph kernels:\n",
      "Graphlet (Shervashidze et al., 2009), Shortest-path (Borgwardt & Kriegel, 2005), and Weisfeiler-\n",
      "Lehman subtree kernel (WL) (Weisfeiler & Lehman, 1968).\n",
      "\n",
      "4.2 CLASSIFICATION RESULTS\n",
      "\n",
      "We evaluate our proposed method on six benchmark datasets and compare with several state-of-the-\n",
      "art approaches. The results are reported in Table 1 where the best results are shown in bold and the\n",
      "second best results are shown with underlines. For our STRUCTPOOL, we perform 10-fold cross\n",
      "validations and report the average accuracy for each dataset. The 10-fold splitting is the same as\n",
      "DGCNN. For all comparing methods, the results are taken from existing work (Ying et al., 2018;\n",
      "Zhang et al., 2018). We can observe that our STRUCTPOOL obtains the best performance on 5 out of\n",
      "6 benchmark datasets. For these 5 datasets, the classiﬁcation results of our method are signiﬁcantly\n",
      "better than all comparing methods, including advanced models DGCNN and DIFFPOOL. Notably,\n",
      "our model outperforms the second-best performance by an average of 3.58% on these 5 datasets.\n",
      "In addition, the graph kernel method WL obtains the best performance on COLLAB dataset and\n",
      "none of these deep models can achieve similar performance. Our model can obtain competitive\n",
      "performance compared with the second best model. This is because many graphs in COLLAB only\n",
      "have simple structures and deep models may be too complex to capture them.\n",
      "\n",
      "4.3 COMPARISONS OF DIFFERENT POOLING METHODS\n",
      "\n",
      "To demonstrate the effectiveness of our proposed pooling technique, we compare different pooling\n",
      "techniques under the same network framework. Speciﬁcally, we compare our STRUCTPOOL with\n",
      "the global sum pool, SORTPOOL, TOPKPOOL, DIFFPOOL, and SAGPOOL. All pooling methods\n",
      "are employed in the network framework introduced in Section 3.5. In addition, the same 10-fold\n",
      "cross validations from DGCNN are used for all pooling methods. We report the results in Table 2\n",
      "and the best results are shown in bold. Obviously, our method achieves the best performance on ﬁve\n",
      "of six datasets, and signiﬁcantly outperforms all comparing pooling techniques. For the dataset EN-\n",
      "ZYMES, our obtained result is competitive since SAGPOOL only slightly outperforms our proposed\n",
      "method by 0.34%. Such observations demonstrate the structural information in graphs is useful for\n",
      "graph pooling and the relationships between different nodes should be explicitly modeled.\n",
      "\n",
      "4.4 STUDY OF COMPUTATIONAL COMPLEXITY\n",
      "\n",
      "As mentioned in Section 3.4, our pro-\n",
      "posed STRUCTPOOL yields O((m +\n",
      "i)n3) computational complexity.\n",
      "The\n",
      "complexity of DIFFPOOL is O(jn3) if\n",
      "we assume it employs j layers of GCNs to\n",
      "obtain the assignment matrix. In our ex-\n",
      "periments, i is usually set to 2 or 3 which\n",
      "\n",
      "Table 3: The prediction accuracy with different iteration\n",
      "number m.\n",
      "\n",
      "Dataset\n",
      "\n",
      "m = 1 m = 3 m = 5 m = 10\n",
      "\n",
      "ENZYMES\n",
      "D&D\n",
      "PROTEINS\n",
      "\n",
      "62.67\n",
      "82.82\n",
      "80.09\n",
      "\n",
      "63.00\n",
      "83.08\n",
      "80.00\n",
      "\n",
      "63.83\n",
      "83.59\n",
      "80.18\n",
      "\n",
      "63.50\n",
      "84.19\n",
      "80.18\n",
      "\n",
      "7\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "is much smaller than n. We conduct experiments to show how different iteration number m affects\n",
      "the prediction accuracy and the results are reported in Table 3. Note that we employ the dense CRF\n",
      "form for all different m. We can observe that the performance generally increases with m increasing,\n",
      "especially for large-scale dataset D&D. We also observe m = 5 is a good trade-off between time\n",
      "complexity and prediction performance. Notably, our method can even outperform other approaches\n",
      "when m = 1. Furthermore, we evaluate the running time of our STRUCTPOOL and compare it with\n",
      "DIFFPOOL. For 500 graphs from large-scale dataset D&D, we set i = j = 3 and show the aver-\n",
      "aging time cost to perform pooling for each graph. The time cost for DIFFPOOL is 0.042 second,\n",
      "while our STRUCTPOOL takes 0.049 second, 0.053 second and 0.058 second for m = 1, m = 3,\n",
      "m = 5 respectively. Even though our STRUCTPOOL has a relatively higher computational cost, it is\n",
      "still reasonable and acceptable given its superior performance.\n",
      "\n",
      "4.5 EFFECTS OF TOPOLOGY INFORMATION\n",
      "\n",
      "in\n",
      "\n",
      "(cid:96) = 5\n",
      "\n",
      "(cid:96) = 1\n",
      "\n",
      "(cid:96) = 10\n",
      "\n",
      "Dataset\n",
      "\n",
      "Table 4: The prediction accuracy using different A(cid:96)\n",
      "STRUCTPOOL.\n",
      "\n",
      "Next, we conduct experiments\n",
      "to show how the topology in-\n",
      "formation A(cid:96) affects the predic-\n",
      "tion performance. We evaluate\n",
      "our STRUCTPOOL with different (cid:96)\n",
      "values and report the results in Ta-\n",
      "ble 4. Note that when (cid:96) is large\n",
      "enough, our STRUCTPOOL considers all pairwise relationships between all nodes, and it is equiva-\n",
      "lent to the dense CRF. For the datasets IMDB-M and PROTEINS, we can observe that the prediction\n",
      "accuracies are generally increasing with the increasing of (cid:96). With the increasing of (cid:96), more pairwise\n",
      "relationships are considered by the model, and hence it is reasonable to obtain better performance.\n",
      "In addition, for the dataset IMDB-B, the results remain similar with different (cid:96), and even (cid:96) = 1\n",
      "yields competitive performance with dense CRF. It is possible that 1-hop pairwise relationships are\n",
      "enough to learn good embeddings for such graph types. Overall, dense CRF consistently produces\n",
      "promising results and is a proper choice in practice.\n",
      "\n",
      "IMDB-B\n",
      "IMDB-M\n",
      "PROTEINS\n",
      "\n",
      "74.70\n",
      "52.47\n",
      "80.18\n",
      "\n",
      "74.30\n",
      "52.00\n",
      "79.83\n",
      "\n",
      "74.40\n",
      "51.67\n",
      "79.61\n",
      "\n",
      "74.60\n",
      "51.53\n",
      "79.73\n",
      "\n",
      "74.70\n",
      "51.96\n",
      "80.36\n",
      "\n",
      "(cid:96) = 15 DENSE\n",
      "\n",
      "4.6 GRAPH ISOMORPHISM NETWORKS WITH STRUCTPOOL\n",
      "\n",
      "PTC\n",
      "\n",
      "Dataset\n",
      "\n",
      "75.10\n",
      "78.50\n",
      "\n",
      "64.60\n",
      "73.46\n",
      "\n",
      "GINS\n",
      "OURS\n",
      "\n",
      "IMDB-B MUTAG COLLAB\n",
      "\n",
      "Table 5: Comparisons with Graph Isomorphism Networks.\n",
      "\n",
      "Isomor-\n",
      "Recently, Graph\n",
      "phism Networks\n",
      "(GINs)\n",
      "are proposed and shown\n",
      "to be more powerful\n",
      "than\n",
      "traditional GNNs (Xu et al.,\n",
      "2019). To demonstrate the\n",
      "effectiveness of our STRUCTPOOL and show its generalizability, we build models based on GINs\n",
      "and evaluate their performance. Speciﬁcally, we employ GINs to learn node representations and\n",
      "perform one layer of the dense form of our STRUCTPOOL, followed by 1D convolutional layers\n",
      "and fully-connected layers as the classiﬁer. The results are reported in the Table 5, where we\n",
      "employ the same 10-fold splitting as GINs (Xu et al., 2019) and the GIN results are taken from\n",
      "its released results. These ﬁve datasets include both bioinformatic data and social media data, and\n",
      "both small-scale data and large-scale data. Obviously, incorporating our proposed STRUCTPOOL in\n",
      "GINs consistently and signiﬁcantly improves the prediction performance. It leads to an average of\n",
      "4.52% prediction accuracy improvement, which is promising.\n",
      "\n",
      "89.40\n",
      "93.59\n",
      "\n",
      "80.20\n",
      "84.06\n",
      "\n",
      "52.30\n",
      "54.60\n",
      "\n",
      "IMDB-M\n",
      "\n",
      "5 CONCLUSIONS\n",
      "\n",
      "Graph pooling is an appealing way to learn good graph-level representations, and several advaned\n",
      "pooling techiques are proposed. However, none of existing graph pooling techniques explicitly\n",
      "considers the relationship between different nodes. We propose a novel graph pooling technique,\n",
      "known as STRUCTPOOL, which is developed based on the conditional random ﬁelds. We consider\n",
      "the graph pooling as a node clustering problem and employ the CRF to build relationships between\n",
      "the assignments of different nodes. In addition, we generalize our method by incorporating the graph\n",
      "topological information so that our method can control the pairwise clique set in our CRFs. Finally,\n",
      "\n",
      "8\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "we evaluate our proposed STRUCTPOOL on several benchmark datasets and our method can achieve\n",
      "new state-of-the-art results on ﬁve out of six datasets.\n",
      "\n",
      "This work was supported in part by National Science Foundation grants DBI-1661289 and IIS-\n",
      "1908198.\n",
      "\n",
      "ACKNOWLEDGEMENT\n",
      "\n",
      "REFERENCES\n",
      "\n",
      "James Atwood and Don Towsley. Diffusion-convolutional neural networks. In Advances in Neural\n",
      "\n",
      "Information Processing Systems, pp. 1993–2001, 2016.\n",
      "\n",
      "Karsten M Borgwardt and Hans-Peter Kriegel. Shortest-path kernels on graphs.\n",
      "\n",
      "In Fifth IEEE\n",
      "\n",
      "international conference on data mining (ICDM’05), pp. 8–pp. IEEE, 2005.\n",
      "\n",
      "Karsten M Borgwardt, Cheng Soon Ong, Stefan Sch¨onauer, SVN Vishwanathan, Alex J Smola, and\n",
      "Hans-Peter Kriegel. Protein function prediction via graph kernels. Bioinformatics, 21(suppl 1):\n",
      "i47–i56, 2005.\n",
      "\n",
      "Lei Cai and Shuiwang Ji. A multi-scale approach for graph link prediction. In Thirty-Fourth AAAI\n",
      "\n",
      "Conference on Artiﬁcial Intelligence, 2020.\n",
      "\n",
      "Paul D Dobson and Andrew J Doig. Distinguishing enzyme structures from non-enzymes without\n",
      "\n",
      "alignments. Journal of molecular biology, 330(4):771–783, 2003.\n",
      "\n",
      "Hongchang Gao, Jian Pei, and Heng Huang. Conditional random ﬁeld enhanced graph convolu-\n",
      "tional neural networks. In Proceedings of the 25th ACM SIGKDD International Conference on\n",
      "Knowledge Discovery & Data Mining, pp. 276–284. ACM, 2019.\n",
      "\n",
      "Hongyang Gao and Shuiwang Ji. Graph u-nets. In International Conference on Machine Learning,\n",
      "\n",
      "pp. 2083–2092, 2019a.\n",
      "\n",
      "Hongyang Gao and Shuiwang Ji. Graph representation learning via hard and channel-wise attention\n",
      "In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge\n",
      "\n",
      "networks.\n",
      "Discovery & Data Mining, pp. 741–749, 2019b.\n",
      "\n",
      "Hongyang Gao, Zhengyang Wang, and Shuiwang Ji. Large-scale learnable graph convolutional\n",
      "In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge\n",
      "\n",
      "networks.\n",
      "Discovery & Data Mining, pp. 1416–1424, 2018.\n",
      "\n",
      "Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural\n",
      "message passing for quantum chemistry. In Proceedings of the 34th International Conference on\n",
      "Machine Learning-Volume 70, pp. 1263–1272. JMLR. org, 2017.\n",
      "\n",
      "Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs.\n",
      "\n",
      "In Advances in Neural Information Processing Systems, pp. 1024–1034, 2017.\n",
      "\n",
      "Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Proceedings of\n",
      "\n",
      "the 3rd International Conference on Learning Representations, 2014.\n",
      "\n",
      "Thomas N Kipf and Max Welling. Semi-supervised classiﬁcation with graph convolutional net-\n",
      "\n",
      "works. In Proceedings of the International Conference on Learning Representations, 2017.\n",
      "\n",
      "Philipp Kr¨ahenb¨uhl and Vladlen Koltun. Efﬁcient inference in fully connected crfs with gaussian\n",
      "\n",
      "edge potentials. In Advances in neural information processing systems, pp. 109–117, 2011.\n",
      "\n",
      "John Lafferty, Andrew McCallum, and Fernando CN Pereira. Conditional random ﬁelds: Probabilis-\n",
      "tic models for segmenting and labeling sequence data. In International conference on machine\n",
      "learning, pp. 282–289, 2001.\n",
      "\n",
      "Junhyun Lee, Inyeop Lee, and Jaewoo Kang. Self-attention graph pooling. In International Confer-\n",
      "\n",
      "ence on Machine Learning, pp. 3734–3743, 2019.\n",
      "\n",
      "9\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "Tengfei Ma, Cao Xiao, Junyuan Shang, and Jimeng Sun. CGNF: Conditional graph neural ﬁelds,\n",
      "\n",
      "2019. URL https://openreview.net/forum?id=ryxMX2R9YQ.\n",
      "\n",
      "Mathias Niepert, Mohamed Ahmed, and Konstantin Kutzkov. Learning convolutional neural net-\n",
      "\n",
      "works for graphs. In International conference on machine learning, pp. 2014–2023, 2016.\n",
      "\n",
      "Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito,\n",
      "Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in\n",
      "pytorch. In Proceedings of the International Conference on Learning Representations, 2017.\n",
      "\n",
      "Meng Qu, Yoshua Bengio, and Jian Tang. GMNN: Graph Markov neural networks. In Kamalika\n",
      "Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International Conference on\n",
      "Machine Learning, volume 97 of Proceedings of Machine Learning Research, pp. 5241–5250,\n",
      "Long Beach, California, USA, 09–15 Jun 2019. PMLR.\n",
      "\n",
      "Nino Shervashidze, SVN Vishwanathan, Tobias Petri, Kurt Mehlhorn, and Karsten Borgwardt. Ef-\n",
      "ﬁcient graphlet kernels for large graph comparison. In Artiﬁcial Intelligence and Statistics, pp.\n",
      "488–495, 2009.\n",
      "\n",
      "Martin Simonovsky and Nikos Komodakis. Dynamic edge-conditioned ﬁlters in convolutional neu-\n",
      "ral networks on graphs. In Proceedings of the IEEE conference on computer vision and pattern\n",
      "recognition, pp. 3693–3702, 2017.\n",
      "\n",
      "Jost Tobias Springenberg, Alexey Dosovitskiy, Thomas Brox, and Martin Riedmiller. Striving for\n",
      "simplicity: The all convolutional net. In Proceedings of the International Conference on Learning\n",
      "Representations, 2014.\n",
      "\n",
      "Petar Veliˇckovi´c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li`o, and Yoshua\n",
      "Bengio. Graph attention networks. In International Conference on Learning Representations,\n",
      "2018. URL https://openreview.net/forum?id=rJXMpikCZ.\n",
      "\n",
      "Oriol Vinyals, Samy Bengio, and Manjunath Kudlur. Order matters: Sequence to sequence for sets.\n",
      "\n",
      "In International Conference on Learning Representations, 2015.\n",
      "\n",
      "Boris Weisfeiler and Andrei A Lehman. A reduction of a graph to a canonical form and an algebra\n",
      "\n",
      "arising during this reduction. Nauchno-Technicheskaya Informatsia, 2(9):12–16, 1968.\n",
      "\n",
      "Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural\n",
      "In International Conference on Learning Representations, 2019. URL https:\n",
      "\n",
      "networks?\n",
      "//openreview.net/forum?id=ryGs6iA5Km.\n",
      "\n",
      "Pinar Yanardag and SVN Vishwanathan. Deep graph kernels.\n",
      "\n",
      "In Proceedings of the 21th ACM\n",
      "SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 1365–1374.\n",
      "ACM, 2015a.\n",
      "\n",
      "Pinar Yanardag and SVN Vishwanathan. A structural smoothing framework for robust graph com-\n",
      "\n",
      "parison. In Advances in neural information processing systems, pp. 2134–2142, 2015b.\n",
      "\n",
      "Zhitao Ying, Jiaxuan You, Christopher Morris, Xiang Ren, Will Hamilton, and Jure Leskovec. Hi-\n",
      "erarchical graph representation learning with differentiable pooling. In Advances in Neural Infor-\n",
      "mation Processing Systems, pp. 4800–4810, 2018.\n",
      "\n",
      "Fisher Yu and Vladlen Koltun. Multi-scale context aggregation by dilated convolutions. In Proceed-\n",
      "\n",
      "ings of the International Conference on Learning Representations, 2016.\n",
      "\n",
      "Muhan Zhang and Yixin Chen. Link prediction based on graph neural networks. In Advances in\n",
      "\n",
      "Neural Information Processing Systems, pp. 5165–5175, 2018.\n",
      "\n",
      "Muhan Zhang, Zhicheng Cui, Marion Neumann, and Yixin Chen. An end-to-end deep learning\n",
      "\n",
      "architecture for graph classiﬁcation. In AAAI, pp. 4438–4445, 2018.\n",
      "\n",
      "Shuai Zheng, Sadeep Jayasumana, Bernardino Romera-Paredes, Vibhav Vineet, Zhizhong Su, Da-\n",
      "long Du, Chang Huang, and Philip HS Torr. Conditional random ﬁelds as recurrent neural net-\n",
      "works. In Proceedings of the IEEE international conference on computer vision, pp. 1529–1537,\n",
      "2015.\n",
      "\n",
      "10\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "A APPENDIX\n",
      "\n",
      "A.1 DATASETS AND EXPERIMENTAL SETTINGS\n",
      "\n",
      "Table 6: Statistics and properties of eight benchmark datasets.\n",
      "\n",
      "ENZYMES\n",
      "\n",
      "D&D\n",
      "\n",
      "COLLAB\n",
      "\n",
      "PROTEINS\n",
      "\n",
      "# of Edges (avg)\n",
      "# of Nodes (avg)\n",
      "# of Graphs\n",
      "# of Classes\n",
      "\n",
      "124.20\n",
      "32.63\n",
      "600\n",
      "6\n",
      "\n",
      "Dataset\n",
      "\n",
      "1431.3\n",
      "284.32\n",
      "1178\n",
      "\n",
      "2\n",
      "\n",
      "2457.78\n",
      "74.49\n",
      "5000\n",
      "\n",
      "3\n",
      "\n",
      "Dataset\n",
      "\n",
      "IMDB-B\n",
      "\n",
      "IMDB-M\n",
      "\n",
      "# of Edges (avg)\n",
      "# of Nodes (avg)\n",
      "# of Graphs\n",
      "# of Classes\n",
      "\n",
      "96.53\n",
      "19.77\n",
      "1000\n",
      "\n",
      "2\n",
      "\n",
      "65.94\n",
      "13.00\n",
      "1500\n",
      "\n",
      "3\n",
      "\n",
      "PTC\n",
      "\n",
      "14.69\n",
      "14.30\n",
      "344\n",
      "2\n",
      "\n",
      "72.82\n",
      "39.06\n",
      "1113\n",
      "\n",
      "2\n",
      "\n",
      "MUTAG\n",
      "\n",
      "19.79\n",
      "17.93\n",
      "188\n",
      "\n",
      "2\n",
      "\n",
      "We report the statistics and properties of eight benchmark datasets in Supplementary Table 6. For\n",
      "our STRUCTPOOL, we implement our models using Pytorch (Paszke et al., 2017) and conduct exper-\n",
      "iments on one GeForce GTX 1080 Ti GPU. The model is trained using Stochastic gradient descent\n",
      "(SGD) with the ADAM optimizer (Kingma & Ba, 2014). For the models built on DGCNN (Zhang\n",
      "et al., 2018) in Section 4.2, 4.3, 4.4, 4.5, we employ GCNs to obtain the node features and the unary\n",
      "energy matrix. All experiments in these sections perform 10-fold cross validations and we report the\n",
      "averaging results. The 10-fold splitting is exactly the same as DGCNN (Zhang et al., 2018). For the\n",
      "non-linear function, we employ tanh for GCNs and relu for 1D convolution layers. For the models\n",
      "built on GINs in Section 4.6, we employ GINs to learn node features and unary energy. Here the 10-\n",
      "fold splitting is exactly the same as GINs. We employ relu for all layers as the non-linear function.\n",
      "For all models, 1D convolutional layers and fully-connected layers are used after our STRUCTPOOL.\n",
      "Hard clustering assignments are employed in all experiments.\n",
      "\n",
      "A.2 EFFECTS OF PAIRWISE ENERGY\n",
      "\n",
      "Table 7: Comparison with the baseline which excludes pairwise energy.\n",
      "\n",
      "Dataset\n",
      "\n",
      "ENZYMES D&D COLLAB\n",
      "\n",
      "PROTEINS\n",
      "\n",
      "IMDB-B IMDB-M\n",
      "\n",
      "BASELINE\n",
      "OURS\n",
      "\n",
      "60.83\n",
      "63.83\n",
      "\n",
      "81.30\n",
      "84.19\n",
      "\n",
      "70.58\n",
      "74.22\n",
      "\n",
      "78.18\n",
      "80.36\n",
      "\n",
      "72.40\n",
      "74.70\n",
      "\n",
      "50.13\n",
      "52.47\n",
      "\n",
      "We conduct experiments to show the importance of the pairwise energy. If the pairwise energy is\n",
      "removed, the relations between different node assignments are not explicitly considered. Then the\n",
      "method is similar to the DIFFPOOL. We compare our method with such a baseline that removes the\n",
      "pairwise energy. Experimental results are reported in Table 7. The network framework is the same\n",
      "as introduced in Section 3.5 and the same 10-fold cross validations from DGCNN are used. Obvi-\n",
      "ously, our proposed method consistently and signiﬁcantly outperforms the baseline which excludes\n",
      "pairwise energy. It indicates the importance and effectiveness of incorporating pairwise energy and\n",
      "considering high-order relationships between different node assignments.\n",
      "\n",
      "A.3 STUDY OF HIERARCHICAL NETWORK STRUCTURE\n",
      "\n",
      "To demonstrate how the network depth and multiple pooling layers affects the prediction perfor-\n",
      "mance, we conduct experiments to evaluate different hierarchical network structures. We ﬁrst deﬁne\n",
      "a network block contains two GCN layers and one STRUCTPOOL layer. Then we compare three\n",
      "\n",
      "11\n",
      "\n",
      "\f",
      "Published as a conference paper at ICLR 2020\n",
      "\n",
      "Table 8: Comparison with different hierarchical network structures.\n",
      "\n",
      "Dataset\n",
      "\n",
      "1 BLOCK\n",
      "\n",
      "2 BLOCKS\n",
      "\n",
      "3 BLOCKS\n",
      "\n",
      "PROTEINS\n",
      "D&D\n",
      "\n",
      "79.73\n",
      "81.87\n",
      "\n",
      "77.42\n",
      "83.59\n",
      "\n",
      "74.95\n",
      "81.63\n",
      "\n",
      "different network settings: 1 block with the ﬁnal classiﬁer, 2 blocks with the ﬁnal classiﬁer, and\n",
      "3 blocks with the ﬁnal classiﬁer. The results are reported in Table 8. For the dataset Proteins, we\n",
      "observe that the network with one block can obtain better performance than deeper networks. We\n",
      "believe the main reason is dataset Proteins is a small-scale dataset with an average number of nodes\n",
      "equal to 39.06. A relatively simpler network is powerful enough to learn its data distribution while\n",
      "stacking multiple GCN layers and pooling layers may lead to a serious overﬁtting problems. For\n",
      "the dataset D&D, the network with 2 blocks performs better than the one with 1 block. Since D&D\n",
      "is relatively large scale, stacking 2 blocks increases the power of network and hence increases the\n",
      "performance. However, going very deep, e.g., stacking 3 blocks, will cause the overﬁtting problem.\n",
      "\n",
      "A.4 STUDY OF GRAPH POOLING RATE\n",
      "\n",
      "Table 9: Comparison with different pooling rates.\n",
      "\n",
      "r = 0.1\n",
      "\n",
      "r = 0.3\n",
      "\n",
      "r = 0.5\n",
      "\n",
      "r = 0.7\n",
      "\n",
      "r = 0.9\n",
      "\n",
      "k\n",
      "ACC\n",
      "\n",
      "91\n",
      "\n",
      "80.77\n",
      "\n",
      "160\n",
      "81.53\n",
      "\n",
      "241\n",
      "81.53\n",
      "\n",
      "331\n",
      "81.97\n",
      "\n",
      "503\n",
      "80.68\n",
      "\n",
      "We follow the DGCNN (Zhang et al., 2018) to select the number of clusters k. Speciﬁcally, we use\n",
      "a pooling rate r ∈ (0, 1) to control k. Then k is set to an integer so that r × 100% of graphs have\n",
      "nodes less than this integer in the current dataset. As suggested in DGCNN, generally, r = 0.9\n",
      "is a proper choice for bioinformatics datasets and r = 0.6 is good for social network datasets. In\n",
      "addition, we conduct experiments to show the performance with the respect to different r values.\n",
      "We set r = 0.1, 0.3, 0.5, 0.7, 0.9 to evaluate the performance on a large-scale social network dataset\n",
      "D&D. The average number of nodes in dataset D&D is 284.32 and the maximum number of nodes\n",
      "is 5748. The results are reported in Table 9 where the ﬁrst row shows different pooling rates, the\n",
      "second row reports the corresponding k values and the ﬁnal row shows the results. For simplicity,\n",
      "we employ the network structure with 1 block and a ﬁnal classiﬁer (as deﬁned in Section A.3). We\n",
      "can observe that the performance drops when r, k is relatively large or small. In addition, the model\n",
      "can obtain competitive performance when r is set to a proper range, for example, r ∈ [0.3, 0.7] for\n",
      "dataset D&D.\n",
      "\n",
      "12\n",
      "\n",
      "\f",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert pdfs to text\n",
    "pdfdir = 'C:/Users/Evan/Documents/7180QueryTool/Data/*.pdf'\n",
    "for research_paper_name in glob.glob(pdfdir):\n",
    "    ProcessText.pdfToText(research_paper_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What model was introduced in this paper?\n"
     ]
    }
   ],
   "source": [
    "# Basic user interface; obtain query\n",
    "print(\"Query: What model was introduced in this paper?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C-SWM.txt\n",
      "C:/Users/Evan/Documents/7180QueryTool/DataTxt\n",
      "C:/Users/Evan/Documents/7180QueryTool/DataTxt/snippet_data\n",
      "C-SWM_3.csv\n",
      "Exit\n"
     ]
    }
   ],
   "source": [
    "# Convert text files to CSV snippets\n",
    "txtdir = 'C:/Users/Evan/Documents/7180QueryTool/DataTxt/*.txt'\n",
    "sentenceNum = 3\n",
    "for research_paper_name in glob.glob(txtdir):\n",
    "    ProcessText.snippetToCsv(research_paper_name, sentenceNum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use USE to obtain best snippets for query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run BERT on best snippets, return results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
