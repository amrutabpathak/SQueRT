




















































Published as a conference paper at ICLR 2020

GRAPH INFERENCE LEARNING FOR SEMI-SUPERVISED
CLASSIFICATION

Chunyan Xu, Zhen Cui∗, Xiaobin Hong, Tong Zhang, and Jian Yang
School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China
{cyx,zhen.cui,xbhong,tong.zhang,csjyang}@njust.edu.cn

Wei Liu
Tencent AI Lab, China
wl2223@columbia.edu

ABSTRACT

In this work, we address semi-supervised classification of graph data, where the
categories of those unlabeled nodes are inferred from labeled nodes as well as
graph structuresRecent works often solve this problem via advanced graph
convolution in a conventionally supervised manner, but the performance could
degrade significantly when labeled data is scarceTo this end, we propose a
Graph Inference Learning (GIL) framework to boost the performance of semi-
supervised node classification by learning the inference of node labels on graph
topologyTo bridge the connection between two nodes, we formally define a
structure relation by encapsulating node attributes, between-node paths, and local
topological structures together, which can make the inference conveniently deduced
from one node to another nodeFor learning the inference process, we further
introduce meta-optimization on structure relations from training nodes to validation
nodes, such that the learnt graph inference capability can be better self-adapted to
testing nodesComprehensive evaluations on four benchmark datasets (including
Cora, Citeseer, Pubmed, and NELL) demonstrate the superiority of our proposed
GIL when compared against state-of-the-art methods on the semi-supervised node
classification task.

1 INTRODUCTION

Graph, which comprises a set of vertices/nodes together with connected edges, is a formal structural
representation of non-regular dataDue to the strong representation ability, it accommodates many
potential applications, e.g., social network (Orsini et al., 2017), world wide data (Page et al., 1999),
knowledge graph (Xu et al., 2017), and protein-interaction network (Borgwardt et al., 2007)Among
these, semi-supervised node classification on graphs is one of the most interesting also popular topics.
Given a graph in which some nodes are labeled, the aim of semi-supervised classification is to infer
the categories of those remaining unlabeled nodes by using various priors of the graph.

While there have been numerous previous works (Brandes et al., 2008; Zhou et al., 2004; Zhu
et al., 2003; Yang et al., 2016; Zhao et al., 2019) devoted to semi-supervised node classification
based on explicit graph Laplacian regularizations, it is hard to efficiently boost the performance of
label prediction due to the strict assumption that connected nodes are likely to share the same label
informationWith the progress of deep learning on grid-shaped images/videos (He et al., 2016),
a few of graph convolutional neural networks (CNN) based methods, including spectral (Kipf &
Welling, 2017) and spatial methods (Niepert et al., 2016; Pan et al., 2018; Yu et al., 2018), have
been proposed to learn local convolution filters on graphs in order to extract more discriminative
node representationsAlthough graph CNN based methods have achieved considerable capabilities
of graph embedding by optimizing filters, they are limited into a conventionally semi-supervised
framework and lack of an efficient inference mechanism on graphsEspecially, in the case of few-shot
learning, where a small number of training nodes are labeled, this kind of methods would drastically
compromise the performanceFor example, the Pubmed graph dataset (Sen et al., 2008) consists
∗Corresponding author: Zhen Cui.

1



Published as a conference paper at ICLR 2020

 
 
 
(b) The process of Graph inference learning 
We extract the local representation from the local subgraph (the circle with dashed line      
The red wave line denote the node reachability from     to      
d t  th  h bilit  f   d  t  th  d    

Figure 1: The illustration of our proposed GIL frameworkFor the problem of graph node labeling, the category
information of these unlabeled nodes depends on the similarity computation between a query node (e.g., vj)
and these labeled reference nodes (e.g., vi)We consider the similarity from three points: node attributes,
the consistency of local topological structures (i.e., the circle with dashed line), and the between-node path
reachability (i.e., the red wave line from vi to vj)Specifically, the local structures as well as node attributes are
encoded as high-level features with graph convolution, while the between-node path reachability is abstracted as
reachable probabilities of random walksTo better make the inference generalize to test nodes, we introduce a
meta-learning strategy to optimize the structure relations learning from training nodes to validation nodes.

of 19,717 nodes and 44,338 edges, but only 0.3% nodes are labeled for the semi-supervised node
classification taskThese aforementioned works usually boil down to a general classification task,
where the model is learnt on a training set and selected by checking a validation setHowever, they
do not put great efforts on how to learn to infer from one node to another node on a topological graph,
especially in the few-shot regime.

In this paper, we propose a graph inference learning (GIL) framework to teach the model itself to
adaptively infer from reference labeled nodes to those query unlabeled nodes, and finally boost the
performance of semi-supervised node classification in the case of a few number of labeled samples.
Given an input graph, GIL attempts to infer the unlabeled nodes from those observed nodes by
building between-node relationsThe between-node relations are structured as the integration of
node attributes, connection paths, and graph topological structuresIt means that the similarity
between two nodes is decided from three aspects: the consistency of node attributes, the consistency
of local topological structures, and the between-node path reachability, as shown in Fig1The local
structures anchored around each node as well as the attributes of nodes therein are jointly encoded
with graph convolution (Defferrard et al., 2016) for the sake of high-level feature extractionFor the
between-node path reachability, we adopt the random walk algorithm to obtain the characteristics
from a labeled reference node vi to a query unlabeled node vj in a given graphBased on the
computed node representation and between-node reachability, the structure relations can be obtained
by computing the similar scores/relationships from reference nodes to unlabeled nodes in a graph.
Inspired by the recent meta-learning strategy (Finn et al., 2017), we learn to infer the structure
relations from a training set to a validation set, which can benefit the generalization capability of the
learned modelIn other words, our proposed GIL attempts to learn some transferable knowledge
underlying in the structure relations from training samples to validation samples, such that the learned
structure relations can be better self-adapted to the new testing stage.

We summarize the main contributions of this work as three folds:

• We propose a novel graph inference learning framework by building structure relations to
infer unknown node labels from those labeled nodes in an end-to-end wayThe structure
relations are well defined by jointly considering node attributes, between-node paths, and
graph topological structures.

• To make the inference model better generalize to test nodes, we introduce a meta-learning
procedure to optimize structure relations, which could be the first time for graph node
classification to the best of our knowledge.

• Comprehensive evaluations on three citation network datasets (including Cora, Citeseer,
and Pubmed) and one knowledge graph data (i.e., NELL) demonstrate the superiority of
our proposed GIL in contrast with other state-of-the-art methods on the semi-supervised
classification task.

2



Published as a conference paper at ICLR 2020

2 RELATED WORK

Graph CNNs: With the rapid development of deep learning methods, various graph convolution
neural networks (Kashima et al., 2003; Morris et al., 2017; Shervashidze et al., 2009; Yanardag
& Vishwanathan, 2015; Jiang et al., 2019; Zhang et al., 2020) have been exploited to analyze
the irregular graph-structured dataFor better extending general convolutional neural networks to
graph domains, two broad strategies have been proposed, including spectral and spatial convolution
methodsSpecifically, spectral filtering methods (Henaff et al., 2015; Kipf & Welling, 2017) develop
convolution-like operators in the spectral domain, and then perform a series of spectral filters
by decomposing the graph LaplacianUnfortunately, the spectral-based approaches often lead
to a high computational complex due to the operation of eigenvalue decomposition, especially
for a large number of graph nodesTo alleviate this computation burden, local spectral filtering
methods (Defferrard et al., 2016) are then proposed by parameterizing the frequency responses as a
Chebyshev polynomial approximationAnother type of graph CNNs, namely spatial methods (Li et al.,
2016; Niepert et al., 2016), can perform the filtering operation by defining the spatial structures of
adjacent verticesVarious approaches can be employed to aggregate or sort neighboring vertices, such
as diffusion CNNs (Atwood & Towsley, 2016), GraphSAGE (Hamilton et al., 2017), PSCN (Niepert
et al., 2016), and NgramCNN (Luo et al., 2017)From the perspective of data distribution, recently,
the Gaussian induced convolution model (Jiang et al., 2019) is proposed to disentangle the aggregation
process through encoding adjacent regions with Gaussian mixture model.

Semi-supervised node classification: Among various graph-related applications, semi-supervised
node classification has gained increasing attention recently, and various approaches have been
proposed to deal with this problem, including explicit graph Laplacian regularization and graph-
embedding approachesSeveral classic algorithms with graph Laplacian regularization contain
the label propagation method using Gaussian random fields (Zhu et al., 2003), the regularization
framework by relying on the local/global consistency (Zhou et al., 2004), and the random walk-
based sampling algorithm for acquiring the context information (Yang et al., 2016)To further
address scalable semi-supervised learning issues (Liu et al., 2012), the Anchor Graph regularization
approach (Liu et al., 2010) is proposed to scale linearly with the number of graph nodes and then
applied to massive-scale graph datasetsSeveral graph convolution network methods (Abu-El-Haija
et al., 2018; Du et al., 2017; Thekumparampil et al., 2018; Velickovic et al., 2018; Zhuang & Ma,
2018) are then developed to obtain discriminative representations of input graphsFor example, Kipf
et al(Kipf & Welling, 2017) proposed a scalable graph CNN model, which can scale linearly in the
number of graph edges and learn graph representations by encoding both local graph structures and
node attributesGraph attention networks (GAT) (Velickovic et al., 2018) are proposed to compute
hidden representations of each node for attending to its neighbors with a self-attention strategy.
By jointly considering the local- and global-consistency information, dual graph convolutional
networks (Zhuang & Ma, 2018) are presented to deal with semi-supervised node classificationThe
critical difference between our proposed GIL and those previous semi-supervised node classification
methods is to adopt a graph inference strategy by defining structure relations on graphs and then
leverage a meta optimization mechanism to learn an inference model, which could be the first time to
the best of our knowledge, while the existing graph CNNs take semi-supervised node classification
as a general classification task.

3 THE PROPOSED MODEL
3.1 PROBLEM DEFINITION

Formally, we denote an undirected/directed graph as G = {V, E ,X ,Y}, where V = {vi}ni=1 is the
finite set of n (or |V|) vertices, E ∈ Rn×n defines the adjacency relationships (i.e., edges) between
vertices representing the topology of G, X ∈ Rn×d records the explicit/implicit attributes/signals of
vertices, and Y ∈ Rn is the vertex labels of C classesThe edge Eij = E(vi, vj) = 0 if and only if
vertices vi, vj are not connected, otherwise Eij 6= 0The attribute matrix X is attached to the vertex
set V , whose i-th row Xvi (or Xi·) represents the attribute of the i-th vertex viIt means that vi ∈ V
carries a vector of d-dimensional signalsAssociated with each node vi ∈ V , there is a discrete label
yi ∈ {1, 2, · · · , C}.
We consider the task of semi-supervised node classification over graph data, where only a small
number of vertices are labeled for the model learning, i.e., |VLabel| � |V|Generally, we have three
node sets: a training set Vtr, a validation set Vval, and a testing set VteIn the standard protocol

3



Published as a conference paper at ICLR 2020

of prior literatures (Yang et al., 2016), the three node sets share the same label spaceWe follow
but do not restrict this protocol for our proposed methodGiven the training and validation node
sets, the aim is to predict the node labels of testing nodes by using node attributes as well as edge
connectionsA sophisticated machine learning technique used in most existing methods (Kipf &
Welling, 2017; Zhou et al., 2004) is to choose the optimal classifier (trained on a training set) after
checking the performance on the validation setHowever, these methods essentially ignore how to
extract transferable knowledge from these known labeled nodes to unlabeled nodes, as the graph
structure itself implies node connectivity/reachabilityMoreover, due to the scarcity of labeled
samples, the performance of such a classifier is usually not satisfyingTo address these issues,
we introduce a meta-learning mechanism (Finn et al., 2017; Ravi & Larochelle, 2017; Sung et al.,
2017) to learn to infer node labels on graphsSpecifically, the graph structure, between-node path
reachability, and node attributes are jointly modeled into the learning processOur aim is to learn to
infer from labeled nodes to unlabeled nodes, so that the learner can perform better on a validation set
and thus classify a testing set more accurately.

3.2 STRUCTURE RELATION
For convenient inference, we specifically build a structure relation between two nodes on the topology
graphThe labeled vertices (in a training set) are viewed as the reference nodes, and their information
can be propagated into those unlabeled vertices for improving the label prediction accuracyFormally,
given a reference node vi ∈ VLabel, we define the score of a query node vj similar to vi as

si→j = fr(fe(Gvi), fe(Gvj ), fP(vi, vj , E)), (1)
where Gvi and Gvj may be understood as the centralized subgraphs around vi and vj , respectively.
fe, fr, fP are three abstract functions that we explain as follows:

• Node representation fe(Gvi) −→ Rdv , encodes the local representation of the centralized
subgraph Gvi around node vi, and may thus be understood as a local filter function on graphs.
This function should not only take the signals of nodes therein as input, but also consider the
local topological structure of the subgraph for more accurate similarity computationTo this
end, we perform the spectral graph convolution on subgraphs to learn discriminative node
features, analogous to the pixel-level feature extraction from convolution maps of gridded
imagesThe details of feature extraction fe are described in Section 4.
• Path reachability fP(vi, vj , E) −→ Rdp , represents the characteristics of path reachability

from vi to vj As there usually exist multiple traversal paths between two nodes, we choose
the function as reachable probabilities of different lengths of walks from vi to vj More
details will be introduced in Section 4.
• Structure relation fr(Rdv ,Rdv ,Rdp) −→ R, is a relational function computing the score

of vj similar to viThis function is not exchangeable for different orders of two nodes,
due to the asymmetric reachable relationship fP If necessary, we may easily revise it as a
symmetry function, e.g., summarizing two traversal directionsThe score function depends
on triple inputs: the local representations extracted from the subgraphs w.r.tfe(Gvi) and
fe(Gvj ), respectively, and the path reachability from vi to vj .

In semi-supervised node classification, we take the training node set Vtr as the reference samples, and
the validation set Vval as the query samples during the training stageGiven a query node vj ∈ Vval,
we can derive the class similarity score of vj w.r.tthe c-th (c = 1, · · · , C) category by weighting
the reference samples Cc = {vk|yvk = c}Formally, we can further revise Eqn(1) and define the
class-to-node relationship function as

sCc→j = φr(FCc→vj
∑
vi∈Cc

wi→j · fe(Gvi), fe(Gvj )), (2)

s.twi→j = φw(fP(vi, vj , E)), (3)
where the function φw maps a reachable vector fP(vi, vj , E) into a weight value, and the function φr
computes the similar score between vj and the c-th class nodesThe normalization factor FCc→vj of
the c-th category w.r.tvj is defined as

FCc→vj =
1∑

vi∈Cc wi→j
(4)

For the relation function φr and the weight function φw, we may choose some subnetworks to
instantiate them in practiceThe detailed implementation of our model can be found in Section 4.

4



Published as a conference paper at ICLR 2020

3.3 INFERENCE LEARNING

According to the class-to-node relationship function in Eqn(2), given a query node vj , we can obtain
a score vector sC→j = [sC1→j , · · · , sCC→j ]ᵀ ∈ RC after computing the relations to all classes The
indexed category with the maximum score is assumed to be the estimated labelThus, we can define
the loss function based on cross entropy as follows:

L = −
∑
vj

C∑
c=1

yj,c log ŷCc→j , (5)

where yj,c is a binary indicator (i.e., 0 or 1) of class label c for node vj , and the softmax operation
is imposed on sCc→j , i.e., ŷCc→j = exp(sCc→j)/

∑C
k=1 exp(sCk→j)Other error functions may be

chosen as the loss function, e.g., mean square errorIn the regime of general classification, the cross
entropy loss is a standard one that performs well.

Given a training set Vtr, we expect that the best performance can be obtained on the validation set
Vval after optimizing the model on VtrGiven a trained/pretrained model Θ = {fe, φw, φr}, we
perform iteratively gradient updates on the training set Vtr to obtain the new model, formally,

Θ′ = Θ− α∇ΘLtr(Θ), (6)

where α is the updating rateNote that, in the computation of class scores, since the reference node
and query node can be both from the training set Vtr, we set the computation weight wi→j = 0 if
i = j in Eqn(3)After several iterates of gradient descent on Vtr, we expect a better performance on
the validation set Vval, i.e., min

Θ
Lval(Θ′)Thus, we can perform the gradient update as follows

Θ = Θ− β∇ΘLval(Θ′), (7)

where β is the learning rate of meta optimization (Finn et al., 2017).

During the training process, we may perform batch sampling from training nodes and validation
nodes, instead of taking all one timeIn the testing stage, we may take all training nodes and perform
the model update according to Eqn(6) like the training processThe updated model is used as the
final model and is then fed into Eqn(2) to infer the class labels for those query nodes.

4 MODULES
In this section, we instantiate all modules (i.e., functions) of the aforementioned structure relation.
The implementation details can be found in the following.

Node Representation fe(Gvi): The local representation at vertex vi can be extracted by performing
the graph convolution operation on subgraph Gvi Similar to gridded images/videos, on which local
convolution kernels are defined as multiple lattices with various receptive fields, the spectral graph
convolution is used to encode the local representations of an input graph in our work.

Given a graph sample G = {V, E ,X}, the normalized graph Laplacian matrix is L = In −
D−1/2ED−1/2 = UΛUT , with a diagonal matrix of its eigenvalues ΛThe spectral graph convo-
lution can be defined as the multiplication of signal X with a filter gθ(Λ) = diag(θ) parameterized
by θ in the Fourier domain: conv(X ) = gθ(L) ∗ X = Ugθ(Λ)UTX , where parameter θ ∈ Rn
is a vector of Fourier coefficientsTo reduce the computational complexity and obtain the local
information, we use an approximate local filter of the Chebyshev polynomial (Defferrard et al.,
2016), gθ(Λ) =

∑K−1
k=0 θkTk(Λ̂), where parameter θ ∈ RK is a vector of Chebyshev coefficients

and Tk(Λ̂) ∈ Rn×n is the Chebyshev polynomial of order k evaluated at Λ̂ = 2Λ/λmax − In,
a diagonal matrix of scaled eigenvaluesThe graph filtering operation can then be expressed as
gθ(Λ) ∗ X =

∑K−1
k=0 θkTk(L̂)X , where Tk(L̂) ∈ Rn×n is the Chebyshev polynomial of order k

evaluated at the scaled Laplacian L̂ = 2L/λmax−InFurther, we can construct multi-scale receptive
fields for each vertex based on the Laplacian matrix L, where each receptive field records hopping
neighborhood relationships around the reference vertex vi, and forms a local centralized subgraph.

Path Reachability fP(vi, vj , E): Here we compute the probabilities of paths from vertex i to vertex
j by employing random walks on graphs, which refers to traversing the graph from vi to vj according
to the probability matrix PFor the input graph G with n vertices, the random-walk transition matrix

5



Published as a conference paper at ICLR 2020

Datasets Nodes Edges Classes Features Label Rates
Cora 2,708 5,429 7 1,433 0.052
Citeseer 3,327 4,732 6 3,703 0.036
Pubmed 19,717 44,338 3 500 0.003
NELL 65,755 266,144 210 5,414 0.001

Table 1: The properties (especially for label rate) of various graph datasets used for the semi-supervised
classification task.

can be defined as P = D−1E , where D ∈ Rn×n is the diagonal degree matrix with Dii =
∑
i Eij .

That is to say, each element Pij is the probability of going from vertex i to vertex j in one step.

The sequence of nodes from vertex i to vertex j is a random walk on the graph, which can be modeled
as a classical Markov chain by considering the set of graph verticesTo represent this formulation,
we show that P tij is the probability of getting from vertex vi to vertex vj in t stepsThis fact is easily
exhibited by considering a t-step path from vertex vi to vertex vj as first taking a single step to some
vertex h, and then taking t− 1 steps to vj The transition probability P t in t steps can be formulated
as

P tij =


Pij if t = 1∑
h

PihP
t−1
h,j if t > 1

, (8)

where each matrix entry P tij denotes the probability of starting at vertex i and ending at vertex j in t
stepsFinally, the node reachability from vi to vj can be written as a dp-dimensional vector:

fP(vi, vj , E) = [Pij , P 2ij , , P
dp
ij ], (9)

where dp refers to the step length of the longest path from vi to vj .

Class-to-Node Relationship sCc→j: To define the node relationship si→j from vi to vj , we simulta-
neously consider the property of path reachability fP(vi, vj , E), local representations fe(Gvi), and
fe(Gvj ) of nodes vi, vj The function φw(fP(vi, vj , E)) in Eqn(3), which is to map the reachable
vector fP(vi, vj , E) ∈ Rdp into a weight value, can be implemented with two 16-dimensional fully
connected layers in our experimentsThe computed value wi→j can be further used to weight the
local features at node vi, fe(Gvi) ∈ Rdv For obtaining the similar score between vj and the c-th
class nodes Cc in Eqn(2), we perform a concatenation of two input features, where one refers to the
weighted features of vertex vi, and another is the local features of vertex vj One fully connected
layer (w.r.tφr) with C-dimensions is finally adopted to obtain the relation regression score.

5 EXPERIMENTS

5.1 EXPERIMENTAL SETTINGS

We evaluate our proposed GIL method on three citation network datasets: Cora, Citeseer, Pubmed (Sen
et al., 2008), and one knowledge graph NELL dataset (Carlson et al., 2010)The statistical properties
of graph data are summarized in Table 1Following the previous protocol in (Kipf & Welling, 2017;
Zhuang & Ma, 2018), we split the graph data into a training set, a validation set, and a testing set.
Taking into account the graph convolution and pooling modules, we may alternately stack them into
a multi-layer Graph convolutional networkThe GIL model consists of two graph convolution layers,
each of which is followed by a mean-pooling layer, a class-to-node relationship regression module,
and a final softmax layerWe have given the detailed configuration of the relationship regression
module in the class-to-node relationship of Section 4The parameter dp in Eqn(9) is set to the
mean length of between-node reachability paths in the input graphThe channels of the 1-st and
2-nd convolutional layers are set to 128 and 256, respectivelyThe scale of the respective filed is 2
in both convolutional layersThe dropout rate is set to 0.5 in the convolution and fully connected
layers to avoid over-fitting, and the ReLU unit is leveraged as a nonlinear activation functionWe
pre-train our proposed GIL model for 200 iterations with the training set, where its initial learning
rate, decay factor, and momentum are set to 0.05, 0.95, and 0.9, respectivelyHere we train the GIL
model using the stochastic gradient descent method with the batch size of 100We further improve
the inference learning capability of the GIL model for 1200 iterations with the validation set, where
the meta-learning rates α and β are both set to 0.001.

6



Published as a conference paper at ICLR 2020

5.2 COMPARISON WITH STATE-OF-THE-ARTS

We compare the GIL approach with several state-of-the-art methods (Monti et al., 2017; Kipf &
Welling, 2017; Zhou et al., 2004; Zhuang & Ma, 2018) over four graph datasets, including Cora,
Citeseer, Pubmed, and NELLThe classification accuracies for all methods are reported in Table 2.
Our proposed GIL can significantly outperform these graph Laplacian regularized methods on four
graph datasets, including Deep walk (Zhou et al., 2004), modularity clustering (Brandes et al., 2008),
Gaussian fields (Zhu et al., 2003), and graph embedding (Yang et al., 2016) methodsFor example,
we can achieve much higher performance than the deepwalk method (Zhou et al., 2004), e.g., 43.2%
vs 74.1% on the Citeseer dataset, 65.3% vs 83.1% on the Pubmed dataset, and 58.1% vs 78.9% on the
NELL datasetWe find that the graph embedding method (Yang et al., 2016), which has considered
both label information and graph structure during sampling, can obtain lower accuracies than our
proposed GIL by 9.4% on the Citeseer dataset and 10.5% on the Cora dataset, respectivelyThis
indicates that our proposed GIL can better optimize structure relations and thus improve the network
generalizationWe further compare our proposed GIL with several existing deep graph embedding
methods, including graph attention network (Velickovic et al., 2018), dual graph convolutional
networks (Zhuang & Ma, 2018), topology adaptive graph convolutional networks (Du et al., 2017),
Multi-scale graph convolution (Abu-El-Haija et al., 2018), etcFor example, our proposed GIL
achieves a very large gain, e.g., 86.2% vs 83.3% (Du et al., 2017) on the Cora dataset, and 78.9%
vs 66.0% (Kipf & Welling, 2017) on the NELL datasetWe evaluate our proposed GIL method on
a large graph dataset with a lower label rate, which can significantly outperform existing baselines
on the Pubmed dataset: 3.1% over DGCN (Zhuang & Ma, 2018), 4.1% over classic GCN (Kipf &
Welling, 2017) and TAGCN (Du et al., 2017), 3.2% over AGNN (Thekumparampil et al., 2018), and
3.6% over N-GCN (Abu-El-Haija et al., 2018)It demonstrates that our proposed GIL performs very
well on various graph datasets by building the graph inference learning process, where the limited
label information and graph structures can be well employed in the predicted framework.

Table 2: Performance comparisons of semi-supervised classification methods.

Methods Cora Citeseer Pubmed NELL
Clustering (Brandes et al., 2008) 59.5 60.1 70.7 21.8
DeepWalk (Zhou et al., 2004) 67.2 43.2 65.3 58.1
Gaussian (Zhu et al., 2003) 68.0 45.3 63.0 26.5
G-embedding (Yang et al., 2016) 75.7 64.7 77.2 61.9
DCNN (Atwood & Towsley, 2016) 76.8 - 73.0 -
GCN (Kipf & Welling, 2017) 81.5 70.3 79.0 66.0
MoNet (Monti et al., 2017) 81.7 - 78.8 -
N-GCN (Abu-El-Haija et al., 2018) 83.0 72.2 79.5 -
GAT (Velickovic et al., 2018) 83.0 72.5 79.0 -
AGNN (Thekumparampil et al., 2018) 83.1 71.7 79.9 -
TAGCN (Du et al., 2017) 83.3 72.5 79.0 -
DGCN (Zhuang & Ma, 2018) 83.5 72.6 80.0 74.2
Our GIL 86.2 74.1 83.1 78.9

5.3 ANALYSIS

Meta-optimization: As can be seen in Table 3, we report the classification accuracies of
semi-supervised classification with several variants of our proposed GIL and the classical GCN
method (Kipf & Welling, 2017) when evaluating them on the Cora datasetFor analyzing the perfor-
mance improvement of our proposed GIL with the graph inference learning process, we report the
classification accuracies of GCN (Kipf & Welling, 2017) and our proposed GIL on the Cora dataset
under two different situations, including “only learning with the training set Vtr" and “with jointly
learning on a training set Vtr and a validation set Vval"“GCN /w jointly learning on Vtr & Vval"
achieves a better result than “GCN /w learning on Vtr" by 3.6%, which demonstrates that the network
performance can be improved by employing validation samplesWhen using structure relations,
“GIL /w learning on Vtr" obtains an improvement of 1.9% (over “GCN /w learning on Vtr”), which
can be attributed to the building connection between nodesThe meta-optimization strategy (“GIL /w
meta-training from Vtr → Vval" vs “GIL /w learning on Vtr”) has a gain of 2.9%, which indicates
that a good inference capability can be learnt through meta-optimizationIt is worth noting that, GIL
adopts a meta-optimization strategy to learn the inference model, which is a process of migrating

7



Published as a conference paper at ICLR 2020

from a training set to a validation setIn other words, the validation set is only used to teach the
model itself how to transfer to unseen dataIn contrast, the conventional methods often employ a
validation set to tune parameters of a certain model of interest.

Table 3: Performance comparisons with several GIL variants and the classical GCN method on the Cora dataset.

Methods Acc(%)

GCN (Kipf & Welling, 2017) /w learning on Vtr 81.4/w jointly learning on Vtr & Vval 84.0

GIL /w learning on Vtr 83.3/w meta-train Vtr → Vval 86.2

GIL+mean pooling
/w 1 convlayer 84.5
/w 2 convlayers 86.2
/w 3 convlayers 85.4

GIL+2 convlayers /w max-pooling 85.2/w mean pooling 86.2

Network settings: We explore the effectiveness of our proposed GIL with the same mean pooling
mechanism, but with different numbers of convolutional layers, i.e., “GIL + mean pooling" with one,
two, and three convolutional layers, respectivelyAs can be seen in Table 3, the proposed GIL with
two convolutional layers can obtain a better performance on the Cora data than the other two network
settings (i.e., GIL with one or three convolutional layers)For example, the performance of ‘GIL /w
1 convlayer + mean pooling" is slightly decreased by 1.7% over “GIL /w 2 convlayers + mean
pooling" on the Cora datasetFurthermore, we report the classification results of our proposed GIL
by using mean and max-pooling mechanisms, respectivelyGIL with mean pooling (i.e., “GIL /w
2 conv layers + mean pooling") can get a better result than the GIL method with max-pooling (i.e.,
“GIL /w 2 conv layers + max-pooling"), e.g., 86.2% vs 85.2% on the Cora graph datasetThe reason
may be that the graph network with two convolutional layers and the mean pooling mechanism can
obtain the optimal graph embeddings, but when increasing the network layers, more parameters of a
certain graph model need to be optimized, which may lead to the over-fitting issue.

Influence of different between-node steps: We compare the classification performance within
different between-node steps for our proposed GIL and GCN (Kipf & Welling, 2017), as illustrated in
Fig2(a)The length of between-node steps can be computed with the shortest path between reference
nodes and query nodesWhen the step between nodes is smaller, both GIL and GCN methods can
predict the category information for a small part of unlabeled nodes in the testing setThe reason
may be that the node category information may be disturbed by its nearest neighboring nodes with
different labels and fewer nodes are within 1 or 2 steps in the testing setThe GIL and GCN methods
can infer the category information for a part of unlabeled nodes by adopting node attributes, when
two nodes are not connected in the graph (i.e., step=∞)By increasing the length of reachability
path, the inference process of the GIL method would become difficult and more graph structure
information may be employed in the predicted processGIL can outperform the classic GCN by
analyzing the accuracies within different between-node steps, which indicates that our proposed GIL
has a better reference capability than GCN by using the meta-optimization mechanism from training
nodes to validation nodes.

1 3 5 7 9 11
step

0.0

0.2

0.4

0.6

0.8

ac
cu

ra
cy

our GIL
GCN

(a)

label rate 0.30% 0.60% 0.90% 1.20% 1.50% 1.80%
GCN 0.792 0.797 0.805 0.824 0.829 0.834
GIL(ours) 0.817 0.824 0.831 0.836 0.838 0.842

1x 2x 3x 4x 5x 6x

77.0%

79.0%

81.0%

83.0%

85.0%

1x 2x 3x 4x 5x 6x

GCN

GIL(ours)

Label rates 

A
ccuracy 

(b)

Figure 2: (a) Performance comparisons within different between-node steps on the Cora datasetThe accuracy
equals to the number of correctly classified nodes divided by all testing samples, and is accumulated from step 1
to step k(b) Performance comparisons with different label rates on the Pubmed dataset.

8



Published as a conference paper at ICLR 2020

Influence of different label rates: We also explore the performance comparisons of the GIL method
with different label rates, and the detailed results on the Pubmed dataset can be shown in Fig2(b).
When label rates increase by multiplication, the performances of GIL and GCN are improved, but the
relative gain becomes narrowThe reason is that, the reachable path lengths between unlabeled nodes
and labeled nodes will be reduced with the increase of labeled nodes, which will weaken the effect
of inference learningIn the extreme case, labels of unlabeled nodes could be determined by those
neighbors with the 1 ∼ 2 step reachabilityIn summary, our proposed GIL method prefers small ratio
labeled nodes on the semi-supervised node classification task.

Inference learning process: Classification errors of different epochs on the validation set of the
Cora dataset can be illustrated in Fig3Classification errors are rapidly decreasing as the number of
iterations increases from the beginning to 400 iterations, while they are with a slow descent from 400
iterations to 1200 iterationsIt demonstrates that the learned knowledge from the training samples
can be transferred for inferring node category information from these reference labeled nodesThe
performance of semi-supervised classification can be further increased by improving the generalized
capability of the Graph CNN model.

the number of iterations 

e
rro

r 

Figure 3: Classification errors of different itera-
tions on the validation set of the Cora dataset.

Table 4: Performance comparisons with different mod-
ules on the Cora dataset, where fe, fP , and fr denote
node representation, path reachability, and structure re-
lation, respectively.

fe fr fP Acc.(%)
- - - 56.0
X - - 81.5
X X - 85.0
X X X 86.2

Module analysis: We evaluate the effectiveness of different modules within our proposed GIL
framework, including node representation fe, path reachability fP , and structure relation frNote
that the last one fr defines on the former two ones, so we consider the cases in Table 4 by adding
modulesWhen not using all modules, only original attributes of nodes are used to predict labels.
The case of only using fe belongs to the GCN method, which can achieve 81.5% on the Cora dataset.
The large gain of using the relation module fr (i.e., from 81.5% to 85.0%) may be contributed to the
ability of inference learning on attributes as well as local topology structures which are implicitly
encoded in feThe path information fP can further boost the performance by 1.2%, e.g., 86.2% vs
85.0%It demonstrates that three different modules of our method can improve the graph inference
learning capability.

Computational complexity: For the computational complexity of our proposed GIL, the cost is
mainly spent on the computations of node representation, between-node reachability, and class-to-
node relationship, which are about O((ntr + nte) ∗ e ∗ din ∗ dout), O((ntr + nte) ∗ e ∗ P ), and
O(ntr ∗ nted2out), respectivelyntr and nte refer to the numbers of training and testing nodes, din
and dout denote the input and output dimensions of node representation, e is about the average degree
of graph node, and P is the step length of node reachabilityCompared with those classic Graph
CNNs (Kipf & Welling, 2017), our proposed GIL has a slightly higher cost due to an extra inference
learning process, but can complete the testing stage with several seconds on these benchmark datasets.

6 CONCLUSION

In this work, we tackled the semi-supervised node classification task with a graph inference learning
method, which can better predict the categories of these unlabeled nodes in an end-to-end framework.
We can build a structure relation for obtaining the connection between any two graph nodes, where
node attributes, between-node paths, and graph structure information can be encapsulated together.
For better capturing the transferable knowledge, our method further learns to transfer the mined
knowledge from the training samples to the validation set, finally boosting the prediction accuracy
of the labels of unlabeled nodes in the testing setThe extensive experimental results demonstrate
the effectiveness of our proposed GIL for solving the semi-supervised learning problem, even in
the few-shot paradigmIn the future, we would extend the graph inference method to handle more
graph-related tasks, such as graph generation and social network analysis.

9



Published as a conference paper at ICLR 2020

ACKNOWLEDGMENT

This work was supported by the National Natural Science Foundation of China (Nos61972204,
61906094, U1713208), the Natural Science Foundation of Jiangsu Province (Grant NosBK20191283
and BK20190019), and Tencent AI Lab Rhino-Bird Focused Research Program (NoJR201922).

REFERENCES
Sami Abu-El-Haija, Amol Kapoor, Bryan Perozzi, and Joonseok LeeN-gcn: Multi-scale graph

convolution for semi-supervised node classificationarXiv preprint arXiv:1802.08888, 2018.

James Atwood and Don TowsleyDiffusion-convolutional neural networksIn NeurIPS, pp1993–
2001, 2016.

Karsten M Borgwardt, Hans-Peter Kriegel, SVN Vishwanathan, and Nicol N SchraudolphGraph ker-
nels for disease outcome prediction from protein-protein interaction networksPacific Symposium
on Biocomputing Pacific Symposium on Biocomputing, pp4–15, 2007.

Ulrik Brandes, Daniel Delling, Marco Gaertler, Robert Gorke, Martin Hoefer, Zoran Nikoloski,
and Dorothea WagnerOn modularity clusteringIEEE transactions on knowledge and data
engineering, 20(2):172–188, 2008.

Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr Settles, Estevam RHruschka Jr., and Tom M.
MitchellToward an architecture for never-ending language learningIn AAAI, 2010.

Michaël Defferrard, Xavier Bresson, and Pierre VandergheynstConvolutional neural networks on
graphs with fast localized spectral filteringIn NeurIPS, pp3844–3852, 2016.

Jian Du, Shanghang Zhang, Guanhang Wu, José MF Moura, and Soummya KarTopology adaptive
graph convolutional networksarXiv preprint arXiv:1710.10370, 2017.

Chelsea Finn, Pieter Abbeel, and Sergey LevineModel-agnostic meta-learning for fast adaptation of
deep networksIn ICML, pp1126–1135, 2017.

Will Hamilton, Zhitao Ying, and Jure LeskovecInductive representation learning on large graphsIn
NeurIPS, pp1025–1035, 2017.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian SunDeep residual learning for image
recognitionIn CVPR, pp770–778, 2016.

Mikael Henaff, Joan Bruna, and Yann LeCunDeep convolutional networks on graph-structured data.
arXiv preprint arXiv:1506.05163, 2015.

Jiatao Jiang, Zhen Cui, Chunyan Xu, and Jian YangGaussian-induced convolution for graphsIn
AAAI, volume 33, pp4007–4014, 2019.

Hisashi Kashima, Koji Tsuda, and Akihiro InokuchiMarginalized kernels between labeled graphs.
In ICML, pp321–328, 2003.

Thomas NKipf and Max WellingSemi-supervised classification with graph convolutional networks.
In ICLR, 2017.

Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard ZemelGated graph sequence neural
networksICLR, 2016.

Wei Liu, Junfeng He, and Shih-Fu ChangLarge graph construction for scalable semi-supervised
learningIn ICML, 2010.

Wei Liu, Jun Wang, and Shih-Fu ChangRobust and scalable graph-based semisupervised learning.
Proceedings of the IEEE, 100(9):2624–2638, 2012.

Zhiling Luo, Ling Liu, Jianwei Yin, Ying Li, and Zhaohui WuDeep learning of graphs with ngram
convolutional neural networksIEEE Transactions on Knowledge and Data Engineering, 29(10):
2125–2139, 2017.

10



Published as a conference paper at ICLR 2020

Federico Monti, Davide Boscaini, Jonathan Masci, Emanuele Rodola, Jan Svoboda, and Michael M
BronsteinGeometric deep learning on graphs and manifolds using mixture model cnnsIn CVPR,
pp5115–5124, 2017.

Christopher Morris, Kristian Kersting, and Petra MutzelGlocalized weisfeiler-lehman graph kernels:
Global-local feature maps of graphsIn ICDM, pp327–336IEEE, 2017.

Mathias Niepert, Mohamed Ahmed, and Konstantin KutzkovLearning convolutional neural networks
for graphsIn ICML, pp2014–2023, 2016.

Francesco Orsini, Daniele Baracchi, and Paolo FrasconiShift aggregate extract networksarXiv
preprint arXiv:1703.05537, 2017.

Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry WinogradThe pagerank citation ranking:
Bringing order to the webTechnical Report 1999-66, 1999.

Shirui Pan, Ruiqi Hu, Guodong Long, Jing Jiang, Lina Yao, and Chengqi ZhangAdversarially
regularized graph autoencoder for graph embeddingIn IJCAI, pp2609–2615, 2018.

Sachin Ravi and Hugo LarochelleOptimization as a model for few-shot learningIn ICLR, 2017.

Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-Rad.
Collective classification in network dataAI magazine, 29(3):93–93, 2008.

Nino Shervashidze, SVN Vishwanathan, Tobias Petri, Kurt Mehlhorn, and Karsten Borgwardt.
Efficient graphlet kernels for large graph comparisonIn Artificial Intelligence and Statistics, pp.
488–495, 2009.

Flood Sung, Li Zhang, Tao Xiang, Timothy Hospedales, and Yongxin YangLearning to learn:
Meta-critic networks for sample efficient learningarXiv preprint arXiv:1706.09529, 2017.

Kiran K Thekumparampil, Chong Wang, Sewoong Oh, and Li-Jia LiAttention-based graph neural
network for semi-supervised learningarXiv preprint arXiv:1803.03735, 2018.

Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, and Yoshua
BengioGraph attention networksICLR, 2018.

Danfei Xu, Yuke Zhu, Christopher B Choy, and Li Fei-FeiScene graph generation by iterative
message passingIn CVPR, pp5410–5419, 2017.

Pinar Yanardag and SVN VishwanathanDeep graph kernelsIn SIGKDD, pp1365–1374, 2015.

Zhilin Yang, William W Cohen, and Ruslan SalakhutdinovRevisiting semi-supervised learning with
graph embeddingsICML, 2016.

Bing Yu, Haoteng Yin, and Zhanxing ZhuSpatio-temporal graph convolutional networks: A deep
learning framework for traffic forecastingIn IJCAI, pp3634–3640, 2018.

Tong Zhang, Zhen Cui, Chunyan Xu, Wenming Zheng, and Jian YangVariational pathway reasoning
for eeg emotion recognitionIn AAAI, 2020.

Wenting Zhao, Zhen Cui, Chunyan Xu, Chengzheng Li, Tong Zhang, and Jian YangHashing graph
convolution for node classificationIn CIKM, 2019.

Dengyong Zhou, Olivier Bousquet, Thomas N Lal, Jason Weston, and Bernhard SchölkopfLearning
with local and global consistencyIn NeurIPS, pp321–328, 2004.

Xiaojin Zhu, Zoubin Ghahramani, and John D LaffertySemi-supervised learning using gaussian
fields and harmonic functionsIn ICML, pp912–919, 2003.

Chenyi Zhuang and Qiang MaDual graph convolutional networks for graph-based semi-supervised
classificationIn WWW, pp499–508, 2018.

11


	Introduction
	Related Work
	The Proposed Model
	Problem Definition
	Structure Relation
	Inference Learning

	Modules
	Experiments
	Experimental Settings
	Comparison with state-of-the-arts
	Analysis

	Conclusion

