




















































Published as a conference paper at ICLR 2020

SHARING KNOWLEDGE IN MULTI-TASK
DEEP REINFORCEMENT LEARNING

Carlo D’Eramo & Davide Tateo
Department of Computer Science
TU Darmstadt, IAS
Hochschulstraße 10, 64289, Darmstadt, Germany
{carlo.deramo,davide.tateo}@tu-darmstadt.de

Andrea Bonarini & Marcello Restelli
Politecnico di Milano, DEIB
Piazza Leonardo da Vinci 32, 20133, Milano
{andrea.bonarini,marcello.restelli}@polimi.it

Jan Peters
TU Darmstadt, IAS
Hochschulstraße 10, 64289, Darmstadt, Germany
Max Planck Institute for Intelligent Systems
Max-Planck-Ring 4, 72076, Tübingen, Germany
jan.peters@tu-darmstadt.de

ABSTRACT

We study the benefit of sharing representations among tasks to enable the effective
use of deep neural networks in Multi-Task Reinforcement LearningWe leverage
the assumption that learning from different tasks, sharing common properties, is
helpful to generalize the knowledge of them resulting in a more effective feature ex-
traction compared to learning a single taskIntuitively, the resulting set of features
offers performance benefits when used by Reinforcement Learning algorithms.
We prove this by providing theoretical guarantees that highlight the conditions
for which is convenient to share representations among tasks, extending the well-
known finite-time bounds of Approximate Value-Iteration to the multi-task setting.
In addition, we complement our analysis by proposing multi-task extensions of
three Reinforcement Learning algorithms that we empirically evaluate on widely
used Reinforcement Learning benchmarks showing significant improvements over
the single-task counterparts in terms of sample efficiency and performance.

1 INTRODUCTION

Multi-Task Learning (MTL) ambitiously aims to learn multiple tasks jointly instead of learning them
separately, leveraging the assumption that the considered tasks have common properties which can be
exploited by Machine Learning (ML) models to generalize the learning of each of themFor instance,
the features extracted in the hidden layers of a neural network trained on multiple tasks have the
advantage of being a general representation of structures common to each otherThis translates into
an effective way of learning multiple tasks at the same time, but it can also improve the learning
of each individual task compared to learning them separately (Caruana, 1997)Furthermore, the
learned representation can be used to perform Transfer Learning (TL), i.eusing it as a preliminary
knowledge to learn a new similar task resulting in a more effective and faster learning than learning
the new task from scratch (Baxter, 2000; Thrun & Pratt, 2012).

The same benefits of extraction and exploitation of common features among the tasks achieved
in MTL, can be obtained in Multi-Task Reinforcement Learning (MTRL) when training a single
agent on multiple Reinforcement Learning (RL) problems with common structures (Taylor & Stone,
2009; Lazaric, 2012)In particular, in MTRL an agent can be trained on multiple tasks in the same

1



Published as a conference paper at ICLR 2020

domain, e.griding a bicycle or cycling while going towards a goal, or on different but similar
domains, e.gbalancing a pendulum or balancing a double pendulum1Considering recent advances
in Deep Reinforcement Learning (DRL) and the resulting increase in the complexity of experimental
benchmarks, the use of Deep Learning (DL) models, e.gdeep neural networks, has become a popular
and effective way to extract common features among tasks in MTRL algorithms (Rusu et al., 2015;
Liu et al., 2016; Higgins et al., 2017)However, despite the high representational capacity of DL
models, the extraction of good features remains challengingFor instance, the performance of the
learning process can degrade when unrelated tasks are used together (Caruana, 1997; Baxter, 2000);
another detrimental issue may occur when the training of a single model is not balanced properly
among multiple tasks (Hessel et al., 2018).

Recent developments in MTRL achieve significant results in feature extraction by means of algorithms
specifically developed to address these issuesWhile some of these works rely on a single deep
neural network to model the multi-task agent (Liu et al., 2016; Yang et al., 2017; Hessel et al., 2018;
Wulfmeier et al., 2019), others use multiple deep neural networks, e.gone for each task and another
for the multi-task agent (Rusu et al., 2015; Parisotto et al., 2015; Higgins et al., 2017; Teh et al., 2017).
Intuitively, achieving good results in MTRL with a single deep neural network is more desirable
than using many of them, since the training time is likely much less and the whole architecture is
easier to implementIn this paper we study the benefits of shared representations among tasksWe
theoretically motivate the intuitive effectiveness of our method, deriving theoretical guarantees that
exploit the theoretical framework provided by Maurer et al(2016), in which the authors present
upper bounds on the quality of learning in MTL when extracting features for multiple tasks in a
single shared representationThe significancy of this result is that the cost of learning the shared
representation decreases with a factor O(1/√T), where T is the number of tasks for many function
approximator hypothesis classesThe main contribution of this work is twofold.

1We derive upper confidence bounds for Approximate Value-Iteration (AVI) and Approximate
Policy-Iteration (API)2 (Farahmand, 2011) in the MTRL setting, and we extend the approx-
imation error bounds in Maurer et al(2016) to the case of multiple tasks with different
dimensionalitiesThen, we show how to combine these results resulting in, to the best
of our knowledge, the first proposed extension of the finite-time bounds of AVI/API to
MTRLDespite being an extension of previous works, we derive these results to justify
our approach showing how the error propagation in AVI/API can theoretically benefit from
learning multiple tasks jointly.

2We leverage these results proposing a neural network architecture, for which these bounds
hold with minor assumptions, that allow us to learn multiple tasks with a single regressor
extracting a common representationWe show an empirical evidence of the consequence of
our bounds by means of a variant of FittedQ-Iteration (FQI) (Ernst et al., 2005), based on our
shared network and for which our bounds apply, that we call Multi FittedQ-Iteration (MFQI).
Then, we perform an empirical evaluation in challenging RL problems proposing multi-
task variants of the Deep Q-Network (DQN) (Mnih et al., 2015) and Deep Deterministic
Policy Gradient (DDPG) (Lillicrap et al., 2015) algorithmsThese algorithms are practical
implementations of the more general AVI/API framework, designed to solve complex
problemsIn this case, the bounds apply to these algorithms only with some assumptions,
e.gstationary sampling distributionThe outcome of the empirical analysis joins the
theoretical results, showing significant performance improvements compared to the single-
task version of the algorithms in various RL problems, including several MuJoCo (Todorov
et al., 2012) domains.

2 PRELIMINARIES

Let B(X ) be the space of bounded measurable functions w.r.tthe σ-algebra σX , and similarly
B(X , L) be the same bounded by L <∞.
A Markov Decision Process (MDP) is defined as a 5-tupleM =< S,A,P,R, γ >, where S is the
state space, A is the action space, P : S × A → S is the transition distribution where P(s′|s, a)

1For simplicity, in this paper we refer to the concepts of task and domain interchangeably.
2All proofs and the theorem for API are in Appendix A.2.

2



Published as a conference paper at ICLR 2020

is the probability of reaching state s′ when performing action a in state s, R : S × A × S →
R is the reward function, and γ ∈ (0, 1] is the discount factorA deterministic policy π maps,
for each state, the action to perform: π : S → AGiven a policy π, the value of an action
a in a state s represents the expected discounted cumulative reward obtained by performing a
in s and following π thereafter: Qπ(s, a) , E[

∑∞
k=0 γ

kri+k+1|si = s, ai = a, π], where ri+1
is the reward obtained after the i-th transitionThe expected discounted cumulative reward is
maximized by following the optimal policy π∗ which is the one that determines the optimal action
values, i.e., the ones that satisfy the Bellman optimality equation (Bellman, 1954): Q∗(s, a) ,∫
S P(s

′|s, a) [R(s, a, s′) + γmaxa′ Q∗(s′, a′)] ds′The solution of the Bellman optimality equation
is the fixed point of the optimal Bellman operator T ∗ : B(S × A) → B(S × A) defined as
(T ∗Q)(s, a) ,

∫
S P(s

′|s, a)[R(s, a, s′) + γmaxa′ Q(s′, a′)]ds′In the MTRL setting, there are
multiple MDPsM(t) =< S(t),A(t),P(t),R(t), γ(t) > where t ∈ {1, , T} and T is the number
of MDPsFor each MDPM(t), a deterministic policy πt : S(t) → A(t) induces an action-value
function Qπtt (s

(t), a(t)) = E[
∑∞
k=0 γ

kr
(t)
i+k+1|si = s(t), ai = a(t), πt]In this setting, the goal is to

maximize the sum of the expected cumulative discounted reward of each task.

In our theoretical analysis of the MTRL problem, the complexity of representation plays a central role.
As done in Maurer et al(2016), we consider the Gaussian complexity, a variant of the well-known
Rademacher complexity, to measure the complexity of the representationGiven a set X̄ ∈ X Tn of n
input samples for each task t ∈ {1, , T}, and a classH composed of k ∈ {1, ,K} functions,
the Gaussian complexity of a random set H(X̄) = {(hk(Xti)) : h ∈ H} ⊆ RKTn is defined as
follows:

G(H(X̄)) = E

[
sup
h∈H

∑
tki

γtkihk(Xti)

∣∣∣∣∣Xti
]
, (1)

where γtki are independent standard normal variablesWe also need to define the following quantity,
taken from Maurer (2016): let γ be a vector of m random standard normal variables, and f ∈ F :
Y → Rm, with Y ⊆ Rn, we define

O(F) = sup
y,y′∈Y,y 6=y′

E

[
sup
f∈F

〈γ, f(y)− f(y′)〉
‖y − y′‖

]
(2)

Equation 2 can be viewed as a Gaussian average of Lipschitz quotients, and appears in the bounds
provided in this workFinally, we define L(F) as the upper bound of the Lipschitz constant of all the
functions f in the function class F .

3 THEORETICAL ANALYSIS

The following theoretical study starts from the derivation of theoretical guarantees for MTRL in the
AVI framework, extending the results of Farahmand (2011) in the MTRL scenarioThen, to bound
the approximation error term in the AVI bound, we extend the result described in Maurer (2006)
to MTRLAs we discuss, the resulting bounds described in this section clearly show the benefit of
sharing representation in MTRLTo the best of our knowledge, this is the first general result for
MTRL; previous works have focused on finite MDPs (Brunskill & Li, 2013) or linear models (Lazaric
& Restelli, 2011).

3.1 MULTI-TASK REPRESENTATION LEARNING

The multi-task representation learning problem consists in learning simultaneously a set of T tasks
µt, modeled as probability measures over the space of the possible input-output pairs (x, y), with
x ∈ X and y ∈ R, being X the input spaceLet w ∈ W : X → RJ , h ∈ H : RJ → RK and
f ∈ F : RK → R be functions chosen from their respective hypothesis classesThe functions
in the hypothesis classes must be Lipschitz continuous functionsLet Z̄ = (Z1, ,ZT ) be the
multi-sample over the set of tasks µ = (µ1, , µT ), where Zt = (Zt1, , Ztn) ∼ µnt and
Zti = (Xti, Yti) ∼ µtWe can formalize our regression problem as the following minimization

3



Published as a conference paper at ICLR 2020

problem:

min

{
1

nT

T∑
t=1

N∑
i=1

`(ft(h(wt(Xti))), Yti) : f ∈ FT , h ∈ H,w ∈ WT
}
, (3)

where we use f = (f1, , fT ), w = (w1, , wT ), and define the minimizers of Equation (3) as ŵ,
ĥ, and f̂ We assume that the loss function ` : R× R→ [0, 1] is 1-Lipschitz in the first argument for
every value of the second argumentWhile this assumption may seem restrictive, the result obtained
can be easily scaled to the general caseTo use the principal result of this section, for a generic loss
function `′, it is possible to use `(·) = `′(·)/�max, where �max is the maximum value of `′The expected
loss over the tasks, given w, h and f is the task-averaged risk:

εavg(w, h, f) =
1

T

T∑
t=1

E [`(ft(h(wt(X))), Y )] (4)

The minimum task-averaged risk, given the set of tasks µ and the hypothesis classesW ,H and F is
ε∗avg, and the corresponding minimizers are w

∗, h∗ and f∗.

3.2 MULTI-TASK APPROXIMATE VALUE ITERATION BOUND

We start by considering the bound for the AVI framework which applies for the single-task scenario.
Theorem 1(Theorem 3.4 of Farahmand (2011)) Let K be a positive integer, and Qmax ≤ Rmax1−γ Then
for any sequence (Qk)Kk=0 ⊂ B(S × A, Qmax) and the corresponding sequence (εk)

K−1
k=0 , where

εk = ‖Qk+1 − T ∗Qk‖2ν , we have:

‖Q∗ −QπK‖1,ρ ≤
2γ

(1− γ)2

[
inf

r∈[0,1]
C

1
2

VI,ρ,ν(K; r)E
1
2 (ε0, , εK−1; r) +

2

1− γ
γKRmax

]
, (5)

where

CVI,ρ,ν(K; r) =

(
1− γ

2

)2
sup

π′1,...,π
′
K

K−1∑
k=0

a
2(1−r)
k

∑
m≥0

γm
(
cVI1,ρ,ν(m,K − k;π′K)

+cVI2,ρ,ν(m+ 1;π
′
k+1, , π

′
K)
)2 , (6)

with E(ε0, , εK−1; r) =
∑K−1
k=0 α

2r
k εk, the two coefficients cVI1,ρ,ν , cVI2,ρ,ν , the distributions ρ

and ν, and the series αk are defined as in Farahmand (2011).

In the multi-task scenario, let the average approximation error across tasks be:

εavg,k(ŵk, ĥk, f̂k) =
1

T

T∑
t=1

‖Qt,k+1 − T ∗t Qt,k‖2ν , (7)

where Qt,k+1 = f̂t,k ◦ ĥk ◦ ŵt,k, and T ∗t is the optimal Bellman operator of task t.
In the following, we extend the AVI bound of Theorem 1 to the multi-task scenario, by computing
the average loss across tasks and pushing inside the average using Jensen’s inequality.
Theorem 2Let K be a positive integer, andQmax ≤ Rmax1−γ Then for any sequence (Qk)

K
k=0 ⊂ B(S×

A, Qmax) and the corresponding sequence (εavg,k)K−1k=0 , where εavg,k =
1

T

∑T
t=1‖Qt,k+1−T ∗t Qt,k‖2ν ,

we have:

1

T

T∑
t=1

‖Q∗t −Q
πK
t ‖1,ρ ≤

2γ

(1− γ)2

[
inf

r∈[0,1]
C

1
2

VI(K; r)E
1
2

avg(εavg,0, , εavg,K−1; r) +
2γKRmax,avg

1− γ

]
(8)

with Eavg =
∑K−1
k=0 α

2r
k εavg,k, γ = max

t∈{1,...,T}
γt, C

1
2

VI(K; r) = max
t∈{1,...,T}

C
1
2

VI,ρ,ν(K; t, r), Rmax,avg =

1

T

∑T
t=1Rmax,t and αk =

{
(1−γ)γK−k−1

1−γK+1 0 ≤ k < K,
(1−γ)γK
1−γK+1 k = K

.

4



Published as a conference paper at ICLR 2020

Remarks Theorem 2 retains most of the properties of Theorem 3.4 of Farahmand (2011), except
that the regression error in the bound is now task-averagedInterestingly, the second term of the
sum in Equation (8) depends on the average maximum reward for each taskIn order to obtain this
result we use an overly pessimistic bound on γ and the concentrability coefficients, however this
approximation is not too loose if the MDPs are sufficiently similar.

3.3 MULTI-TASK APPROXIMATION ERROR BOUND

We bound the task-averaged approximation error εavg at each AVI iteration k involved in (8) following
a derivation similar to the one proposed by Maurer et al(2016), obtaining:
Theorem 3Let µ,W , H and F be defined as above and assume 0 ∈ H and f(0) = 0,∀f ∈ F .
Then for δ > 0 with probability at least 1− δ in the draw of Z̄ ∼

∏T
t=1 µ

n
t we have that

εavg(ŵ, ĥ, f̂) ≤L(F)
(
c1
L(H) supl∈{1,...,T}G(W(Xl))

n
+ c2

supw‖w(X̄)‖O(H)
nT

+c3
minp∈P G(H(p))

nT

)
+ c4

suph,w‖h(w(X̄))‖O(F)
n
√
T

+

√
8 ln( 3δ )

nT
+ ε∗avg(9)

Remarks The assumptions 0 ∈ H and f(0) = 0 for all f ∈ F are not essential for the proof and
are only needed to simplify the resultFor reasonable function classes, the Gaussian complexity
G(W(Xl)) is O(

√
n)If supw‖w(X̄)‖ and suph,w‖h(w(X̄))‖ can be uniformly bounded, then

they are O(
√
nT )For some function classes, the Gaussian average of Lipschitz quotients O(·) can

be bounded independently from the number of samplesGiven these assumptions, the first and the
fourth term of the right hand side of Equation (9), which represent respectively the cost of learning the
meta-state space w and the task-specific f mappings, are both O(1/√n)The second term represents
the cost of learning the multi-task representation h and is O(1/√nT), thus vanishing in the multi-task
limit T → ∞The third term can be removed if ∀h ∈ H,∃p0 ∈ P : h(p) = 0; even when this
assumption does not hold, this term can be ignored for many classes of interest, e.gneural networks,
as it can be arbitrarily small.

The last term to be bounded in (9) is the minimum average approximation error ε∗avg at each AVI
iteration kRecalling that the task-averaged approximation error is defined as in (7), applying
Theorem 5.3 by Farahmand (2011) we obtain:
Lemma 4LetQ∗t,k,∀t ∈ {1, , T} be the minimizers of ε∗avg,k, ťk = arg maxt∈{1,...,T}‖Q∗t,k+1−
T ∗t Qt,k‖2ν , and bk,i = ‖Qťk,i+1 − T

∗
ť
Qťk,i‖ν , then:

ε∗avg,k ≤

(
‖Q∗ťk,k+1 − (T

∗
ť )

k+1Qťk,0‖ν +
k−1∑
i=0

(γťkCAE(ν; ťk, P ))
i+1bk,k−1−i

)2
, (10)

with CAE defined as in Farahmand (2011).

Final remarks The bound for MTRL is derived by composing the results in Theorems 2 and 3, and
Lemma 4The results above highlight the advantage of learning a shared representationThe bound
in Theorem 2 shows that a small approximation error is critical to improve the convergence towards
the optimal action-value function, and the bound in Theorem 3 shows that the cost of learning the
shared representation at each AVI iteration is mitigated by using multiple tasksThis is particularly
beneficial when the feature representation is complex, e.gdeep neural networks.

3.4 DISCUSSION

As stated in the remarks of Equation (9), the benefit of MTRL is evinced by the second component
of the bound, i.ethe cost of learning h, which vanishes with the increase of the number of tasks.
Obviously, adding more tasks require the shared representation to be large enough to include all
of them, undesirably causing the term suph,w‖h(w(X̄))‖ in the fourth component of the bound to
increaseThis introduces a tradeoff between the number of features and number of tasks; however, for

5



Published as a conference paper at ICLR 2020

hh

w
1

w
1

w
2

w
2

w
T

w
T

f
1
f
1

f
2
f
2

f
T
f
T

Input Output

x
1

x
2

x
T

y
1

y
2

y
T

.

.
.
.

.

.
.
.

(a) Shared network

0 25 50
# Iterations

0.15
0.20
0.25
0.30
0.35
0.40
0.45
0.50

Q
*

Q
K

FQI
MULTI

0 25 50
# Iterations

0.05
0.00
0.05
0.10
0.15

Pe
rfo

rm
an

ce

(b) FQI vs MFQI

0 25 50
# Iterations

0.15
0.20
0.25
0.30
0.35
0.40
0.45
0.50

Q
*

Q
K

1
2
4
8

(c) #Task analysis

Figure 1: (a) The architecture of the neural network we propose to learn T tasks simultaneously.
The wt block maps each input xt from task µt to a shared set of layers h which extracts a common
representation of the tasksEventually, the shared representation is specialized in block ft and the
output yt of the network is computedNote that each block can be composed of arbitrarily many
layers(b) Results of FQI and MFQI averaged over 4 tasks in Car-On-Hill, showing ‖Q∗ −QπK‖ on
the left, and the discounted cumulative reward on the right(c) Results of MFQI showing ‖Q∗−QπK‖
for increasing number of tasksBoth results in (b) and (c) are averaged over 100 experiments, and
show the 95% confidence intervals.

a reasonable number of tasks the number of features used in the single-task case is enough to handle
them, as we show in some experiments in Section 5Notably, since the AVI/API framework provided
by Farahmand (2011) provides an easy way to include the approximation error of a generic function
approximator, it is easy to show the benefit in MTRL of the bound in Equation (9)Despite being just
multi-task extensions of previous works, our results are the first one to theoretically show the benefit
of sharing representation in MTRLMoreover, they serve as a significant theoretical motivation,
besides to the intuitive ones, of the practical algorithms that we describe in the following sections.

4 SHARING REPRESENTATIONS

We want to empirically evaluate the benefit of our theoretical study in the problem of jointly learning
T different tasks µt, introducing a neural network architecture for which our bounds holdFollowing
our theoretical framework, the network we propose extracts representationswt from inputs xt for each
task µt, mapping them to common features in a set of shared layers h, specializing the learning of
each task in respective separated layers ft, and finally computing the output yt = (ft ◦ h ◦wt)(xt) =
ft(h(wt(xt))) (Figure 1(a))The idea behind this architecture is not new in the literatureFor
instance, similar ideas have already been used in DQN variants to improve exploration on the same
task via bootstrapping (Osband et al., 2016) and to perform MTRL (Liu et al., 2016).

The intuitive and desirable property of this architecture is the exploitation of the regularization effect
introduced by the shared representation of the jointly learned tasksIndeed, unlike learning a single
task that may end up in overfitting, forcing the model to compute a shared representation of the tasks
helps the regression process to extract more general features, with a consequent reduction in the
variance of the learned functionThis intuitive justification for our approach, joins the theoretical
benefit proven in Section 3Note that our architecture can be used in any MTRL problem involving a
regression process; indeed, it can be easily used in value-based methods as a Q-function regressor,
or in policy search as a policy regressorIn both cases, the targets are learned for each task µt
in its respective output block ftRemarkably, as we show in the experimental Section 5, it is
straightforward to extend RL algorithms to their multi-task variants only through the use of the
proposed network architecture, without major changes to the algorithms themselves.

5 EXPERIMENTAL RESULTS

To empirically evince the effect described by our bounds, we propose an extension of FQI (Ernst
et al., 2005; Riedmiller, 2005), that we call MFQI, for which our AVI bounds applyThen, to
empirically evaluate our approach in challenging RL problems, we introduce multi-task variants
of two well-known DRL algorithms: DQN (Mnih et al., 2015) and DDPG (Lillicrap et al., 2015),
which we call Multi Deep Q-Network (MDQN) and Multi Deep Deterministic Policy Gradient
(MDDPG) respectivelyNote that for these methodologies, our AVI and API bounds hold only with

6



Published as a conference paper at ICLR 2020

0 25 50
#Epochs

20
40
60
80

Pe
rfo

rm
an

ce

Cart-Pole

0 25 50
#Epochs

100
90
80
70
60

Acrobot

0 25 50
#Epochs

100
95
90
85
80
75
70
65

Mountain-Car

0 25 50
#Epochs

0.0
0.1
0.2
0.3
0.4

Car-On-Hill

0 25 50
#Epochs

0.6
0.4
0.2
0.0

Inverted-Pendulum

DQN
MULTI

(a) Multi-task

0 25 50
#Epochs

100
90
80
70
60

Pe
rfo

rm
an

ce

Acrobot

No initialization
Unfreeze-0
Unfreeze-10
No unfreeze

(b) Transfer

Figure 2: Discounted cumulative reward averaged over 100 experiments of DQN and MDQN for
each task and for transfer learning in the Acrobot problemAn epoch consists of 1, 000 steps, after
which the greedy policy is evaluated for 2, 000 stepsThe 95% confidence intervals are shown.

the simplifying assumption that the samples are i.i.d.; nevertheless they are useful to show the benefit
of our method also in complex scenarios, e.gMuJoCo (Todorov et al., 2012)We remark that in
these experiments we are only interested in showing the benefit of learning multiple tasks with a
shared representation w.r.tlearning a single task; therefore, we only compare our methods with
the single task counterparts, ignoring other works on MTRL in literatureExperiments have been
developed using the MushroomRL library (D’Eramo et al., 2020), and run on an NVIDIA R© DGX
StationTM and Intel R© AI DevCloudRefer to Appendix B for all the details and our motivations
about the experimental settings.

5.1 MULTI FITTED Q-ITERATION

As a first empirical evaluation, we consider FQI, as an example of an AVI algorithm, to show the
effect described by our theoretical AVI bounds in experimentsWe consider the Car-On-Hill problem
as described in Ernst et al(2005), and select four different tasks from it changing the mass of the
car and the value of the actions (details in Appendix B)Then, we run separate instances of FQI
with a single task network for each task respectively, and one of MFQI considering all the tasks
simultaneouslyFigure 1(b) shows the L1-norm of the difference between Q∗ and QπK averaged
over all the tasksIt is clear how MFQI is able to get much closer to the optimal Q-function, thus
giving an empirical evidence of the AVI bounds in Theorem 2For completeness, we also show the
advantage of MFQI w.r.tFQI in performanceThen, in Figure 1(c) we provide an empirical evidence
of the benefit of increasing the number of tasks in MFQI in terms of both quality and stability.

5.2 MULTI DEEP Q-NETWORK

As in Liu et al(2016), our MDQN uses separate replay memories for each task and the batch
used in each training step is built picking the same number of samples from each replay memory.
Furthermore, a step of the algorithm consists of exactly one step in each taskThese are the only
minor changes to the vanilla DQN algorithm we introduce, while all other aspects, such as the use of
the target network, are not modifiedThus, the time complexity of MDQN is considerably lower than
vanilla DQN thanks to the learning of T tasks with a single model, but at the cost of a higher memory
complexity for the collection of samples for each taskWe consider five problems with similar
state spaces, sparse rewards and discrete actions: Cart-Pole, Acrobot, Mountain-Car, Car-On-Hill,
and Inverted-PendulumThe implementation of the first three problems is the one provided by the
OpenAI Gym library Brockman et al(2016), while Car-On-Hill is described in Ernst et al(2005)
and Inverted-Pendulum in Lagoudakis & Parr (2003).

Figure 2(a) shows the performance of MDQN w.r.tto vanilla DQN that uses a single-task network
structured as the multi-task one in the case with T = 1The first three plots from the left show good
performance of MDQN, which is both higher and more stable than DQNIn Car-On-Hill, MDQN is
slightly slower than DQN to reach the best performance, but eventually manages to be more stable.
Finally, the Inverted-Pendulum experiment is clearly too easy to solve for both approaches, but it is
still useful for the shared feature extraction in MDQNThe described results provide important hints
about the better quality of the features extracted by MDQN w.r.tDQNTo further demonstrate this,
we evaluate the performance of DQN on Acrobot, arguably the hardest of the five problems, using
a single-task network with the shared parameters in h initialized with the weights of a multi-task

7



Published as a conference paper at ICLR 2020

0 50 100
#Epochs

20
30
40
50
60
70
80
90

100

Pe
rfo

rm
an

ce

Inverted-Pendulum

DDPG
MULTI

0 50 100
#Epochs

100
200
300
400
500
600
700
800

Inverted-Double-Pendulum

0 50 100
#Epochs

100
80
60
40
20

0
Inverted-Pendulum-Swingup

(a) Multi-task for pendulums

0 50 100
#Epochs

200

400

600

800

Pe
rfo

rm
an

ce

Inverted-Double-Pendulum

No initialization
Unfreeze-0
No unfreeze

(b) Transfer for pendulums

0 50 100
#Epochs

0
5

10
15
20
25
30
35

Pe
rfo

rm
an

ce

Hopper

0 50 100
#Epochs

0
10
20
30
40
50
60
70

Walker

0 50 100
#Epochs

0
5

10
15
20
25
30
35
40

Half-Cheetah

DDPG
MULTI

(c) Multi-task for walkers

0 50 100
#Epochs

0

10

20

30

40

Pe
rfo

rm
an

ce

Hopper
No initialization
Unfreeze-0
No unfreeze

(d) Transfer for walkers

Figure 3: Discounted cumulative reward averaged over 40 experiments of DDPG and MDDPG for
each task and for transfer learning in the Inverted-Double-Pendulum and Hopper problemsAn
epoch consists of 10, 000 steps, after which the greedy policy is evaluated for 5, 000 stepsThe 95%
confidence intervals are shown.

network trained with MDQN on the other four problemsArbitrarily, the pre-trained weights can be
adjusted during the learning of the new task or can be kept fixed and only the remaining randomly
initialized parameters in w and f are trainedFrom Figure 2(b), the advantages of initializing the
weights are clearIn particular, we compare the performance of DQN without initialization w.r.t.
DQN with initialization in three settings: in Unfreeze-0 the initialized weights are adjusted, in No-
Unfreeze they are kept fixed, and in Unfreeze-10 they are kept fixed until epoch 10 after which they
start to be optimizedInterestingly, keeping the shared weights fixed shows a significant performance
improvement in the earliest epochs, but ceases to improve soonOn the other hand, the adjustment of
weights from the earliest epochs shows improvements only compared to the uninitialized network
in the intermediate stages of learningThe best results are achieved by starting to adjust the shared
weights after epoch 10, which is approximately the point at which the improvement given by the
fixed initialization starts to lessen.

5.3 MULTI DEEP DETERMINISTIC POLICY GRADIENT

In order to show how the flexibility of our approach easily allows to perform MTRL in policy search
algorithms, we propose MDDPG as a multi-task variant of DDPGAs an actor-critic method, DDPG
requires an actor network and a critic networkIntuitively, to obtain MDDPG both the actor and critic
networks should be built following our proposed structureWe perform separate experiments on two
sets of MuJoCo Todorov et al(2012) problems with similar continuous state and action spaces: the
first set includes Inverted-Pendulum, Inverted-Double-Pendulum, and Inverted-Pendulum-Swingup as
implemented in the pybullet library, whereas the second set includes Hopper-Stand, Walker-Walk,
and Half-Cheetah-Run as implemented in the DeepMind Control SuiteTassa et al(2018)Figure 3(a)
shows a relevant improvement of MDDPG w.r.tDDPG in the pendulum tasksIndeed, while in
Inverted-Pendulum, which is the easiest problem among the three, the performance of MDDPG is
only slightly better than DDPG, the difference in the other two problems is significantThe advantage
of MDDPG is confirmed in Figure 3(c) where it performs better than DDPG in Hopper and equally
good in the other two tasksAgain, we perform a TL evaluation of DDPG in the problems where
it suffers the most, by initializing the shared weights of a single-task network with the ones of a
multi-task network trained with MDDPG on the other problemsFigures 3(b) and 3(d) show evident
advantages of pre-training the shared weights and a significant difference between keeping them fixed
or not.

8



Published as a conference paper at ICLR 2020

6 RELATED WORKS

Our work is inspired from both theoretical and empirical studies in MTL and MTRL literatureIn
particular, the theoretical analysis we provide follows previous results about the theoretical properties
of multi-task algorithmsFor instance, Cavallanti et al(2010) and Maurer (2006) prove the theoretical
advantages of MTL based on linear approximationMore in detail, Maurer (2006) derives bounds on
MTL when a linear approximator is used to extract a shared representation among tasksThen, Maurer
et al(2016), which we considered in this work, describes similar results that extend to the use of
non-linear approximatorsSimilar studies have been conducted in the context of MTRLAmong the
others, Lazaric & Restelli (2011) and Brunskill & Li (2013) give theoretical proofs of the advantage
of learning from multiple MDPs and introduces new algorithms to empirically support their claims,
as done in this work.

Generally, contributions in MTRL assume that properties of different tasks, e.gdynamics and reward
function, are generated from a common generative modelAbout this, interesting analyses consider
Bayesian approaches; for instance Wilson et al(2007) assumes that the tasks are generated from a
hierarchical Bayesian model, and likewise Lazaric & Ghavamzadeh (2010) considers the case when
the value functions are generated from a common prior distributionSimilar considerations, which
however does not use a Bayesian approach, are implicitly made in Taylor et al(2007), Lazaric et al.
(2008), and also in this work.

In recent years, the advantages of MTRL have been empirically evinced also in DRL, especially
exploiting the powerful representational capacity of deep neural networksFor instance, Parisotto
et al(2015) and Rusu et al(2015) propose to derive a multi-task policy from the policies learned by
DQN experts trained separately on different tasksRusu et al(2015) compares to a therein introduced
variant of DQN, which is very similar to our MDQN and the one in Liu et al(2016), showing how
their method overcomes it in the Atari benchmark Bellemare et al(2013)Further developments,
extend the analysis to policy search (Yang et al., 2017; Teh et al., 2017), and to multi-goal RL (Schaul
et al., 2015; Andrychowicz et al., 2017)Finally, Hessel et al(2018) addresses the problem of
balancing the learning of multiple tasks with a single deep neural network proposing a method that
uniformly adapts the impact of each task on the training updates of the agent.

7 CONCLUSION

We have theoretically proved the advantage in RL of using a shared representation to learn multiple
tasks w.r.tlearning a single taskWe have derived our results extending the AVI/API bounds (Farah-
mand, 2011) to MTRL, leveraging the upper bounds on the approximation error in MTL provided
in Maurer et al(2016)The results of this analysis show that the error propagation during the
AVI/API iterations is reduced according to the number of tasksThen, we proposed a practical way of
exploiting this theoretical benefit which consists in an effective way of extracting shared representa-
tions of multiple tasks by means of deep neural networksTo empirically show the advantages of our
method, we carried out experiments on challenging RL problems with the introduction of multi-task
extensions of FQI, DQN, and DDPG based on the neural network structure we proposedAs desired,
the favorable empirical results confirm the theoretical benefit we described.

9



Published as a conference paper at ICLR 2020

ACKNOWLEDGMENTS

This project has received funding from the European Union’s Horizon 2020 research and innovation
programme under grant agreement No#640554 (SKILLS4ROBOTS) and No#713010 (GOAL-
Robots)This project has also been supported by grants from NVIDIA, the NVIDIA DGX Station,
and the Intel R© AI DevCloudThe authors thank Alberto Maria Metelli, Andrea Tirinzoni and Matteo
Papini for their helpful insights during the development of the project.

REFERENCES
Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob

McGrew, Josh Tobin, OpenAI Pieter Abbeel, and Wojciech ZarembaHindsight experience replay.
In Advances in Neural Information Processing Systems, pp5048–5058, 2017.

Jonathan BaxterA model of inductive bias learningJournal of Artificial Intelligence Research, 12:
149–198, 2000.

Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael BowlingThe arcade learning environ-
ment: An evaluation platform for general agentsJournal of Artificial Intelligence Research, 47:
253–279, 2013.

Richard BellmanThe theory of dynamic programmingTechnical report, RAND Corp Santa Monica
CA, 1954.

Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech ZarembaOpenai gym, 2016.

Emma Brunskill and Lihong LiSample complexity of multi-task reinforcement learningIn
Proceedings of the Twenty-Ninth Conference on Uncertainty in Artificial Intelligence, 2013.

Rich CaruanaMultitask learningMachine learning, 28(1):41–75, 1997.

Giovanni Cavallanti, Nicolo Cesa-Bianchi, and Claudio GentileLinear algorithms for online
multitask classificationJournal of Machine Learning Research, 11(Oct):2901–2934, 2010.

Carlo D’Eramo, Davide Tateo, Andrea Bonarini, Marcello Restelli, and Jan PetersMushroomrl:
Simplifying reinforcement learning researcharXiv:2001.01102, 2020.

Damien Ernst, Pierre Geurts, and Louis WehenkelTree-based batch mode reinforcement learning.
Journal of Machine Learning Research, 6(Apr):503–556, 2005.

Amir-massoud FarahmandRegularization in reinforcement learning2011.

Matteo Hessel, Hubert Soyer, Lasse Espeholt, Wojciech Czarnecki, Simon Schmitt, and Hado van
HasseltMulti-task deep reinforcement learning with popartarXiv:1809.04474, 2018.

Irina Higgins, Arka Pal, Andrei Rusu, Loic Matthey, Christopher Burgess, Alexander Pritzel, Matthew
Botvinick, Charles Blundell, and Alexander LerchnerDarla: Improving zero-shot transfer in
reinforcement learningIn International Conference on Machine Learning, pp1480–1490, 2017.

Michail G Lagoudakis and Ronald ParrLeast-squares policy iterationJournal of machine learning
research, 4(Dec):1107–1149, 2003.

Alessandro LazaricTransfer in reinforcement learning: a framework and a surveyIn Reinforcement
Learning, pp143–173Springer, 2012.

Alessandro Lazaric and Mohammad GhavamzadehBayesian multi-task reinforcement learningIn
ICML-27th International Conference on Machine Learning, pp599–606Omnipress, 2010.

Alessandro Lazaric and Marcello RestelliTransfer from multiple mdpsIn Advances in Neural
Information Processing Systems, pp1746–1754, 2011.

Alessandro Lazaric, Marcello Restelli, and Andrea BonariniTransfer of samples in batch rein-
forcement learningIn Proceedings of the 25th international conference on Machine learning, pp.
544–551ACM, 2008.

10



Published as a conference paper at ICLR 2020

Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan WierstraContinuous control with deep reinforcement learningarXiv
preprint arXiv:1509.02971, 2015.

Lydia Liu, Urun Dogan, and Katja HofmannDecoding multitask dqn in the world of minecraftIn
European Workshop on Reinforcement Learning, 2016.

Andreas MaurerBounds for linear multi-task learningJournal of Machine Learning Research, 7
(Jan):117–139, 2006.

Andreas MaurerA chain rule for the expected suprema of gaussian processesTheoretical Computer
Science, 650:109–122, 2016.

Andreas Maurer, Massimiliano Pontil, and Bernardino Romera-ParedesThe benefit of multitask
representation learningThe Journal of Machine Learning Research, 17(1):2853–2884, 2016.

Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare,
Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et alHuman-level control
through deep reinforcement learningNature, 518(7540):529, 2015.

Ian Osband, Charles Blundell, Alexander Pritzel, and Benjamin Van RoyDeep exploration via
bootstrapped dqnIn Advances in neural information processing systems, pp4026–4034, 2016.

Emilio Parisotto, Jimmy Lei Ba, and Ruslan SalakhutdinovActor-mimic: Deep multitask and
transfer reinforcement learningarXiv preprint arXiv:1511.06342, 2015.

Martin RiedmillerNeural fitted q iteration–first experiences with a data efficient neural reinforcement
learning methodIn European Conference on Machine Learning, pp317–328Springer, 2005.

Andrei A Rusu, Sergio Gomez Colmenarejo, Caglar Gulcehre, Guillaume Desjardins, James Kirk-
patrick, Razvan Pascanu, Volodymyr Mnih, Koray Kavukcuoglu, and Raia HadsellPolicy
distillationarXiv preprint arXiv:1511.06295, 2015.

Tom Schaul, Daniel Horgan, Karol Gregor, and David SilverUniversal value function approximators.
In International Conference on Machine Learning, pp1312–1320, 2015.

Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de Las Casas, David Budden,
Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq, Timothy PLillicrap, and Martin ARiedmiller.
Deepmind control suiteCoRR, abs/1801.00690, 2018.

Matthew E Taylor and Peter StoneTransfer learning for reinforcement learning domains: A survey.
Journal of Machine Learning Research, 10(Jul):1633–1685, 2009.

Matthew E Taylor, Peter Stone, and Yaxin LiuTransfer learning via inter-task mappings for temporal
difference learningJournal of Machine Learning Research, 8(Sep):2125–2167, 2007.

Yee Teh, Victor Bapst, Wojciech M Czarnecki, John Quan, James Kirkpatrick, Raia Hadsell, Nicolas
Heess, and Razvan PascanuDistral: Robust multitask reinforcement learningIn Advances in
Neural Information Processing Systems, pp4496–4506, 2017.

Sebastian Thrun and Lorien PrattLearning to learnSpringer Science & Business Media, 2012.

Emanuel Todorov, Tom Erez, and Yuval TassaMujoco: A physics engine for model-based control.
In 2012 IEEE/RSJ International Conference on Intelligent Robots and SystemsIEEE, 2012.

Aaron Wilson, Alan Fern, Soumya Ray, and Prasad TadepalliMulti-task reinforcement learning: a
hierarchical bayesian approachIn Proceedings of the 24th international conference on Machine
learning, pp1015–1022ACM, 2007.

Markus Wulfmeier, Abbas Abdolmaleki, Roland Hafner, Jost Tobias Springenberg, Michael Neunert,
Tim Hertweck, Thomas Lampe, Noah Siegel, Nicolas Heess, and Martin RiedmillerRegularized
hierarchical policies for compositional transfer in roboticsarXiv:1906.11228, 2019.

Zhaoyang Yang, Kathryn E Merrick, Hussein A Abbass, and Lianwen JinMulti-task deep reinforce-
ment learning for continuous action controlIn IJCAI, pp3301–3307, 2017.

11



Published as a conference paper at ICLR 2020

A PROOFS

A.1 APPROXIMATED VALUE-ITERATION BOUNDS

Proof of Theorem 2We compute the average expected loss across tasks:

1

T

T∑
t=1

‖Q∗t −Q
πK
t ‖1,ρ

≤ 1
T

T∑
t=1

2γt
(1− γt)2

[
inf

r∈[0,1]
C

1
2

VI,ρ,ν(K; t, r)E
1
2 (εt,0, , εt,K−1; t, r) +

2

1− γt
γKt Rmax,t

]

≤ 2γ
(1− γ)2

1

T

T∑
t=1

[
inf

r∈[0,1]
C

1
2

VI,ρ,ν(K; t, r)E
1
2 (εt,0, , εt,K−1; t, r) +

2

1− γt
γKt Rmax,t

]

≤ 2γ
(1− γ)2

[
1

T

T∑
t=1

(
inf

r∈[0,1]
C

1
2

VI,ρ,ν(K; t, r)E
1
2 (εt,0, , εt,K−1; t, r)

)
+

2

1− γ
γKRmax,avg

]

≤ 2γ
(1− γ)2

[
inf

r∈[0,1]

1

T

T∑
t=1

(
C

1
2

VI,ρ,ν(K; t, r)E
1
2 (εt,0, , εt,K−1; t, r)

)
+

2

1− γ
γKRmax,avg

]

≤ 2γ
(1− γ)2

[
inf

r∈[0,1]
C

1
2

VI(K; r)
1

T

T∑
t=1

(
E 12 (εt,0, , εt,K−1; t, r)

)
+

2

1− γ
γKRmax,avg

]
(11)

with γ = max
t∈{1,...,T}

γt, C
1
2

VI(K; r) = max
t∈{1,...,T}

C
1
2

VI,ρ,ν(K; t, r), and Rmax,avg = 1/T
∑T
t=1Rmax,t.

Considering the term 1/T
∑T
t=1

[
E 12 (εt,0, , εt,K−1; t, r)

]
= 1/T

∑T
t=1

(∑K−1
k=0 α

2r
t,kεt,k

) 1
2

let

αk =

{
(1−γ)γK−k−1

1−γK+1 0 ≤ k < K,
(1−γ)γK
1−γK+1 k = K

,

then we bound

1

T

T∑
t=1

(
K−1∑
k=0

α2rt,kεt,k

) 1
2

≤ 1
T

T∑
t=1

(
K−1∑
k=0

α2rk εt,k

) 1
2

.

Using Jensen’s inequality:

1

T

T∑
t=1

(
K−1∑
k=0

α2rk εt,k

) 1
2

≤

(
K−1∑
k=0

α2rk
1

T

T∑
t=1

εt,k

) 1
2

.

So, now we can write (11) as

1

T

T∑
t=1

‖Q∗t −Q
πK
t ‖1,ρ ≤

2γ

(1− γ)2

[
inf

r∈[0,1]
C

1
2

VI(K; r)E
1
2

avg(εavg,0, , εavg,K−1; r)

+
2

1− γ
γKRmax,avg

]
,

with εavg,k = 1/T
∑T
t=1 εt,k and Eavg(εavg,0, , εavg,K−1; r) =

∑K−1
k=0 α

2r
k εavg,k.

Proof of Lemma 4Let us start from the definition of optimal task-averaged risk:

ε∗avg,k =
1

T

T∑
t=1

‖Q∗t,k+1 − T ∗t Qt,k‖2ν ,

12



Published as a conference paper at ICLR 2020

where Q∗t,k, with t ∈ [1, T ], are the minimizers of εavg,k.

Consider the task ť such that

ťk = arg max
t∈{1,...,T}

‖Q∗t,k+1 − T ∗t Qt,k‖2ν ,

we can write the following inequality:√
ε∗avg,k ≤ ‖Q

∗
ťk,k+1

− T ∗ť Qťk,k‖ν .

By the application of Theorem 5.3 by Farahmand (2011) to the right hand side, and defining
bk,i = ‖Qťk,i+1 − T

∗
ť
Qťk,i‖ν , we obtain:√

ε∗avg,k ≤ ‖Q
∗
ťk,k+1

− (T ∗ť )
k+1Qťk,0‖ν +

k−1∑
i=0

(γťkCAE(ν; ťk, P ))
i+1bk,k−1−i.

Squaring both sides yields the result:

ε∗avg,k ≤

(
‖Q∗ťk,k+1 − (T

∗
ť )

k+1Qťk,0‖ν +
k−1∑
i=0

(γťkCAE(ν; ťk, P ))
i+1bk,k−1−i

)2
.

A.2 APPROXIMATED POLICY-ITERATION BOUNDS

We start by considering the bound for the API framework:

Theorem 5(Theorem 3.2 of Farahmand (2011)) Let K be a positive integer, and Qmax ≤ Rmax1−γ Then
for any sequence (Qk)K−1k=0 ⊂ B(S × A, Qmax) and the corresponding sequence (εk)

K−1
k=0 , where

εk = ‖Qk −Qπk‖2ν , we have:

‖Q∗ −QπK‖1,ρ ≤
2γ

(1− γ)2

[
inf

r∈[0,1]
C

1
2

PI,ρ,ν(K; r)E
1
2 (ε0, , εK−1; r) + γ

K−1Rmax

]
, (12)

where

CPI,ρ,ν(K; r) =(
1− γ

2

)2
sup

π′0,...,π
′
K

K−1∑
k=0

a
2(1−r)
k

∑
m≥0

γmcPI1,ρ,ν(K − k − 1,m+ 1;π′k+1)+

∑
m≥1

γmcPI2,ρ,ν(K − k − 1,m;π′k+1, π′k) + cPI3,ρ,ν

2 ;
(13)

with E(ε0, , εK−1; r) =
∑K−1
k=0 α

2r
k εk, the three coefficients cPI1,ρ,ν , cPI2,ρ,ν , cPI3,ρ,ν , the distri-

butions ρ and ν, and the series αk are defined as in Farahmand (2011).

From Theorem 5, by computing the average loss across tasks and pushing inside the average using
Jensen’s inequality, we derive the API bounds averaged on multiple tasks.

Theorem 6Let K be a positive integer, andQmax ≤ Rmax1−γ Then for any sequence (Qk)
K−1
k=0 ⊂ B(S×

A, Qmax) and the corresponding sequence (εavg,k)K−1k=0 , where εavg,k =
1

T

∑T
t=1‖Qt,k −Q

πk
t ‖2ν , we

have:

1

T

T∑
t=1

‖Q∗t −Q
πK
t ‖1,ρ ≤

2γ

(1− γ)2

[
inf

r∈[0,1]
C

1
2

PI(K; r)E
1
2

avg(εavg,0, , εavg,K−1; r)

+γK−1Rmax,avg
]
, (14)

13



Published as a conference paper at ICLR 2020

with Eavg =
∑K−1
k=0 α

2r
k εavg,k, γ = max

t∈{1,...,T}
γt, C

1
2

PI(K; r) = max
t∈{1,...,T}

C
1
2

PI,ρ,ν(K; t, r), Rmax,avg =

1

T

∑T
t=1Rmax,t and αk =

{
(1−γ)γK−k−1

1−γK+1 0 ≤ k < K,
(1−γ)γK
1−γK+1 k = K

.

Proof of Theorem 6The proof is very similar to the one for AVIWe compute the average expected
loss across tasks:

1

T

T∑
t=1

‖Q∗t −Q
πK
t ‖1,ρ

≤ 1
T

T∑
t=1

2γt
(1− γt)2

[
inf

r∈[0,1]
C

1
2

PI,ρ,ν(K; t, r)E
1
2 (εt,0, , εt,K−1; t, r) + γ

K−1
t Rmax,t

]

≤ 2γ
(1− γ)2

1

T

T∑
t=1

[
inf

r∈[0,1]
C

1
2

PI,ρ,ν(K; t, r)E
1
2 (εt,0, , εt,K−1; t, r) + γ

K−1
t Rmax,t

]

≤ 2γ
(1− γ)2

[
1

T

T∑
t=1

(
inf

r∈[0,1]
C

1
2

PI,ρ,ν(K; t, r)E
1
2 (εt,0, , εt,K−1; t, r)

)
+ γK−1Rmax,avg

]

≤ 2γ
(1− γ)2

[
inf

r∈[0,1]

1

T

T∑
t=1

(
C

1
2

PI,ρ,ν(K; t, r)E
1
2 (εt,0, , εt,K−1; t, r)

)
+ γK−1Rmax,avg

]

≤ 2γ
(1− γ)2

[
inf

r∈[0,1]
C

1
2

PI(K; r)
1

T

T∑
t=1

(
E 12 (εt,0, , εt,K−1; t, r)

)
+ γK−1Rmax,avg

]
(15)

Using Jensen’s inequality as in the AVI scenario, we can write (15) as:

1

T

T∑
t=1

‖Q∗t −Q
πK
t ‖1,ρ ≤

2γ

(1− γ)2

[
inf

r∈[0,1]
C

1
2

PI(K; r)E
1
2

avg(εavg,0, , εavg,K−1; r)

+γK−1Rmax,avg
]
, (16)

with εavg,k = 1/T
∑T
t=1 εt,k and Eavg(εavg,0, , εavg,K−1; r) =

∑K−1
k=0 α

2r
k εavg,k.

A.3 APPROXIMATION BOUNDS

Proof of Theorem 3Let w∗1 , , w
∗
T , h

∗ and f∗1 , , f
∗
T be the minimizers of ε

∗
avg, then:

εavg(ŵ, ĥ, f̂)− ε∗avg =

(
εavg(ŵ, ĥ, f̂)−

1

nT

∑
ti

`(f̂t(ĥ(ŵt(Xti))), Yti)

)
︸ ︷︷ ︸

A

+

(
1

nT

∑
ti

`(f̂t(ĥ(ŵt(Xti))), Yti)−
1

nT

∑
ti

`(f∗t (h
∗(w∗t (Xti))), Yti)

)
︸ ︷︷ ︸

B

+

(
1

nT

∑
ti

`(f∗t (h
∗(w∗t (Xti))), Yti)− ε∗avg

)
︸ ︷︷ ︸

C

(17)

We proceed to bound the three components individually:

• C can be bounded using Hoeffding’s inequality, with probability 1− δ/2 by
√

ln(2/δ)/(2nT ),
as it contains only nT random variables bounded in the interval [0, 1];

14



Published as a conference paper at ICLR 2020

• B can be bounded by 0, by definition of ŵ, ĥ and f̂ , as they are the minimizers of Equa-
tion (3);

• the bounding of A is less straightforward and is described in the following.

We define the following auxiliary function spaces:

• W ′ = {x ∈ X → (wt(xti)) : (w1, , wT ) ∈ WT },

• F ′ =
{
y ∈ RKTn → (ft(yti)) : (f1, , fT ) ∈ FT

}
,

and the following auxiliary sets:

• S =
{

(`(ft(h(wt(Xti))), Yti)) : f ∈ FT , h ∈ H, w ∈ WT
}
⊆ RTn,

• S′ = F ′(H(W ′(X̄))) =
{

(ft(h(wt(Xti)))) : f ∈ FT , h ∈ H, w ∈ WT
}
⊆ RTn,

• S′′ = H(W ′(X̄)) =
{

(h(wt(Xti))) : h ∈ H, w ∈ WT
}
⊆ RKTn,

which will be useful in our proof.

Using Theorem 9 by Maurer et al(2016), we can write:

εavg(ŵ, ĥ, f̂)−
1

nT

∑
ti

`(f̂t(ĥ(ŵt(Xti))), Yti)

≤ sup
w∈WT ,h∈H,f∈FT

(
εavg(w, h, f)−

1

nT

∑
ti

`(ft(h(wt(Xti))), Yti)

)

≤
√

2πG(S)

nT
+

√
9 ln(2δ )

2nT
, (18)

then by Lipschitz property of the loss function ` and the contraction lemma Corollary 11 Maurer et al.
(2016): G(S) ≤ G(S′)By Theorem 12 by Maurer et al(2016), for universal constants c′1 and c′2:

G(S′) ≤ c′1L(F ′)G(S′′) + c′2D(S′′)O(F ′) + min
y∈Y

G(F(y)), (19)

where L(F ′) is the largest value for the Lipschitz constants in the function space F ′, and D(S′′) is
the Euclidean diameter of the set S′′.

Using Theorem 12 by Maurer et al(2016) again, for universal constants c′′1 and c
′′
2 :

G(S′′) ≤ c′′1L(H)G(W ′(X̄)) + c′′2D(W ′(X̄))O(H) + min
p∈P

G(H(p))(20)

Putting (19) and (20) together:

G(S′) ≤ c′1L(F ′)
(
c′′1L(H)G(W ′(X̄)) + c′′2D(W ′(X̄))O(H) + min

p∈P
G(H(p))

)
+ c′2D(S

′′)O(F ′) + min
y∈Y

G(F(y))

= c′1c
′′
1L(F ′)L(H)G(W ′(X̄)) + c′1c′′2L(F ′)D(W ′(X̄))O(H) + c′1L(F ′) min

p∈P
G(H(p))

+ c′2D(S
′′)O(F ′) + min

y∈Y
G(F(y))(21)

At this point, we have to bound the individual terms in the right hand side of (21), following the same
procedure proposed by Maurer et al(2016).

15



Published as a conference paper at ICLR 2020

Firstly, to bound L(F ′), let y, y′ ∈ RKTn, where y = (yti) with yti ∈ RK and y′ = (y′ti) with
y′ti ∈ RK We can write the following:

‖f(y)− f(y′)‖2 =
∑
ti

(ft(yti)− ft(y′ti))
2

≤ L(F)2
∑
ti

‖yti − y′ti‖2

= L(F)2‖y − y′‖2, (22)

whence L(F ′) ≤ L(F).
Then, we bound:

G(W ′(X̄)) = E

[
sup

w∈WT

∑
kti

γktiwtk(Xti)

∣∣∣∣∣Xti
]
≤
∑
t

sup
l∈{1,...,T}

E

[
sup
w∈W

∑
ki

γkliwk(Xli)

∣∣∣∣∣Xli
]

= T sup
l∈{1,...,T}

G(W(Xl))(23)

Then, since it is possible to bound the Euclidean diameter using the norm of the supremum value in
the set, we bound D(S′′) ≤ 2 suph,w‖h(w(X̄))‖ and D(W ′(X̄)) ≤ 2 supw∈WT ‖w(X̄)‖.

Also, we bound O(F ′):

E
[

sup
g∈F ′
〈γ, g(y)− g(y′)〉

]
= E

[
sup

f∈FT

∑
ti

γti (ft(yti)− ft(y′ti))

]

=
∑
t

E

[
sup
f∈F

∑
i

γi (f(yti)− f(y′ti))

]

≤
√
T

∑
t

E

[
sup
f∈F

∑
i

γi (f(yti)− f(y′ti))

]2 12

≤
√
T

(∑
t

O(F)2
∑
i

‖yti − y′ti‖2
) 1

2

=
√
TO(F)‖y − y′‖, (24)

whence O(F ′) ≤
√
TO(F).

To minimize the last term, it is possible to choose y0 = 0, as f(0) = 0,∀f ∈ F , resulting in
miny∈Y G(F(y)) = G(F(0)) = 0.
Then, substituting in (21), and recalling that G(S) ≤ G(S′):

G(S) ≤ c′1c′′1L(F)L(H)T sup
l∈{1,...,T}

G(W(Xl)) + 2c′1c′′2L(F) sup
w∈WT

‖w(X̄)‖O(H)

+ c′1L(F) min
p∈P

G(H(p)) + 2c′2 sup
h,w
‖h(w(X̄))‖

√
TO(F)(25)

16



Published as a conference paper at ICLR 2020

Now, the first term A of (17) can be bounded substituting (25) in (18):

εavg(ŵ, ĥ, f̂)−
1

nT

∑
ti

`(f̂t(ĥ(ŵt(Xti))), Yti)

≤
√

2π

nT

(
c′1c
′′
1L(F)L(H)T sup

l∈{1,...,T}
G(W(Xl)) + 2c′1c′′2L(F) sup

w∈WT
‖w(X̄)‖O(H)

+ c′1L(F) min
p∈P

G(H(p)) + 2c′2 sup
h,w
‖h(w(X̄))‖

√
TO(F)

)
+

√
9 ln(2δ )

2nT

= c1
L(F)L(H) supl∈{1,...,T}G(W(Xl))

n
+ c2

supw‖w(X̄)‖L(F)O(H)
nT

+ c3
L(F) minp∈P G(H(p))

nT
+ c4

suph,w‖h(w(X̄))‖O(F)
n
√
T

+

√
9 ln( 2δ )

2nT
.

A union bound between A, B and C of (17) completes the proof:

εavg(ŵ, ĥ, f̂)− ε∗avg ≤ c1
L(F)L(H) supl∈{1,...,T}G(W(Xl))

n

+ c2
supw‖w(X̄)‖L(F)O(H)

nT

+ c3
L(F) minp∈P G(H(p))

nT

+ c4
suph,w‖h(w(X̄))‖O(F)

n
√
T

+

√
8 ln(3δ )

nT
.

B ADDITIONAL DETAILS OF EMPIRICAL EVALUATION

B.1 MULTI FITTED Q-ITERATION

We consider Car-On-Hill problem with discount factor 0.95 and horizon 100Running Adam
optimizer with learning rate 0.001 and using a mean squared loss, we train a neural network composed
of 2 shared layers of 30 neurons each, with sigmoidal activation function, as described in Riedmiller
(2005)We select 8 tasks for the problem changing the mass of the car m and the value of the
discrete actions a (Table 1)Figure 1(b) is computed considering the first four tasks, while Figure 1(c)
considers task 1 in the result with 1 task, tasks 1 and 2 for the result with 2 tasks, tasks 1, 2, 3, and 4
for the result with 4 tasks, and all the tasks for the result with 8 tasksTo run FQI and MFQI, for each
task we collect transitions running an extra-tree trained following the procedure and setting in Ernst
et al(2005), using an �-greedy policy with � = 0.1, to obtain a small, but representative datasetThe
optimal Q-function for each task is computed by tree-search3 for 100 states uniformly picked from
the state space, and the 2 discrete actions, for a total of 200 state-action tuples.

B.2 MULTI DEEP Q-NETWORK

The five problems we consider for this experiment are: Cart-Pole, Acrobot, Mountain-Car, Car-On-
Hill, and Inverted-Pendulum4The discount factors are respectively 0.99, 0.99, 0.99, 0.95, and 0.95.
The horizons are respectively 500, 1, 000, 1, 000, 100, and 3, 000The network we use consists of 80
ReLu units for each wt, t ∈ {1, , T} block, with T = 5Then, the shared block h consists of one

3We follow the method described in Ernst et al(2005).
4The IDs of the problems in the OpenAI Gym library are: CartPole-v0, Acrobot-v1, and MountainCar-v0.

17



Published as a conference paper at ICLR 2020

Task Mass Action set
1 1.0 {−4.0; 4.0}
2 0.8 {−4.0; 4.0}
3 1.0 {−4.5; 4.5}
4 1.2 {−4.5; 4.5}
5 1.0 {−4.125; 4.125}
6 1.0 {−4.25; 4.25}
7 0.8 {−4.375; 4.375}
8 0.85 {−4.0; 4.0}

Table 1: Different values of the mass of the car and available actions chosen for the Car-On-Hill tasks
in the MFQI empirical evaluation.

layer with 80 ReLu units and another one with 80 sigmoid unitsEventually, each ft has a number of
linear units equal to the number of discrete actions a(t)i , i ∈ {1, ,#A(t)} of task µt which outputs
the action-value Qt(s, a

(t)
i ) = yt(s, a

(t)
i ) = ft(h(wt(s)), a

(t)
i ),∀s ∈ S(t)The use of sigmoid units

in the second layer of h is due to our choice to extract meaningful shared features bounded between 0
and 1 to be used as input of the last linear layer, as in most RL approachesIn practice, we have also
found that sigmoid units help to reduce task interference in multi-task networks, where instead the
linear response of ReLu units cause a problematic increase in the feature valuesFurthermore, the use
of a bounded feature space reduces the suph,w‖h(w(X̄))‖ term in the upper bound of Theorem 3,
corresponding to the upper bound of the diameter of the feature space, as shown in Appendix A.
The initial replay memory size for each task is 100 and the maximum size is 5, 000We use Huber
loss with Adam optimizer using learning rate 10−3 and batch size of 100 samples for each taskThe
target network is updated every 100 stepsThe exploration is ε-greedy with ε linearly decaying from
1 to 0.01 in the first 5, 000 steps.

B.3 MULTI DEEP DETERMINISTIC POLICY GRADIENT

The two set of problems we consider for this experiment are: one including Inverted-Pendulum,
Inverted-Double-Pendulum, and Inverted-Pendulum-Swingup, and another one including Hopper-
Stand, Walker-Walk, and Half-Cheetah-Run5The discount factors are 0.99 and the horizons are
1, 000 for all problemsThe actor network is composed of 600 ReLu units for eachwt, t ∈ {1, , T}
block, with T = 3The shared block h has 500 units with ReLu activation function as for MDQN.
Finally, each ft has a number of tanh units equal to the number of dimensions of the continuous
actions a(t) ∈ A(t) of task µt which outputs the policy πt(s) = yt(s) = ft(h(wt(s))),∀s ∈ S(t).
On the other hand, the critic network consists of the same wt units of the actor, except for the use of
sigmoidal units in the h layer, as in MDQNIn addition to this, the actions a(t) are given as input to
hFinally, each ft has a single linear unit Qt(s, a(t)) = yt(s, a(t)) = ft(h(wt(s), a(t))),∀s ∈ S(t).
The initial replay memory size for each task is 64 and the maximum size is 50, 000We use Huber
loss to update the critic network and the policy gradient to update the actor networkIn both cases
the optimization is performed with Adam optimizer and batch size of 64 samples for each taskThe
learning rate of the actor is 10−4 and the learning rate of the critic is 10−3Moreover, we apply
`2-penalization to the critic network using a regularization coefficient of 0.01The target networks are
updated with soft-updates using τ = 10−3The exploration is performed using the action computed
by the actor network adding a noise generated with an Ornstein-Uhlenbeck process with θ = 0.15
and σ = 0.2Note that most of these values are taken from the original DDPG paper Lillicrap et al.
(2015), which optimizes them for the single-task scenario.

5The IDs of the problems in the pybullet library are: InvertedPendulumBulletEnv-v0,
InvertedDoublePendulumBulletEnv-v0, and InvertedPendulumSwingupBulletEnv-v0The names of the
domain and the task of the problems in the DeepMind Control Suite are: hopper-stand, walker-walk, and
cheetah-run.

18


	Introduction
	Preliminaries
	Theoretical analysis
	Multi-task representation learning
	Multi-task Approximate Value Iteration bound
	Multi-task approximation error bound
	Discussion

	Sharing representations
	Experimental results
	Multi Fitted Q-Iteration
	Multi Deep Q-Network
	Multi Deep Deterministic Policy Gradient

	Related works
	Conclusion
	Proofs
	Approximated Value-Iteration bounds
	Approximated Policy-Iteration bounds
	Approximation bounds

	Additional details of empirical evaluation
	Multi Fitted Q-Iteration
	Multi Deep Q-Network
	Multi Deep Deterministic Policy Gradient


