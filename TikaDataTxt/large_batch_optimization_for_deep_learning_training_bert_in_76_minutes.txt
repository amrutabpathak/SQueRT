




















































Published as a conference paper at ICLR 2020

LARGE BATCH OPTIMIZATION FOR DEEP LEARNING:
TRAINING BERT IN 76 MINUTES

Yang You2, Jing Li1, Sashank Reddi1, Jonathan Hseu1, Sanjiv Kumar1, Srinadh Bhojanapalli1

Xiaodan Song1, James Demmel2, Kurt Keutzer2, Cho-Jui Hsieh1,3
Yang You was a student researcher at Google BrainThis project was done when he was at Google Brain.

Google1, UC Berkeley2, UCLA3
{youyang, demmel, keutzer}@cs.berkeley.edu, {jingli, sashank, jhseu, sanjivk, bsrinadh, xiaodansong, chojui}@google.com

ABSTRACT

Training large deep neural networks on massive datasets is computationally very
challengingThere has been recent surge in interest in using large batch stochastic
optimization methods to tackle this issueThe most prominent algorithm in this
line of research is LARS, which by employing layerwise adaptive learning rates
trains RESNET on ImageNet in a few minutesHowever, LARS performs poorly for
attention models like BERT, indicating that its performance gains are not consistent
across tasksIn this paper, we first study a principled layerwise adaptation strategy
to accelerate training of deep neural networks using large mini-batchesUsing this
strategy, we develop a new layerwise adaptive large batch optimization technique
called LAMB; we then provide convergence analysis of LAMB as well as LARS,
showing convergence to a stationary point in general nonconvex settingsOur
empirical results demonstrate the superior performance of LAMB across various
tasks such as BERT and RESNET-50 training with very little hyperparameter tuning.
In particular, for BERT training, our optimizer enables use of very large batch sizes
of 32868 without any degradation of performanceBy increasing the batch size to
the memory limit of a TPUv3 Pod, BERT training time can be reduced from 3 days
to just 76 minutes (Table 1)The LAMB implementation is available online1.

1 INTRODUCTION

With the advent of large scale datasets, training large deep neural networks, even using computation-
ally efficient optimization methods like Stochastic gradient descent (SGD), has become particularly
challengingFor instance, training state-of-the-art deep learning models like BERT and ResNet-50
takes 3 days on 16 TPUv3 chips and 29 hours on 8 Tesla P100 gpus respectively (Devlin et al., 2018;
He et al., 2016)Thus, there is a growing interest to develop optimization solutions to tackle this
critical issueThe goal of this paper is to investigate and develop optimization techniques to accelerate
training large deep neural networks, mostly focusing on approaches based on variants of SGD.

Methods based on SGD iteratively update the parameters of the model by moving them in a scaled
(negative) direction of the gradient calculated on a minibatchHowever, SGD’s scalability is limited
by its inherent sequential natureOwing to this limitation, traditional approaches to improve SGD
training time in the context of deep learning largely resort to distributed asynchronous setup (Dean
et al., 2012; Recht et al., 2011)However, the implicit staleness introduced due to the asynchrony
limits the parallelization of the approach, often leading to degraded performanceThe feasibility of
computing gradient on large minibatches in parallel due to recent hardware advances has seen the
resurgence of simply using synchronous SGD with large minibatches as an alternative to asynchronous
SGDHowever, naïvely increasing the batch size typically results in degradation of generalization
performance and reduces computational benefits (Goyal et al., 2017).

Synchronous SGD on large minibatches benefits from reduced variance of the stochastic gradients
used in SGDThis allows one to use much larger learning rates in SGD, typically of the order square
root of the minibatch sizeSurprisingly, recent works have demonstrated that up to certain minibatch
sizes, linear scaling of the learning rate with minibatch size can be used to further speed up the

1https://github.com/tensorflow/addons/blob/master/tensorflow_addons/
optimizers/lamb.py

1

https://github.com/tensorflow/addons/blob/master/tensorflow_addons/optimizers/lamb.py
https://github.com/tensorflow/addons/blob/master/tensorflow_addons/optimizers/lamb.py


Published as a conference paper at ICLR 2020

training Goyal et al(2017)These works also elucidate two interesting aspects to enable the use of
linear scaling in large batch synchronous SGD: (i) linear scaling of learning rate is harmful during the
initial phase; thus, a hand-tuned warmup strategy of slowly increasing the learning rate needs to be
used initially, and (ii) linear scaling of learning rate can be detrimental beyond a certain batch size.
Using these tricks, Goyal et al(2017) was able to drastically reduce the training time of ResNet-50
model from 29 hours to 1 hour using a batch size of 8192While these works demonstrate the
feasibility of this strategy for reducing the wall time for training large deep neural networks, they
also highlight the need for an adaptive learning rate mechanism for large batch learning.

Variants of SGD using layerwise adaptive learning rates have been recently proposed to address this
problemThe most successful in this line of research is the LARS algorithm (You et al., 2017), which
was initially proposed for training RESNETUsing LARS, ResNet-50 can be trained on ImageNet in
just a few minutes! However, it has been observed that its performance gains are not consistent across
tasksFor instance, LARS performs poorly for attention models like BERTFurthermore, theoretical
understanding of the adaptation employed in LARS is largely missingTo this end, we study and
develop new approaches specially catered to the large batch setting of our interest.

ContributionsMore specifically, we make the following main contributions in this paper.

• Inspired by LARS, we investigate a general adaptation strategy specially catered to large
batch learning and provide intuition for the strategy.

• Based on the adaptation strategy, we develop a new optimization algorithm (LAMB) for
achieving adaptivity of learning rate in SGDFurthermore, we provide convergence analysis
for both LARS and LAMB to achieve a stationary point in nonconvex settingsWe highlight
the benefits of using these methods for large batch settings.

• We demonstrate the strong empirical performance of LAMB across several challenging tasks.
Using LAMB we scale the batch size in training BERT to more than 32k without degrading
the performance; thereby, cutting the time down from 3 days to 76 minutesOurs is the first
work to reduce BERT training wall time to less than couple of hours.

• We also demonstrate the efficiency of LAMB for training state-of-the-art image classification
models like RESNETTo the best of our knowledge, ours is first adaptive solver that can
achieve state-of-the-art accuracy for RESNET-50 as adaptive solvers like Adam fail to obtain
the accuracy of SGD with momentum for these tasks.

1.1 RELATED WORK

The literature on optimization for machine learning is vast and hence, we restrict our attention to the
most relevant works hereEarlier works on large batch optimization for machine learning mostly
focused on convex models, benefiting by a factor of square root of batch size using appropriately large
learning rateSimilar results can be shown for nonconvex settings wherein using larger minibatches
improves the convergence to stationary points; albeit at the cost of extra computationHowever,
several important concerns were raised with respect to generalization and computational performance
in large batch nonconvex settingsIt was observed that training with extremely large batch was
difficult (Keskar et al., 2016; Hoffer et al., 2017)Thus, several prior works carefully hand-tune
training hyper-parameters, like learning rate and momentum, to avoid degradation of generalization
performance (Goyal et al., 2017; Li, 2017; You et al., 2018; Shallue et al., 2018).

(Krizhevsky, 2014) empirically found that simply scaling the learning rate linearly with respect to
batch size works better up to certain batch sizesTo avoid optimization instability due to linear scaling
of learning rate, Goyal et al(2017) proposed a highly hand-tuned learning rate which involves a
warm-up strategy that gradually increases the LR to a larger value and then switching to the regular
LR policy (e.gexponential or polynomial decay)Using LR warm-up and linear scaling, Goyal et al.
(2017) managed to train RESNET-50 with batch size 8192 without loss in generalization performance.
However, empirical study (Shallue et al., 2018) shows that learning rate scaling heuristics with the
batch size do not hold across all problems or across all batch sizes.

More recently, to reduce hand-tuning of hyperparameters, adaptive learning rates for large batch
training garnered significant interestsSeveral recent works successfully scaled the batch size to large
values using adaptive learning rates without degrading the performance, thereby, finishing RESNET-
50 training on ImageNet in a few minutes (You et al., 2018; Iandola et al., 2016; Codreanu et al.,
2017; Akiba et al., 2017; Jia et al., 2018; Smith et al., 2017; Martens & Grosse, 2015; Devarakonda

2



Published as a conference paper at ICLR 2020

et al., 2017; Mikami et al., 2018; Osawa et al., 2018; You et al., 2019; Yamazaki et al., 2019)To the
best of our knowledge, the fastest training result for RESNET-50 on ImageNet is due to Ying et al.
(2018), who achieve 76+% top-1 accuracyBy using the LARS optimizer and scaling the batch size to
32K on a TPUv3 Pod, Ying et al(2018) was able to train RESNET-50 on ImageNet in 2.2 minutes.
However, it was empirically observed that none of these performance gains hold in other tasks such
as BERT training (see Section 4).

2 PRELIMINARIES

NotationFor any vector xt ∈ Rd, either xt,j or [xt]j are used to denote its jth coordinate where
j ∈ [d]Let I be the d×d identity matrix, and let I = [I1, I2, ..., Ih] be its decomposition into column
submatrices Ii = d× dhFor x ∈ Rd, let x(i) be the block of variables corresponding to the columns
of Ii i.e., x(i) = I>i x ∈ Rdi for i = {1, 2, · · · , h}For any function f : Rd → R, we use ∇if(x) to
denote the gradient with respect to x(i)For any vectors u, v ∈ Rd, we use u2 and u/v to denote
elementwise square and division operators respectivelyWe use ‖.‖ and ‖.‖1 to denote l2-norm and
l1-norm of a vector respectively.

We start our discussion by formally stating the problem setupIn this paper, we study nonconvex
stochastic optimization problems of the form

min
x∈Rd

f(x) := Es∼P[`(x, s)] +
λ

2
‖x‖2, (1)

where ` is a smooth (possibly nonconvex) function and P is a probability distribution on the domain
S ⊂ RkHere, x corresponds to model parameters, ` is the loss function and P is an unknown data
distribution.

We assume function `(x) is Li-smooth with respect to x(i), i.e., there exists a constant Li such that

‖∇i`(x, s)−∇i`(y, s)‖ ≤ Li‖x(i) − y(i)‖, ∀ x, y ∈ Rd, and s ∈ S, (2)

for all i ∈ [h]We use L = (L1, · · · , Lh)> to denote the h-dimensional vector of Lipschitz constants.
We use L∞ and Lavg to denote maxi Li and

∑
i
Li
h respectivelyWe assume the following bound

on the variance in stochastic gradients: E‖∇i`(x, s)−∇if(x)‖2 ≤ σ2i for all x ∈ Rd and i ∈ [h].
Furthermore, we also assume E‖[∇`(x, s)]i − [∇f(x)]i‖2 ≤ σ̃2i for all x ∈ Rd and i ∈ [d]We use
σ = (σ1, · · · , σh)> and σ̃ = (σ̃1, · · · , σ̃d)> to denote the vectors of standard deviations of stochastic
gradient per layer and per dimension respectivelyFinally, we assume that the gradients are bounded
i.e., [∇l(x, s)]j ≤ G for all i ∈ [d], x ∈ Rd and s ∈ S Note that such assumptions are typical in the
analysis of stochastic first-order methods (cf(Ghadimi & Lan, 2013a; Ghadimi et al., 2014)).

Stochastic gradient descent (SGD) is one of the simplest first-order algorithms for solving problem in
Equation 1The update at the tth iteration of SGD is of the following form:

xt+1 = xt − ηt
1

|St|
∑
st∈St

∇`(xt, st) + λxt, (SGD)

where St is set of b random samples drawn from the distribution PFor very large batch settings, the
following is a well-known result for SGD.

Theorem 1 ((Ghadimi & Lan, 2013b))With large batch b = T and using appropriate learning rate,
we have the following for the iterates of SGD:

E
[
‖∇f(xa)‖2

]
≤ O

(
(f(x1)− f(x∗))L∞

T
+
‖σ‖2

T

)
.

where x∗ is an optimal solution to the problem in equation 1 and xa is an iterate uniformly randomly
chosen from {x1, · · · , xT }.

However, tuning the learning rate ηt in SGD, especially in large batch settings, is difficult in practice.
Furthermore, the dependence on L∞ (the maximum of smoothness across dimension) can lead to
significantly slow convergenceIn the next section, we discuss algorithms to circumvent this issue.

3



Published as a conference paper at ICLR 2020

3 ALGORITHMS

In this section, we first discuss a general strategy to adapt the learning rate in large batch settings.
Using this strategy, we discuss two specific algorithms in the later part of the sectionSince our
primary focus is on deep learning, our discussion is centered around training a h-layer neural network.

General StrategySuppose we use an iterative base algorithm A (e.gSGD or ADAM) in the small
batch setting with the following layerwise update rule:

xt+1 = xt + ηtut,

where ut is the update made by A at time step tWe propose the following two changes to the update
for large batch settings:

1The update is normalized to unit l2-normThis is ensured by modifying the update to the
form ut/‖ut‖Throughout this paper, such a normalization is done layerwise i.e., the update
for each layer is ensured to be unit l2-norm.

2The learning rate is scaled by φ(‖xt‖) for some function φ : R+ → R+Similar to the
normalization, such a scaling is done layerwise.

Suppose the base algorithm A is SGD, then the modification results in the following update rule:

x
(i)
t+1 = x

(i)
t − ηt

φ(‖x(i)t ‖)
‖g(i)t ‖

g
(i)
t , (3)

for all layers i ∈ [h] and where x(i)t and g
(i)
t are the parameters and the gradients of the i

th layer at
time step tThe normalization modification is similar to one typically used in normalized gradient
descent except that it is done layerwiseNote that the modification leads to a biased gradient update;
however, in large-batch settings, it can be shown that this bias is smallIt is intuitive that such a
normalization provides robustness to exploding gradients (where the gradient can be arbitrarily large)
and plateaus (where the gradient can be arbitrarily small)Normalization of this form essentially
ignores the size of the gradient and is particularly useful in large batch settings where the direction of
the gradient is largely preserved.

The scaling term involving φ ensures that the norm of the update is of the same order as that of
the parameterWe found that this typically ensures faster convergence in deep neural networks.
In practice, we observed that a simple function of φ(z) = min{max{z, γl}, γu} works wellIt is
instructive to consider the case where φ(z) = zIn this scenario, the overall change in the learning

rate is ‖x
(i)
t ‖

‖g(i)t ‖
, which can also be interpreted as an estimate on the inverse of Lipschitz constant of the

gradient (see equation 2)We now discuss different instantiations of the strategy discussed aboveIn
particular, we focus on two algorithms: LARS (3.1) and the proposed method, LAMB (3.2).

3.1 LARS ALGORITHM

The first instantiation of the general strategy is LARS algorithm (You et al., 2017), which is obtained
by using momentum optimizer as the base algorithm A in the frameworkLARS was earlier proposed
for large batch learning for RESNET on ImageNetIn general, it is observed that the using (heavy-ball)
momentum, one can reduce the variance in the stochastic gradients at the cost of little biasThe
pseudocode for LARS is provide in Algorithm 1.

We now provide convergence analysis for LARS in general nonconvex setting stated in this paperFor
the sake of simplicity, we analyze the case where β1 = 0 and λ = 0 in Algorithm 1However, our
analysis should extend to the general case as wellWe will defer all discussions about the convergence
rate to the end of the section.

Theorem 2Let ηt = η =
√

2(f(x1)−f(x∗))
α2u‖L‖1T

for all t ∈ [T ], b = T , αl ≤ φ(v) ≤ αu for all v > 0
where αl, αu > 0Then for xt generated using LARS (Algorithm 1), we have the following bound(

E

[
1√
h

h∑
i=1

‖∇if(xa)‖

])2
≤ O

(
(f(x1)− f(x∗))Lavg

T
+
‖σ‖21
Th

)
,

where x∗ is an optimal solution to the problem in equation 1 and xa is an iterate uniformly randomly
chosen from {x1, · · · , xT }.

4



Published as a conference paper at ICLR 2020

Algorithm 1 LARS
Input: x1 ∈ Rd, learning rate {ηt}Tt=1, parameter
0 < β1 < 1, scaling function φ, � > 0
Set m0 = 0
for t = 1 to T do

Draw b samples St from P
Compute gt = 1|St|

∑
st∈St ∇`(xt, st)

mt = β1mt−1 + (1− β1)(gt + λxt)
x

(i)
t+1 = x

(i)
t − ηt

φ(‖x(i)t ‖)

‖m(i)t ‖
m

(i)
t for all i ∈ [h]

end for

Algorithm 2 LAMB
Input: x1 ∈ Rd, learning rate {ηt}Tt=1, parameters
0 < β1, β2 < 1, scaling function φ, � > 0
Set m0 = 0, v0 = 0
for t = 1 to T do

Draw b samples St from P.
Compute gt = 1|St|

∑
st∈St ∇`(xt, st).

mt = β1mt−1 + (1− β1)gt
vt = β2vt−1 + (1− β2)g2t
mt = mt/(1− βt1)
vt = vt/(1− βt2)
Compute ratio rt = mt√vt+�

x
(i)
t+1 = x

(i)
t − ηt

φ(‖x(i)t ‖)

‖r(i)t +λx
(i)
t ‖

(r
(i)
t + λx

(i)
t )

end for

3.2 LAMB ALGORITHM

The second instantiation of the general strategy is obtained by using ADAM as the base algorithm A.
ADAM optimizer is popular in deep learning community and has shown to have good performance
for training state-of-the-art language models like BERTUnlike LARS, the adaptivity of LAMB is
two-fold: (i) per dimension normalization with respect to the square root of the second moment used
in ADAM and (ii) layerwise normalization obtained due to layerwise adaptivityThe pseudocode for
LAMB is provided in Algorithm 2When β1 = 0 and β2 = 0, the algorithm reduces to be Sign SGD
where the learning rate is scaled by square root of the layer dimension (Bernstein et al., 2018).

The following result provides convergence rate for LAMB in general nonconvex settingsSimilar to
the previous case, we focus on the setting where β1 = 0 and λ = 0As before, our analysis extends
to the general case; however, the calculations become messy.

Theorem 3Let ηt = η =
√

2(f(x1)−f(x∗))
α2u‖L‖1T

for all t ∈ [T ], b = T , di = d/h for all i ∈ [h], and
αl ≤ φ(v) ≤ αu for all v > 0 where αl, αu > 0Then for xt generated using LAMB (Algorithm 2),
we have the following bounds:

1When β2 = 0, we have(
E
[

1√
d
‖∇f(xa)‖1

])2
≤ O

(
(f(x1)− f(x∗))Lavg

T
+
‖σ̃‖21
Th

)
,

2When β2 > 0, we have

E[‖∇f(xa)‖2] ≤ O

(√
G2d

h(1− β2)
×

[√
2(f(x1)− f(x∗))‖L‖1

T
+
‖σ̃‖1√
T

])
,

where x∗ is an optimal solution to the problem in equation 1 and xa is an iterate uniformly randomly
chosen from {x1, · · · , xT }.

Discussion on convergence ratesWe first start our discussion with the comparison of convergence
rate of LARS with that of SGD (Theorem 1)The convergence rates of LARS and SGD differ in
two ways: (1) the convergence criterion is (E[

∑h
i=1 ‖∇if‖])2 as opposed to E[‖∇f‖2] in SGD and

(2) the dependence on L and σ in the convergence rateBriefly, the convergence rate of LARS is
better than SGD when the gradient is denser than curvature and stochasticityThis convergence rate
comparison is similar in spirit to the one obtained in (Bernstein et al., 2018)Assuming that the
convergence criterion in Theorem 1 and Theorem 2 is of similar order (which happens when gradients
are fairly dense), convergence rate of LARS and LAMB depend on Lavg instead of L∞ and are thus,
significantly better than that of SGDA more quantitative comparison is provided in Section C of
the AppendixThe comparison of LAMB (with β2 = 0) with SGD is along similar linesWe obtain
slightly worse rates for the case where β2 > 0; although, we believe that its behavior should be better
than the case β2 = 0We leave this investigation to future work.

5



Published as a conference paper at ICLR 2020

4 EXPERIMENTS

We now present empirical results comparing LAMB with existing optimizers on two important
large batch training tasks: BERT and RESNET-50 trainingWe also compare LAMB with existing
optimizers for small batch size (< 1K) and small dataset (e.gCIFAR, MNIST) (see Appendix).

Experimental SetupTo demonstrate its robustness, we use very minimal hyperparameter tuning for
the LAMB optimizerThus, it is possible to achieve better results by further tuning the hyperparameters.
The parameters β1 and β2 in Algorithm 2 are set to 0.9 and 0.999 respectively in all our experiments;
we only tune the learning rateWe use a polynomially decaying learning rate of ηt = η0×(1−t/T ) in
Algorithm 2), which is the same as in BERT baselineThis setting also works for all other applications
in this paperFurthermore, for BERT and RESNET-50 training, we did not tune the hyperparameters
of LAMB while increasing the batch sizeWe use the square root of LR scaling rule to automatically
adjust learning rate and linear-epoch warmup schedulingWe use TPUv3 in all the experimentsA
TPUv3 Pod has 1024 chips and can provide more than 100 petaflops performance for mixed precision
computingTo make sure we are comparing with solid baselines, we use grid search to tune the
hyper-parameters for ADAM, ADAGRAD, ADAMW (ADAM with weight decay), and LARSWe also
tune weight decay for ADAMWAll the hyperparameter tuning settings are reported in the Appendix.
Due to space constraints, several experimental details are relegated to the Appendix.

4.1 BERT TRAINING

We first discuss empirical results for speeding up BERT trainingFor this experiment, we use the same
dataset as Devlin et al(2018), which is a concatenation of Wikipedia and BooksCorpus with 2.5B
and 800M words respectivelyWe specifically focus on the SQuAD task2 in this paperThe F1 score
on SQuAD-v1 is used as the accuracy metric in our experimentsAll our comparisons are with respect
to the baseline BERT model by Devlin et al(2018)To train BERT, Devlin et al(2018) first train the
model for 900k iterations using a sequence length of 128 and then switch to a sequence length of
512 for the last 100k iterationsThis results in a training time of around 3 days on 16 TPUv3 chips.
The baseline BERT model3 achieves a F1 score of 90.395To ensure a fair comparison, we follow
the same SQuAD fine-tune procedure of Devlin et al(2018) without modifying any configuration
(including number of epochs and hyperparameters)As noted earlier, we could get even better results
by changing the fine-tune configurationFor instance, by just slightly changing the learning rate in
the fine-tune stage, we can obtain a higher F1 score of 91.688 for the batch size of 16K using LAMB.
We report a F1 score of 91.345 in Table 1, which is the score obtained for the untuned versionBelow
we describe two different training choices for training BERT and discuss the corresponding speedups.

For the first choice, we maintain the same training procedure as the baseline except for changing the
training optimizer to LAMBWe run with the same number of epochs as the baseline but with batch
size scaled from 512 to 32KThe choice of 32K batch size (with sequence length 512) is mainly
due to memory limits of TPU PodOur results are shown in Table 1By using the LAMB optimizer,
we are able to achieve a F1 score of 91.460 in 15625 iterations for a batch size of 32768 (14063
iterations for sequence length 128 and 1562 iterations for sequence length 512)With 32K batch size,
we reduce BERT training time from 3 days to around 100 minutesWe achieved 49.1 times speedup
by 64 times computational resources (76.7% efficiency)We consider the speedup is great because we
use the synchronous data-parallelismThere is a communication overhead coming from transferring
of the gradients over the interconnectFor RESNET-50, researchers are able to achieve 90% scaling
efficiency because RESNET-50 has much fewer parameters (# parameters is equal to #gradients) than
BERT (25 million versus 300 million).

To obtain further improvements, we use the Mixed-Batch Training procedure with LAMBRecall
that BERT training involves two stages: the first 9/10 of the total epochs use a sequence length of 128,
while the last 1/10 of the total epochs use a sequence length of 512For the second stage training,
which involves a longer sequence length, due to memory limits, a maximum batch size of only
32768 can be used on a TPUv3 PodHowever, we can potentially use a larger batch size for the
first stage because of a shorter sequence lengthIn particular, the batch size can be increased to
131072 for the first stageHowever, we did not observe any speedup by increasing the batch size from
65536 to 131072 for the first stage, thus, we restrict the batch size to 65536 for this stageBy using
this strategy, we are able to make full utilization of the hardware resources throughout the training

2https://rajpurkar.github.io/SQuAD-explorer/
3Pre-trained BERT model can be downloaded from https://github.com/google-research/bert

6



Published as a conference paper at ICLR 2020

Table 1: We use the F1 score on SQuAD-v1 as the accuracy metricThe baseline F1 score is the
score obtained by the pre-trained model (BERT-Large) provided on BERT’s public repository (as of
February 1st, 2019)We use TPUv3s in our experimentsWe use the same setting as the baseline: the
first 9/10 of the total epochs used a sequence length of 128 and the last 1/10 of the total epochs used
a sequence length of 512All the experiments run the same number of epochsDev set means the test
dataIt is worth noting that we can achieve better results by manually tuning the hyperparameters.
The data in this table is collected from the untuned version.

Solver batch size steps F1 score on dev set TPUs Time

Baseline 512 1000k 90.395 16 81.4h
LAMB 512 1000k 91.752 16 82.8h
LAMB 1k 500k 91.761 32 43.2h
LAMB 2k 250k 91.946 64 21.4h
LAMB 4k 125k 91.137 128 693.6m
LAMB 8k 62500 91.263 256 390.5m
LAMB 16k 31250 91.345 512 200.0m
LAMB 32k 15625 91.475 1024 101.2m

LAMB 64k/32k 8599 90.584 1024 76.19m

procedureIncreasing the batch size is able to warm-up and stabilize the optimization process (Smith
et al., 2017), but decreasing the batch size brings chaos to the optimization process and can cause
divergenceIn our experiments, we found a technique that is useful to stabilize the second stage
optimizationBecause we switched to a different optimization problem, it is necessary to re-warm-up
the optimizationInstead of decaying the learning rate at the second stage, we ramp up the learning
rate from zero again in the second stage (re-warm-up)As with the first stage, we decay the learning
rate after the re-warm-up phaseWith this method, we only need 8599 iterations and finish BERT
training in 76 minutes (100.2% efficiency).

Comparison with ADAMW and LARSTo ensure that our approach is compared to a solid
baseline for the BERT training, we tried three different strategies for tuning ADAMW: (1) ADAMW
with default hyperparameters (see Devlin et al(2018)) (2) ADAMW with the same hyperparameters
as LAMB, and (3) ADAMW with tuned hyperparametersADAMW stops scaling at the batch size of
16K because it is not able to achieve the target F1 score (88.1 vs 90.4)The tuning information of
ADAMW is shown in the AppendixFor 64K/32K mixed-batch training, even after extensive tuning
of the hyperparameters, we fail to get any reasonable result with ADAMW optimizerWe conclude
that ADAMW does not work well in large-batch BERT training or is at least hard to tuneWe also
observe that LAMB performs better than LARS for all batch sizes (see Table 2).

Table 2: LAMB achieves a higher performance (F1 score) than LARS for all the batch sizesThe
baseline achieves a F1 score of 90.390Thus, LARS stops scaling at the batch size of 16K.

Batch Size 512 1K 2K 4K 8K 16K 32K

LARS 90.717 90.369 90.748 90.537 90.548 89.589 diverge
LAMB 91.752 91.761 91.946 91.137 91.263 91.345 91.475

4.2 IMAGENET TRAINING WITH RESNET-50.

ImageNet training with ResNet-50 is an industry standard metric that is being used in MLPerf4.
The baseline can get 76.3% top-1 accuracy in 90 epochs (Goyal et al., 2017)All the successful
implementations are based on momentum SGD (He et al., 2016; Goyal et al., 2017) or LARS optimizer
(Ying et al., 2018; Jia et al., 2018; Mikami et al., 2018; You et al., 2018; Yamazaki et al., 2019).
Before our study, we did not find any paper reporting a state-of-the-art accuracy achieved by ADAM,

4https://mlperf.org/

7



Published as a conference paper at ICLR 2020

ADAGRAD, or ADAMW optimizerIn our experiments, even with comprehensive hyper-parameter
tuning, ADAGRAD/ADAM/ADAMW (with batch size 16K) only achieves 55.38%/66.04%/67.27%
top-1 accuracyAfter adding learning rate scheme of Goyal et al(2017), the top-1 accuracy of
ADAGRAD/ADAM/ADAMW was improved to 72.0%/73.48%/73.07%However, they are still much
lower than 76.3%The details of the tuning information are in the AppendixTable 3 shows that
LAMB can achieve the target accuracyBeyond a batch size of 8K, LAMB’s accuracy is higher than
the momentumLAMB’s accuracy is also slightly better than LARSAt a batch size of 32K, LAMB
achieves 76.4% top-1 accuracy while LARS achieves 76.3%At a batch size of 2K, LAMB is able to
achieve 77.11% top-1 accuracy while LARS achieves 76.6%.

Table 3: Top-1 validation accuracy of ImageNet/RESNET-50 training at the batch size of 16K (90
epochs)The performance of momentum was reported by (Goyal et al., 2017)+ means adding the
learning rate scheme of Goyal et al(2017) to the optimizer: (1) 5-epoch warmup to stablize the initial
stage; and (2) multiply the learning rate by 0.1 at 30th, 60th, and 80th epochThe target accuracy is
around 0.763 (Goyal et al., 2017)All the adaptive solvers were comprehensively tunedThe tuning
information was in the Appendix.

optimizer adagrad/adagrad+ adam/adam+ adamw/adamw+ momentum lamb

Accuracy 0.5538/0.7201 0.6604/0.7348 0.6727/0.7307 0.7520 0.7666

4.3 HYPERPARAMETERS FOR SCALING THE BATCH SIZE

For BERT and ImageNet training, we did not tune the hyperparameters of LAMB optimizer when
increasing the batch sizeWe use the square root LR scaling rule and linear-epoch warmup scheduling
to automatically adjust learning rateThe details can be found in Tables 4 and 5

Table 4: Untuned LAMB for BERT training across different batch sizes (fixed #epochs)We use
square root LR scaling and linear-epoch warmupFor example, batch size 32K needs to finish 15625
iterationsIt uses 0.2×15625 = 3125 iterations for learning rate warmupBERT’s baseline achieved a
F1 score of 90.395We can achieve an even higher F1 score if we manually tune the hyperparameters.

Batch Size 512 1K 2K 4K 8K 16K 32K

Learning Rate 523.0×103
5

22.5×103
5

22.0×103
5

21.5×103
5

21.0×103
5

20.5×103
5

20.0×103

Warmup Ratio 1320
1

160
1
80

1
40

1
20

1
10

1
5

F1 score 91.752 91.761 91.946 91.137 91.263 91.345 91.475
Exact Match 85.090 85.260 85.355 84.172 84.901 84.816 84.939

Table 5: Untuned LAMB for ImageNet training with RESNET-50 for different batch sizes (90 epochs).
We use square root LR scaling and linear-epoch warmupThe baseline Goyal et al(2017) gets 76.3%
top-1 accuracy in 90 epochsStanford DAWN Bench (Coleman et al., 2017) baseline achieves 93%
top-5 accuracyLAMB achieves both of themLAMB can achieve an even higher accuracy if we
manually tune the hyperparameters.

Batch Size 512 1K 2K 4K 8K 16K 32K

Learning Rate 423.0×100
4

22.5×100
4

22.0×100
4

21.5×100
4

21.0×100
4

20.5×100
4

20.0×100
Warmup Epochs 0.3125 0.625 1.25 2.5 5 10 20
Top-5 Accuracy 0.9335 0.9349 0.9353 0.9332 0.9331 0.9322 0.9308
Top-1 Accuracy 0.7696 0.7706 0.7711 0.7692 0.7689 0.7666 0.7642

5 CONCLUSION

Large batch techniques are critical to speeding up deep neural network trainingIn this paper, we
propose the LAMB optimizer, which supports adaptive elementwise updating and layerwise learning

8



Published as a conference paper at ICLR 2020

ratesFurthermore, LAMB is a general purpose optimizer that works for both small and large batches.
We also provided theoretical analysis for the LAMB optimizer, highlighting the cases where it
performs better than standard SGDLAMB achieves a better performance than existing optimizers for
a wide range of applicationsBy using LAMB, we are able to scale the batch size of BERT pre-training
to 64K without losing accuracy, thereby, reducing the BERT training time from 3 days to around 76
minutesLAMB is also the first large batch adaptive solver that can achieve state-of-the-art accuracy
on ImageNet training with RESNET-50.

6 ACKNOWLEDGEMENT
We want to thank the comments from George Dahl and Jeff DeanWe want to thank Michael Banfield,
Dehao Chen, Youlong Cheng, Sameer Kumar, and Zak Stone for TPU Pod support.

REFERENCES
Takuya Akiba, Shuji Suzuki, and Keisuke FukudaExtremely large minibatch sgd: Training resnet-50

on imagenet in 15 minutesarXiv preprint arXiv:1711.04325, 2017.

Yoshua BengioPractical recommendations for gradient-based training of deep architecturesIn
Neural networks: Tricks of the trade, pp437–478Springer, 2012.

Jeremy Bernstein, Yu-Xiang Wang, Kamyar Azizzadenesheli, and Anima Anandkumarsignsgd:
compressed optimisation for non-convex problemsCoRR, abs/1802.04434, 2018.

Valeriu Codreanu, Damian Podareanu, and Vikram SaletoreScale out for large minibatch sgd:
Residual network training on imagenet-1k with improved accuracy and reduced time to trainarXiv
preprint arXiv:1711.04291, 2017.

Cody Coleman, Deepak Narayanan, Daniel Kang, Tian Zhao, Jian Zhang, Luigi Nardi, Peter Bailis,
Kunle Olukotun, Chris Ré, and Matei ZahariaDawnbench: An end-to-end deep learning bench-
mark and competitionTraining, 100(101):102, 2017.

Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao, Andrew Senior,
Paul Tucker, Ke Yang, Quoc V Le, et alLarge scale distributed deep networksIn Advances in
neural information processing systems, pp1223–1231, 2012.

Aditya Devarakonda, Maxim Naumov, and Michael GarlandAdabatch: Adaptive batch sizes for
training deep neural networksarXiv preprint arXiv:1712.02029, 2017.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina ToutanovaBert: Pre-training of deep
bidirectional transformers for language understandingarXiv preprint arXiv:1810.04805, 2018.

Timothy DozatIncorporating nesterov momentum into adam2016.

Saeed Ghadimi and Guanghui LanStochastic first- and zeroth-order methods for nonconvex
stochastic programmingSIAM Journal on Optimization, 23(4):2341–2368, 2013adoi: 10.1137/
120880811.

Saeed Ghadimi and Guanghui LanStochastic first-and zeroth-order methods for nonconvex stochastic
programmingSIAM Journal on Optimization, 23(4):2341–2368, 2013b.

Saeed Ghadimi, Guanghui Lan, and Hongchao ZhangMini-batch stochastic approximation methods
for nonconvex stochastic composite optimizationMathematical Programming, 155(1-2):267–305,
2014.

Priya Goyal, Piotr Dollár, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola,
Andrew Tulloch, Yangqing Jia, and Kaiming HeAccurate, large minibatch sgd: Training imagenet
in 1 hourarXiv preprint arXiv:1706.02677, 2017.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian SunDeep residual learning for image
recognitionIn Proceedings of the IEEE conference on computer vision and pattern recognition,
pp770–778, 2016.

Elad Hoffer, Itay Hubara, and Daniel SoudryTrain longer, generalize better: closing the generalization
gap in large batch training of neural networksarXiv preprint arXiv:1705.08741, 2017.

9



Published as a conference paper at ICLR 2020

Forrest N Iandola, Matthew W Moskewicz, Khalid Ashraf, and Kurt KeutzerFirecaffe: near-linear
acceleration of deep neural network training on compute clustersIn Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition, pp2592–2600, 2016.

Xianyan Jia, Shutao Song, Wei He, Yangzihao Wang, Haidong Rong, Feihu Zhou, Liqiang Xie,
Zhenyu Guo, Yuanzhou Yang, Liwei Yu, et alHighly scalable deep learning training system with
mixed-precision: Training imagenet in four minutesarXiv preprint arXiv:1807.11205, 2018.

Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter
TangOn large-batch training for deep learning: Generalization gap and sharp minimaarXiv
preprint arXiv:1609.04836, 2016.

Alex KrizhevskyOne weird trick for parallelizing convolutional neural networksarXiv preprint
arXiv:1404.5997, 2014.

Mu LiScaling Distributed Machine Learning with System and Algorithm Co-designPhD thesis,
Intel, 2017.

James Martens and Roger GrosseOptimizing neural networks with kronecker-factored approximate
curvatureIn International conference on machine learning, pp2408–2417, 2015.

Hiroaki Mikami, Hisahiro Suganuma, Yoshiki Tanaka, Yuichi Kageyama, et alImagenet/resnet-50
training in 224 secondsarXiv preprint arXiv:1811.05233, 2018.

Yurii E NesterovA method for solving the convex programming problem with convergence rate o
(1/kˆ 2)In Doklakadnauk Sssr, volume 269, pp543–547, 1983.

Kazuki Osawa, Yohei Tsuji, Yuichiro Ueno, Akira Naruse, Rio Yokota, and Satoshi Matsuoka.
Second-order optimization method for large mini-batch: Training resnet-50 on imagenet in 35
epochsarXiv preprint arXiv:1811.12019, 2018.

Benjamin Recht, Christopher Re, Stephen Wright, and Feng NiuHogwild: A lock-free approach to
parallelizing stochastic gradient descentIn Advances in neural information processing systems,
pp693–701, 2011.

Christopher J Shallue, Jaehoon Lee, Joe Antognini, Jascha Sohl-Dickstein, Roy Frostig, and George E
DahlMeasuring the effects of data parallelism on neural network trainingarXiv preprint
arXiv:1811.03600, 2018.

Samuel L Smith, Pieter-Jan Kindermans, and Quoc V LeDon’t decay the learning rate, increase the
batch sizearXiv preprint arXiv:1711.00489, 2017.

Ilya Sutskever, James Martens, George Dahl, and Geoffrey HintonOn the importance of initialization
and momentum in deep learningIn International conference on machine learning, pp1139–1147,
2013.

Masafumi Yamazaki, Akihiko Kasagi, Akihiro Tabuchi, Takumi Honda, Masahiro Miwa, Naoto
Fukumoto, Tsuguchika Tabaru, Atsushi Ike, and Kohta NakashimaYet another accelerated sgd:
Resnet-50 training on imagenet in 74.7 secondsarXiv preprint arXiv:1903.12650, 2019.

Chris Ying, Sameer Kumar, Dehao Chen, Tao Wang, and Youlong ChengImage classification at
supercomputer scalearXiv preprint arXiv:1811.06992, 2018.

Yang You, Igor Gitman, and Boris GinsburgScaling sgd batch size to 32k for imagenet training.
arXiv preprint arXiv:1708.03888, 2017.

Yang You, Zhao Zhang, Cho-Jui Hsieh, James Demmel, and Kurt KeutzerImagenet training in
minutesIn Proceedings of the 47th International Conference on Parallel Processing, pp1ACM,
2018.

Yang You, Jonathan Hseu, Chris Ying, James Demmel, Kurt Keutzer, and Cho-Jui HsiehLarge-batch
training for lstm and beyondarXiv preprint arXiv:1901.08256, 2019.

10



Published as a conference paper at ICLR 2020

APPENDIX

A PROOF OF THEOREM 2

ProofWe analyze the convergence of LARS for general minibatch size hereRecall that the update
of LARS is the following

x
(i)
t+1 = x

(i)
t − ηtφ(‖x

(i)
t ‖)

g
(i)
t

‖g(i)t ‖
,

for all i ∈ [h]For simplicity of notation, we reason the
Since the function f is L-smooth, we have the following:

f(xt+1) ≤ f(xt) + 〈∇if(xt), x(i)t+1 − x
(i)
t 〉+

h∑
i=1

Li
2
‖x(i)t+1 − x

(i)
t ‖2

= f(xt)− ηt
h∑
i=1

di∑
j=1

φ(‖x(i)t ‖)×

(
[∇if(xt)]j ×

g
(i)
t,j

‖g(i)t ‖

)
+

h∑
i=1

Liη
2
t φ

2(‖x(i)t ‖)
2

≤ f(xt)− ηt
h∑
i=1

di∑
j=1

φ(‖x(i)t ‖)×

(
[∇if(xt)]j ×

(
g
(i)
t,j

‖g(i)t ‖
− [∇if(xt)]j
‖∇if(xt)‖

+
[∇if(xt)]j
‖∇if(xt)‖

))
+
η2tα

2
u

2
‖L‖1

= f(xt)− ηt
h∑
i=1

φ(‖x(i)t ‖)× ‖∇if(xt)‖ − ηt
h∑
i=1

di∑
j=1

(
[∇if(xt)]j ×

(
g
(i)
t,j

‖g(i)t ‖
− [∇if(xt)]j
‖∇if(xt)‖

))
+
η2tα

2
u

2
‖L‖1

(4)

The first inequality follows from the lipschitz continuous nature of the gradientLet ∆(i)t = g
(i)
t −

∇if(xt)Then the above inequality can be rewritten in the following manner:

f(xt+1) ≤ f(xt)− ηt
h∑
i=1

φ(‖x(i)t ‖)‖∇if(xt)‖

− ηt
h∑
i=1

di∑
j=1

φ(‖x(i)t ‖)×

(
[∇if(xt)]j ×

(
(∆

(i)
t,j + [∇if(xt)]j)

‖∆(i)t +∇if(xt)‖
− [∇if(xt)]j
‖∇if(xt)‖

))
+
η2tα

2
u

2
‖L‖1

= f(xt)− ηt
h∑
i=1

φ(‖x(i)t ‖)‖∇if(xt)‖

− ηt
h∑
i=1

φ(‖x(i)t ‖)×

(
〈∆(i)t +∇if(xt),∇if(xt)〉
‖∆(i)t +∇if(xt)‖

− ‖∇if(xt)‖

)
+
η2tα

2
u

2
‖L‖1

= f(xt)− ηt
h∑
i=1

φ(‖x(i)t ‖)‖∇if(xt)‖

+ ηt

h∑
i=1

φ(‖x(i)t ‖)×

(
‖∇if(xt)‖‖∆(i)t +∇if(xt)‖ − 〈∆

(i)
t +∇if(xt),∇if(xt)〉

‖∆(i)t +∇if(xt)‖

)
+
η2tα

2
u

2
‖L‖1

= f(xt)− ηt
h∑
i=1

φ(‖x(i)t ‖)‖∇if(xt)‖+
η2tα

2
u

2
‖L‖1

+ ηt

h∑
i=1

φ(‖x(i)t ‖)×

(
‖∇if(xt)‖‖∆(i)t +∇if(xt)‖ − ‖∆

(i)
t +∇if(xt)‖2 + 〈∆

(i)
t ,∆

(i)
t +∇if(xt)〉

‖∆(i)t +∇if(xt)‖

)
.

(5)

11



Published as a conference paper at ICLR 2020

Using Cauchy-Schwarz inequality in the above inequality, we have:

f(xt+1) ≤ f(xt)− ηt
h∑
i=1

φ(‖x(i)t ‖)‖∇if(xt)‖

+ ηt

h∑
i=1

φ(‖x(i)t ‖)×
(
‖∇if(xt)‖ − ‖∆(i)t +∇if(xt)‖+ ‖∆

(i)
t ‖
)

+
η2tα

2
u

2
‖L‖1

≤ f(xt)− ηt
h∑
i=1

φ(‖x(i)t ‖)‖∇if(xt)‖+ 2ηt
h∑
i=1

φ(‖x(i)t ‖)× ‖∆
(i)
t ‖+

η2tα
2
u

2
‖L‖1

Taking expectation, we obtain the following:

E[f(xt+1)] ≤ f(xt)− ηt
h∑
i=1

φ(‖x(i)t ‖)‖∇if(xt)‖+ 2ηt
h∑
i=1

φ(‖x(i)t ‖)× E[‖∆
(i)
t ‖] +

η2tα
2
u

2
‖L‖1

≤ f(xt)− ηtαl
h∑
i=1

‖∇if(xt)‖+ 2ηtαu
‖σ‖1√
b

+
η2tα

2
u

2
‖L‖1(6)

Summing the above inequality for t = 1 to T and using telescoping sum, we have the following
inequality:

E[f(xT+1)] ≤ f(x1)− ηαl
T∑
t=1

h∑
i=1

E[‖∇if(xt)‖] + 2ηT
αu‖σ‖1√

b
+
η2α2uT

2
‖L‖1.

Rearranging the terms of the above inequality, and dividing by ηTαl, we have:

1

T

T∑
t=1

h∑
i=1

E[‖∇if(xt)‖] ≤
f(x1)− E[f(xT+1)]

Tηαl
+

2αu‖σ‖1√
bαl

+
ηα2u
2αl
‖L‖1

≤ f(x1)− f(x
∗)

Tηαl
+

2αu‖σ‖1
αl
√
b

+
ηα2u
2αl
‖L‖1.

B PROOF OF THEOREM 3

ProofWe analyze the convergence of LAMB for general minibatch size hereRecall that the update
of LAMB is the following

x
(i)
t+1 = x

(i)
t − ηtφ(‖x

(i)
t ‖)

r
(i)
t

‖r(i)t ‖
,

for all i ∈ [h]For simplicity of notation, we reason the
Since the function f is L-smooth, we have the following:

f(xt+1) ≤ f(xt) + 〈∇if(xt), x(i)t+1 − x
(i)
t 〉+

h∑
i=1

Li
2
‖x(i)t+1 − x

(i)
t ‖2

= f(xt)−ηt
h∑
i=1

di∑
j=1

φ(‖x(i)t ‖)×

(
[∇if(xt)]j ×

r
(i)
t,j

‖r(i)t ‖

)
︸ ︷︷ ︸

T1

+

h∑
i=1

Liα
2
uη

2
t

2
(7)

12



Published as a conference paper at ICLR 2020

The above inequality simply follows from the lipschitz continuous nature of the gradientWe bound
term T1 in the following manner:

T1 ≤ −ηt
h∑
i=1

di∑
j=1

φ(‖x(i)t ‖)×

(
[∇if(xt)]j ×

r
(i)
t,j

‖r(i)t ‖

)

≤ −ηt
h∑
i=1

di∑
j=1

√
1− β2
G2di

(
φ(‖x(i)t ‖)× [∇if(xt)]j × g

(i)
t,j

)

− ηt
h∑
i=1

di∑
j=1

(
φ(‖x(i)t ‖)× [∇if(xt)]j ×

r
(i)
t,j

‖r(i)t ‖

)
1(sign(∇if(xt)]j) 6= sign(r(i)t,j))

(8)

This follows from the fact that ‖r(i)t ‖ ≤
√

di
1−β2 and

√
vt ≤ GIf β2 = 0, then T1 can be bounded

as follows:

T1 ≤ −ηt
h∑
i=1

di∑
j=1

√
1

di

(
φ(‖x(i)t ‖)× |[∇if(xt)]j |

)

− ηt
h∑
i=1

di∑
j=1

(
φ(‖x(i)t ‖)× [∇if(xt)]j ×

r
(i)
t,j

‖r(i)t ‖

)
1(sign(∇if(xt)]j) 6= sign(r(i)t,j))

The rest of the proof for β2 = 0 is similar to argument for the case β2 > 0, which is shown below.
Taking expectation, we have the following:

E[T1] ≤ −ηt
h∑
i=1

di∑
j=1

√
1− β2
G2di

E
[
φ(‖x(i)t ‖)×

(
[∇if(xt)]j × g(i)t,j

)]

− ηt
h∑
i=1

di∑
j=1

E

[
φ(‖x(i)t ‖)×

(
[∇if(xt)]j ×

r
(i)
t,j

‖r(i)t ‖

)
1(sign(∇if(xt)]j) 6= sign(g(i)t,j ))

]

≤ −ηt
h∑
i=1

di∑
j=1

√
1− β2
G2di

E
[(
φ(‖x(i)t ‖)× [∇if(xt)]j × g

(i)
t,j

)]

+ ηt

h∑
i=1

di∑
j=1

E
[
αu|[∇if(xt)]j |1(sign(∇if(xt)]j) 6= sign(g(i)t,j ))

]

≤ −ηt
h∑
i=1

di∑
j=1

√
1− β2
G2di

E
[
φ(‖x(i)t ‖)×

(
[∇if(xt)]j × g(i)t,j

)]

− ηt
h∑
i=1

di∑
j=1

αu|[∇if(xt)]j |P(sign(∇if(xt)]j) 6= sign(g(i)t,j ))

(9)

Using the bound on the probability that the signs differ, we get:

E[T1] ≤ −ηtαl

√
h(1− β2)
G2d

‖∇f(xt)‖2 + ηtαu
h∑
i=1

di∑
j=1

σi,j√
b
.

Substituting the above bound on T1 in equation 7, we have the following bound:

E[f(xt+1)] ≤ f(xt)− ηtαl

√
h(1− β2)
G2d

‖∇f(xt)‖2 + ηtαu
‖σ̃‖1√
b

+
η2tα

2
u‖L‖1
2

(10)

13



Published as a conference paper at ICLR 2020

Algorithm 3 N-LAMB
Input: x1 ∈ Rd, learning rate {ηt}Tt=1, parame-
ters 0 < β1, β2 < 1, scaling function φ, � > 0,
parameters 0 < {βt1}Tt=1 < 1
Set m0 = 0, v0 = 0
for t = 1 to T do

Draw b samples St from P.
Compute gt = 1|St|

∑
st∈St ∇`(xt, st).

mt = β1mt−1 + (1− β1)gt
m̂ =

βt+11 mt

1−Πt+1i=1β
i
1

+
(1−βt1)gt

1−Πti=1β
i
1

vt = β2vt−1 + (1− β2)g2t
v̂ = β2vt

1−βt2
Compute ratio rt = m̂√

v̂+�

x
(i)
t+1 = x

(i)
t − ηt

φ(‖x(i)t ‖)

‖r(i)t +λx
(i)
t ‖

(r
(i)
t + λxt)

end for

Algorithm 4 NN-LAMB
Input: x1 ∈ Rd, learning rate {ηt}Tt=1, parameters
0 < β1, β2 < 1, scaling function φ, � > 0, parame-
ters 0 < {βt1}Tt=1 < 1
Set m0 = 0, v0 = 0
for t = 1 to T do

Draw b samples St from P.
Compute gt = 1|St|

∑
st∈St ∇`(xt, st).

mt = β1mt−1 + (1− β1)gt
m̂ =

βt+11 mt

1−Πt+1i=1β
i
1

+
(1−βt1)gt

1−Πti=1β
i
1

vt = β2vt−1 + (1− β2)g2t
v̂ =

βt+12 vt

1−Πt+1i=1β
i
2

+
(1−βt2)g

2
t

1−Πti=1β
i
2

Compute ratio rt = m̂√
v̂+�

x
(i)
t+1 = x

(i)
t − ηt

φ(‖x(i)t ‖)

‖r(i)t +λx
(i)
t ‖

(r
(i)
t + λxt)

end for

Summing the above inequality for t = 1 to T and using telescoping sum, we have the following
inequality:

E[f(xT+1)] ≤ f(x1)− ηtαl

√
h(1− β2)
G2d

T∑
t=1

E[‖∇f(xt)‖2] + ηTαu
‖σ̃‖1√
b

+
η2α2uT

2
‖L‖1.

Rearranging the terms of the above inequality, and dividing by ηTαl, we have:√
h(1− β2)
G2d

1

T

T∑
t=1

E[‖∇f(xt)‖2] ≤
f(x1)− E[f(xT+1)]

Tηαl
+
αu‖σ̃‖1
αl
√
b

+
η

2
‖L‖1

≤ f(x1)− f(x
∗)

Tηαl
+
αu‖σ̃‖1
αl
√
b

+
ηα2u
2αl
‖L‖1.

C COMPARISON OF CONVERGENCE RATES OF LARS AND SGD

Inspired by the comparison used by (Bernstein et al., 2018) for comparing SIGN SGD with SGD, we
define the following quantities:(

h∑
i=1

‖∇if(xt)‖

)2
=
ψ(∇f(xt))d‖∇f(xt)‖2

h
≥ ψgd‖∇f(xt)‖

2

h

‖L‖21 ≤
ψLd

2‖L‖2∞
h2

‖σ‖21 =
ψσd‖σ‖2

h
.

Then LARS convergence rate can be written in the following manner:

(E[‖∇f(xa)‖)2 ≤ O
(

(f(x1)− f(x∗))L∞
T

ψL
ψ2g

+
‖σ‖2

T

ψ2σ
ψ2g

)
.

If ψL � ψ2g and ψσ � ψ2g then LARS (i.e., gradient is more denser than curvature or stochasticity),
we gain over SGDOtherwise, SGD’s upper bound on convergence rate is better.

14



Published as a conference paper at ICLR 2020

Figure 1: This figure shows N-LAMB and NN-LAMB can achieve a comparable accuracy compared
to LAMB optimizerTheir performances are much better than momentum solverThe result of
momentum optimizer was reported by Goyal et al(2017)For Nadam, we use the learning rate recipe
of (Goyal et al., 2017): (1) 5-epoch warmup to stablize the initial stage; and (2) multiply the learning
rate by 0.1 at 30th, 60th, and 80th epochThe target accuracy is around 0.763 (Goyal et al., 2017).
We also tuned the learning rate of Nadam in {1e-4, 2e-4, ..., 9e-4, 1e-3, 2e-3, ..., 9e-3, 1e-2}.

D N-LAMB: NESTEROV MOMENTUM FOR LAMB

Sutskever et al(2013) report that Nesterov’s accelerated gradient (NAG) proposed by Nesterov (1983)
is conceptually and empirically better than the regular momentum method for convex, non-stochastic
objectivesDozat (2016) incorporated Nesterov’s momentum into Adam optimizer and proposed
the Nadam optimizerSpecifically, only the first moment of Adam was modified and the second
moment of Adam was unchangedThe results on several applications (Word2Vec, Image Recognition,
and LSTM Language Model) showed that Nadam optimizer improves the speed of convergence
and the quality of the learned modelsWe also tried using Nesterov’s momentum to replace the
regular momentum of LAMB optimizer’s first momentIn this way, we got a new algorithm named
as N-LAMB (Nesterov LAMB)The complete algorithm is in Algorithm 3We can also Nesterov’s
momentum to replace the regular momentum of LAMB optimizer’s second momentWe refer to this
algorithm as NN-LAMB (Nesterov’s momentum for both the first moment and the second moment).
The details of NN-LAMB were shown in Algorithm 4.

Dozat (2016) suggested the best performance of Nadam was achieved by β1 = 0.975, β2 = 0.999, and
� = 1e-8We used the same settings for N-LAMB and NN-LAMBWe scaled the batch size to 32K
for ImageNet training with ResNet-50Our experimental results show that N-LAMB and NN-LAMB
can achieve a comparable accuracy compared to LAMB optimizerTheir performances are much
better than momentum solver (Figure 1).

E LAMB WITH LEARNING RATE CORRECTION

There are two operations at each iteration in original Adam optimizer (let us call it adam-correction):

mt = mt/(1− βt1)

vt = vt/(1− βt2)

It has an impact on the learning rate by ηt := ηt∗
√

(1− βt2)/(1− βt1)According to our experimental
results, adam-correction essentially has the same effect as learning rate warmup (see Figure 2)The
warmup function often was implemented in the modern deep learning systemThus, we can remove
adam-correction from the LAMB optimizerWe did not observe any drop in the test or validation
accuracy for BERT and ImageNet training.

15



Published as a conference paper at ICLR 2020

Figure 2: The figure shows that adam-correction has the same effect as learning rate warmupWe
removed adam-correction from the LAMB optimizerWe did not observe any drop in the test or
validation accuracy for BERT and ImageNet training.

Figure 3: We tried different norms in LAMB optimizerHowever, we did not observe a significant
difference in the validation accuracy of ImageNet training with ResNet-50We use L2 norm as the
default.

F LAMB WITH DIFFERENT NORMS

We need to compute the matrix/tensor norm for each layer when we do the parameter updating in
the LAMB optimizerWe tried different norms in LAMB optimizerHowever, we did not observe
a significant difference in the validation accuracy of ImageNet training with ResNet-50In our
experiments, the difference in validation accuracy is less than 0.1 percent (Figure 3)We use L2 norm
as the default.

G REGULAR BATCH SIZES FOR SMALL DATASETS: MNIST AND CIFAR-10.

According to DAWNBench, DavidNet (a custom 9-layer Residual ConvNet) is the fastest model
for CIFAR-10 dataset (as of April 1st, 2019)5The baseline uses the momentum SGD optimizer.
Table 6 and Figure 4 show the test accuracy of CIFAR-10 training with DavidNetThe PyTorch
implementation (momentum SGD optimizer) on GPUs was reported on Standford DAWNBench’s
website, which achieves 94.06% in 24 epochsThe Tensorflow implementation (momentum SGD
optimizer) on TPU achieves a 93.72% accuracy in 24 epochs6We use the implementation of
TensorFlow on TPUsLAMB optimizer is able to achieve 94.08% test accuracy in 24 epochs, which
is better than other adaptive optimizers and momentum SGDEven on the smaller tasks like MNIST
training with LeNet, LAMB is able to achieve a better accuracy than existing solvers (Table 7).

5https://dawn.cs.stanford.edu/benchmark/CIFAR10/train.html
6https://github.com/fenwickslab/dl_tutorials/blob/master/tutorial3_cifar10_davidnet_fix.ipynb

16



Published as a conference paper at ICLR 2020

Figure 4: LAMB is better than the existing solvers (batch size = 512)We make sure all the solvers are
carefully tunedThe learning rate tuning space of Adam, AdamW, Adagrad and LAMB is {0.0001,
0.0002, 0.0004, 0.0006, 0.0008, 0.001, 0.002, 0.004, 0.006, 0.008, 0.01, 0.02, 0.04, 0.06, 0.08, 0.1,
0.2, 0.4, 0.6, 0.8, 1, 2, 4, 6, 8, 10, 15, 20, 25, 30, 35, 40, 45, 50}The momentum optimizer was tuned
by the baseline implementerThe weight decay term of AdamW was tuned by {0.0001, 0.001, 0.01,
0.1, 1.0}.

Table 6: CIFAR-10 training with DavidNet (batch size = 512)All of them run 24 epochs and finish
the training under one minute on one cloud TPUWe make sure all the solvers are carefully tuned.
The learning rate tuning space of Adam, AdamW, Adagrad and LAMB is {0.0001, 0.0002, 0.0004,
0.0006, 0.0008, 0.001, 0.002, 0.004, 0.006, 0.008, 0.01, 0.02, 0.04, 0.06, 0.08, 0.1, 0.2, 0.4, 0.6, 0.8,
1, 2, 4, 6, 8, 10, 15, 20, 25, 30, 35, 40, 45, 50}The momentum optimizer was tuned by the baseline
implementerThe weight decay term of AdamW was tuned by {0.0001, 0.001, 0.01, 0.1, 1.0}.

Optimizer ADAGRAD ADAM ADAMW momentum LAMB

Test Accuracy 0.9074 0.9225 0.9271 0.9372 0.9408

H IMPLEMENTATION DETAILS AND ADDITIONAL RESULTS

There are several hyper-parameters in LAMB optimizerAlthough users do not need to tune them,
we explain them to help users to have a better understandingβ1 is used for decaying the running
average of the gradientβ2 is used for decaying the running average of the square of gradientThe
default setting for other parameters: weight decay rate λ=0.01, β1=0.9, β2=0.999, �=1e-6We did not
tune β1 and β2However, our experiments show that tuning them may get a higher accuracy.

Based on our experience, learning rate is the most important hyper-parameter that affects the learning
efficiency and final accuracyBengio (2012) suggests that it is often the single most important
hyper-parameter and that it always should be tunedThus, to make sure we have a solid baseline, we
carefully tune the learning rate of ADAM, ADAMW, ADAGRAD, and momentum SGD

In our experiments, we found that the validation loss is not reliable for large-batch trainingA lower
validation loss does not necessarily lead to a higher validation accuracy (Figure 5)Thus, we use the
test/val accuracy or F1 score on dev set to evaluate the optimizers.

H.0.1 BERT

Table 8 shows some of the tuning information from BERT training with ADAMW optimizerADAMW
stops scaling at the batch size of 16KThe target F1 score is 90.5LAMB achieves a F1 score of
91.345The table shows the tuning information of ADAMWIn Table 8, we report the best F1 score
we observed from our experiments.

The loss curves of BERT training by LAMB for different batch sizes are shown in Figure 6We
observe that the loss curves are almost identical to each other, which means our optimizer scales well
with the batch size.

The training loss curve of BERT mixed-batch pre-training with LAMB is shown in Figure 7This
figure shows that LAMB can make the training converge smoothly at the batch size of 64K.

Figure 8 shows that we can achieve 76.8% scaling efficiency by scaling the batch size (49.1 times
speedup by 64 times computational resources) and 101.8% scaling efficiency with mixed-batch (65.2
times speedup by 64 times computational resources)

17



Published as a conference paper at ICLR 2020

Table 7: Test Accuracy by MNIST training with LeNet (30 epochs for Batch Size = 1024)The
tuning space of learning rate for all the optimizers is {0.0001, 0.001, 0.01, 0.1}We use the same
learning rate warmup and decay schedule for all of them.

Optimizer Momentum Addgrad ADAM ADAMW LAMB

Average accuracy over 5 runs 0.9933 0.9928 0.9936 0.9941 0.9945

Figure 5: Our experiments show that even the validation loss is not reliable in the large-scale training.
A lower validation loss may lead to a worse accuracyThus, we use the test/val accuracy or F1 score
on dev set to evaluate the optimizers.

H.0.2 IMAGENET

Figures 9 - 14 show the LAMB trust ratio at different iterations for ImageNet training with ResNet-50.
From these figures we can see that these ratios are very different from each other for different layers.
LAMB uses the trust ratio to help the slow learners to train faster.

H.1 BASELINE TUNING DETAILS FOR IMAGENET TRAINING WITH RESNET-50

If you are not interested in the baseline tuning details, please skip this section.

Goyal et al(2017) suggested a proper learning rate warmup and decay scheme may help improve
the ImageNet classification accuracyWe included these techniques in Adam/AdamW/AdaGrad
tuningSpecifically, we use the learning rate recipe of Goyal et al(2017): (1) 5-epoch warmup
to stablize the initial stage; and (2) multiply the learning rate by 0.1 at 30th, 60th, and 80th
epochThe target accuracy is around 76.3% (Goyal et al., 2017)There techniques help to im-
prove the accuracy of Adam/AdamW/AdaGrad to around 73%However, even with these techniques,
Adam/AdamW/AdaGrad stil can not achieve the target validation accuracy.

To make sure our baseline is solid, we carefully tuned the hyper-parametersTable 9 shows the tuning
information of standard AdagradTable 10 shows the tuning information of adding the learning rate
scheme of Goyal et al(2017) to standard AdagradTable 11 shows the tuning information of standard
AdamTable shows the tuning information of adding the learning rate scheme of Goyal et al(2017)
to standard AdamIt is tricky to tune the AdamW optimizer since both the L2 regularization and
weight decay have the effect on the performanceThus we have four tuning sets.

The first tuning set is based on AdamW with default L2 regularizationWe tune the learning rate and
weight decayThe tuning information is in Figures 13, 14, 15, and 16.

The second tuning set is based on AdamW with disabled L2 regularizationWe tune the learning rate
and weight decayThe tuning information is in Figures 17, 18, 19, and 20.

18



Published as a conference paper at ICLR 2020

Table 8: ADAMW stops scaling at the batch size of 16KThe target F1 score is 90.5LAMB achieves
a F1 score of 91.345The table shows the tuning information of ADAMWIn this table, we report the
best F1 score we observed from our experiments.

Solver batch size warmup steps LR last step infomation F1 score on dev set

ADAMW 16K 0.05×31250 0.0001 loss=8.04471, step=28126 diverged
ADAMW 16K 0.05×31250 0.0002 loss=7.89673, step=28126 diverged
ADAMW 16K 0.05×31250 0.0003 loss=8.35102, step=28126 diverged
ADAMW 16K 0.10×31250 0.0001 loss=2.01419, step=31250 86.034
ADAMW 16K 0.10×31250 0.0002 loss=1.04689, step=31250 88.540
ADAMW 16K 0.10×31250 0.0003 loss=8.05845, step=20000 diverged
ADAMW 16K 0.20×31250 0.0001 loss=1.53706, step=31250 85.231
ADAMW 16K 0.20×31250 0.0002 loss=1.15500, step=31250 88.110
ADAMW 16K 0.20×31250 0.0003 loss=1.48798, step=31250 85.653

Figure 6: This figure shows the training loss curve of LAMB optimizerWe just want to use this figure
to show that LAMB can make the training converge smoothlyEven if we scale the batch size to the
extremely large cases, the loss curves are almost identical to each other.

Then we add the learning rate scheme of Goyal et al(2017) to AdamW and refer to it as AdamW+.

The third tuning set is based on AdamW+ with default L2 regularizationWe tune the learning rate
and weight decayThe tuning information is Figure 21 and 22.

The fourth tuning set is based on AdamW+ with disabled L2 regularizationWe tune the learning rate
and weight decayThe tuning information is in Figures 23, 24, 25.

Based on our comprehensive tuning results, we conclude the existing adaptive solvers do not perform
well on ImageNet training or at least it is hard to tune them.

19



Published as a conference paper at ICLR 2020

Figure 7: This figure shows the training loss curve of LAMB optimizerThis figure shows that LAMB
can make the training converge smoothly at the extremely large batch size (e.g64K).

Figure 8: We achieve 76.8% scaling efficiency (49 times speedup by 64 times computational resources)
and 101.8% scaling efficiency with a mixed, scaled batch size (65.2 times speedup by 64 times
computational resources)1024-mixed means the mixed-batch training on 1024 TPUs.

Figure 9: The LAMB trust ratio.

20



Published as a conference paper at ICLR 2020

Figure 10: The LAMB trust ratio.

Figure 11: The LAMB trust ratio.

Figure 12: The LAMB trust ratio.

Figure 13: The LAMB trust ratio.

21



Published as a conference paper at ICLR 2020

Figure 14: The LAMB trust ratio.

Table 9: The accuracy information of tuning default AdaGrad optimizer for ImageNet training with
ResNet-50 (batch size = 16384, 90 epochs, 7038 iterations).

Learning Rate Top-1 Validation Accuracy

0.0001 0.0026855469
0.001 0.015563965
0.002 0.022684732
0.004 0.030924479
0.008 0.04486084
0.010 0.054158527
0.020 0.0758667
0.040 0.1262614
0.080 0.24037679
0.100 0.27357993
0.200 0.458313
0.400 0.553833
0.800 0.54103595
1.000 0.5489095
2.000 0.47680664
4.000 0.5295207
6.000 0.36950684
8.000 0.31081137
10.00 0.30670166
12.00 0.3091024
14.00 0.3227946
16.00 0.0063680015
18.00 0.11287435
20.00 0.21602376
30.00 0.08315023
40.00 0.0132039385
50.00 0.0009969076

22



Published as a conference paper at ICLR 2020

Table 10: The accuracy information of tuning AdaGrad optimizer for ImageNet training with ResNet-
50 (batch size = 16384, 90 epochs, 7038 iterations)We use the learning rate recipe of (Goyal et al.,
2017): (1) 5-epoch warmup to stablize the initial stage; and (2) multiply the learning rate by 0.1 at
30th, 60th, and 80th epochThe target accuracy is around 0.763 (Goyal et al., 2017).

Learning Rate Top-1 Validation Accuracy

0.0001 0.0011189779
0.001 0.00793457
0.002 0.012573242
0.004 0.019022623
0.008 0.027079264
0.010 0.029012045
0.020 0.0421346
0.040 0.06618246
0.080 0.10970052
0.100 0.13429768
0.200 0.26550293
0.400 0.41918945
0.800 0.5519816
1.000 0.58614093
2.000 0.67252606
4.000 0.70306396
6.000 0.709493
8.000 0.7137858
10.00 0.71797687
12.00 0.7187703
14.00 0.72007245
16.00 0.7194214
18.00 0.7149251
20.00 0.71293133
30.00 0.70458984
40.00 0.69085693
50.00 0.67976886

23



Published as a conference paper at ICLR 2020

Table 11: The accuracy information of tuning default Adam optimizer for ImageNet training with
ResNet-50 (batch size = 16384, 90 epochs, 7038 iterations)The target accuracy is around 0.763
(Goyal et al., 2017).

Learning Rate Top-1 Validation Accuracy

0.0001 0.5521
0.0002 0.6089
0.0004 0.6432
0.0006 0.6465
0.0008 0.6479
0.001 0.6604
0.002 0.6408
0.004 0.5687
0.006 0.5165
0.008 0.4812
0.010 0.3673

Table 12: The accuracy information of tuning Adam optimizer for ImageNet training with ResNet-50
(batch size = 16384, 90 epochs, 7038 iterations)We use the learning rate recipe of (Goyal et al.,
2017): (1) 5-epoch warmup to stablize the initial stage; and (2) multiply the learning rate by 0.1 at
30th, 60th, and 80th epochThe target accuracy is around 0.763 (Goyal et al., 2017).

Learning Rate Top-1 Validation Accuracy

0.0001 0.410319
0.0002 0.55263263
0.0004 0.6455485
0.0006 0.6774495
0.0008 0.6996867
0.001 0.71010333
0.002 0.73476154
0.004 0.73286945
0.006 0.72648114
0.008 0.72214764
0.010 0.71466064
0.012 0.7081502
0.014 0.6993001
0.016 0.69108075
0.020 0.67997235
0.040 0.58658856
0.060 0.51090497
0.080 0.45174155
0.100 0.40297446

24



Published as a conference paper at ICLR 2020

Table 13: The accuracy information of tuning default AdamW optimizer for ImageNet training with
ResNet-50 (batch size = 16384, 90 epochs, 7038 iterations)The target accuracy is around 0.763
(Goyal et al., 2017).

learning rate weight decay L2 regularization Top-1 Validation Accuracy

0.0001 0.00001 default (0.01) 0.53312176
0.0002 0.00001 default (0.01) 0.5542806
0.0004 0.00001 default (0.01) 0.48769125
0.0006 0.00001 default (0.01) 0.46317545
0.0008 0.00001 default (0.01) 0.40903726
0.001 0.00001 default (0.01) 0.42401123
0.002 0.00001 default (0.01) 0.33870444
0.004 0.00001 default (0.01) 0.12339274
0.006 0.00001 default (0.01) 0.122924805
0.008 0.00001 default (0.01) 0.08099365
0.010 0.00001 default (0.01) 0.016764322
0.012 0.00001 default (0.01) 0.032714844
0.014 0.00001 default (0.01) 0.018147787
0.016 0.00001 default (0.01) 0.0066731772
0.018 0.00001 default (0.01) 0.010294597
0.020 0.00001 default (0.01) 0.008260091
0.025 0.00001 default (0.01) 0.008870442
0.030 0.00001 default (0.01) 0.0064493814
0.040 0.00001 default (0.01) 0.0018107096
0.050 0.00001 default (0.01) 0.003540039

25



Published as a conference paper at ICLR 2020

Table 14: The accuracy information of tuning default AdamW optimizer for ImageNet training with
ResNet-50 (batch size = 16384, 90 epochs, 7038 iterations)The target accuracy is around 0.763
(Goyal et al., 2017).

learning rate weight decay L2 regularization Top-1 Validation Accuracy

0.0001 0.0001 default (0.01) 0.55489093
0.0002 0.0001 default (0.01) 0.56514484
0.0004 0.0001 default (0.01) 0.4986979
0.0006 0.0001 default (0.01) 0.47595215
0.0008 0.0001 default (0.01) 0.44685873
0.001 0.0001 default (0.01) 0.41029868
0.002 0.0001 default (0.01) 0.2808024
0.004 0.0001 default (0.01) 0.08111572
0.006 0.0001 default (0.01) 0.068115234
0.008 0.0001 default (0.01) 0.057922363
0.010 0.0001 default (0.01) 0.05222575
0.012 0.0001 default (0.01) 0.017313639
0.014 0.0001 default (0.01) 0.029785156
0.016 0.0001 default (0.01) 0.016540527
0.018 0.0001 default (0.01) 0.00575765
0.020 0.0001 default (0.01) 0.0102335615
0.025 0.0001 default (0.01) 0.0060831704
0.030 0.0001 default (0.01) 0.0036417644
0.040 0.0001 default (0.01) 0.0010782877
0.050 0.0001 default (0.01) 0.0037638347

26



Published as a conference paper at ICLR 2020

Table 15: The accuracy information of tuning default AdamW optimizer for ImageNet training with
ResNet-50 (batch size = 16384, 90 epochs, 7038 iterations)The target accuracy is around 0.763
(Goyal et al., 2017).

learning rate weight decay L2 regularization Top-1 Validation Accuracy

0.0001 0.001 default (0.01) 0.21142578
0.0002 0.001 default (0.01) 0.4289144
0.0004 0.001 default (0.01) 0.13537598
0.0006 0.001 default (0.01) 0.33803305
0.0008 0.001 default (0.01) 0.32611084
0.001 0.001 default (0.01) 0.22194417
0.002 0.001 default (0.01) 0.1833903
0.004 0.001 default (0.01) 0.08256022
0.006 0.001 default (0.01) 0.020507812
0.008 0.001 default (0.01) 0.018269857
0.010 0.001 default (0.01) 0.007507324
0.012 0.001 default (0.01) 0.020080566
0.014 0.001 default (0.01) 0.010762532
0.016 0.001 default (0.01) 0.0021362305
0.018 0.001 default (0.01) 0.007954915
0.020 0.001 default (0.01) 0.005859375
0.025 0.001 default (0.01) 0.009724935
0.030 0.001 default (0.01) 0.0019124349
0.040 0.001 default (0.01) 0.00390625
0.050 0.001 default (0.01) 0.0009969076

27



Published as a conference paper at ICLR 2020

Table 16: The accuracy information of tuning default AdamW optimizer for ImageNet training with
ResNet-50 (batch size = 16384, 90 epochs, 7038 iterations)The target accuracy is around 0.763
(Goyal et al., 2017).

learning rate weight decay L2 regularization Top-1 Validation Accuracy

0.0001 0.01 default (0.01) 0.0009765625
0.0002 0.01 default (0.01) 0.0009969076
0.0004 0.01 default (0.01) 0.0010172526
0.0006 0.01 default (0.01) 0.0009358724
0.0008 0.01 default (0.01) 0.0022379558
0.001 0.01 default (0.01) 0.001566569
0.002 0.01 default (0.01) 0.009480794
0.004 0.01 default (0.01) 0.0033569336
0.006 0.01 default (0.01) 0.0029907227
0.008 0.01 default (0.01) 0.0018513998
0.010 0.01 default (0.01) 0.009134929
0.012 0.01 default (0.01) 0.0022176106
0.014 0.01 default (0.01) 0.0040690103
0.016 0.01 default (0.01) 0.0017293295
0.018 0.01 default (0.01) 0.00061035156
0.020 0.01 default (0.01) 0.0022379558
0.025 0.01 default (0.01) 0.0017089844
0.030 0.01 default (0.01) 0.0014241537
0.040 0.01 default (0.01) 0.0020345051
0.050 0.01 default (0.01) 0.0012817383

28



Published as a conference paper at ICLR 2020

Table 17: The accuracy information of tuning default AdamW optimizer for ImageNet training with
ResNet-50 (batch size = 16384, 90 epochs, 7038 iterations)The target accuracy is around 0.763
(Goyal et al., 2017).

learning rate weight decay L2 regularization Top-1 Validation Accuracy

0.0001 0.00001 disable 0.48917642
0.0002 0.00001 disable 0.58152264
0.0004 0.00001 disable 0.63460284
0.0006 0.00001 disable 0.64849854
0.0008 0.00001 disable 0.6598918
0.001 0.00001 disable 0.6662801
0.002 0.00001 disable 0.67266846
0.004 0.00001 disable 0.6692708
0.006 0.00001 disable 0.6573079
0.008 0.00001 disable 0.6639404
0.010 0.00001 disable 0.65230304
0.012 0.00001 disable 0.6505534
0.014 0.00001 disable 0.64990234
0.016 0.00001 disable 0.65323895
0.018 0.00001 disable 0.67026776
0.020 0.00001 disable 0.66086835
0.025 0.00001 disable 0.65425617
0.030 0.00001 disable 0.6476237
0.040 0.00001 disable 0.55478925
0.050 0.00001 disable 0.61869305

29



Published as a conference paper at ICLR 2020

Table 18: The accuracy information of tuning default AdamW optimizer for ImageNet training with
ResNet-50 (batch size = 16384, 90 epochs, 7038 iterations)The target accuracy is around 0.763
(Goyal et al., 2017).

learning rate weight decay L2 regularization Top-1 Validation Accuracy

0.0001 0.0001 disable 0.5033366
0.0002 0.0001 disable 0.5949707
0.0004 0.0001 disable 0.62561035
0.0006 0.0001 disable 0.6545207
0.0008 0.0001 disable 0.66326904
0.001 0.0001 disable 0.6677043
0.002 0.0001 disable 0.67244464
0.004 0.0001 disable 0.6702881
0.006 0.0001 disable 0.66033936
0.008 0.0001 disable 0.66426593
0.010 0.0001 disable 0.66151935
0.012 0.0001 disable 0.6545817
0.014 0.0001 disable 0.65509033
0.016 0.0001 disable 0.6529338
0.018 0.0001 disable 0.65651447
0.020 0.0001 disable 0.65334064
0.025 0.0001 disable 0.655009
0.030 0.0001 disable 0.64552814
0.040 0.0001 disable 0.6425374
0.050 0.0001 disable 0.5988159

30



Published as a conference paper at ICLR 2020

Table 19: The accuracy information of tuning default AdamW optimizer for ImageNet training with
ResNet-50 (batch size = 16384, 90 epochs, 7038 iterations)The target accuracy is around 0.763
(Goyal et al., 2017).

learning rate weight decay L2 regularization Top-1 Validation Accuracy

0.0001 0.001 disable 0.4611206
0.0002 0.001 disable 0.0076293945
0.0004 0.001 disable 0.29233804
0.0006 0.001 disable 0.57295734
0.0008 0.001 disable 0.5574748
0.001 0.001 disable 0.5988566
0.002 0.001 disable 0.586263
0.004 0.001 disable 0.62076825
0.006 0.001 disable 0.61503094
0.008 0.001 disable 0.4697876
0.010 0.001 disable 0.619751
0.012 0.001 disable 0.54243976
0.014 0.001 disable 0.5429077
0.016 0.001 disable 0.55281574
0.018 0.001 disable 0.5819295
0.020 0.001 disable 0.5938924
0.025 0.001 disable 0.541097
0.030 0.001 disable 0.45890298
0.040 0.001 disable 0.56193036
0.050 0.001 disable 0.5279134

31



Published as a conference paper at ICLR 2020

Table 20: The accuracy information of tuning default AdamW optimizer for ImageNet training with
ResNet-50 (batch size = 16384, 90 epochs, 7038 iterations)The target accuracy is around 0.763
(Goyal et al., 2017).

learning rate weight decay L2 regularization Top-1 Validation Accuracy

0.0001 0.01 disable 0.0009969076
0.0002 0.01 disable 0.0008951823
0.0004 0.01 disable 0.00095621747
0.0006 0.01 disable 0.0012817383
0.0008 0.01 disable 0.016886393
0.001 0.01 disable 0.038146973
0.002 0.01 disable 0.0015258789
0.004 0.01 disable 0.0014241537
0.006 0.01 disable 0.081441246
0.008 0.01 disable 0.028116861
0.010 0.01 disable 0.011820476
0.012 0.01 disable 0.08138021
0.014 0.01 disable 0.010111491
0.016 0.01 disable 0.0041910806
0.018 0.01 disable 0.0038248699
0.020 0.01 disable 0.002746582
0.025 0.01 disable 0.011555989
0.030 0.01 disable 0.0065104165
0.040 0.01 disable 0.016438803
0.050 0.01 disable 0.007710775

32



Published as a conference paper at ICLR 2020

Table 21: The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet-
50 (batch size = 16384, 90 epochs, 7038 iterations)We use the learning rate recipe of (Goyal et al.,
2017): (1) 5-epoch warmup to stablize the initial stage; and (2) multiply the learning rate by 0.1 at
30th, 60th, and 80th epochThe target accuracy is around 0.763 (Goyal et al., 2017).

learning rate weight decay L2 regularization Top-1 Validation Accuracy

0.0001 0.01 default (0.01) 0.0009969076
0.0002 0.01 default (0.01) 0.0009969076
0.0004 0.01 default (0.01) 0.0009969076
0.0006 0.01 default (0.01) 0.0009358724
0.0008 0.01 default (0.01) 0.0009969076
0.001 0.01 default (0.01) 0.0009765625
0.002 0.01 default (0.01) 0.0010172526
0.004 0.01 default (0.01) 0.0010172526
0.006 0.01 default (0.01) 0.0010172526
0.008 0.01 default (0.01) 0.0010172526
0.0001 0.001 default (0.01) 0.0010172526
0.0002 0.001 default (0.01) 0.0010172526
0.0004 0.001 default (0.01) 0.0010172526
0.0006 0.001 default (0.01) 0.0009969076
0.0008 0.001 default (0.01) 0.0010172526
0.001 0.001 default (0.01) 0.0010172526
0.002 0.001 default (0.01) 0.0010172526
0.004 0.001 default (0.01) 0.0038452148
0.006 0.001 default (0.01) 0.011881511
0.008 0.001 default (0.01) 0.0061442056

33



Published as a conference paper at ICLR 2020

Table 22: The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet-
50 (batch size = 16384, 90 epochs, 7038 iterations)We use the learning rate recipe of (Goyal et al.,
2017): (1) 5-epoch warmup to stablize the initial stage; and (2) multiply the learning rate by 0.1 at
30th, 60th, and 80th epochThe target accuracy is around 0.763 (Goyal et al., 2017).

learning rate weight decay L2 regularization Top-1 Validation Accuracy

0.0001 0.0001 default (0.01) 0.3665975
0.0002 0.0001 default (0.01) 0.5315755
0.0004 0.0001 default (0.01) 0.6369222
0.0006 0.0001 default (0.01) 0.6760457
0.0008 0.0001 default (0.01) 0.69557697
0.001 0.0001 default (0.01) 0.7076009
0.002 0.0001 default (0.01) 0.73065186
0.004 0.0001 default (0.01) 0.72806805
0.006 0.0001 default (0.01) 0.72161865
0.008 0.0001 default (0.01) 0.71816
0.0001 0.00001 default (0.01) 0.49804688
0.0002 0.00001 default (0.01) 0.6287028
0.0004 0.00001 default (0.01) 0.6773885
0.0006 0.00001 default (0.01) 0.67348224
0.0008 0.00001 default (0.01) 0.6622111
0.001 0.00001 default (0.01) 0.6468709
0.002 0.00001 default (0.01) 0.5846761
0.004 0.00001 default (0.01) 0.4868978
0.006 0.00001 default (0.01) 0.34969077
0.008 0.00001 default (0.01) 0.31193033

34



Published as a conference paper at ICLR 2020

Table 23: The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet-
50 (batch size = 16384, 90 epochs, 7038 iterations)We use the learning rate recipe of (Goyal et al.,
2017): (1) 5-epoch warmup to stablize the initial stage; and (2) multiply the learning rate by 0.1 at
30th, 60th, and 80th epochThe target accuracy is around 0.763 (Goyal et al., 2017).

learning rate weight decay L2 regularization Top-1 Validation Accuracy

0.0001 0.01 disable 0.0010172526
0.0002 0.01 disable 0.0009765625
0.0004 0.01 disable 0.0010172526
0.0006 0.01 disable 0.0009969076
0.0008 0.01 disable 0.0010172526
0.001 0.01 disable 0.0009765625
0.002 0.01 disable 0.0009969076
0.004 0.01 disable 0.0009969076
0.006 0.01 disable 0.0009765625
0.008 0.01 disable 0.0010172526
0.0001 0.001 disable 0.0009765625
0.0002 0.001 disable 0.0010172526
0.0004 0.001 disable 0.0010172526
0.0006 0.001 disable 0.0010172526
0.0008 0.001 disable 0.0010172526
0.001 0.001 disable 0.0009969076
0.002 0.001 disable 0.0010579427
0.004 0.001 disable 0.0016886393
0.006 0.001 disable 0.019714355
0.008 0.001 disable 0.1329956

35



Published as a conference paper at ICLR 2020

Table 24: The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet-
50 (batch size = 16384, 90 epochs, 7038 iterations)We use the learning rate recipe of (Goyal et al.,
2017): (1) 5-epoch warmup to stablize the initial stage; and (2) multiply the learning rate by 0.1 at
30th, 60th, and 80th epochThe target accuracy is around 0.763 (Goyal et al., 2017).

learning rate weight decay L2 regularization Top-1 Validation Accuracy

0.0001 0.0001 disable 0.28515625
0.0002 0.0001 disable 0.44055176
0.0004 0.0001 disable 0.56815594
0.0006 0.0001 disable 0.6234741
0.0008 0.0001 disable 0.6530762
0.001 0.0001 disable 0.6695964
0.002 0.0001 disable 0.70048016
0.004 0.0001 disable 0.71698
0.006 0.0001 disable 0.72021484
0.008 0.0001 disable 0.7223918
0.010 0.0001 disable 0.72017413
0.012 0.0001 disable 0.72058105
0.014 0.0001 disable 0.7188924
0.016 0.0001 disable 0.71695966
0.018 0.0001 disable 0.7154134
0.020 0.0001 disable 0.71358234
0.025 0.0001 disable 0.7145386
0.030 0.0001 disable 0.7114258
0.040 0.0001 disable 0.7066447
0.050 0.0001 disable 0.70284015

36



Published as a conference paper at ICLR 2020

Table 25: The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet-
50 (batch size = 16384, 90 epochs, 7038 iterations)We use the learning rate recipe of (Goyal et al.,
2017): (1) 5-epoch warmup to stablize the initial stage; and (2) multiply the learning rate by 0.1 at
30th, 60th, and 80th epochThe target accuracy is around 0.763 (Goyal et al., 2017).

learning rate weight decay L2 regularization Top-1 Validation Accuracy

0.0001 0.00001 disable 0.31247965
0.0002 0.00001 disable 0.4534912
0.0004 0.00001 disable 0.57765704
0.0006 0.00001 disable 0.6277669
0.0008 0.00001 disable 0.65321857
0.001 0.00001 disable 0.6682129
0.002 0.00001 disable 0.69938153
0.004 0.00001 disable 0.7095947
0.006 0.00001 disable 0.710612
0.008 0.00001 disable 0.70857745
0.010 0.00001 disable 0.7094116
0.012 0.00001 disable 0.70717365
0.014 0.00001 disable 0.7109375
0.016 0.00001 disable 0.7058309
0.018 0.00001 disable 0.7052409
0.020 0.00001 disable 0.7064412
0.025 0.00001 disable 0.7035319
0.030 0.00001 disable 0.6994629
0.040 0.00001 disable 0.6972656
0.050 0.00001 disable 0.6971232

37


	Introduction
	Related Work

	Preliminaries
	Algorithms
	Lars Algorithm
	Lamb Algorithm

	Experiments
	Bert Training
	ImageNet Training with ResNet-50.
	Hyperparameters for scaling the batch size

	Conclusion
	Acknowledgement
	Proof of Theorem 2
	Proof of Theorem 3
	Comparison of Convergence Rates of Lars and Sgd
	N-LAMB: Nesterov Momentum for LAMB
	LAMB with learning rate correction
	LAMB with different norms
	Regular Batch Sizes for Small Datasets: MNIST and CIFAR-10.
	Implementation Details and Additional Results
	BERT
	ImageNet

	Baseline tuning details for ImageNet training with ResNet-50


