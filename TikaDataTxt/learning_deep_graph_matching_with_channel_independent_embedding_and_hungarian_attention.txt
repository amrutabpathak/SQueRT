




















































Published as a conference paper at ICLR 2020

LEARNING DEEP GRAPH MATCHING VIA CHANNEL-
INDEPENDENT EMBEDDING AND HUNGARIAN ATTEN-
TION

Tianshu Yu†, Runzhong Wang‡, Junchi Yan‡, Baoxin Li†
†Arizona State University
‡Shanghai Jiao Tong University
{tianshuy,baoxin.li}@asu.edu
{runzhong.wang,yanjunchi}@sjtu.edu.cn

ABSTRACT

Graph matching aims to establishing node-wise correspondence between two
graphs, which is a classic combinatorial problem and in general NP-completeUn-
til very recently, deep graph matching methods start to resort to deep networks to
achieve unprecedented matching accuracyAlong this direction, this paper makes
two complementary contributions which can also be reused as plugin in existing
works: i) a novel node and edge embedding strategy which stimulates the multi-
head strategy in attention models and allows the information in each channel to
be merged independentlyIn contrast, only node embedding is accounted in pre-
vious works; ii) a general masking mechanism over the loss function is devised to
improve the smoothness of objective learning for graph matchingUsing Hungar-
ian algorithm, it dynamically constructs a structured and sparsely connected layer,
taking into account the most contributing matching pairs as hard attentionOur
approach performs competitively, and can also improve state-of-the-art methods
as plugin, regarding with matching accuracy on three public benchmarks.

1 INTRODUCTION

Without loss of generality, we consider the bijection problem for graph matching: given graph G1
and G2 of equal size n, graph matching seeks to find the one-vs-one node correspondence1:

max
x

x>Kx s.tPx = 1 (1)

where x = vec(X) ∈ {0, 1}n2 which is the column-wise vectorized form of the permutation ma-
trix X that encodes the node-to-node correspondence between two graphs, and K ∈ Rn

2×n2
+ is

the so-called affinity matrix2, respectivelyNote P is a selection matrix encoding the one-to-one
correspondence constraintThis problem is called Lawler’s QAP (Lawler, 1963) and has attracted
enormous attention for its generally NP-complete (Hartmanis, 1982) challenge, as well as a wide
spectrum of applications in computer vision, graphics, machine learning and operational research
etcIn particular, Koopmans-Beckmann’s QAP (Loiola et al., 2007) with objective tr(X>F1XF2)
is a special case of Eq(1), which can be converted to Lawler’s QAP by K = F2⊗F1 and Fi refers
to the weighted adjacency matrixA series of solvers haven been developed to solve graph match-
ing problem (Leordeanu & Hebert, 2005; Cho et al., 2010; Bernard et al., 2018; Yan et al., 2015;
Yu et al., 2018)All these methods are based on deterministic optimization, which are conditioned
with pre-defined affinity matrix and no learning paradigm is involvedThis fact greatly limits the
performance and broad application w.r.tdifferent problem settings considering its NP-hard nature.

Recently, the seminal work namely deep graph matching (DGM) (Zanfir & Sminchisescu, 2018) is
proposed to exploit the high capacity of deep networks for graph matching, which achieves state-
of-the-art performanceThis is in contrast to some early works which incorporate learning strategy

1We assume graphs are of equal size for narrative simplicityOne can easily handle unbalanced graph size
by adding dummy nodes as a common protocol in graph matching literature (Cho et al., 2010).

2Aia:jb typically encodes the affinity between pair (i, j) and (a, b) where node i, j ∈ G1 and a, b ∈ G2.

1



Published as a conference paper at ICLR 2020

separately in local stages (Caetano et al., 2009; Cho et al., 2013)On the other hand, Graph Convolu-
tional Networks (GCN) (Kipf & Welling, 2017) brings about new capability on tasks over graph-like
data, as it naturally integrates the intrinsic graph structure in a general updating rule:

H(l+1) = σ
(
ÂH(l)W(l)

)
(2)

where Â is the normalized connectivity matrixH(l) and W(l) are the features and weights at layer
l, respectivelyNode embedding is updated by aggregation from 1-neighboring nodes, which is akin
to the convolution operator in CNNBy taking advantages of both DGM and GCN, Wang et al.
(2019) and Zhang & Lee (2019) incorporate permutation loss instead of displacement loss in (Zanfir
& Sminchisescu, 2018), with notable improvement across both synthetic and real data.

Note that Eq(1) involves both node and edge information, which exactly correspond to the diag-
onal and off-diagonal elements in K, respectivelyEdges can carry informative multi-dimensional
attributes (namely weights) which are fundamental to graph matchingHowever existing embed-
ding based graph matching methods (Wang et al., 2019; Xu et al., 2019) are focused on the explicit
modeling of node level features, whereby the edges are only used as topological node connection for
message passing in GCNBesides, edge attributes are neither well modeled in the embedding-free
model (Zanfir & Sminchisescu, 2018) since the edge information is derived from the concatena-
tion of node featuresTo our best knowledge, there is no deep graph matching method explicitly
incorporating edge attributesIn contrast, edge attributes e.glength and orientation are widely
used in traditional graph matching models (Cho et al., 2010; Yan et al., 2015; Yu et al., 2018) for
constructing the affinity matrix KSuch a gap shall be filled in the deep graph matching pipeline.

Another important consideration refers to the design of loss functionThere are mainly two forms in
existing deep graph matching works: i) displacement loss (Zanfir & Sminchisescu, 2018) similar to
the use in optical flow estimation (Ren et al., 2017); ii) the so-called permutation loss (Wang et al.,
2019) involving iterative Sinkhorn procedure followed by a cross-entropy lossResults in (Wang
et al., 2019) show the latter is an effective improvement against the former regression based loss.
However, we argue that the continuous Sinkhorn procedure (in training stage) is yet an unnatural
approximation to Hungarian sampling (in testing stage) for discretizationIf the network is equipped
with a continuous loss function (e.gcross-entropy), we argue that the training process will make a
great “meaningless effort” to enforce some network output digits of the final matching matrix into
binary and neglect the resting digits which might have notable impact on accuracy.

This paper strikes an endeavor on the above two gaps and makes the following main contributions:

i) We propose a new approach for edge embedding via channel-wise operation, namely channel-
independent embedding (CIE)The hope is to effectively explore the edge attribute and simulate
the multi-head strategy in attention models (Veličković et al., 2018) by decoupling the calculations
parallel and orthogonal to channel directionIn fact, edge attribute information has not been consid-
ered in existing embedding based graph matching methods (Wang et al., 2019; Xu et al., 2019).

ii) We devise a new mechanism to adjust the loss function based on the Hungarian method which is
widely used for linear assignment problem, as termed by Hungarian attentionIt resorts to dynami-
cally generating sparse matching mask according to Hungarian sampling during training, rather than
approximating Hungarian sampling with a differentiable functionAs such, the Hungarian attention
introduces higher smoothness against traditional loss functions to ease the training.

iii) The empirical results on three public benchmarks shows that the two proposed techniques are
orthogonal and beneficial to existing techniquesSpecifically, on the one hand, our CIE module
can effectively boost the accuracy by exploring the edge attributes which otherwise are not consid-
ered in state-of-the-art deep graph matching methods; on the other hand, our Hungarian attention
mechanism also shows generality and it is complementary to existing graph matching loss.

2 RELATED WORKS

Graph embeddingTo handle graph-like data, early works adopt recursive neural networks (RNNs)
treating input as directed acyclic graphs (Sperduti & Starita, 1997; Frasconi et al., 1998)Gori et al.
(2005); Scarselli et al(2008) generalized early models to graph neural networks (GNNs) so as to be
directly applied on cyclic, directed or undirected graphsLi et al(2016) further improved this line

2



Published as a conference paper at ICLR 2020

of model by replacing standard RNNs with gated recurrent units (GRUs) (Cho et al., 2013)Inspired
by the great success of convolutional neural networks (CNNs) (Simonyan & Zisserman, 2014; He
et al., 2016), researchers have made tremendous effort on applying convolution operator to graphs
(Bruna et al., 2014; Kipf & Welling, 2017; Gong & Cheng, 2019)Bruna et al(2014) defined
a convolution operator in Fourier domain which is obtained by performing eigen-decomposition
on graph LaplacianHowever, such convolution will affect the whole spatial domain once taking
inverse Fourier transformationThis method was improved by Chebyshev expansion to approximate
filters (Defferrard et al., 2016)Kipf & Welling (2017) propose a graph convolutional operator
over 1-neighbor nodes derived from graph spectral theory, which is invariant to node permutation
and achieved significant performance on semi-supervised learning tasksThere are series of works
following GCN, such as GraphSAGE (Hamilton et al., 2017), GAT (Veličković et al., 2018) and
MPNN (Gilmer et al., 2017)Refer to (Cai et al., 2018) for a more comprehensive survey.

While the aforementioned models are focused on learning node state/embedding, a parallel line of
work seek to learn edge embedding by taking into account the information carried on edges (Li et al.,
2016; Gilmer et al., 2017; Gong & Cheng, 2019)Edges are intrinsic portion of graphs, and thus
edge embedding can be essential to reveal the relation among nodesGilmer et al(2017) introduce
a general embedding network incorporating edge information and node-edge information merging,
and a serious of works fall into this framework e.gGated GNN (Li et al., 2016), Tensor GNN
(Schütt et al., 2017) and EGNN (Gong & Cheng, 2019)An improved version is devised in Chen
et al(2019) by interpreting this framework as maximizing mutual information across layers.

Loss for combinatorial learningFor the relatively easy linear assignment problem, it has been
known that Sinkhorn algorithm (Sinkhorn, 1964) is the approximate and differentiable version of
Hungarian algorithm (Mena et al., 2017)The Sinkhorn Network (Adams & Zemel, 2011) is de-
veloped given known assignment cost, whereby doubly-stochastic regulation is performed on input
non-negative square matrixPatrini et al(2018) devise the Sinkhorn AutoEncoder to minimize
Wasserstein distance, and Emami & Ranka (2018) propose to learning a linear assignment solver
via reinforcement learningFor permutation prediction, DeepPermNet (Santa Cruz et al., 2018)
adopts the Sinkhorn layer on top of a deep convolutional networkHowever this method cannot be
directly applied for graph matching as it is not invariant to input permutations which is conditioned
on a predefined node permutation as referenceIn particular, existing supervised methods on combi-
natorial learning are generally cross-entropy-basedPointer Net (Vinyals et al., 2015) incorporates
cross-entropy loss on learning heuristics for combinatorial problemsMilan et al(2017) propose an
objective-based loss, where the gradients are only updated if the objective improves after update.

Learning for graph matchingThe early effort (Caetano et al., 2009) aims to incorporate learning
to graph matchingThe key is to learn a more effective affinity function with given correspon-
dence as supervisionWhile the ability by only learning affinity is limited, Cho et al(2013) pro-
pose a matching function learning paradigm using histogram-based attributes with Structured-SVM
(Tsochantaridis et al., 2005)A recent work (Zanfir & Sminchisescu, 2018) is a breakthrough to
introduce deep learning paradigm into graph matching task, which utilizes a neural network to learn
the affinity functionThe learning procedure is explicitly derived from the factorization of affinity
matrix (Zhou & De la Torre, 2012), which makes the interpretation of the network behavior possible.
However, the displacement loss in (Zanfir & Sminchisescu, 2018) measures the pixel-wise transla-
tion which is similar to optical-flow (Dosovitskiy et al., 2015), being essentially a regression task
instead of combinaotiral optimizationSeeing this limitation, Wang et al(2019) employ element-
wise binary cross-entropy, termed as permutation lossThis loss has proved capable of capturing
the combinatorial nature rather than pixel offset, and achieves improvement over displacement loss.
Node embedding is also used in (Wang et al., 2019) to explore the structure information.

3 THE PROPOSED LEARNING APPROACH FOR GRAPH MATCHING

3.1 APPROACH OVERVIEW

An overall structure of our approach is illustrated in Fig1In line with (Wang et al., 2019), we em-
ploy VGG16 (Simonyan & Zisserman, 2014) to extract features from input images and bi-linearly
interpolate the features at key points (provided by datasets)We concatenate lower-level (Relu4 2)
and higher-level (Relu5 1) features to incorporate local and contextual informationFor an image
with k key points, the feature is denoted as H ∈ Rk×d, where d is the feature dimensionUnless

3



Published as a conference paper at ICLR 2020

CNN

Gaussian	
kernel

im
ag
e	
1 initial	node	embedding

initial	edge	embedding

CIE node	embedding

edge	embedding

cross	
graph

node	embedding

edge	embedding

node	embedding

edge	embedding

CIE

CNN

Gaussian	
kernel

initial	node	embedding

initial	edge	embedding

CIE node	embedding

edge	embedding

cross	
graph

node	embedding

edge	embedding

CIE node	embedding

edge	embedding

af
fin
ity

af
fin
ity

im
ag
e	
2

similarity	matrix doubly-stochastic	matrix

Sinkhorn Hungarian	
attention

ground	truth	matching

lo
ss

Figure 1: Architecture overview of the proposed deep graph matching networks that consist of the
proposed channel-independent embedding and Hungarian attention layer over the loss function.
otherwise specified, the adjacency matrix A ∈ Rk×k is consequentially constructed via Delaunay
triangulation (Delaunay et al., 1934), which is a widely adopted strategy to produce sparsely con-
nected graphTo introduce more rich edge information, we also generate k×k m-dimensional edge
features E ∈ Rm×k×kE can be initialized with some basic edge information (e.glength and
angle and other attributes) or a commutative function Eij = p(Hi,Hj) = p(Hj ,Hi) ∈ Rm, where
Hi refers to the feature of node iNote for directed graph, the commutative property is not required.

The features H and E, together with the adjacency A, are then fed into GNN modulePairs of
features are processed in a Siamese fashion (Bromley et al., 1994)Standard GCN’s message passing
rule simply updates node embedding as shown in Eq(2)In contrast, each GNN layer in our model
computes a new pair of node and edge embeddings simultaneously:

H(l+1) = fi(H
(l),E(l),A;W l0), E

(l+1) = g(H(l),E(l),A;W l1) (3)
where W l0 and W

l
1 are the learnable parameters at layer lThe edge information is essential to

provide structural feature enhancing graph matchingWe initialize H(0) = H and E(0) = E in
our settingWe will discuss the details of functions f and g in Sec3.2Following state-of-the-art
work (Wang et al., 2019), we also compute the cross-graph affinity followed by a column/row-wise
softmax activation and a Sinkhorn layer (Adams & Zemel, 2011):

Mij = exp
(
τH>(1)iΛH(2)j

)
, S = Sinkhorn(M) (4)

Note here M ∈ Rk×k is the node-level similarity matrix encoding similarity between two graphs,
differing from the edege-level affinity matrix K in Eq1τ is the weighting parameter of similarity,
Λ contains learnable parameters and H(1)i is the node i’s embedding from graph G1The output
S ∈ [0, 1]k×k,S1 = 1,S>1 = 1 is a so-called doubly-stochastic matrixHere Sinkhorn(·) denotes
the following update iteratively to project M into doubly stochastic polygon:

M(t+1) = M(t) − 1
n

M(t)11> − 1
n

11>M(t) +
1

n2
11>M(t)11> − 1

n
11> (5)

The Sinkhorn layer is shown to be an approximation of Hungarian algorithm which produces discrete
matching output (Kuhn, 1955)As there are only matrix multiplication and normalization operators
involved in Sinkhorn layer, it is differentiableIn practice, Eq(5) converges rapidly within 10
iterations for decades of nodesLess iterations involved, more precise back-propagated gradients
can be achievedWe employ a cross-graph node embedding strategy following (Wang et al., 2019):

H
(l)
(1) = fc

(
cat(H

(l)
(1),SH

(l)
(2))
)
, H

(l)
(2) = fc

(
cat(H

(l)
(2),S

>H
(l)
(2))
)

(6)

where fc is a network and cat(·, ·) is the concatenation operatorH(i) is the node feature of graph i.
This procedure seeks to merge similar features from another graph into the node feature in current
graphIt is similar to the feature transfer strategy in (Aberman et al., 2018) for sparse correspon-
dence, which employs a feature merging method analogous to style transfer (Li et al., 2017).

As Sinkhorn layer does not necessarily output binary digits, we employ Hungarian algorithm (Kuhn,
1955) to discretize matching output S in testingThe testing differs from the training due to the
Hungarian discretizationWe introduce a novel attention-like mechanism termed as Hungarian
attention, along with existing loss functions (will be detailed in Sec3.3)The final training loss is
as follows, where SG andH correspond to binary true matching and Hungarian attention loss.

minH(S,SG) (7)

4



Published as a conference paper at ICLR 2020

𝐄(#)

𝐇(#)

Linear

Linear

Γ'

Γ(

𝐄(#)*)

𝐇(#)*)

𝑚×𝑘×𝑘

𝑚×𝑘

𝑚 channels

𝜎

𝜎

Figure 2: Illustration of the proposed CIE layer for embedding based deep graph matchingThe
operation “Linear” refers to the linear mapping, e.gH(l)w →W(l)2 H

(l)
w in Eq (9).

3.2 CHANNEL-INDEPENDENT EMBEDDING

We detail the updating rule in Eq(3)We propose a method to merge edge features into node
features and perform matching on nodesEdge information acts an important role in modeling
relational data, whereby such relation can be complex thus should be encoded with high-dimensional
featureTo this end, Gilmer et al(2017) introduce a general embedding layer, which takes node and
edge features and outputs a message to node v, then fuses the message and the current embedding:

m(l)v = σ

( ∑
w∈Nv

ft (Evw) H
(l)
w + W

(l)H(l)

)
, H(t+1)v = ut

(
H(t)v ,m

(l)
v

)
(8)

where Evw is the feature corresponding to edge (v, w)In the realization of Eq(8) (Gilmer et al.,
2017), m(l)v and H

(l)
v are fed to GRU (Cho et al., 2014) as a sequential inputThere are several

variants which take into account specific tasks (Li et al., 2016; Schütt et al., 2017; Chen et al., 2019).
Among these, Li et al(2016) generates a transformation matrix for each edge and Schütt et al.
(2017) resorts to merge embedding via fully connected neural networksWhile edge-wise merging
is straightforward, the representation ability is also limitedOn the other hand, fully connected
merging strategy will result in high computational cost and instability for back-propagationTo
address these issues, we propose to merge embedding in a channel-wise fashion, which is termed as
Channel-Independent Embedding (CIE)Concretely, the updating rule is written as:

H(l+1)v = σ

 ∑
w∈Nv

ΓN

(
W

(l)
1 E

(l)
vw ◦W

(l)
2 H

(l)
w

)
︸ ︷︷ ︸

channel-wise operator/function

+ σ (W(l)0 H(l)v ) (9)
E(l+1)vw = σ

(
W

(l)
1 E

(l)
vw

)
(10)

where ΓN(· ◦ ·) is a channel-wise operator/function (above the underbrace), and it performs calcu-
lation per-channel and the output channel dimension is the same as inputThe second σ(·) term is
the message a node passes to itself, which is necessary in keeping the node information contextually
consistent through each CIE layerIn this fashion, CIE is thus a procedure to aggregate node and
edge embedding in each channel independently, which requires the dimensions of node (W(l)2 H

(l)
w )

and edge (W(l)1 E
(l)
vw) representations to be equalSimilarly, we also propose an corresponding up-

dating rule of edge embedding by substituting Eq(10):

E(l+1)vw = σ
(

ΓE

(
W

(l)
1 E

(l)
vw ◦ h

(
H(l)v ,H

(l)
w

)))
+ σ

(
W

(l)
1 E

(l)
vw

)
(11)

where h(·, ·) is commutative h(X,Y) = h(Y,X)Eq(11) is supplementary to Eq(9).
Fig2 shows a schematic diagram of CIE layer, which is motivated from two perspectivesFirst,
CIE is motivated by counterparts in CNN (Qiu et al., 2017; Tran et al., 2018) which decouple a 3D
convolution into two 2D ones (e.ga 3× 3× 3 convolution can be decomposed to a 1× 3× 3 and
a 3 × 1 × 1 convolutions)In this sense, the number of parameters can be significantly reduced.
As shown in Fig2, node and edge embedding is first manipulated along the channel direction via
a linear layer, then operated via ΓN and ΓE orthogonal to the channel directionInstead of merging
node and edge as a whole, CIE layer decouples it into two operationsSecond, CIE is also motivated
by the triumph of multi-head structure (e.ggraph attention (Veličković et al., 2018)), the key of

5



Published as a conference paper at ICLR 2020

0 1 2 3 4 5

a

b

c

d

e

f

1.4 1.7 3.8 3.8 0.8 0.8

1.3 3.6 0.9 0.8 2.1 6.7

2.5 0.4 4.3 1.2 0.7 1.0

0.4 1.8 1.9 1.1 2.7 3.8

1.1 1.5 0.5 0.4 0.6 3.6

0.8 0.7 2.7 2.1 5.5 1.0

similarity matrix

0 1 2 3 4 5

a

b

c

d

e

f

0.2 0.1 0.2 0.4 0.1 0.0

0.1 0.3 0.1 0.1 0.1 0.3

0.3 0.0 0.3 0.1 0.1 0.1

0.1 0.2 0.1 0.1 0.2 0.2

0.2 0.2 0.1 0.1 0.1 0.3

0.1 0.1 0.2 0.2 0.4 0.0

doubly-stochastic matrix

0 1 2 3 4 5

a

b

c

d

e

f

0.0 0.0 0.0 1.0 0.0 0.0

0.0 1.0 0.0 0.0 0.0 0.0

1.0 0.0 0.0 0.0 0.0 0.0

0.0 0.0 1.0 0.0 0.0 0.0

0.0 0.0 0.0 0.0 0.0 1.0

0.0 0.0 0.0 0.0 1.0 0.0

permutation matrix

0 1 2 3 4 5

a

b

c

d

e

f

0.0 0.0 0.0 1.0 0.0 0.0

1.0 0.0 0.0 0.0 0.0 0.0

0.0 1.0 0.0 0.0 0.0 0.0

0.0 0.0 1.0 0.0 0.0 0.0

0.0 0.0 0.0 0.0 0.0 1.0

0.0 0.0 0.0 0.0 1.0 0.0

ground truth matrix

0 1 2 3 4 5

a

b

c

d

e

f

0.0 0.0 0.0 1.0 0.0 0.0

1.0 1.0 0.0 0.0 0.0 0.0

1.0 1.0 0.0 0.0 0.0 0.0

0.0 0.0 1.0 0.0 0.0 0.0

0.0 0.0 0.0 0.0 0.0 1.0

0.0 0.0 0.0 0.0 1.0 0.0

attention activation

0 1 2 3 4 5

a

b

c

d

e

f

0.0 0.0 0.0 0.4 0.0 0.0

0.1 0.3 0.0 0.0 0.0 0.0

0.3 0.0 0.0 0.0 0.0 0.0

0.0 0.0 0.1 0.0 0.0 0.0

0.0 0.0 0.0 0.0 0.0 0.3

0.0 0.0 0.0 0.0 0.4 0.0

output matrix

Sinkhorn Hungarian

Attention Pooling Hungarian Attention

Figure 3: A working example illustrating of our proposed Hungarian attention pipeline starting from
similarity matrixSinkhorn algorithm solves similarity matrix into a doubly-stochastic matrix in a
differentiable wayA discrete permutation matrix is further obtained via Hungarian algorithmOur
proposed Hungarian attention, taking the ground truth matching matrix into account, focuses on
the “important” digits either labeled true or being mis-classifiedThe output matrix is obtained by
attention pooling from doubly-stochastic matrix, where we compute a loss on it.
which is to conduct unit calculation multiple times and concatenate the resultsMulti-head proved
effective to further improve the performance since it is capable of capturing information at different
scales or aspectsTraditional neural node-edge message passing algorithms (Gilmer et al., 2017; Li
et al., 2016; Schütt et al., 2017) typically produce a unified transformation matrix for all the channels.
On the other hand, in Eq(9) (10) and (11), one can consider that the basic operator in each channel
is repeated d times in a multi-head fashionThe cross-channel information exchange, as signified in
Eq(9) (10) and (11), only happens before the channel-wise operator (i.eweights W(l)i as the cross-
channel matrices)The main difference between CIE and traditional multi-head approaches e.g.
(Veličković et al., 2018) is that CIE assumes the channel-independence of two embedded features
(node and edge), while traditional ones only take one input under head-independence assumption.

3.3 HUNGARIAN ATTENTION MECHANISM

For most graph matching algorithms, the output is in a continuous domainThough there are some
alternatives that deliver discrete solutions by adding more constraints or introducing numerical con-
tinuation (Zhou & De la Torre, 2012; Yu et al., 2018), the main line of methods is to incorporate a
sampling procedure (e.gwinner-take-all and Hungarian)Among them, the Hungarian algorithm
(Kuhn, 1955) is a widely adopted, for its efficiency and theoretical optimality.

However, the Hungarian algorithm incurs a gap between training (loss function) and testing stages
(Hungarian sampling)We compare the permutation loss (Wang et al., 2019) for concrete analysis:

LCE = −
∑

i∈G1,j∈G2

(
SGij log Sij +

(
1− SGij

)
log (1− Sij)

)
(12)

Note Eq(12) is an element-wise version of binary cross-entropyDuring training, this loss tends
to drag the digits in S into binary format and is likely trapped to local optimaThis is because this
loss will back-propagate the gradients of training samples that are easy to learn in the early training
stageIn later iterations, this loss is then hard to give up the digits that have become binaryIn fact,
the similar phenomenon is also investigated in the focal loss (Lin et al., 2017) in comparison to the
traditional cross-entropy lossDuring the testing stage, however, the Hungarian algorithm has no
preference on the case if digits in S are close to 0− 1 or notIt binarizes S anywayTherefore, the
effort of Eq(12) to drag S into binary might be meaningless.

This issue is likely to be solved by integrating Hungarian algorithm during the training stageUn-
fortunately, Hungarian algorithm is undifferentiable and its behavior is difficult to mimic with a
differentiable counterpartIn this paper, instead of finding a continuous approximation of Hungar-
ian algorithm, we treat it as a black box and dynamically generate network structure (sparse link)

6



Published as a conference paper at ICLR 2020

Table 1: Accuracy on Pascal VOC (best in bold)White and gray background refer to results on test-
ing and training, respectivelyCompared methods include GMN (Zanfir & Sminchisescu, 2018),
GAT (Veličković et al., 2018), EPN (Gong & Cheng, 2019), PCA/PIA (Wang et al., 2019).

method aero bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv Ave
GMN-D 31.9 47.2 51.9 40.8 68.7 72.2 53.6 52.8 34.6 48.6 72.3 47.7 54.8 51.0 38.6 75.1 49.5 45.0 83.0 86.3 55.3
GMN-P 31.1 46.2 58.2 45.9 70.6 76.4 61.2 61.7 35.5 53.7 58.9 57.5 56.9 49.3 34.1 77.5 57.1 53.6 83.2 88.6 57.9
GAT-P 46.4 60.5 60.9 51.8 79.0 70.9 62.7 70.1 39.7 63.9 66.2 63.8 65.8 62.8 39.5 82.0 66.9 50.1 78.5 90.3 63.6
GAT-H 47.2 61.6 63.2 53.3 79.7 70.1 65.3 70.5 38.4 64.7 62.9 65.1 66.2 62.5 41.1 78.8 67.1 61.6 81.4 91.0 64.6
EPN-P 47.6 65.2 62.2 52.7 77.8 69.5 63.4 69.6 37.8 62.8 63.6 63.9 64.6 61.9 39.9 80.5 66.7 45.5 77.6 90.6 63.2
PIA-D 39.7 57.7 58.6 47.2 74.0 74.5 62.1 66.6 33.6 61.7 65.4 58.0 67.1 58.9 41.9 77.7 64.7 50.5 81.8 89.9 61.6
PIA-P 41.5 55.8 60.9 51.9 75.0 75.8 59.6 65.2 33.3 65.9 62.8 62.7 67.7 62.1 42.9 80.2 64.3 59.5 82.7 90.1 63.0

PCA-P 40.9 55.0 65.8 47.9 76.9 77.9 63.5 67.4 33.7 65.5 63.6 61.3 68.9 62.8 44.9 77.5 67.4 57.5 86.7 90.9 63.8
PCA-H 49.8 60.7 63.9 52.6 79.8 72.5 63.8 71.2 38.4 62.5 71.7 65.4 66.6 62.5 40.5 84.7 66.1 47.9 80.5 91.1 64.6

PCA+-P 46.6 61.0 62.3 53.9 78.2 72.5 64.4 70.5 39.0 63.5 74.8 65.2 65.0 61.6 40.8 83.2 67.1 50.5 79.6 91.6 64.6
CIE2-P 50.9 65.5 68.0 57.0 81.0 75.9 70.3 73.4 41.1 66.7 53.2 68.3 68.4 63.5 45.3 84.8 69.7 57.2 79.8 91.6 66.9
CIE2-H 51.2 68.4 69.5 57.3 82.5 73.5 69.5 74.0 40.3 67.8 60.0 69.7 70.3 65.1 44.7 86.9 70.7 57.3 84.2 92.2 67.4
CIE1-P 52.1 69.4 69.9 58.9 80.6 76.3 71.0 74.2 41.1 68.0 60.4 69.7 70.7 65.1 46.1 85.1 70.4 61.6 80.7 91.7 68.1
CIE1-H 51.2 69.2 70.1 55.0 82.8 72.8 69.0 74.2 39.6 68.8 71.8 70.0 71.8 66.8 44.8 85.2 69.9 65.4 85.2 92.4 68.9
PCA-P 75.8 99.2 83.3 74.7 98.7 96.3 74.3 87.8 80.9 85.7 100.0 83.7 83.8 98.7 66.5 99.1 80.7 99.7 98.2 97.0 88.2
CIE1-P 56.5 84.0 73.5 58.0 91.5 81.1 67.8 76.8 46.4 72.2 98.0 73.9 73.6 77.9 46.1 94.8 72.7 93.6 93.7 91.6 76.2
CIE1-H 59.4 88.1 75.9 58.0 94.3 81.9 69.4 78.9 49.5 78.2 99.7 78.1 78.0 82.1 47.4 95.8 75.7 97.6 96.0 91.1 78.7

according to its outputConcretely, the sparse link is calculated as:

Z = Atten
(
Hungarian(S),SG

)
= P ∪Q (13)

where the attention mechanism Atten is fulfilled by an element-wise “logic OR” functionFig3
shows an example of Hungarian attention procedure, and Eq(13) highlights the most contributing
digit locations: positive digits P = S where Hungarian agrees with the ground-truth; negative digits
Q = Hungarian(S) \ SG where Hungarian differs from ground-truthWhile GT (positive digits)
naturally points out the digits that must be considered, negative ones indicate the digits that most
hinder the matching (most impeding ones among all mis-matchings)Thus we need only minimize
the loss at Z, without considering the rest of digitsAs we note that this mechanism only focuses on
a small portion of the matching matrix which is analogous to producing hard attention, we term it
Hungarian attentionNow that with the attention mask Z, the Hungarian attention loss becomes:

HCE = −
∑

i∈G1,j∈G2

Zij
(
SGij log Sij +

(
1− SGij

)
log (1− Sij)

)
(14)

Note that Hungarian attention mechanism can also be applied to other loss functions once the match-
ing score is calculated in an element-wise fashionOur experiment also studies Hungarian attention
loss when casted on focal loss (Lin et al., 2017) and a specifically designed margin loss.

Finally we give a brief qualitative analysis on why Hungarian attention can improve matching loss.
As discrete graph matching problem is actually built upon Delta function over permutation vertices
(1 at ground-truth matching and 0 otherwise) (Yu et al., 2018), learning of graph matching with per-
mutation loss is actually to approximate such functions with continuous counterpartsUnfortunately,
more precise approximation to Delta function will result in higher non-smoothness, as discussed in
Yu et al(2018)For highly non-smooth objective, the network is more likely trapped at local optima.
Hungarian attention, however, focuses on a small portion of the output locations, thus does not care
about if most of the output digits are in {0, 1}In this sense, Hungarian attention allows moderate
smoothness of the objective, thus optimizer with momentum is likely to avoid local optima.

4 EXPERIMENTS

Experiments are conducted on three benchmarks widely used for learning-based graph matching:
CUB2011 dataset (Welinder et al., 2010) following the protocol in (Choy et al., 2016), Pascal VOC
keypoint matching (Everingham et al., 2010; Bourdev & Malik, 2009) which is challenging and
Willow Object Class dataset (Cho et al., 2013)Mean matching accuracy is adopted for evaluation:

Acc =
1

k

∑
i∈G1,j∈G2

AND
(
Hungarian(S)ij ,S

G
ij

)
(15)

The algorithm abbreviation is in the form “X-Y”, where “X” and “Y” refer to the network structure
(e.gCIE) and loss function (e.gHungarian attention loss), respectivelySpecifically, D, P and H

7



Published as a conference paper at ICLR 2020

correspond to displacement used in (Zanfir & Sminchisescu, 2018), permutation as adopted in (Wang
et al., 2019) and Hungarian attention over permutation loss devised by this paper, respectively.

Peer methodsWe compare our method with the following selected counterparts: 1) HARG (Cho
et al., 2013)This shallow learning method is based on hand-crafted feature and Structured SVM;
2) GMN (Zanfir & Sminchisescu, 2018)This is a seminal work incorporating graph matching and
deep learning, and the solver is upon spectral matching (Leordeanu & Hebert, 2005)While the
loss of this method is displacement loss, we also report the results of GMN by replacing its loss
with permutation loss (GMN-P); 3) PIA/PCA (Wang et al., 2019)PCA and PIA correspond to
the algorithms with and without cross-graph node embedding, respectivelyReaders are referred
to Wang et al(2019) for more details; We further replace the GNN layer in our framework with:
4) GAT (Veličković et al., 2018)Graph attention network is an attention mechanism on graphs,
which reweights the embedding according to attention score; 5) EPN (Gong & Cheng, 2019)This
method exploits multi-dimensional edge embedding and can further be applied on directed graphs.
The edge dimension is set to 32 in our experimentsFinally, we term our network structure CIE for
shortTo investigate the capacity of edge embedding update, we also devise a version without edge
embedding, in which connectivity is initialized as reciprocal of the edge length then normalized,
rather than AThis model is called PCA+ since the node embedding strategy follows PCA.

Implementation detailsAs the node number of each graph might vary, we add dummy nodes for
each graph pair such that the node number reaches the maximal graph size in a mini-batch in line
with the protocol in (Wang et al., 2019)In either training or testing stages, these dummy nodes will
not be updated or countedThe activation function in Eq(9) (10) and (11) is set as Relu (Nair &
Hinton, 2010) in all experimentsSpecifically, the node and edge embedding is implemented by:

H
(l+1)
·q = σ

((
A�

(
W

(l)
1 E

(l)
)
·q

)(
W

(l)
2 H

(l)
)
·q

)
+ σ

((
W

(l)
0 H

(l)
)
·q

)
(16a)

E
(l+1)
·q = σ

(∣∣∣∣(W(l)0 H(l))·q 	 (W(l)0 H(l))>·q
∣∣∣∣�E(l)·q )+ σ((W(l)1 E(l))·q

)
(16b)

where � and 	 refer to element-wise product and pairwise difference, respectivelyH·q is the
qth channel of HIn CIE1 setting, only node-level merging Eq(16a) is considered and the edge
feature is updated as Eq(10)In CIE2 setting, we also replace the edge update Eq(11) with Eq.
(16b)Note edge embedding is used in both CIE1 and CIE2 and note PCA-H can be regarded as
the pure node embedding version of our approachThe edge feature is initiated as reciprocal of the
edge lengthFor training, batch size is set to 8We employ SGD optimizer (Bottou, 2010) with
momentum 0.9Two CIE layers are stacked after VGG16.

CUB2011 test CUB2011 consists of 11,788 images from 200 kinds of birds with 15 annotated
partsWe randomly sample image pairs from the dataset following the implementation released by
Choy et al(2016)We do not use the pre-alignment of poses during testing, because their alignment
result is not publicly availableTherefore, there exists significant variation in pose, articulation and
appearance across images, in both training and testing phaseImages are cropped around bounding
box and resized to 256 × 256 before fed into the networkInstead of evaluating the performance
in a retrieval fashion (Zanfir & Sminchisescu, 2018), we directly evaluate the matching accuracy
since the semantic key-points are pre-givenWe test two settings: 1) intra-classDuring training,
we randomly sample images, with each pair sampled from the same category (out of 200 bird cate-
gories)In testing, 2,000 image pairs (100 pairs for each category) are sampled; 2) cross-classWe
analogously sample image pairs without considering the category information and 5,000 randomly
sampled image pairs are employed for testingWhile the first setting is for a class-aware situation,
the second setting is considered for testing the class-agnostic caseResults are shown in Table 3.

We see our method surpasses all the competing methods in terms of matching accuracyBesides,
almost all the selected algorithms can reach over 90% accuracy, indicating that this dataset contains
mostly “easy” learning samplesIn this case, the Hungarian attention can slightly improve the
performance since easy gradients agree with descending trend of the loss on the whole dataset.

Pascal VOC test The Pascal VOC dataset with Key-point annotation (Bourdev & Malik, 2009)
contains 7,020 training images and 1,682 testing images with 20 classes in totalTo the best of
our knowledge, this is the largest and most challenging dataset for graph matching in computer vi-
sionEach image is cropped around its object bounding box and is resized to 256× 256The node

8



Published as a conference paper at ICLR 2020

0 5 10 15 20 25 30

Epoch

0.2

0.25

0.3

0.35

0.4

0.45

0.5

0.55

0.6

0.65

0.7

A
cc

ur
ac

y

0.5

1

1.5

2

2.5

3

L
os

s

Acc: CIE
1
-P

Acc: CIE
1
-H

Loss: CIE
1
-P

Loss: CIE
1
-H

(a) Accuracy/loss vstraining epoch.
Focal Margin Perm

0.6

0.62

0.64

0.66

0.68

0.7

A
cc

ur
ac

y

no Hung
with Hung

(b) Ablation study by Hungarian attention.

Figure 4: Performance study on Pascal VOCNote in (a) the loss is calculated on all matching digits
for both CIE1-P and CIE1-HNote around 10th epoch, the accuracy of CIE1-P almost reaches the
highest, but the loss keeps descending until 30th epochThis indicates that in most of the latter
epochs, P-loss performs “meaningless” back-propagation to drag the output to binaryH-loss, by
accommodating smoothness, can emphasize most contributing digits and achieves higher accuracy.
size of this dataset varies from 6 to 23 and there are various scale, pose and illumination perturba-
tionsExperimental results are summarized in Table 1We see in either setting, CIE significantly
outperforms all peer algorithmsSpecifically, CIE1-H achieves the best performance and has 0.8%
improvement w.r.taverage accuracy over CIE1-PFor each class, CIE1-H and CIE1-P carve up most
of the top performanceWe also note that CIE1-H has a close performance on “table” compared
with GMN-DSince P-loss is naturally not as robust as D-loss on symmetric objects, P-loss showed
great degradation over D-loss on “table” (as discussed in (Wang et al., 2019))However, with the
help of Hungarian link, H-loss can maintain relatively high accuracy despite natural flaw of P-loss.
This observation indicates that H-loss can focus on “difficult” examplesWe also note that CIE1
produces better results against CIE2, which implies that updating edge embedding is less effective
compared to a singleton node updating strategyWe can also see from Table 1 that PCA-P has much
higher performance on training samples than CIE1-H, which is to the contrary of the result on testing
samplesThis might indicate that PCA-P overfits the training samples.

Accuracy/loss vstraining epochWe further show the typical training behavior of P-loss and H-
loss on Pascal VOC dataset in Fig430 epochs are involved in a whole training processAccuracy
is evaluated on testing samples after each epoch while loss is the average loss value within each
epochIn the early training stage, the loss of CIE1-P immediately dropsOn the other hand, CIE1-H
hesitates for several epochs to find the most effective descending directionOn the late stage, we
observe that even though P-loss (Eq(12)) calculates much more digits than H-loss (Eq(14)), the
loss values are oppositeThis counter-intuitive fact strongly indicates that P-loss makes meaningless
effort, which is not helpful to improve the performance, at late stageThe proposed H-loss, on the
other hand, is capable of avoiding easy but meaningless gradients.

Effect of Hungarian attention mechanismWe also conduct experiments to show the improve-
ment of Hungarian attention over several loss functions (with and without Hungarian attention):
Hungarian attention is applied on Focal loss (Focal) (Lin et al., 2017) as:

Lfocal =

{
−αZij(1− Sij)γ log(Sij), SGij = 1
−(1− α)ZijSγij log(1− Sij), SGij = 0

(17)

where controlling parameters α = 0.75 and γ = 2 in our settingWe also design a margin loss
(Margin) with Hungarian attention under a max-margin ruleNote we insert the Hungarian attention
mask Zij into Eq(17) and Eq(18) based on the vanilla forms.

Lmargin =
{

Zij ×max(1− Sij − β, 0), SGij = 1
Zij ×max(Sij − β, 0), SGij = 0

(18)

where we set the margin value β = 0.2Loss of Eq(18) is valid because after Softmax and
Sinkhorn operations, Sij ∈ [0, 1]We also show permutation loss (Perm) (Wang et al., 2019).
Result can be found in Fig4 (b) whereby the average accuracy on Pascal VOC is reportedAll the
settings are under CIE1For either loss, the proposed Hungarian attention can further enhance the
accuracy, which is further visualized by a pair of matching results under P-loss and H-loss in Fig5.

9



Published as a conference paper at ICLR 2020

(a) Reference Image (b) P-loss: 7/10 (c) H-loss: 8/10
Figure 5: Visualization of a matching result: 10 key points in each image with 7 and 8 correct
matchings dispalyed, respectivelyDifferent colors across images indicate node correspondence.
The larger size of dot, the larger is the predicted value Sij (a) The reference image(b) Result on
the target image from CIE1-P(c) Result on the target image from CIE1-HWe see though H-loss
i.eHungarian attention loss outputs smaller predicted values, it delivers a more accurate matching.

Table 2: Accuracy (%) on Willow Object.
method face mbike car duck wbottle
HARG 91.2 44.4 58.4 55.2 66.6

GMN-V 98.1 65.0 72.9 74.3 70.5
GMN-W 99.3 71.4 74.3 82.8 76.7
PCA-V 100.0 69.8 78.6 82.4 95.1
PCA-W 100.0 76.7 84.0 93.5 96.9
CIE-V 99.9 71.5 75.4 73.2 97.6
CIE-W 100.0 90.0 82.2 81.2 97.6

Table 3: Accuracy (%) on CUB.
method intra-class cross-class
GMN-D 89.6 89.9
GMN-P 90.4 90.8
GAT-P 93.2 93.4
PCA-P 92.9 93.5
PCA-H 93.7 93.5
CIE-P 94.1 93.8
CIE-H 94.4 94.2

Willow Object Class test We test the transfer ability on Willow Object Class (Cho et al., 2013).
It contains 256 images3 of 5 categories in total, with three categories (face, duck and winebottle)
collected from Caltech-256 and resting two (car and motorbike) from Pascal VOC 2007This dataset
is considered to have bias compared with Pascal VOC since images in the same category are with
relatively fixed pose and background is much cleanerWe crop the object inside its bounding box and
resize it to 256 × 256 as CNN inputWhile HARG is trained from scratch following the protocol
in (Cho et al., 2013), all the resting counterparts are either directly pre-trained from the previous
section or fine-tuned upon the pre-trained modelsWe term the method “X-V” or “X-W” to indicate
pre-trained model on Pascal VOC or fine-tuned on Willow, respectivelyCIE refers to CIE1-H for
shortResults in Table 2 suggest that our method is competitive to state-of-the-art.

5 CONCLUSION

We have presented a novel and effective approach for learning based graph matchingOn one hand,
the novelty of our method partially lies in the development of the Hungarian attention, which in-
trinsically adapts the matching problemIt is further observed from the experiments that Hungarian
attention can improve several matching-oriented loss functions, which might bring about potential
for a series of combinatorial problemsOn the other hand, we also devise the channel independent
embedding (CIE) technique for deep graph matching, which decouples the basic merging opera-
tions and is shown robust in learning effective graph representationExtensive experimental results
on multiple matching benchmarks show the leading performance of our solver, and highlight the
orthogonal contribution of the two proposed components on top of existing techniques.

ACKNOWLEDGMENTS

Tianshu Yu and Baoxin Li were supported in part by a grant from ONRAny opinions expressed
in this material are those of the authors and do not necessarily reflect the views of ONRRunzhong
Wang and Junchi Yan were supported in part by NSFC 61972250 and U19B2035.

3The data size is too small to train a deep modelHence we only evaluate the transfer ability on this dataset.

10



Published as a conference paper at ICLR 2020

REFERENCES
Kfir Aberman, Jing Liao, Mingyi Shi, Dani Lischinski, Baoquan Chen, and Daniel Cohen-OrNeural

best-buddies: Sparse cross-domain correspondenceSIGGRAPH, 2018.

Ryan Prescott Adams and Richard S ZemelRanking via sinkhorn propagationarXiv:1106.1925,
2011.

Florian Bernard, Christian Theobalt, and Michael MoellerDs*: Tighter lifting-free convex relax-
ations for quadratic matching problemsIn CVPR, 2018.

Léon BottouLarge-scale machine learning with stochastic gradient descentIn COMPSTAT2010.

Lubomir Bourdev and Jitendra MalikPoselets: Body part detectors trained using 3d human pose
annotationsIn ICCV, 2009.

Jane Bromley, Isabelle Guyon, Yann LeCun, Eduard Säckinger, and Roopak ShahSignature verifi-
cation using a “siamese” time delay neural networkIn NIPS, 1994.

Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCunSpectral networks and locally
connected networks on graphsIn ICLR, 2014.

Tibério S Caetano, Julian J McAuley, Li Cheng, Quoc V Le, and Alex J SmolaLearning graph
matchingPAMI, 31(6):1048–1058, 2009.

Hongyun Cai, Vincent W Zheng, and Kevin Chen-Chuan ChangA comprehensive survey of graph
embedding: Problems, techniques, and applicationsTKDE, 30(9):1616–1637, 2018.

Pengfei Chen, Weiwen Liu, Chang-Yu Hsieh, Guangyong Chen, and Shengyu ZhangUtilizing
edge features in graph neural networks via variational information maximizationarXiv preprint
arXiv:1906.05488, 2019.

Kyunghyun Cho, Bart Van Merriënboer, Dzmitry Bahdanau, and Yoshua BengioOn the properties
of neural machine translation: Encoder-decoder approachesarXiv preprint arXiv:1409.1259,
2014.

Minsu Cho, Jungmin Lee, and Kyoung Mu LeeReweighted random walks for graph matchingIn
ECCV, 2010.

Minsu Cho, Karteek Alahari, and Jean PonceLearning graphs to matchIn CVPR, 2013.

Christopher B Choy, JunYoung Gwak, Silvio Savarese, and Manmohan ChandrakerUniversal
correspondence networkIn NIPS, 2016.

Michaël Defferrard, Xavier Bresson, and Pierre VandergheynstConvolutional neural networks on
graphs with fast localized spectral filteringIn NIPS, 2016.

Boris Delaunay et alSur la sphere videIzvAkadNauk SSSR, Otdelenie Matematicheskii i
Estestvennyka Nauk, 7(793-800):1–2, 1934.

Alexey Dosovitskiy, Philipp Fischer, Eddy Ilg, Philip Hausser, Caner Hazirbas, Vladimir Golkov,
Patrick Van Der Smagt, Daniel Cremers, and Thomas BroxFlownet: Learning optical flow with
convolutional networksIn ICCV, 2015.

Patrick Emami and Sanjay RankaLearning permutations with sinkhorn policy gradient.
arXiv:1805.07010, 2018.

Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman.
The pascal visual object classes (voc) challengeIJCV, 88(2):303–338, 2010.

Paolo Frasconi, Marco Gori, and Alessandro SperdutiA general framework for adaptive processing
of data structuresTNN, 9(5):768–786, 1998.

Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E DahlNeural
message passing for quantum chemistryIn ICML, 2017.

11



Published as a conference paper at ICLR 2020

Liyu Gong and Qiang ChengExploiting edge features for graph neural networksIn CVPR, 2019.

Marco Gori, Gabriele Monfardini, and Franco ScarselliA new model for learning in graph domains.
In IJCNN, 2005.

Will Hamilton, Zhitao Ying, and Jure LeskovecInductive representation learning on large graphs.
In NIPS, 2017.

Juris HartmanisComputers and intractability: a guide to the theory of np-completeness (michael r.
garey and david sjohnson)Siam Review, 24(1):90, 1982.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian SunDeep residual learning for image recog-
nitionIn CVPR, 2016.

Thomas N Kipf and Max WellingSemi-supervised classification with graph convolutional net-
worksIn ICLR, 2017.

Harold W KuhnThe hungarian method for the assignment problemNaval research logistics
quarterly, 2(1-2):83–97, 1955.

Eugene L LawlerThe quadratic assignment problemManagement science, 9(4):586–599, 1963.

Marius Leordeanu and Martial HebertA spectral technique for correspondence problems using
pairwise constraintsIn ICCV, 2005.

Yijun Li, Chen Fang, Jimei Yang, Zhaowen Wang, Xin Lu, and Ming-Hsuan YangUniversal style
transfer via feature transformsIn NIPS, 2017.

Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard ZemelGated graph sequence neural
networksIn ICLR, 2016.

Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr DollárFocal loss for dense object
detectionIn ICCV, 2017.

Eliane Maria Loiola, Nair Maria Maia de Abreu, Paulo Oswaldo Boaventura-Netto, Peter Hahn, and
Tania QueridoA survey for the quadratic assignment problemEuropean journal of operational
research, 176(2):657–690, 2007.

Gonzalo Mena, David Belanger, Gonzalo Muñoz, and Jasper SnoekSinkhorn networks: Using
optimal transport techniques to learn permutationsNIPS Workshop in Optimal Transport and
Machine Learning, 2017.

Anton Milan, Seyed Hamid Rezatofighi, Ravi Garg, Anthony RDick, and Ian DReidData-driven
approximations to np-hard problemsIn AAAI, 2017.

Vinod Nair and Geoffrey E HintonRectified linear units improve restricted boltzmann machines.
In ICML, 2010.

Giorgio Patrini, Marcello Carioni, Patrick Forre, Samarth Bhargav, Max Welling, Rianne van den
Berg, Tim Genewein, and Frank NielsenSinkhorn autoencodersarXiv:1810.01118, 2018.

Zhaofan Qiu, Ting Yao, and Tao MeiLearning spatio-temporal representation with pseudo-3d
residual networksIn ICCV, 2017.

Zhe Ren, Junchi Yan, Bingbing Ni, Bin Liu, Xiaokang Yang, and Hongyuan ZhaUnsupervised
deep learning for optical flow estimationIn AAAI, 2017.

Rodrigo Santa Cruz, Basura Fernando, Anoop Cherian, and Stephen GouldVisual permutation
learningTPAMI, 2018.

Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini.
The graph neural network modelTNN, 20(1):61–80, 2008.

Kristof T Schütt, Farhad Arbabzadah, Stefan Chmiela, Klaus R Müller, and Alexandre Tkatchenko.
Quantum-chemical insights from deep tensor neural networksNature communications, 8:13890,
2017.

12



Published as a conference paper at ICLR 2020

Karen Simonyan and Andrew ZissermanVery deep convolutional networks for large-scale image
recognitionarXiv preprint arXiv:1409.1556, 2014.

Richard SinkhornA relationship between arbitrary positive matrices and doubly stochastic matri-
cesAoMS, 1964.

Alessandro Sperduti and Antonina StaritaSupervised neural networks for the classification of
structuresTNN, 8(3):714–735, 1997.

Du Tran, Heng Wang, Lorenzo Torresani, Jamie Ray, Yann LeCun, and Manohar PaluriA closer
look at spatiotemporal convolutions for action recognitionIn CVPR, 2018.

Ioannis Tsochantaridis, Thorsten Joachims, Thomas Hofmann, and Yasemin AltunLarge margin
methods for structured and interdependent output variablesJMLR, 6(Sep):1453–1484, 2005.

Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, and Yoshua
BengioGraph attention networksIn ICLR, 2018.

Oriol Vinyals, Meire Fortunato, and Navdeep JaitlyPointer networksIn NIPS, 2015.

Runzhong Wang, Junchi Yan, and Xiaokang YangLearning combinatorial embedding networks for
deep graph matchingIn ICCV, 2019.

PWelinder, SBranson, TMita, CWah, FSchroff, SBelongie, and PPeronaCaltech-UCSD
Birds 200Technical Report CNS-TR-2010-001, California Institute of Technology, 2010.

Hongteng Xu, Dixin Luo, and Lawrence CarinGromov-wasserstein learning for graph matching
and node embeddingIn ICML, 2019.

Junchi Yan, Chao Zhang, Hongyuan Zha, Wei Liu, Xiaokang Yang, and Stephen M ChuDiscrete
hyper-graph matchingIn CVPR, 2015.

Tianshu Yu, Junchi Yan, Yilin Wang, Wei Liu, and Baoxin LiGeneralizing graph matching beyond
quadratic assignment modelIn NIPS, 2018.

Andrei Zanfir and Cristian SminchisescuDeep learning of graph matchingIn CVPR, 2018.

Zhen Zhang and Wee Sun LeeDeep graphical feature learning for the feature matching problem.
In ICCV, 2019.

Feng Zhou and Fernando De la TorreFactorized graph matchingIn CVPR, 2012.

A APPENDIX

A.1 SYNTHETIC TEST

Synthetic graphs are generated for training and testing following the protocol in (Cho et al., 2010).
Specifically, Kpt keypoints are generated for a pair of graphs with a 1024-dimensional random
feature for each node, which is sampled from uniform distribution U(−1, 1)Disturbance is also
applied to graph pairs including: Gaussian node feature noise from N (0, σ2ft); random affine trans-

formation

[
s cos θ −s sin θ tx
s sin θ s cos θ ty

0 0 1

]
with s ∼ U(0.8, 1.2), θ ∼ U(−60, 60), tx, ty ∼ U(−10, 10)

followed by Gaussian coordinate position noise N (0, σ2co)By default we assign Kpt = 25, σft =
1.5, σco = 5Two graphs share the same structureWe generate 10 random distributions for each
testResults are shown in Fig6The performance of PCA and CIE is reportedWe see our
method significantly outperformed PCAIt can further be noticed that Hungarian attention can help
to achieve an even higher accuracyReaders are referred to Wang et al(2019) for some other results
on synthetic test.

However, we also notice that the way to generate synthetic graphs is much different from the dis-
tribution of real-world dataFor real-world data, on one hand, there is strong correlation on the

13



Published as a conference paper at ICLR 2020

1.2 1.25 1.3 1.35 1.4 1.45 1.5 1.55 1.6

Noise

0.55

0.6

0.65

0.7

0.75

0.8

0.85

0.9

0.95

1

A
cc

ur
ac

y

PCA-P
CIE

1
-P

CIE
1
-H

5 10 15 20 25 30 35 40 45

Node size

0.6

0.65

0.7

0.75

0.8

0.85

0.9

0.95

1

A
cc

ur
ac

y

PCA-P
CIE

1
-P

CIE
1
-H

Figure 6: Results on synthetic test where two different loss functions are compared in ablative study.

neighboring node featuresThis is the reason why the message passing from nearby node features
worksHowever, the features of synthetic data are randomly generated and there is no correlation
between neighboring node featuresTherefore, message passing mechanism is not very effective to
reveal the relation or pattern among local nodes for synthetic dataOn the other hand, features of
real-world data typically lie on a manifold embedded in high dimensional space, hence is low di-
mensionalHowever, randomly generated features will span the whole space and show no patterns.

Taking into account the aforementioned factors, we believe there is a demand for a novel strategy to
generate more reasonable synthetic dataThis can be one of the future works.

A.2 COMPARISON OF PASCAL VOC AND WILLOW

As we claim that Willow dataset is biased compared with Pascal VOC dataset, we qualitatively show
some randomly selected examples in Fig7We select several images with the same class “car” from
both datasetsWe also choose images with “bird” from Pascal VOC and “duck” from Willow since
they somewhat share similar semantic informationWe see in either case, Pascal VOC contains more
variation and degradation compared with Willow in terms of pose, scale, appearance, etcIn general,
Willow dataset is easier for algorithms to learnWhile there is a significant performance gap of PCA
over these two datasets, the performance of CIE on Willow without fine-tune (Table 2) is consistent
to the performance on Pascal VOC (Table 1)As such, we infer the performance degradation of CIE
on “duck” in Willow test (Table 2) is due to such biasThe pre-trained CIE on Pascal VOC tends
to produce more stable and higher average accuracy on all types of images, rather than focusing on
“easy-to-learn” samples by PCAThis is a different learning strategy from PCA.

14



Published as a conference paper at ICLR 2020

(a) Car images from Pascal VOC

(b) Car images from Willow

(c) Bird images from Pascal VOC

(d) Duck images from Willow

Figure 7: Image examples from Pascal VOC and Willow.

15


	Introduction
	Related works
	The proposed learning approach for graph matching
	Approach Overview
	Channel-independent embedding
	Hungarian attention mechanism

	Experiments
	Conclusion
	Appendix
	Synthetic test
	Comparison of Pascal VOC and Willow


