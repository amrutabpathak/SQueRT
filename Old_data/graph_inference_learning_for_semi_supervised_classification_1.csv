b'Published as a conference paper at ICLR 2020'
<EOS>
b'GRAPH INFERENCE LEARNING FOR'
<EOS>
b'SEMI-SUPERVISED\nCLASSIFICATION'
<EOS>
b'Chunyan Xu, Zhen Cui, Xiaobin Hong, Tong Zhang, and Jian Yang'
<EOS>
b'School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China'
<EOS>
b'cyx,zhen.cui,xbhong,tong.zhang,csjyangnjust.edu.cn'
<EOS>
b'Wei Liu\nTencent AI Lab'
<EOS>
b', China\nwl2223columbia.edu'
<EOS>
b'ABSTRACT'
<EOS>
b'In this work, we address semi-supervised classi\xef\xac\x81cation of graph data, where the\ncategories of those unlabeled nodes are inferred from labeled nodes as well as\ngraph structures.'
<EOS>
b'Recent works often solve this problem via advanced graph\nconvolution in a conventionally supervised manner, but the performance could\ndegrade signi\xef\xac\x81cantly when labeled data is scarce.'
<EOS>
b'To this end, we propose a\nGraph Inference Learning GIL framework to boost the performance of semi-'
<EOS>
b'supervised node classi\xef\xac\x81cation by learning the inference of node labels on graph\ntopology.'
<EOS>
b'To bridge the connection between two nodes, we formally de\xef\xac\x81ne a\nstructure relation by encapsulating node attributes, between-node paths, and local\ntopological structures together, which can make the inference conveniently deduced\nfrom one node to another node.'
<EOS>
b'For learning the inference process, we further\nintroduce meta-optimization on structure relations from training nodes to validation\nnodes, such that the learnt graph inference capability can be better self-adapted to\ntesting nodes.'
<EOS>
b'Comprehensive evaluations on four benchmark datasets including\nCora, Citeseer, Pubmed, and NELL demonstrate the superiority of our proposed\nGIL when compared against state-of-the-art methods on the semi-supervised node\nclassi\xef\xac\x81cation task.'
<EOS>
b'1'
<EOS>
b'INTRODUCTION'
<EOS>
b'Graph, which comprises a set of verticesnodes together with connected edges, is a formal structural\nrepresentation of non-regular data.'
<EOS>
b'Due to the strong representation ability, it accommodates many\npotential applications, e.g., social network Orsini et al., 2017, world wide data Page et al., 1999,\nknowledge graph Xu et al., 2017, and protein-interaction network Borgwardt et al., 2007.'
<EOS>
b'Among\nthese, semi-supervised node classi\xef\xac\x81cation on graphs is one of the most interesting also popular topics.'
<EOS>
b'Given a graph in which some nodes are labeled, the aim of semi-supervised classi\xef\xac\x81cation is to infer\nthe categories of those remaining unlabeled nodes by using various priors of the graph.'
<EOS>
b'While there have been numerous previous works'
<EOS>
b'Brandes et al., 2008'
<EOS>
b'Zhou et al.,'
<EOS>
b'2004 Zhu'
<EOS>
b'et al., 2003'
<EOS>
b'Yang et al., 2016'
<EOS>
b'Zhao et al., 2019'
<EOS>
b'devoted to semi-supervised node classi\xef\xac\x81cation\nbased on explicit graph Laplacian regularizations, it is hard to ef\xef\xac\x81ciently boost the performance of\nlabel prediction due to the strict assumption that connected nodes are likely to share the same label\ninformation.'
<EOS>
b'With the progress of deep learning on grid-shaped imagesvideos'
<EOS>
b'He et al., 2016,'
<EOS>
b'a few of graph convolutional neural networks CNN based methods, including spectral Kipf \nWelling, 2017 and spatial methods'
<EOS>
b'Niepert et al., 2016'
<EOS>
b'Pan et al., 2018'
<EOS>
b'Yu et al., 2018,'
<EOS>
b'have\nbeen proposed to learn local convolution \xef\xac\x81lters on graphs in order to extract more discriminative\nnode representations.'
<EOS>
b'Although graph CNN based methods have achieved considerable capabilities\nof graph embedding by optimizing \xef\xac\x81lters, they are limited into a conventionally semi-supervised\nframework and lack of an ef\xef\xac\x81cient inference mechanism on graphs.'
<EOS>
b'Especially, in the case of few-shot\nlearning, where a small number of training nodes are labeled, this kind of methods would drastically\ncompromise the performance.'
<EOS>
b'For example, the Pubmed graph dataset Sen et al., 2008'
<EOS>
b'consists'
<EOS>
b'Corresponding author Zhen Cui.'
<EOS>
b'1'
<EOS>
b'Published as a conference paper at ICLR 2020'
<EOS>
b'Figure 1'
<EOS>
b'The illustration of our proposed GIL framework.'
<EOS>
b'For the problem of graph node labeling, the category\ninformation of these unlabeled nodes depends on the similarity computation between a query node'
<EOS>
b'e.g., vj\nand these labeled reference nodes'
<EOS>
b'e.g., vi.'
<EOS>
b'We consider the similarity from three points node attributes,\nthe consistency of local topological structures i.e., the circle with dashed line, and'
<EOS>
b'the between-node path\nreachability'
<EOS>
b'i.e., the red wave line from vi to vj.'
<EOS>
b'Speci\xef\xac\x81cally, the local structures as well as node attributes are\nencoded as high-level features with graph convolution, while the between-node path reachability is abstracted as\nreachable probabilities of random walks.'
<EOS>
b'To better make the inference generalize to test nodes, we introduce a\nmeta-learning strategy to optimize the structure relations learning from training nodes to validation nodes.'
<EOS>
b'of 19,717 nodes and 44,338 edges, but only 0.3 nodes are labeled for the semi-supervised node\nclassi\xef\xac\x81cation task.'
<EOS>
b'These aforementioned works usually boil down to a general classi\xef\xac\x81cation task,\nwhere the model is learnt on a training set and selected by checking a validation set.'
<EOS>
b'However, they\ndo not put great efforts on how to learn to infer from one node to another node on a topological graph,\nespecially in the few-shot regime.'
<EOS>
b'In this paper, we propose a graph inference learning GIL framework to teach the model itself to\nadaptively infer from reference labeled nodes to those query unlabeled nodes, and \xef\xac\x81nally boost the\nperformance of semi-supervised node classi\xef\xac\x81cation in the case of a few number of labeled samples.'
<EOS>
b'Given an input graph, GIL attempts to infer the unlabeled nodes from those observed nodes by\nbuilding between-node relations.'
<EOS>
b'The between-node relations are structured as the integration of\nnode attributes, connection paths, and graph topological structures.'
<EOS>
b'It means that the similarity\nbetween two nodes is decided from three aspects the consistency of node attributes, the consistency\nof local topological structures, and the between-node path reachability, as shown in Fig.'
<EOS>
b'1.'
<EOS>
b'The local\nstructures anchored around each node as well as the attributes of nodes therein are jointly encoded\nwith graph convolution Defferrard et al., 2016 for the sake of high-level feature extraction.'
<EOS>
b'For the\nbetween-node path reachability, we adopt the random walk algorithm to obtain the characteristics\nfrom a labeled reference node vi to a query unlabeled node vj in a given graph.'
<EOS>
b'Based on the\ncomputed node representation and between-node reachability, the structure relations can be obtained\nby computing the similar scoresrelationships from reference nodes to unlabeled nodes in a graph.'
<EOS>
b'Inspired by the recent meta-learning strategy Finn et al., 2017, we learn to infer the structure\nrelations from a training set to a validation set, which can bene\xef\xac\x81t the generalization capability of the\nlearned model.'
<EOS>
b'In other words, our proposed GIL attempts to learn some transferable knowledge\nunderlying in the structure relations from training samples to validation samples, such that the learned\nstructure relations can be better self-adapted to the new testing stage.'
<EOS>
b'We summarize the main contributions of this work as three folds'
<EOS>
b'We propose a novel graph inference learning framework by building structure relations to\ninfer unknown node labels from those labeled nodes in an end-to-end way.'
<EOS>
b'The structure\nrelations are well de\xef\xac\x81ned by jointly considering node attributes, between-node paths, and\ngraph topological structures.'
<EOS>
b'To make the inference model better generalize to test nodes, we introduce a meta-learning\nprocedure to optimize structure relations, which could be the \xef\xac\x81rst time for graph node'
<EOS>
b'classi\xef\xac\x81cation to the best of our knowledge.'
<EOS>
b'Comprehensive evaluations on three citation network datasets including Cora, Citeseer,\nand Pubmed and one knowledge graph data i.e., NELL demonstrate the superiority of\nour proposed GIL in contrast with other state-of-the-art methods on the semi-supervised\nclassi\xef\xac\x81cation task.'
<EOS>
b'2'
<EOS>
b'b'
<EOS>
b'The process of Graph inference learning.'
<EOS>
b'We extract the local representation from the local subgraph the circle with dashed line     The red wave line denote the node reachability from     to     .'
<EOS>
b'dt th'
<EOS>
b'hbilit f  d t th d'
<EOS>
b'Published as a conference paper at ICLR 2020'
<EOS>
b'2'
<EOS>
b'RELATED WORK'
<EOS>
b'Graph CNNs'
<EOS>
b'With the rapid development of deep learning methods, various graph convolution'
<EOS>
b'neural networks Kashima et al., 2003 Morris et al., 2017 Shervashidze et al.,'
<EOS>
b'2009 Yanardag\n Vishwanathan, 2015 Jiang et al., 2019'
<EOS>
b'Zhang et al., 2020 have been exploited to analyze\nthe irregular graph-structured data.'
<EOS>
b'For better extending general convolutional neural networks to\ngraph domains, two broad strategies have been proposed, including spectral and spatial convolution\nmethods.'
<EOS>
b'Speci\xef\xac\x81cally, spectral \xef\xac\x81ltering methods'
<EOS>
b'Henaff et al., 2015'
<EOS>
b'Kipf  Welling, 2017 develop\nconvolution-like operators in the spectral domain, and then perform a series of spectral \xef\xac\x81lters\nby decomposing the graph Laplacian.'
<EOS>
b'Unfortunately, the spectral-based approaches often lead\nto a high computational complex due to the operation of eigenvalue decomposition, especially\nfor a large number of graph nodes.'
<EOS>
b'To alleviate this computation burden, local spectral \xef\xac\x81ltering\nmethods Defferrard et al., 2016 are then proposed by parameterizing the frequency responses as a\nChebyshev polynomial approximation.'
<EOS>
b'Another type of graph CNNs,'
<EOS>
b'namely spatial methods Li et al.,'
<EOS>
b'2016 Niepert et al., 2016, can perform the \xef\xac\x81ltering operation by de\xef\xac\x81ning the spatial structures of\nadjacent vertices.'
<EOS>
b'Various approaches can be employed to aggregate or sort neighboring vertices, such\nas diffusion CNNs Atwood  Towsley, 2016, GraphSAGE Hamilton'
<EOS>
b'et al., 2017, PSCN Niepert\net al., 2016, and NgramCNN'
<EOS>
b'Luo et al., 2017.'
<EOS>
b'From the perspective of data distribution, recently,\nthe Gaussian induced convolution model Jiang et al., 2019 is proposed to disentangle the aggregation\nprocess through encoding adjacent regions with Gaussian mixture model.'
<EOS>
b'Semi-supervised node classi\xef\xac\x81cation Among various graph-related applications'
<EOS>
b', semi-supervised\nnode classi\xef\xac\x81cation has gained increasing attention recently, and various approaches have been\nproposed to deal with this problem, including explicit graph Laplacian regularization and graph-'
<EOS>
b'embedding approaches.'
<EOS>
b'Several classic algorithms with graph Laplacian regularization contain'
<EOS>
b'the label propagation method using Gaussian random \xef\xac\x81elds Zhu et al., 2003'
<EOS>
b', the regularization\nframework by relying on the localglobal consistency Zhou et al., 2004, and the random walk-'
<EOS>
b'based sampling algorithm for acquiring the context information Yang et al., 2016.'
<EOS>
b'To further\naddress scalable semi-supervised learning issues Liu et al., 2012,'
<EOS>
b'the Anchor Graph regularization'
<EOS>
b'approach Liu et al., 2010 is proposed to scale linearly with the number of graph nodes and then\napplied to massive-scale graph datasets.'
<EOS>
b'Several graph convolution network methods'
<EOS>
b'Abu-El-Haija\net al., 2018'
<EOS>
b'Du et al.,'
<EOS>
b'2017'
<EOS>
b'Thekumparampil et al., 2018'
<EOS>
b'Velickovic et al., 2018 Zhuang  Ma,'
<EOS>
b'2018 are then developed to obtain discriminative representations of input graphs.'
<EOS>
b'For example, Kipf'
<EOS>
b'et al.'
<EOS>
b'Kipf  Welling, 2017 proposed a scalable graph CNN model, which can scale linearly in the\nnumber of graph edges and learn graph representations by encoding both local graph structures and'
<EOS>
b'node attributes.'
<EOS>
b'Graph attention networks GAT Velickovic et al., 2018 are proposed to compute\nhidden representations of each node for attending to its neighbors with a self-attention strategy.'
<EOS>
b'By jointly considering the local- and global-consistency information, dual graph convolutional\nnetworks Zhuang  Ma, 2018 are presented to deal with semi-supervised node classi\xef\xac\x81cation.'
<EOS>
b'The\ncritical difference between our proposed GIL and those previous semi-supervised node classi\xef\xac\x81cation'
<EOS>
b'methods is to adopt a graph inference strategy by de\xef\xac\x81ning structure relations on graphs and'
<EOS>
b'then\nleverage a meta optimization mechanism to learn an inference model, which could be the \xef\xac\x81rst time to\nthe best of our knowledge, while the existing graph CNNs take semi-supervised node classi\xef\xac\x81cation\nas a general classi\xef\xac\x81cation task.'
<EOS>
b'3'
<EOS>
b'THE PROPOSED MODEL'
<EOS>
b'3.1 PROBLEM DEFINITION'
<EOS>
b'Formally, we denote an undirecteddirected graph as G  V, E, X , Y, where V  vin\ni1 is the\n\xef\xac\x81nite set of n or V vertices, E  Rnn de\xef\xac\x81nes'
<EOS>
b'the adjacency relationships i.e., edges between\nvertices representing the topology of G,'
<EOS>
b'X  Rnd records the explicitimplicit attributessignals of\nvertices, and Y'
<EOS>
b'Rn is the vertex labels of C classes.'
<EOS>
b'The edge'
<EOS>
b'Eij  Evi, vj  0'
<EOS>
b'if and only if\nvertices vi, vj are not connected, otherwise Eij cid54 0.'
<EOS>
b'The attribute matrix X is attached to the vertex\nset V,'
<EOS>
b'whose i-th row Xvi or Xi represents the attribute of the i-th vertex vi.'
<EOS>
b'It means that vi  V\ncarries a vector of d-dimensional signals.'
<EOS>
b'Associated with each node vi'
<EOS>
b'V'
<EOS>
b', there is a discrete label'
<EOS>
b'yi  1, 2,    , C.'
<EOS>
b'We consider the task of semi-supervised node classi\xef\xac\x81cation over graph data, where only a small\nnumber of vertices are labeled for the model learning, i.e., VLabel'
<EOS>
b'cid28 V. Generally, we have three'
<EOS>
b'node sets a training set Vtr, a validation set Vval, and a testing set Vte.'
<EOS>
b'In the standard protocol\n\n3\n\n\x0cPublished as a conference paper at ICLR 2020\n\nof prior literatures'
<EOS>
b'Yang et al., 2016,'
<EOS>
b'the three node sets share the same label space.'
<EOS>
b'We follow\nbut do not restrict this protocol for our proposed method.'
<EOS>
b'Given the training and validation node'
<EOS>
b'sets, the aim is to predict the node labels of testing nodes by using node attributes as well as edge\nconnections.'
<EOS>
b'A sophisticated machine learning technique used in most existing methods'
<EOS>
b'Kipf'
<EOS>
b'Welling, 2017'
<EOS>
b'Zhou et al., 2004 is to choose the optimal classi\xef\xac\x81er trained on a training set after\nchecking the performance on the validation set.'
<EOS>
b'However, these methods essentially ignore how to\nextract transferable knowledge from these known labeled nodes to unlabeled nodes, as the graph\nstructure itself implies node connectivityreachability.'
<EOS>
b'Moreover, due to the scarcity of labeled\nsamples, the performance of such a classi\xef\xac\x81er is usually not satisfying.'
<EOS>
b'To address these issues,\nwe introduce a meta-learning mechanism Finn et al., 2017 Ravi  Larochelle, 2017'
<EOS>
b'Sung et al.,'
<EOS>
b'2017 to learn to infer node labels on graphs.'
<EOS>
b'Speci\xef\xac\x81cally, the graph structure, between-node path\nreachability, and node attributes are jointly modeled into the learning process.'
<EOS>
b'Our aim is to learn to\ninfer from labeled nodes to unlabeled nodes, so that the learner can perform better on a validation set\nand thus classify a testing set more accurately.'
<EOS>
b'3.2 STRUCTURE RELATION'
<EOS>
b'For convenient inference, we speci\xef\xac\x81cally build a structure relation between two nodes on the topology\ngraph.'
<EOS>
b'The labeled vertices in a training set are viewed as the reference nodes, and their information\ncan be propagated into those unlabeled vertices for improving the label prediction accuracy.'
<EOS>
b'Formally,\ngiven a reference node vi'
<EOS>
b'VLabel, we de\xef\xac\x81ne the score of a query node vj similar to vi as\n\n1\nwhere Gvi and Gvj may be understood as the centralized subgraphs around vi and vj, respectively.'
<EOS>
b'fe, fr, fP are three abstract functions that we explain as follows\n\nsij  frfeGvi , feGvj , fP'
<EOS>
b'vi, vj, E,'
<EOS>
b'Node representation feGvi  Rdv , encodes the local representation of the centralized\nsubgraph Gvi around node vi, and may thus be understood as a local \xef\xac\x81lter function on graphs.'
<EOS>
b'This function should not only take the signals of nodes therein as input, but also consider the\nlocal topological structure of the subgraph for more accurate similarity computation.'
<EOS>
b'To this\nend, we perform the spectral graph convolution on subgraphs to learn discriminative node\nfeatures, analogous to the pixel-level feature extraction from convolution maps of gridded\nimages.'
<EOS>
b'The details of feature extraction fe are described in Section 4.'
<EOS>
b'Path reachability'
<EOS>
b'fP'
<EOS>
b'vi, vj, E  Rdp , represents the characteristics of path reachability\nfrom vi to vj.'
<EOS>
b'As there usually exist multiple traversal paths between two nodes, we choose\nthe function as reachable probabilities of different lengths of walks from vi to vj.'
<EOS>
b'More\ndetails will be introduced in Section 4.'
<EOS>
b'Structure relation frRdv , Rdv , Rdp   R, is a relational function computing the score\nof vj similar to vi.'
<EOS>
b'This function is not exchangeable for different orders of two nodes,\ndue to the asymmetric reachable relationship'
<EOS>
b'fP .'
<EOS>
b'If necessary, we may easily revise it as a\nsymmetry function, e.g., summarizing two traversal directions.'
<EOS>
b'The score function depends\non triple inputs the local representations extracted from the subgraphs w.r.t. feGvi and\nfeGvj , respectively, and the path reachability from vi to vj.'
<EOS>
b'In semi-supervised node classi\xef\xac\x81cation, we take the training node set Vtr as the reference samples, and\nthe validation set Vval as the query samples during the training stage.'
<EOS>
b'Given a query node vj  Vval,\nwe can derive the class similarity score of vj w.r.t'
<EOS>
b'.'
<EOS>
b'the c-th c  1'
<EOS>
b',    , C category by weighting'
<EOS>
b'the reference samples'
<EOS>
b'Cc  vkyvk'
<EOS>
b'c. Formally, we can further revise Eqn. 1 and de\xef\xac\x81ne the\nclass-to-node relationship function as\n\nsCcj  \xcf\x86rFCcvj\n\nwij  feGvi, feGvj ,\n\ncid88'
<EOS>
b'viCc'
<EOS>
b's.t. wij'
<EOS>
b'\xcf\x86wfP vi, vj, E,\n\n3'
<EOS>
b'where the function \xcf\x86w maps a reachable vector fP vi, vj'
<EOS>
b', E into a weight value, and the function \xcf\x86r\ncomputes the similar score between vj and the c-th class nodes.'
<EOS>
b'The normalization factor FCcvj of\nthe c-th category'
<EOS>
b'w.r.t. vj is de\xef\xac\x81ned as\n\n2'
<EOS>
b'4'
<EOS>
b'For the relation function \xcf\x86r and the weight function \xcf\x86w, we may choose some subnetworks to\ninstantiate them in practice.'
<EOS>
b'The detailed implementation of our model can be found in Section 4.'
<EOS>
b'FCcvj'
<EOS>
b'cid80'
<EOS>
b'1\n\n.'
<EOS>
b'viCc'
<EOS>
b'wij'
<EOS>
b'4'
<EOS>
b'Published as a conference paper at ICLR 2020\n\n3.3'
<EOS>
b'INFERENCE LEARNING'
<EOS>
b'According to the class-to-node relationship function in Eqn. 2, given a query node vj, we can obtain\na score vector sCj  sC1j,    , sCC'
<EOS>
b'jcid124  RC after computing the relations to all classes .'
<EOS>
b'The\nindexed category with the maximum score is assumed to be the estimated label.'
<EOS>
b'Thus, we can de\xef\xac\x81ne'
<EOS>
b'the loss function based on cross entropy as follows\n\nL  \n\nyj,c log \xcb\x86yCcj'
<EOS>
b',\n\ncid88'
<EOS>
b'C'
<EOS>
b'cid88'
<EOS>
b'vj'
<EOS>
b'c1'
<EOS>
b'5'
<EOS>
b'6'
<EOS>
b'7'
<EOS>
b'where yj,c is a binary indicator i.e., 0 or 1 of class label c for node vj, and the softmax operation\nis imposed on sCcj, i.e., \xcb\x86yCcj'
<EOS>
b'expsCcj cid80C'
<EOS>
b'k1 expsCkj.'
<EOS>
b'Other error functions may be\nchosen as the loss function, e.g., mean square error.'
<EOS>
b'In the regime of general classi\xef\xac\x81cation, the cross\nentropy loss is a standard one that performs well.'
<EOS>
b'Given a training set'
<EOS>
b'Vtr, we expect that the best performance can be obtained on the validation set'
<EOS>
b'Vval after optimizing the model on Vtr.'
<EOS>
b'Given a trainedpretrained model \xce\x98  fe, \xcf\x86w, \xcf\x86r, we\nperform iteratively gradient updates on the training set Vtr to obtain the new model, formally,\n\n\xce\x98cid48'
<EOS>
b'\xce\x98'
<EOS>
b'\xce\xb1\xce\x98Ltr\xce\x98,\n\nwhere \xce\xb1 is the updating rate.'
<EOS>
b'Note that, in the computation of class scores, since the reference node\nand query node can be both from the training set'
<EOS>
b'Vtr, we set the computation weight'
<EOS>
b'wij  0'
<EOS>
b'if\ni  j in Eqn.'
<EOS>
b'3.'
<EOS>
b'After several iterates of gradient descent on Vtr, we expect a better performance on\nthe validation set Vval, i.e., min'
<EOS>
b'\xce\x98'
<EOS>
b'Lval\xce\x98cid48.'
<EOS>
b'Thus, we can perform the gradient update as follows\n\nwhere \xce\xb2 is the learning rate of meta optimization Finn et al., 2017.'
<EOS>
b'\xce\x98'
<EOS>
b'\xce\x98'
<EOS>
b'\xce\xb2\xce\x98Lval\xce\x98cid48,\n\nDuring the training process, we may perform batch sampling from training nodes and validation\nnodes, instead of taking all one time.'
<EOS>
b'In the testing stage, we may take all training nodes and perform\nthe model update according to Eqn.'
<EOS>
b'6 like the training process.'
<EOS>
b'The updated model is used as the\n\xef\xac\x81nal model and is then fed into Eqn. 2 to infer the class labels for those query nodes.'
<EOS>
b'4 MODULES'
<EOS>
b'In this section, we instantiate all modules i.e., functions of the aforementioned structure relation.'
<EOS>
b'The implementation details can be found in the following.'
<EOS>
b'Node Representation feGvi The local representation at vertex vi can be extracted by performing\nthe graph convolution operation on subgraph Gvi .'
<EOS>
b'Similar to gridded imagesvideos, on which local\nconvolution kernels are de\xef\xac\x81ned as multiple lattices with various receptive \xef\xac\x81elds, the spectral graph\nconvolution is used to encode the local representations of an input graph in our work.'
<EOS>
b'Given a graph sample G  V, E, X , the normalized graph Laplacian matrix is L'
<EOS>
b'In \nD12ED12  U\xce\x9bUT , with a diagonal matrix of its eigenvalues'
<EOS>
b'\xce\x9b.'
<EOS>
b'The spectral graph convo-'
<EOS>
b'lution can be de\xef\xac\x81ned as the multiplication of signal X with a \xef\xac\x81lter g\xce\xb8\xce\x9b'
<EOS>
b'diag\xce\xb8 parameterized\nby \xce\xb8 in the Fourier domain'
<EOS>
b'convX'
<EOS>
b'g\xce\xb8L  X'
<EOS>
b'Ug\xce\xb8\xce\x9bUT X , where parameter \xce\xb8  Rn\nis a vector of Fourier coef\xef\xac\x81cients.'
<EOS>
b'To reduce the computational complexity and obtain the local\ninformation, we use an approximate local \xef\xac\x81lter of the Chebyshev polynomial Defferrard et al.,\n2016, g\xce\xb8\xce\x9b  cid80K1\nk0'
<EOS>
b'\xce\xb8kTk\xcb\x86\xce\x9b, where parameter \xce\xb8  RK is a vector of Chebyshev coef\xef\xac\x81cients'
<EOS>
b'and Tk\xcb\x86\xce\x9b  Rnn is the Chebyshev polynomial of order k evaluated at \xcb\x86\xce\x9b'
<EOS>
b'2\xce\x9b\xce\xbbmax'
<EOS>
b'In,\na diagonal matrix of scaled eigenvalues.'
<EOS>
b'The graph \xef\xac\x81ltering operation can then be expressed as\ng\xce\xb8\xce\x9b  X  cid80K1'
<EOS>
b'k0 \xce\xb8kTk\xcb\x86LX , where Tk\xcb\x86L'
<EOS>
b'Rnn is the Chebyshev polynomial of order k'
<EOS>
b'evaluated at the scaled Laplacian \xcb\x86L'
<EOS>
b'2L\xce\xbbmax'
<EOS>
b'In.'
<EOS>
b'Further, we can construct multi-scale receptive\n\xef\xac\x81elds for each vertex based on the Laplacian matrix L, where each receptive \xef\xac\x81eld records hopping\nneighborhood relationships around the reference vertex vi, and forms a local centralized subgraph.'
<EOS>
b'Path Reachability'
<EOS>
b'fP'
<EOS>
b'vi, vj,'
<EOS>
b'E'
<EOS>
b'Here we compute the probabilities of paths from vertex i to vertex'
<EOS>
b'j by employing random walks on graphs, which refers to traversing the graph from vi to vj according\nto the probability matrix P.'
<EOS>
b'For the input graph G with n vertices, the random-walk transition matrix'
<EOS>
b'5'
<EOS>
b'Published as a conference paper at ICLR 2020'
<EOS>
b'Datasets Nodes\n2,708'
<EOS>
b'Cora\n3,327'
<EOS>
b'Citeseer\n19,717'
<EOS>
b'Pubmed\nNELL'
<EOS>
b'65,755'
<EOS>
b'Edges\n5,429\n4,732\n44,338'
<EOS>
b'266,144'
<EOS>
b'Classes'
<EOS>
b'7\n6'
<EOS>
b'3'
<EOS>
b'210'
<EOS>
b'Features'
<EOS>
b'1,433\n3,703\n500'
<EOS>
b'5,414'
<EOS>
b'Label Rates\n0.052'
<EOS>
b'0.036'
<EOS>
b'0.003'
<EOS>
b'0.001'
<EOS>
b'Table 1'
<EOS>
b'The properties especially for label rate of various graph datasets used for the semi-supervised\nclassi\xef\xac\x81cation task.'
<EOS>
b'can be de\xef\xac\x81ned as P  D1E, where D  Rnn is the diagonal degree matrix with Dii  cid80'
<EOS>
b'That is to say, each element Pij is the probability of going from vertex i to vertex j in one step.'
<EOS>
b'i Eij.'
<EOS>
b'The sequence of nodes from vertex i to vertex j is a random walk on the graph, which can be modeled\nas a classical Markov chain by considering the set of graph vertices.'
<EOS>
b'To represent this formulation,\nwe show that P t'
<EOS>
b'ij is the probability of getting from vertex vi to vertex vj in t steps.'
<EOS>
b'This fact is easily\nexhibited by considering a t-step path from vertex vi to vertex vj as \xef\xac\x81rst taking a single step to some\nvertex h, and then taking t  1 steps to vj.'
<EOS>
b'The transition probability P t in t steps can be formulated\nas\n\nP t'
<EOS>
b'ij'
<EOS>
b'PihP t1'
<EOS>
b'h,j\n\n\n\n\n\n\nPij'
<EOS>
b'cid88'
<EOS>
b'h'
<EOS>
b'if t'
<EOS>
b'1'
<EOS>
b'if t  1 ,'
<EOS>
b'where each matrix entry P t\nsteps.'
<EOS>
b'Finally, the node reachability from vi to vj can be written as a dp-dimensional vector'
<EOS>
b'ij denotes the probability of starting at vertex i and ending at vertex j in t'
<EOS>
b'ij, . . . , P dp\nij ,\nwhere dp refers to the step length of the longest path from vi to vj.'
<EOS>
b'fP'
<EOS>
b'vi, vj, E  Pij,'
<EOS>
b'P 2\n\nClass-to-Node Relationship sCcj'
<EOS>
b'To de\xef\xac\x81ne the node relationship sij from vi to vj, we simulta-'
<EOS>
b'neously consider the property of path reachability'
<EOS>
b'fP vi, vj, E, local representations feGvi, and\nfeGvj  of nodes'
<EOS>
b'vi, vj.'
<EOS>
b'The function \xcf\x86wfP vi, vj, E in Eqn. 3, which is to map the reachable\nvector'
<EOS>
b'fP vi, vj, E  Rdp into a weight value, can be implemented with two 16-dimensional fully\nconnected layers in our experiments.'
<EOS>
b'The computed value wij can be further used to weight the\nlocal features at node vi, feGvi  Rdv .'
<EOS>
b'For obtaining the similar score between vj and the c-th\nclass nodes Cc in Eqn. 2, we perform a concatenation of two input features, where one refers to the\nweighted features of vertex vi, and another is the local features of vertex vj.'
<EOS>
b'One fully connected\nlayer'
<EOS>
b'w.r.t. \xcf\x86r with C-dimensions is \xef\xac\x81nally adopted to obtain the relation regression score.'
<EOS>
b'8\n\n9'
<EOS>
b'5'
<EOS>
b'EXPERIMENTS'
<EOS>
b'5.1 EXPERIMENTAL SETTINGS'
<EOS>
b'We evaluate our proposed GIL method on three citation network datasets Cora, Citeseer, Pubmed Sen\net al., 2008, and one knowledge graph NELL dataset Carlson et al., 2010.'
<EOS>
b'The statistical properties\nof graph data are summarized in Table 1.'
<EOS>
b'Following the previous protocol in Kipf  Welling, 2017'
<EOS>
b'Zhuang  Ma, 2018'
<EOS>
b', we split the graph data into a training set, a validation set, and a testing set.'
<EOS>
b'Taking into account the graph convolution and pooling modules, we may alternately stack them into\na multi-layer Graph convolutional network.'
<EOS>
b'The GIL model consists of two graph convolution layers,\neach of which is followed by a mean-pooling layer, a class-to-node relationship regression module,\nand a \xef\xac\x81nal softmax layer.'
<EOS>
b'We have given the detailed con\xef\xac\x81guration of the relationship regression\nmodule in the class-to-node relationship of Section 4.'
<EOS>
b'The parameter dp in Eqn.'
<EOS>
b'9 is set to the\nmean length of between-node reachability paths in the input graph.'
<EOS>
b'The channels of the 1-st and\n2-nd convolutional layers are set to 128 and 256, respectively.'
<EOS>
b'The scale of the respective \xef\xac\x81led is 2\nin both convolutional layers.'
<EOS>
b'The dropout rate is set to 0.5 in the convolution and fully connected\nlayers to avoid over-\xef\xac\x81tting, and the ReLU unit is leveraged as a nonlinear activation function.'
<EOS>
b'We\npre-train our proposed GIL model for 200 iterations with the training set, where its initial learning\nrate, decay factor, and momentum are set to 0.05, 0.95, and 0.9, respectively.'
<EOS>
b'Here we train the GIL\nmodel using the stochastic gradient descent method with the batch size of 100.'
<EOS>
b'We further improve\nthe inference learning capability of the GIL model for 1200 iterations with the validation set, where\nthe meta-learning rates \xce\xb1 and \xce\xb2 are both set to 0.001.'
<EOS>
b'6'
<EOS>
b'Published as a conference paper at ICLR 2020'
<EOS>
b'5.2 COMPARISON WITH STATE-OF-THE-ARTS'
<EOS>
b'We compare the GIL approach with several state-of-the-art methods Monti et al.,'
<EOS>
b'2017 Kipf'
<EOS>
b'Welling, 2017'
<EOS>
b'Zhou et al., 2004'
<EOS>
b'Zhuang  Ma, 2018 over four graph datasets, including Cora,\nCiteseer, Pubmed, and NELL.'
<EOS>
b'The classi\xef\xac\x81cation accuracies for all methods are reported in Table 2.'
<EOS>
b'Our proposed GIL can signi\xef\xac\x81cantly outperform these graph'
<EOS>
b'Laplacian regularized methods on four\ngraph datasets, including Deep walk Zhou et al., 2004, modularity clustering Brandes et al., 2008'
<EOS>
b',\nGaussian \xef\xac\x81elds Zhu et al., 2003, and graph embedding Yang et al., 2016 methods.'
<EOS>
b'For example,\nwe can achieve much higher performance than the deepwalk method Zhou et al., 2004, e.g., 43.2\nvs 74.1 on the Citeseer dataset, 65.3 vs 83.1 on the Pubmed dataset, and 58.1 vs 78.9 on the\nNELL dataset.'
<EOS>
b'We \xef\xac\x81nd that the graph embedding method Yang et al., 2016, which has considered\nboth label information and graph structure during sampling, can obtain lower accuracies than our\nproposed GIL by 9.4 on the Citeseer dataset and 10.5 on the Cora dataset, respectively.'
<EOS>
b'This\nindicates that our proposed GIL can better optimize structure relations and thus improve the network\ngeneralization.'
<EOS>
b'We further compare our proposed GIL with several existing deep graph embedding\nmethods, including graph attention network Velickovic et al., 2018, dual graph convolutional'
<EOS>
b'networks Zhuang  Ma, 2018, topology adaptive graph convolutional networks Du et al., 2017,\nMulti-scale graph convolution Abu-El-Haija et al., 2018,'
<EOS>
b'etc.'
<EOS>
b'For example, our proposed GIL\nachieves a very large gain, e.g., 86.2 vs 83.3 Du et al., 2017 on the Cora dataset, and 78.9'
<EOS>
b'vs 66.0 Kipf  Welling, 2017 on the NELL dataset.'
<EOS>
b'We evaluate our proposed GIL method on\na large graph dataset with a lower label rate, which can signi\xef\xac\x81cantly outperform existing baselines\non the Pubmed dataset 3.1 over DGCN Zhuang  Ma, 2018, 4.1 over classic GCN Kipf'
<EOS>
b'Welling, 2017 and TAGCN Du et al., 2017, 3.2 over AGNN Thekumparampil et al., 2018, and\n3.6 over N-GCN Abu-El-Haija et al., 2018.'
<EOS>
b'It demonstrates that our proposed GIL performs very'
<EOS>
b'well on various graph datasets by building the graph inference learning process, where the limited\nlabel information and graph structures can be well employed in the predicted framework.'
<EOS>
b'Table 2 Performance comparisons of semi-supervised classi\xef\xac\x81cation methods.'
<EOS>
b'Methods'
<EOS>
b'Clustering Brandes et al., 2008'
<EOS>
b'DeepWalk'
<EOS>
b'Zhou et al., 2004'
<EOS>
b'Gaussian Zhu et al., 2003'
<EOS>
b'G-embedding Yang et al., 2016'
<EOS>
b'DCNN Atwood  Towsley, 2016'
<EOS>
b'GCN Kipf'
<EOS>
b'Welling, 2017'
<EOS>
b'MoNet Monti et al., 2017'
<EOS>
b'N-GCN Abu-El-Haija et al., 2018'
<EOS>
b'GAT Velickovic et al., 2018'
<EOS>
b'AGNN Thekumparampil et al., 2018'
<EOS>
b'TAGCN Du et al., 2017'
<EOS>
b'DGCN Zhuang  Ma, 2018'
<EOS>
b'Our GIL'
<EOS>
b'Cora\n59.5\n67.2'
<EOS>
b'68.0\n75.7'
<EOS>
b'76.8'
<EOS>
b'81.5'
<EOS>
b'81.7'
<EOS>
b'83.0'
<EOS>
b'83.0'
<EOS>
b'83.1'
<EOS>
b'83.3\n83.5'
<EOS>
b'86.2'
<EOS>
b'Citeseer'
<EOS>
b'60.1'
<EOS>
b'43.2'
<EOS>
b'45.3'
<EOS>
b'64.7'
<EOS>
b'-\n70.3'
<EOS>
b'-'
<EOS>
b'72.2'
<EOS>
b'72.5'
<EOS>
b'71.7'
<EOS>
b'72.5'
<EOS>
b'72.6'
<EOS>
b'74.1'
<EOS>
b'Pubmed NELL\n70.7'
<EOS>
b'65.3'
<EOS>
b'63.0'
<EOS>
b'77.2'
<EOS>
b'73.0'
<EOS>
b'79.0'
<EOS>
b'78.8\n79.5\n79.0'
<EOS>
b'79.9'
<EOS>
b'79.0'
<EOS>
b'80.0'
<EOS>
b'83.1'
<EOS>
b'21.8'
<EOS>
b'58.1\n26.5'
<EOS>
b'61.9\n-\n66.0\n-\n-\n-\n-\n-\n74.2\n78.9'
<EOS>
b'5.3 ANALYSIS'
<EOS>
b'Meta-optimization'
<EOS>
b'As can be seen in Table 3, we report the classi\xef\xac\x81cation accuracies of\nsemi-supervised classi\xef\xac\x81cation with several variants of our proposed GIL and the classical GCN\nmethod'
<EOS>
b'Kipf  Welling, 2017 when evaluating them on the Cora dataset.'
<EOS>
b'For analyzing the perfor-\nmance improvement of our proposed GIL with the graph inference learning process, we report the\nclassi\xef\xac\x81cation accuracies of GCN Kipf  Welling, 2017 and our proposed GIL on the Cora dataset\nunder two different situations, including only learning with the training set Vtr and with jointly\nlearning on a training set Vtr and a validation set Vval.'
<EOS>
b'GCN w jointly learning on Vtr  Vval\nachieves a better result than GCN w learning on Vtr by 3.6, which demonstrates that the network\nperformance can be improved by employing validation samples.'
<EOS>
b'When using structure relations,\nGIL w learning on Vtr obtains an improvement of 1.9 over GCN w learning on Vtr, which\ncan be attributed to the building connection between nodes.'
<EOS>
b'The meta-optimization strategy GIL w\nmeta-training from Vtr  Vval vs GIL w learning on Vtr has a gain of 2.9, which indicates\nthat a good inference capability can be learnt through meta-optimization.'
<EOS>
b'It is worth noting that, GIL\nadopts a meta-optimization strategy to learn the inference model, which is a process of migrating\n\n7\n\n\x0cPublished as a conference paper at ICLR 2020\n\nfrom a training set to a validation set.'
<EOS>
b'In other words, the validation set is only used to teach the\nmodel itself how to transfer to unseen data.'
<EOS>
b'In contrast, the conventional methods often employ a\nvalidation set to tune parameters of a certain model of interest.'
<EOS>
b'Table 3 Performance comparisons with several GIL variants and the classical GCN method on the Cora dataset.'
<EOS>
b'GCN Kipf'
<EOS>
b'Welling, 2017'
<EOS>
b'Methods'
<EOS>
b'GIL'
<EOS>
b'GILmean pooling'
<EOS>
b'GIL2 conv.'
<EOS>
b'layers'
<EOS>
b'w learning on Vtr'
<EOS>
b'w jointly learning on Vtr  Vval\nw learning on Vtr\nw'
<EOS>
b'meta-train Vtr  Vval'
<EOS>
b'w 1 conv.'
<EOS>
b'layer'
<EOS>
b'w 2 conv.'
<EOS>
b'layers\nw 3 conv.'
<EOS>
b'layers'
<EOS>
b'w max-pooling'
<EOS>
b'w mean'
<EOS>
b'pooling'
<EOS>
b'Acc.'
<EOS>
b'81.4\n84.0'
<EOS>
b'83.3'
<EOS>
b'86.2'
<EOS>
b'84.5\n86.2\n85.4\n85.2\n86.2'
<EOS>
b'Network settings'
<EOS>
b'We explore the effectiveness of our proposed GIL with the same mean'
<EOS>
b'pooling\nmechanism, but with different numbers of convolutional layers, i.e., GIL'
<EOS>
b'mean pooling with one,\ntwo, and three convolutional layers, respectively.'
<EOS>
b'As can be seen in Table 3, the proposed GIL with\ntwo convolutional layers can obtain a better performance on the Cora data than the other two network\nsettings i.e., GIL with one or three convolutional layers.'
<EOS>
b'For example, the performance of GIL w\n1 conv.'
<EOS>
b'layer  mean pooling is slightly decreased by 1.7 over GIL w 2 conv.'
<EOS>
b'layers  mean\npooling on the Cora dataset.'
<EOS>
b'Furthermore, we report the classi\xef\xac\x81cation results of our proposed GIL\nby using mean and max-pooling mechanisms, respectively.'
<EOS>
b'GIL with mean pooling'
<EOS>
b'i.e., GIL w'
<EOS>
b'2 conv layers  mean pooling can get a better result than the GIL method with max-pooling i.e.,'
<EOS>
b'GIL w 2'
<EOS>
b'conv layers  max-pooling,'
<EOS>
b'e.g., 86.2 vs 85.2 on the Cora graph dataset.'
<EOS>
b'The reason\nmay be that the graph network with two convolutional layers and the mean pooling mechanism can\nobtain the optimal graph embeddings, but when increasing the network layers, more parameters of a\ncertain graph model need to be optimized, which may lead to the over-\xef\xac\x81tting issue.'
<EOS>
b'In\xef\xac\x82uence of different between-node steps'
<EOS>
b'We compare the classi\xef\xac\x81cation performance within\ndifferent between-node steps for our proposed GIL and GCN Kipf  Welling, 2017, as illustrated in\nFig.'
<EOS>
b'2a.'
<EOS>
b'The length of between-node steps can be computed with the shortest path between reference\nnodes and query nodes.'
<EOS>
b'When the step between nodes is smaller, both GIL and GCN methods can\npredict the category information for a small part of unlabeled nodes in the testing set.'
<EOS>
b'The reason\nmay be that the node category information may be disturbed by its nearest neighboring nodes with\ndifferent labels and fewer nodes are within 1 or 2 steps in the testing set.'
<EOS>
b'The GIL and GCN methods\ncan infer the category information for a part of unlabeled nodes by adopting node attributes, when\ntwo nodes are not connected in the graph i.e., step.'
<EOS>
b'By increasing the length of reachability\npath, the inference process of the GIL method would become dif\xef\xac\x81cult and more graph structure'
<EOS>
b'information may be employed in the predicted process.'
<EOS>
b'GIL can outperform the classic GCN by\nanalyzing the accuracies within different between-node steps, which indicates that our proposed GIL\nhas a better reference capability than GCN by using the meta-optimization mechanism from training\nnodes to validation nodes.'
<EOS>
b'a\n\nb\n\nFigure 2 a Performance comparisons within different between-node steps on the Cora dataset.'
<EOS>
b'The accuracy\nequals to the number of correctly classi\xef\xac\x81ed nodes divided by all testing samples, and is accumulated from step 1\nto step k. b Performance comparisons with different label rates on the Pubmed dataset.'
<EOS>
b'8'
<EOS>
b'1357911step0.00.20.40.60.8accuracyour GILGCNlabel'
<EOS>
b'rate0.300.600.901.201.501.80GCN0.7920.7970.8050.8240.8290.834GILours0.8170.8240.8310.8360.8380.8421x2x3x4x5x6x77.079.081.083.085.01x2x3x4x5x6xGCNGILoursLabel rates Accuracy'
<EOS>
b'Published as a conference paper at ICLR 2020\n\nIn\xef\xac\x82uence of different label rates'
<EOS>
b'We also explore the performance comparisons of the GIL method\nwith different label rates, and the detailed results on the Pubmed dataset can be shown in Fig.'
<EOS>
b'2b.'
<EOS>
b'When label rates increase by multiplication, the performances of GIL and GCN are improved, but the\nrelative gain becomes narrow.'
<EOS>
b'The reason is that, the reachable path lengths between unlabeled nodes\nand labeled nodes will be reduced with the increase of labeled nodes, which will weaken the effect\nof inference learning.'
<EOS>
b'In the extreme case, labels of unlabeled nodes could be determined by those\nneighbors with the 1  2 step reachability.'
<EOS>
b'In summary, our proposed GIL method prefers small ratio\nlabeled nodes on the semi-supervised node classi\xef\xac\x81cation task.'
<EOS>
b'Inference learning process Classi\xef\xac\x81cation errors of different epochs on the validation set of the\nCora dataset can be illustrated in Fig.'
<EOS>
b'3.'
<EOS>
b'Classi\xef\xac\x81cation errors are rapidly decreasing as the number of\niterations increases from the beginning to 400 iterations, while they are with a slow descent from 400\niterations to 1200 iterations.'
<EOS>
b'It demonstrates that the learned knowledge from the training samples\ncan be transferred for inferring node category information from these reference labeled nodes.'
<EOS>
b'The\nperformance of semi-supervised classi\xef\xac\x81cation can be further increased by improving the generalized\ncapability of the Graph CNN model.'
<EOS>
b'Table 4 Performance comparisons with different mod-\nules on the Cora dataset, where fe, fP , and fr denote\nnode representation, path reachability, and structure'
<EOS>
b're-'
<EOS>
b'lation, respectively.'
<EOS>
b'fP'
<EOS>
b'fr\nfe'
<EOS>
b'-\n-\n-\ncid88 -\n-\ncid88'
<EOS>
b'cid88 -'
<EOS>
b'cid88 cid88'
<EOS>
b'cid88'
<EOS>
b'Acc.'
<EOS>
b'56.0\n81.5\n85.0'
<EOS>
b'86.2'
<EOS>
b'Figure 3 Classi\xef\xac\x81cation errors of different itera-'
<EOS>
b'tions on the validation set of the Cora dataset.'
<EOS>
b'Module analysis'
<EOS>
b'We evaluate the effectiveness of different modules within our proposed GIL\nframework, including node representation fe, path reachability fP , and structure relation fr.'
<EOS>
b'Note'
<EOS>
b'that the last one fr de\xef\xac\x81nes on the former two ones, so we consider the cases in Table 4 by adding\nmodules.'
<EOS>
b'When not using all modules, only original attributes of nodes are used to predict labels.'
<EOS>
b'The case of only using fe belongs to the GCN method, which can achieve 81.5 on the Cora dataset.'
<EOS>
b'The large gain of using the relation module fr i.e., from 81.5 to 85.0 may be contributed to the\nability of inference learning on attributes as well as local topology structures which are implicitly\nencoded in fe.'
<EOS>
b'The path information fP can further boost the performance by 1.2, e.g., 86.2 vs\n85.0.'
<EOS>
b'It demonstrates that three different modules of our method can improve the graph inference\nlearning capability.'
<EOS>
b'Computational complexity'
<EOS>
b'For the computational complexity of our proposed GIL, the cost is\nmainly spent on the computations of node representation, between-node reachability, and class-to-'
<EOS>
b'node relationship, which are about Ontr'
<EOS>
b'nte'
<EOS>
b'e'
<EOS>
b'din'
<EOS>
b'dout, Ontr'
<EOS>
b'nte  e  P , and'
<EOS>
b'Ontr'
<EOS>
b'nted2'
<EOS>
b'out, respectively.'
<EOS>
b'ntr and nte refer to the numbers of training and testing nodes, din\nand dout denote the input and output dimensions of node representation'
<EOS>
b', e is about the average degree\nof graph node, and P is the step length of node reachability.'
<EOS>
b'Compared with those classic Graph'
<EOS>
b'CNNs Kipf  Welling, 2017, our proposed GIL has a slightly higher cost due to an extra inference\nlearning process, but can complete the testing stage with several seconds on these benchmark datasets.'
<EOS>
b'6 CONCLUSION'
<EOS>
b'In this work, we tackled the semi-supervised node classi\xef\xac\x81cation task with a graph inference learning'
<EOS>
b'method, which can better predict the categories of these unlabeled nodes in an end-to-end framework.'
<EOS>
b'We can build a structure relation for obtaining the connection between any two graph nodes, where\nnode attributes, between-node paths, and graph structure information can be encapsulated together.'
<EOS>
b'For better capturing the transferable knowledge, our method further learns to transfer the mined\nknowledge from the training samples to the validation set, \xef\xac\x81nally boosting the prediction accuracy\nof the labels of unlabeled nodes in the testing set.'
<EOS>
b'The extensive experimental results demonstrate\nthe effectiveness of our proposed GIL for solving the semi-supervised learning problem, even in\nthe few-shot paradigm.'
<EOS>
b'In the future, we would extend the graph inference method to handle more\ngraph-related tasks, such as graph generation and social network analysis.'
<EOS>
b'9'
<EOS>
b'the number of iterations error \x0cPublished as a conference paper at ICLR 2020'
<EOS>
b'ACKNOWLEDGMENT'
<EOS>
b'This work was supported by the National Natural Science Foundation of China'
<EOS>
b'Nos.'
<EOS>
b'61972204,'
<EOS>
b'61906094, U1713208, the Natural Science Foundation of Jiangsu Province Grant Nos.'
<EOS>
b'BK20191283\nand BK20190019, and Tencent AI Lab Rhino-Bird'
<EOS>
b'Focused'
<EOS>
b'Research Program'
<EOS>
b'No.'
<EOS>
b'JR201922.'
<EOS>
b'REFERENCES\n\n2001, 2016.'
<EOS>
b'Sami Abu-El-Haija, Amol Kapoor, Bryan Perozzi, and Joonseok Lee.'
<EOS>
b'N-gcn Multi-scale graph\n\nconvolution for semi-supervised node classi\xef\xac\x81cation.'
<EOS>
b'arXiv preprint arXiv1802.08888, 2018.'
<EOS>
b'James Atwood and Don Towsley.'
<EOS>
b'Diffusion-convolutional neural networks.'
<EOS>
b'In NeurIPS, pp.'
<EOS>
b'1993'
<EOS>
b'Karsten M Borgwardt, Hans-Peter Kriegel, SVN Vishwanathan, and Nicol N Schraudolph.'
<EOS>
b'Graph ker-'
<EOS>
b'nels for disease outcome prediction from protein-protein interaction networks.'
<EOS>
b'Paci\xef\xac\x81c Symposium\non Biocomputing Paci\xef\xac\x81c Symposium on Biocomputing, pp. 415, 2007.'
<EOS>
b'Ulrik Brandes, Daniel Delling, Marco Gaertler, Robert Gorke, Martin Hoefer, Zoran Nikoloski,\nand Dorothea Wagner.'
<EOS>
b'On modularity clustering.'
<EOS>
b'IEEE transactions on knowledge and data\nengineering, 202172188, 2008.'
<EOS>
b'Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr Settles, Estevam R. Hruschka Jr., and Tom M.\n\nMitchell.'
<EOS>
b'Toward an architecture for never-ending language learning.'
<EOS>
b'In AAAI, 2010.'
<EOS>
b'Micha\xc3\xabl Defferrard, Xavier Bresson, and Pierre Vandergheynst.'
<EOS>
b'Convolutional neural networks on\n\ngraphs with fast localized spectral \xef\xac\x81ltering.'
<EOS>
b'In NeurIPS, pp. 38443852, 2016.'
<EOS>
b'Jian Du, Shanghang Zhang, Guanhang Wu, Jos\xc3\xa9 MF Moura, and Soummya Kar.'
<EOS>
b'Topology adaptive\n\ngraph convolutional networks.'
<EOS>
b'arXiv preprint arXiv1710.10370, 2017.'
<EOS>
b'Chelsea Finn, Pieter Abbeel, and Sergey Levine.'
<EOS>
b'Model-agnostic meta-learning for fast adaptation of\n\ndeep networks.'
<EOS>
b'In ICML, pp.'
<EOS>
b'11261135, 2017.'
<EOS>
b'Will Hamilton, Zhitao Ying, and Jure Leskovec.'
<EOS>
b'Inductive representation learning on large graphs.'
<EOS>
b'In\n\nNeurIPS, pp. 10251035, 2017.'
<EOS>
b'Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.'
<EOS>
b'Deep residual learning for image\n\nrecognition.'
<EOS>
b'In CVPR, pp. 770778, 2016.'
<EOS>
b'Mikael Henaff, Joan Bruna, and Yann LeCun.'
<EOS>
b'Deep convolutional networks on graph-structured data.'
<EOS>
b'arXiv preprint arXiv1506.05163, 2015.'
<EOS>
b'Jiatao Jiang, Zhen Cui, Chunyan Xu, and Jian Yang.'
<EOS>
b'Gaussian-induced convolution for graphs.'
<EOS>
b'In\n\nAAAI, volume 33, pp. 40074014, 2019.'
<EOS>
b'Hisashi Kashima, Koji Tsuda, and Akihiro Inokuchi.'
<EOS>
b'Marginalized kernels between labeled graphs.'
<EOS>
b'In ICML, pp. 321328, 2003.'
<EOS>
b'Thomas N. Kipf and Max Welling.'
<EOS>
b'Semi-supervised classi\xef\xac\x81cation with graph convolutional networks.'
<EOS>
b'In ICLR, 2017.'
<EOS>
b'networks.'
<EOS>
b'ICLR, 2016.'
<EOS>
b'learning.'
<EOS>
b'In ICML, 2010.'
<EOS>
b'Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel.'
<EOS>
b'Gated graph sequence neural'
<EOS>
b'Wei Liu, Junfeng He, and Shih-Fu Chang.'
<EOS>
b'Large graph construction for scalable semi-supervised'
<EOS>
b'Wei Liu, Jun Wang, and Shih-Fu Chang.'
<EOS>
b'Robust and scalable graph-based semisupervised learning.'
<EOS>
b'Proceedings of the IEEE, 100926242638, 2012.'
<EOS>
b'Zhiling Luo, Ling Liu, Jianwei Yin, Ying Li, and Zhaohui Wu.'
<EOS>
b'Deep learning of graphs with ngram\nconvolutional neural networks.'
<EOS>
b'IEEE Transactions on Knowledge and Data Engineering, 2910\n21252139, 2017.'
<EOS>
b'10'
<EOS>
b'Published as a conference paper at ICLR 2020'
<EOS>
b'Federico Monti, Davide Boscaini, Jonathan Masci, Emanuele Rodola, Jan Svoboda, and Michael M\nBronstein.'
<EOS>
b'Geometric deep learning on graphs and manifolds using mixture model cnns.'
<EOS>
b'In CVPR,\npp. 51155124, 2017.'
<EOS>
b'Christopher Morris, Kristian Kersting, and Petra Mutzel.'
<EOS>
b'Glocalized weisfeiler-lehman graph kernels\n\nGlobal-local feature maps of graphs.'
<EOS>
b'In ICDM, pp.'
<EOS>
b'327336.'
<EOS>
b'IEEE, 2017.'
<EOS>
b'Mathias Niepert, Mohamed Ahmed, and Konstantin Kutzkov.'
<EOS>
b'Learning convolutional neural networks\n\nfor graphs.'
<EOS>
b'In ICML, pp. 20142023, 2016.'
<EOS>
b'Francesco Orsini, Daniele Baracchi, and Paolo Frasconi.'
<EOS>
b'Shift aggregate extract networks.'
<EOS>
b'arXiv\n\npreprint arXiv1703.05537, 2017.'
<EOS>
b'Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry Winograd.'
<EOS>
b'The pagerank citation ranking'
<EOS>
b'Bringing order to the web.'
<EOS>
b'Technical Report 1999-66, 1999.'
<EOS>
b'Shirui Pan, Ruiqi Hu, Guodong Long, Jing Jiang, Lina Yao, and Chengqi Zhang.'
<EOS>
b'Adversarially\n\nregularized graph autoencoder for graph embedding.'
<EOS>
b'In IJCAI, pp. 26092615, 2018.'
<EOS>
b'Sachin Ravi and Hugo Larochelle.'
<EOS>
b'Optimization as a model for few-shot learning.'
<EOS>
b'In ICLR, 2017.'
<EOS>
b'Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-Rad.'
<EOS>
b'Collective classi\xef\xac\x81cation in network data.'
<EOS>
b'AI magazine, 2939393, 2008.'
<EOS>
b'Nino Shervashidze, SVN Vishwanathan, Tobias Petri, Kurt Mehlhorn, and Karsten Borgwardt.\nEf\xef\xac\x81cient graphlet kernels for large graph comparison.'
<EOS>
b'In Arti\xef\xac\x81cial Intelligence and Statistics, pp.\n488495, 2009.'
<EOS>
b'Flood Sung, Li Zhang, Tao Xiang, Timothy Hospedales, and Yongxin Yang.'
<EOS>
b'Learning to learn\n\nMeta-critic networks for sample ef\xef\xac\x81cient learning.'
<EOS>
b'arXiv preprint arXiv1706.09529, 2017.'
<EOS>
b'Kiran K Thekumparampil, Chong Wang,'
<EOS>
b'Sewoong'
<EOS>
b'Oh, and Li-Jia Li.'
<EOS>
b'Attention-based graph neural\n\nnetwork for semi-supervised learning.'
<EOS>
b'arXiv preprint arXiv1803.03735, 2018.'
<EOS>
b'Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li\xc3\xb2, and Yoshua\n\nBengio.'
<EOS>
b'Graph attention networks.'
<EOS>
b'ICLR, 2018.'
<EOS>
b'Danfei Xu, Yuke Zhu, Christopher B Choy, and Li Fei-Fei.'
<EOS>
b'Scene graph generation by iterative\n\nmessage passing.'
<EOS>
b'In CVPR, pp.'
<EOS>
b'54105419, 2017.'
<EOS>
b'Pinar Yanardag and SVN Vishwanathan.'
<EOS>
b'Deep graph kernels.'
<EOS>
b'In SIGKDD, pp. 13651374, 2015.'
<EOS>
b'Zhilin Yang, William W Cohen, and Ruslan Salakhutdinov.'
<EOS>
b'Revisiting semi-supervised learning with\n\ngraph embeddings.'
<EOS>
b'ICML, 2016.'
<EOS>
b'Bing Yu, Haoteng Yin, and Zhanxing Zhu.'
<EOS>
b'Spatio-temporal graph convolutional networks A deep\n\nlearning framework for traf\xef\xac\x81c forecasting.'
<EOS>
b'In IJCAI, pp. 36343640, 2018.'
<EOS>
b'Tong Zhang, Zhen Cui, Chunyan Xu, Wenming Zheng, and Jian Yang.'
<EOS>
b'Variational pathway reasoning\n\nfor eeg emotion recognition.'
<EOS>
b'In AAAI, 2020.'
<EOS>
b'Wenting Zhao, Zhen Cui, Chunyan Xu, Chengzheng Li, Tong Zhang, and Jian Yang.'
<EOS>
b'Hashing graph\n\nconvolution for node classi\xef\xac\x81cation.'
<EOS>
b'In CIKM, 2019.'
<EOS>
b'Dengyong Zhou, Olivier Bousquet, Thomas N Lal, Jason Weston, and Bernhard Sch\xc3\xb6lkopf.'
<EOS>
b'Learning\n\nwith local and global consistency.'
<EOS>
b'In NeurIPS, pp. 321328, 2004.'
<EOS>
b'Xiaojin Zhu, Zoubin Ghahramani, and John D Lafferty.'
<EOS>
b'Semi-supervised learning using gaussian\n\n\xef\xac\x81elds and harmonic functions.'
<EOS>
b'In ICML, pp.'
<EOS>
b'912919, 2003.'
<EOS>
b'Chenyi Zhuang and Qiang Ma.'
<EOS>
b'Dual graph convolutional networks for graph-based semi-supervised\n\nclassi\xef\xac\x81cation.'
<EOS>
b'In WWW, pp. 499508, 2018.'
<EOS>
b'11'
<EOS>
