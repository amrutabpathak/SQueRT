b'Published as a conference paper at ICLR 2020\n\nSHARING KNOWLEDGE IN MULTI-TASK'
<EOS>
b'DEEP REINFORCEMENT LEARNING'
<EOS>
b'Carlo DEramo'
<EOS>
b'Davide'
<EOS>
b'Tateo\nDepartment of Computer Science'
<EOS>
b'TU Darmstadt, IAS'
<EOS>
b'Hochschulstra\xc3\x9fe 10, 64289, Darmstadt, Germany'
<EOS>
b'carlo.deramo,davide.tateotu-darmstadt.de\n\nAndrea'
<EOS>
b'Bonarini  Marcello Restelli\nPolitecnico di Milano, DEIB'
<EOS>
b'Piazza Leonardo da Vinci 32, 20133, Milano\nandrea.bonarini,'
<EOS>
b'marcello.restellipolimi.it'
<EOS>
b'Jan Peters'
<EOS>
b'TU Darmstadt, IAS'
<EOS>
b'Hochschulstra\xc3\x9fe 10, 64289, Darmstadt, Germany'
<EOS>
b'Max Planck Institute for Intelligent Systems'
<EOS>
b'Max-Planck-Ring 4, 72076, T\xc3\xbcbingen, Germany'
<EOS>
b'jan.peterstu-darmstadt.de'
<EOS>
b'ABSTRACT'
<EOS>
b'We study the bene\xef\xac\x81t of sharing representations among tasks to enable the effective\nuse of deep neural networks in Multi-Task Reinforcement Learning.'
<EOS>
b'We leverage\nthe assumption that learning from different tasks, sharing common properties, is\nhelpful to generalize the knowledge of them resulting in a more effective feature'
<EOS>
b'ex-'
<EOS>
b'traction compared to learning a single task.'
<EOS>
b'Intuitively, the resulting set of features\noffers performance bene\xef\xac\x81ts when used by Reinforcement Learning algorithms.'
<EOS>
b'We prove this by providing theoretical guarantees that highlight the conditions\nfor which is convenient to share representations among tasks, extending the well-'
<EOS>
b'known \xef\xac\x81nite-time bounds of Approximate Value-Iteration to the multi-task setting.'
<EOS>
b'In addition, we complement our analysis by proposing multi-task extensions of\nthree Reinforcement Learning algorithms that we empirically evaluate on widely\nused Reinforcement Learning benchmarks showing signi\xef\xac\x81cant improvements over\nthe single-task counterparts in terms of sample ef\xef\xac\x81ciency and performance.'
<EOS>
b'1'
<EOS>
b'INTRODUCTION\n\nMulti-Task Learning MTL ambitiously aims to learn multiple tasks jointly instead of learning them\nseparately, leveraging the assumption that the considered tasks have common properties which can be\nexploited by Machine Learning ML models to generalize the learning of each of them.'
<EOS>
b'For instance,\nthe features extracted in the hidden layers of a neural network trained on multiple tasks have the\nadvantage of being a general representation of structures common to each other.'
<EOS>
b'This translates into\nan effective way of learning multiple tasks at the same time, but it can also improve the learning\nof each individual task compared to learning them separately Caruana, 1997.'
<EOS>
b'Furthermore, the\nlearned representation can be used to perform Transfer Learning TL, i.e. using it as a preliminary\nknowledge to learn a new similar task resulting in a more effective and faster learning than learning'
<EOS>
b'the new task from scratch Baxter, 2000 Thrun  Pratt, 2012.'
<EOS>
b'The same bene\xef\xac\x81ts of extraction and exploitation of common features among the tasks achieved\nin MTL, can be obtained in Multi-Task Reinforcement Learning MTRL when training a single\nagent on multiple Reinforcement Learning RL problems with common structures Taylor  Stone,'
<EOS>
b'2009 Lazaric, 2012.'
<EOS>
b'In particular, in MTRL an agent can be trained on multiple tasks in the same\n\n1'
<EOS>
b'Published as a conference paper at ICLR 2020'
<EOS>
b'domain,'
<EOS>
b'e.g. riding a bicycle or cycling while going towards a goal, or on different but similar\ndomains,'
<EOS>
b'e.g. balancing a pendulum or balancing a double pendulum1.'
<EOS>
b'Considering recent advances\nin Deep Reinforcement Learning DRL and the resulting increase in the complexity of experimental\nbenchmarks, the use of Deep Learning DL models, e.g. deep neural networks, has become a popular\nand effective way to extract common features among tasks in MTRL algorithms Rusu et al., 2015'
<EOS>
b'Liu et al., 2016 Higgins et al., 2017.'
<EOS>
b'However, despite the high representational capacity of DL\nmodels, the extraction of good features remains challenging.'
<EOS>
b'For instance, the performance of the\nlearning process can degrade when unrelated tasks are used together'
<EOS>
b'Caruana, 1997 Baxter, 2000'
<EOS>
b'another detrimental issue may occur when the training of a single model is not balanced properly\namong multiple tasks Hessel et al., 2018.'
<EOS>
b'Recent developments in MTRL achieve signi\xef\xac\x81cant results in feature extraction by means of algorithms\nspeci\xef\xac\x81cally developed to address these issues.'
<EOS>
b'While some of these works rely on a single deep\nneural network to model the multi-task agent Liu et al., 2016 Yang et al., 2017 Hessel et al., 2018'
<EOS>
b'Wulfmeier et al., 2019, others use multiple deep neural networks, e.g. one for each task and another\nfor the multi-task agent Rusu et al., 2015 Parisotto et al., 2015 Higgins et al., 2017'
<EOS>
b'Teh et al., 2017.'
<EOS>
b'Intuitively, achieving good results in MTRL with a single deep neural network is more desirable\nthan using many of them, since the training time is likely much less and the whole architecture is\neasier to implement.'
<EOS>
b'In this paper we study the bene\xef\xac\x81ts of shared representations among tasks.'
<EOS>
b'We\ntheoretically motivate the intuitive effectiveness of our method, deriving theoretical guarantees that\nexploit the theoretical framework provided by Maurer et al. 2016, in which the authors present\nupper bounds on the quality of learning in MTL when extracting features for multiple tasks in a\nsingle shared representation.'
<EOS>
b'The signi\xef\xac\x81cancy of this result is that the cost of learning the shared\nrepresentation decreases with a factor O1\nT , where T is the number of tasks for many function\napproximator hypothesis classes.'
<EOS>
b'The main contribution of this work is twofold.'
<EOS>
b'1.'
<EOS>
b'We derive upper con\xef\xac\x81dence bounds for Approximate Value-Iteration AVI and Approximate\nPolicy-Iteration API2 Farahmand, 2011 in the MTRL setting, and we extend the approx-\nimation error bounds in Maurer et al. 2016 to the case of multiple tasks with different\ndimensionalities.'
<EOS>
b'Then, we show how to combine these results resulting in, to the best\nof our knowledge, the \xef\xac\x81rst proposed extension of the \xef\xac\x81nite-time bounds of AVIAPI to\nMTRL.'
<EOS>
b'Despite being an extension of previous works, we derive these results to justify'
<EOS>
b'our approach showing how the error propagation in AVIAPI can theoretically bene\xef\xac\x81t from\nlearning multiple tasks jointly.'
<EOS>
b'2.'
<EOS>
b'We leverage these results proposing a neural network architecture, for which these bounds\nhold with minor assumptions, that allow us to learn multiple tasks with a single regressor\nextracting a common representation.'
<EOS>
b'We show an empirical evidence of the consequence of\nour bounds by means of a variant of Fitted Q-Iteration FQI Ernst et al., 2005, based on our\nshared network and for which our bounds apply, that we call Multi Fitted Q-Iteration MFQI.'
<EOS>
b'Then, we perform an empirical evaluation in challenging RL problems proposing multi-\ntask variants of the Deep Q-Network DQN Mnih et al., 2015 and Deep Deterministic'
<EOS>
b'Policy Gradient DDPG Lillicrap et al., 2015 algorithms.'
<EOS>
b'These algorithms are practical\nimplementations of the more general AVIAPI framework, designed to solve complex\nproblems.'
<EOS>
b'In this case, the bounds apply to these algorithms only with some assumptions,\ne.g. stationary sampling distribution.'
<EOS>
b'The outcome of the empirical analysis joins the\ntheoretical results, showing signi\xef\xac\x81cant performance improvements compared to the single-\ntask version of the algorithms in various RL problems, including several MuJoCo'
<EOS>
b'Todorov'
<EOS>
b'et al.,'
<EOS>
b'2012 domains.'
<EOS>
b'2 PRELIMINARIES'
<EOS>
b'Let BX  be the space of'
<EOS>
b'bounded measurable functions w.r.t.'
<EOS>
b'the \xcf\x83-algebra \xcf\x83X , and similarly'
<EOS>
b'BX , L be the same bounded by L  .'
<EOS>
b'A Markov Decision Process MDP is de\xef\xac\x81ned as a 5-tuple M  S, A, P, R, \xce\xb3 , where S is the\nstate space, A is the action space, P  S'
<EOS>
b'A  S is the transition distribution where Pscid48s, a\n\n1For simplicity, in this paper we refer to the concepts of task and domain interchangeably.'
<EOS>
b'2All proofs and the theorem for API are in Appendix A.2.'
<EOS>
b'2'
<EOS>
b'Published as a conference paper at ICLR 2020\n\nis the probability of reaching state scid48 when performing action a in state s, R'
<EOS>
b'S'
<EOS>
b'A  S'
<EOS>
b'R is the reward function, and \xce\xb3  0, 1 is the discount factor.'
<EOS>
b'A deterministic policy \xcf\x80 maps,\nfor each state, the action to perform \xcf\x80'
<EOS>
b'S'
<EOS>
b'A.'
<EOS>
b'Given a policy \xcf\x80, the value of an action'
<EOS>
b'a in a state s represents the expected discounted cumulative reward obtained by performing a\nin s and following \xcf\x80 thereafter Q\xcf\x80s, a cid44 Ecid80'
<EOS>
b'k0 \xce\xb3krik1si  s, ai'
<EOS>
b'a, \xcf\x80, where ri1\nis the reward obtained after the i-th transition.'
<EOS>
b'The expected discounted cumulative reward is\nmaximized by following the optimal policy \xcf\x80 which is the one that determines the optimal action\nvalues, i.e., the ones that satisfy the Bellman optimality equation Bellman, 1954'
<EOS>
b'Qs, a cid44'
<EOS>
b'cid82'
<EOS>
b'S Pscid48s, a Rs, a, scid48  \xce\xb3 maxacid48 Qscid48, acid48 dscid48.'
<EOS>
b'The solution of the Bellman optimality equation\nis the \xef\xac\x81xed point of the optimal Bellman operator T   BS'
<EOS>
b'A  BS  A de\xef\xac\x81ned as\nT Qs, a cid44 cid82'
<EOS>
b'S Pscid48s, aRs, a, scid48  \xce\xb3 maxacid48 Qscid48, acid48dscid48.'
<EOS>
b'In the MTRL setting, there are\nmultiple MDPs'
<EOS>
b'Mt  S t,'
<EOS>
b'At, P t, Rt,'
<EOS>
b'\xce\xb3t'
<EOS>
b'where t  1, . . .'
<EOS>
b', T  and T is the number\nof MDPs.'
<EOS>
b'For each MDP Mt, a deterministic policy \xcf\x80t'
<EOS>
b'S t'
<EOS>
b'At induces an action-value\nfunction'
<EOS>
b'Q\xcf\x80t'
<EOS>
b'ik1si'
<EOS>
b'st, ai  at, \xcf\x80t.'
<EOS>
b'In this setting, the goal is to\nmaximize the sum of the expected cumulative discounted reward of each task.'
<EOS>
b't st, at  Ecid80'
<EOS>
b'k0 \xce\xb3krt'
<EOS>
b'In our theoretical analysis of the MTRL problem, the complexity of representation plays a central role.'
<EOS>
b'As done in Maurer et al.'
<EOS>
b'2016, we consider the Gaussian complexity, a variant of the well-known\nRademacher complexity, to measure the complexity of the representation.'
<EOS>
b'Given a set X  X T n of n\ninput samples for each task t  1, . . . , T , and a class H composed of k  1, . . . , K functions,\nthe Gaussian complexity of a random set H X'
<EOS>
b'hkXti  h'
<EOS>
b'H  RKT n is de\xef\xac\x81ned as\nfollows'
<EOS>
b'GH'
<EOS>
b'X'
<EOS>
b'E'
<EOS>
b'\xce\xb3tkihkXti'
<EOS>
b'cid34'
<EOS>
b'sup'
<EOS>
b'hH'
<EOS>
b'cid88'
<EOS>
b'tki'
<EOS>
b'cid35'
<EOS>
b',\n\ncid12'
<EOS>
b'cid12\ncid12'
<EOS>
b'Xti\ncid12'
<EOS>
b'cid12'
<EOS>
b'where \xce\xb3tki are independent standard normal variables.'
<EOS>
b'We also need to de\xef\xac\x81ne the following quantity,\ntaken from Maurer 2016 let \xce\xb3 be a vector of m random standard normal variables, and f  F'
<EOS>
b'Y'
<EOS>
b'Rm, with Y'
<EOS>
b'Rn, we de\xef\xac\x81ne\n\nOF'
<EOS>
b'sup'
<EOS>
b'y,ycid48Y,ycid54ycid48'
<EOS>
b'cid34'
<EOS>
b'E'
<EOS>
b'sup'
<EOS>
b'f F'
<EOS>
b'cid104\xce\xb3, f'
<EOS>
b'y'
<EOS>
b'f'
<EOS>
b'ycid48cid105'
<EOS>
b'cid107y'
<EOS>
b'ycid48cid107'
<EOS>
b'cid35\n\n.'
<EOS>
b'1'
<EOS>
b'2'
<EOS>
b'Equation 2 can be viewed as a Gaussian average of Lipschitz quotients, and appears in the bounds\nprovided in this work.'
<EOS>
b'Finally, we de\xef\xac\x81ne LF as the upper bound of the Lipschitz constant of all the\nfunctions f in the function class F.\n\n3 THEORETICAL ANALYSIS'
<EOS>
b'The following theoretical study starts from the derivation of theoretical guarantees for MTRL in the\nAVI framework, extending the results of Farahmand 2011 in the MTRL scenario.'
<EOS>
b'Then, to bound\nthe approximation error term in the AVI bound, we extend the result described in Maurer 2006\nto MTRL.'
<EOS>
b'As we discuss, the resulting bounds described in this section clearly show the bene\xef\xac\x81t of\nsharing representation in MTRL.'
<EOS>
b'To the best of our knowledge, this is the \xef\xac\x81rst general result for\nMTRL previous works have focused on'
<EOS>
b'\xef\xac\x81nite MDPs Brunskill  Li, 2013 or linear models Lazaric\n Restelli, 2011.'
<EOS>
b'3.1 MULTI-TASK REPRESENTATION LEARNING'
<EOS>
b'The multi-task representation learning problem consists in learning simultaneously a set of T tasks'
<EOS>
b'\xc2\xb5t, modeled as probability measures over the space of the possible input-output pairs x, y, with\nx  X and y  R, being X the input space.'
<EOS>
b'Let w  W  X'
<EOS>
b'RJ , h'
<EOS>
b'H'
<EOS>
b'RJ  RK and\nf'
<EOS>
b'F  RK'
<EOS>
b'R be functions chosen from their respective hypothesis classes.'
<EOS>
b'The functions\nin the hypothesis classes must be Lipschitz continuous functions.'
<EOS>
b'Let Z  Z1, . . .'
<EOS>
b', ZT  be the\nmulti-sample over the set of tasks'
<EOS>
b'\xc2\xb5'
<EOS>
b'\xc2\xb51, . . . , \xc2\xb5T , where Zt  Zt1, . . . ,'
<EOS>
b'Ztn  \xc2\xb5n'
<EOS>
b't and\nZti  Xti, Yti  \xc2\xb5t.'
<EOS>
b'We can formalize our regression problem as the following minimization'
<EOS>
b'3'
<EOS>
b'Published as a conference paper at ICLR 2020'
<EOS>
b'problem'
<EOS>
b'min'
<EOS>
b'cid40'
<EOS>
b'1'
<EOS>
b'nT'
<EOS>
b'T'
<EOS>
b'cid88'
<EOS>
b'N'
<EOS>
b'cid88'
<EOS>
b't1'
<EOS>
b'i1\n\ncid96fthwtXti, Yti  f'
<EOS>
b'F T , h  H, w'
<EOS>
b'W T\n\n,\n\n3'
<EOS>
b'cid41'
<EOS>
b'where we use f  f1, . . . , fT , w  w1, . . .'
<EOS>
b', wT , and de\xef\xac\x81ne the minimizers of Equation 3 as \xcb\x86w,\n\xcb\x86h, and \xcb\x86f .'
<EOS>
b'We assume that the loss function cid96  R  R  0, 1 is 1-Lipschitz in the \xef\xac\x81rst argument for\nevery value of the second argument.'
<EOS>
b'While this assumption may seem restrictive, the result obtained\ncan be easily scaled to the general case.'
<EOS>
b'To use the principal result of this section, for a generic loss\nfunction cid96cid48, it is possible to use cid96  cid96cid48cid15max, where cid15max is the maximum value of cid96cid48.'
<EOS>
b'The expected\nloss over the tasks, given w, h and f is the task-averaged risk\n\n\xce\xb5avgw, h, f  \n\nE cid96fthwtX, Y \n\n4\n\n1'
<EOS>
b'T\n\nT'
<EOS>
b'cid88'
<EOS>
b't1'
<EOS>
b'The minimum task-averaged risk, given the set of tasks \xc2\xb5 and the hypothesis classes W, H and F is'
<EOS>
b'\xce\xb5'
<EOS>
b'avg, and'
<EOS>
b'the corresponding minimizers are w, h and f .'
<EOS>
b'3.2 MULTI-TASK APPROXIMATE VALUE ITERATION BOUND'
<EOS>
b'We start by considering the bound for the AVI framework which applies for the single-task scenario.'
<EOS>
b'Theorem 1.'
<EOS>
b'Theorem 3.4 of Farahmand 2011 Let K be a positive integer, and Qmax  Rmax\n1\xce\xb3 .'
<EOS>
b'Then\nk0  BS  A, Qmax and the corresponding sequence \xce\xb5kK1\nfor any sequence QkK'
<EOS>
b'k0 , where\n\xce\xb5k'
<EOS>
b'cid107Qk1'
<EOS>
b'T Qkcid1072'
<EOS>
b'\xce\xbd, we have'
<EOS>
b'cid107Q'
<EOS>
b'Q\xcf\x80K'
<EOS>
b'cid1071,\xcf\x81'
<EOS>
b'2\xce\xb3'
<EOS>
b'cid20\n\n1'
<EOS>
b'\xce\xb32\n\n1\n2'
<EOS>
b'inf'
<EOS>
b'r0,1'
<EOS>
b'C'
<EOS>
b'VI,\xcf\x81,'
<EOS>
b'\xce\xbdK'
<EOS>
b'rE'
<EOS>
b'2 \xce\xb50, . . .'
<EOS>
b', \xce\xb5K1 r'
<EOS>
b'cid21'
<EOS>
b'\xce\xb3KRmax'
<EOS>
b',\n\n5'
<EOS>
b'2'
<EOS>
b'1'
<EOS>
b'\xce\xb3'
<EOS>
b'where\n\nCVI,\xcf\x81,\xce\xbdK'
<EOS>
b'r'
<EOS>
b'cid18 1'
<EOS>
b'\xce\xb3'
<EOS>
b'cid192\n\n2'
<EOS>
b'sup'
<EOS>
b'1,'
<EOS>
b'...,\xcf\x80cid48\n\xcf\x80cid48'
<EOS>
b'K'
<EOS>
b'K1'
<EOS>
b'cid88'
<EOS>
b'k0'
<EOS>
b'a21r'
<EOS>
b'k\n\ncid88'
<EOS>
b'\xce\xb3mcid16'
<EOS>
b'm0'
<EOS>
b'cVI1,\xcf\x81,\xce\xbdm, K'
<EOS>
b'k \xcf\x80cid48'
<EOS>
b'K\n\n1'
<EOS>
b'cVI2,\xcf\x81,'
<EOS>
b'\xce\xbdm'
<EOS>
b'1 \xcf\x80cid48'
<EOS>
b'k1, . . .'
<EOS>
b', \xcf\x80cid48'
<EOS>
b'K\n\n\n\n,\n\n6'
<EOS>
b'2'
<EOS>
b'cid17\n\nwith E\xce\xb50,'
<EOS>
b'.'
<EOS>
b'. .'
<EOS>
b', \xce\xb5K1 r  cid80K1\nand \xce\xbd, and the series \xce\xb1k are de\xef\xac\x81ned as in Farahmand 2011.'
<EOS>
b'k0 \xce\xb12r'
<EOS>
b'k \xce\xb5k, the two coef\xef\xac\x81cients cVI1,\xcf\x81,\xce\xbd, cVI2,\xcf\x81,\xce\xbd, the distributions \xcf\x81'
<EOS>
b'In the multi-task scenario, let the average approximation error across tasks be\n\n\xce\xb5avg'
<EOS>
b',k \xcb\x86wk, \xcb\x86hk, \xcb\x86fk \n\ncid107Qt,k1'
<EOS>
b'T'
<EOS>
b't Qt,kcid1072'
<EOS>
b'\xce\xbd,\n\n7'
<EOS>
b'1'
<EOS>
b'T\n\nT'
<EOS>
b'cid88'
<EOS>
b't1'
<EOS>
b'is the optimal Bellman operator of task'
<EOS>
b't.'
<EOS>
b'where Qt,k1  \xcb\x86ft,k  \xcb\x86hk  \xcb\x86wt,k, and T'
<EOS>
b't'
<EOS>
b'In the following, we extend the AVI bound of Theorem 1 to the multi-task scenario, by computing\nthe average loss across tasks and pushing inside the average using Jensens inequality.'
<EOS>
b'Theorem 2.'
<EOS>
b'Let K be a positive integer, and Qmax  Rmax\n1\xce\xb3 .'
<EOS>
b'Then for any sequence QkK'
<EOS>
b'A, Qmax and the corresponding sequence \xce\xb5avg,kK1'
<EOS>
b'we have'
<EOS>
b'k0'
<EOS>
b'BS'
<EOS>
b't Qt,kcid1072'
<EOS>
b'\xce\xbd,\n\nk0 , where \xce\xb5avg,k \n\nt1cid107Qt,k1T'
<EOS>
b'cid80'
<EOS>
b'T\n\n1'
<EOS>
b'T\n\n1'
<EOS>
b'T\n\nT'
<EOS>
b'cid88'
<EOS>
b't1'
<EOS>
b'cid107Q'
<EOS>
b't'
<EOS>
b'Q\xcf\x80K'
<EOS>
b't'
<EOS>
b'cid1071,\xcf\x81'
<EOS>
b'2\xce\xb3'
<EOS>
b'cid20\n\n1'
<EOS>
b'\xce\xb32\n\ninf'
<EOS>
b'r0,1'
<EOS>
b'1'
<EOS>
b'2'
<EOS>
b'1'
<EOS>
b'2'
<EOS>
b'C'
<EOS>
b'VIK'
<EOS>
b'rE'
<EOS>
b'avg\xce\xb5avg,0, . . .'
<EOS>
b', \xce\xb5avg,K1 r \n\n2\xce\xb3KRmax,avg'
<EOS>
b'cid21\n\n1'
<EOS>
b'\xce\xb3\n\n8\n\nwith Eavg  cid80K1'
<EOS>
b'k0'
<EOS>
b'\xce\xb12r'
<EOS>
b'k \xce\xb5avg,k, \xce\xb3'
<EOS>
b'max'
<EOS>
b'cid40'
<EOS>
b'1\xce\xb3\xce\xb3Kk1'
<EOS>
b't1,'
<EOS>
b'...'
<EOS>
b',T'
<EOS>
b'1'
<EOS>
b'T'
<EOS>
b'cid80'
<EOS>
b'T'
<EOS>
b't1'
<EOS>
b'Rmax,t and \xce\xb1k'
<EOS>
b'1\xce\xb3K1'
<EOS>
b'1\xce\xb3\xce\xb3K'
<EOS>
b'1\xce\xb3K1\n\n0'
<EOS>
b'k'
<EOS>
b'K,\n\n.'
<EOS>
b'k'
<EOS>
b'K'
<EOS>
b'\xce\xb3t, C'
<EOS>
b'VIK r'
<EOS>
b'max'
<EOS>
b'C'
<EOS>
b'VI,\xcf\x81,'
<EOS>
b'\xce\xbdK'
<EOS>
b't, r, Rmax,'
<EOS>
b'avg \n\nt1,'
<EOS>
b'...'
<EOS>
b',T'
<EOS>
b'1'
<EOS>
b'2'
<EOS>
b'1'
<EOS>
b'2'
<EOS>
b'4'
<EOS>
b'Published as a conference paper at ICLR 2020'
<EOS>
b'Remarks'
<EOS>
b'Theorem 2 retains most of the properties of Theorem 3.4 of Farahmand 2011, except\nthat the regression error in the bound is now task-averaged.'
<EOS>
b'Interestingly, the second term of the\nsum in Equation 8 depends on the average maximum reward for each task.'
<EOS>
b'In order to obtain this\nresult we use an overly pessimistic bound on \xce\xb3 and the concentrability coef\xef\xac\x81cients, however this\napproximation is not too loose if the MDPs are suf\xef\xac\x81ciently similar.'
<EOS>
b'3.3 MULTI-TASK APPROXIMATION ERROR BOUND'
<EOS>
b'We bound the task-averaged approximation error \xce\xb5avg at each AVI iteration k involved in 8 following\na derivation similar to the one proposed by Maurer et al. 2016, obtaining'
<EOS>
b'Theorem 3.'
<EOS>
b'Let \xc2\xb5, W, H and F be de\xef\xac\x81ned as above and assume 0'
<EOS>
b'H and f 0  0,'
<EOS>
b'f'
<EOS>
b'F.'
<EOS>
b'Then for \xce\xb4  0 with probability at least 1  \xce\xb4 in the draw of Z'
<EOS>
b'cid81T'
<EOS>
b't1'
<EOS>
b'\xc2\xb5n'
<EOS>
b't'
<EOS>
b'we have that'
<EOS>
b'\xce\xb5avg \xcb\x86w, \xcb\x86h, \xcb\x86f'
<EOS>
b'LF\n\nc1'
<EOS>
b'cid18'
<EOS>
b'LH supl1,...'
<EOS>
b',T'
<EOS>
b'GWXl'
<EOS>
b'supwcid107w'
<EOS>
b'Xcid107OH'
<EOS>
b'minpP'
<EOS>
b'GHp\n\ncid19'
<EOS>
b'suph,wcid107hw'
<EOS>
b'Xcid107OF'
<EOS>
b'c3'
<EOS>
b'nT'
<EOS>
b'n\n\n c4\n\n c2'
<EOS>
b'n'
<EOS>
b'T'
<EOS>
b'nT'
<EOS>
b'cid115\n\n\n\n8 ln 3'
<EOS>
b'\xce\xb4'
<EOS>
b'nT'
<EOS>
b'\xce\xb5'
<EOS>
b'avg.'
<EOS>
b'9'
<EOS>
b'Remarks'
<EOS>
b'The assumptions 0  H and f 0  0 for all f  F are not essential for the proof and\nare only needed to simplify the result.'
<EOS>
b'For reasonable function classes, the Gaussian complexity'
<EOS>
b'n.'
<EOS>
b'If supwcid107w Xcid107 and suph,'
<EOS>
b'wcid107hw Xcid107 can be uniformly bounded, then\nGWXl is O'
<EOS>
b'they are O'
<EOS>
b'nT .'
<EOS>
b'For some function classes, the Gaussian average of Lipschitz quotients O can\nbe bounded independently from the number of samples.'
<EOS>
b'Given these assumptions, the \xef\xac\x81rst and the\nfourth term of the right hand side of Equation 9, which represent respectively the cost of learning the\nmeta-state space w and the task-speci\xef\xac\x81c f mappings, are both O1'
<EOS>
b'n.'
<EOS>
b'The second term represents\nthe cost of learning the multi-task representation h and is O1'
<EOS>
b'nT , thus vanishing in the multi-task\nlimit'
<EOS>
b'T'
<EOS>
b'.'
<EOS>
b'The third term can be removed if h  H, p0  P'
<EOS>
b'hp  0'
<EOS>
b'even when this\nassumption does not hold, this term can be ignored for many classes of interest,'
<EOS>
b'e.g. neural networks,\nas it can be arbitrarily small.'
<EOS>
b'The last term to be bounded in 9 is the minimum average approximation error \xce\xb5'
<EOS>
b'avg at each AVI'
<EOS>
b'iteration k. Recalling that the task-averaged approximation error is de\xef\xac\x81ned as in 7, applying'
<EOS>
b'Theorem 5.3 by Farahmand 2011'
<EOS>
b'we obtain'
<EOS>
b'Lemma 4.'
<EOS>
b'Let Q'
<EOS>
b'T'
<EOS>
b't Qt,kcid1072\n\nt,k, t  1, . . .'
<EOS>
b', T  be the minimizers of \xce\xb5'
<EOS>
b'avg,k, \xcb\x87tk'
<EOS>
b'arg maxt1,'
<EOS>
b'...'
<EOS>
b',T'
<EOS>
b'cid107Q\n\n\xce\xbd, and bk,i'
<EOS>
b'cid107Q\xcb\x87tk,i1'
<EOS>
b'T'
<EOS>
b'\xcb\x87t Q\xcb\x87tk,icid107\xce\xbd,'
<EOS>
b'then\n\nt,k1 \n\n\n\ncid32'
<EOS>
b'cid332'
<EOS>
b'\xce\xb5'
<EOS>
b'avg,k'
<EOS>
b'cid107Q'
<EOS>
b'\xcb\x87tk,k1'
<EOS>
b'T'
<EOS>
b'\xcb\x87t k1Q\xcb\x87tk,0cid107\xce\xbd'
<EOS>
b'\xce\xb3\xcb\x87tk CAE\xce\xbd \xcb\x87tk, P i1bk,'
<EOS>
b'k1i\n\n,\n\n10'
<EOS>
b'k1'
<EOS>
b'cid88\n\ni0'
<EOS>
b'with CAE de\xef\xac\x81ned as in Farahmand 2011.'
<EOS>
b'Final remarks'
<EOS>
b'The bound for MTRL is derived by composing the results in Theorems 2 and 3, and\nLemma 4.'
<EOS>
b'The results above highlight the advantage of learning a shared representation.'
<EOS>
b'The bound\nin Theorem 2 shows that a small approximation error is critical to improve the convergence towards\nthe optimal action-value function, and the bound in Theorem 3 shows that the cost of learning the\nshared representation at each AVI iteration is mitigated by using multiple tasks.'
<EOS>
b'This is particularly\nbene\xef\xac\x81cial when the feature representation is complex,'
<EOS>
b'e.g. deep neural networks.'
<EOS>
b'3.4 DISCUSSION'
<EOS>
b'As stated in the remarks of Equation 9, the bene\xef\xac\x81t of MTRL is evinced by the second component\nof the bound, i.e. the cost of learning h, which vanishes with the increase of the number of tasks.'
<EOS>
b'Obviously, adding more tasks require the shared representation to be large enough to include all\nof them, undesirably causing the term suph,'
<EOS>
b'wcid107hw Xcid107 in the fourth component of the bound to\nincrease.'
<EOS>
b'This introduces a tradeoff between the number of features and number of tasks however, for\n\n5'
<EOS>
b'Published as a conference paper at ICLR 2020'
<EOS>
b'a Shared network'
<EOS>
b'b FQI vs MFQI'
<EOS>
b'c'
<EOS>
b'Task analysis'
<EOS>
b'Figure 1'
<EOS>
b'a The architecture of the neural network we propose to learn T tasks simultaneously.'
<EOS>
b'The wt block maps each input xt from task \xc2\xb5t to a shared set of layers h which extracts a common\nrepresentation of the tasks.'
<EOS>
b'Eventually, the shared representation is specialized in block ft and the\noutput yt of the network is computed.'
<EOS>
b'Note that each block can be composed of arbitrarily many\nlayers.'
<EOS>
b'b Results of FQI and MFQI averaged over 4 tasks in Car-On-Hill, showing cid107Q  Q\xcf\x80K cid107 on\nthe left, and the discounted cumulative reward on the right.'
<EOS>
b'c Results of MFQI showing cid107Q Q\xcf\x80K cid107\nfor increasing number of tasks.'
<EOS>
b'Both results in b and c are averaged over 100 experiments, and\nshow the 95 con\xef\xac\x81dence intervals.'
<EOS>
b'a reasonable number of tasks the number of features used in the single-task case is enough to handle\nthem, as we show in some experiments in Section 5.'
<EOS>
b'Notably, since the AVIAPI framework provided\nby Farahmand 2011 provides an easy way to include the approximation error of a generic function'
<EOS>
b'approximator, it is easy to show the bene\xef\xac\x81t in MTRL of the bound in Equation 9.'
<EOS>
b'Despite being just\nmulti-task extensions of previous works, our results are the \xef\xac\x81rst one to theoretically show the bene\xef\xac\x81t\nof sharing representation in MTRL.'
<EOS>
b'Moreover, they serve as a signi\xef\xac\x81cant theoretical motivation,\nbesides to the intuitive ones, of the practical algorithms that we describe in the following sections.'
<EOS>
b'4 SHARING REPRESENTATIONS'
<EOS>
b'We want to empirically evaluate the bene\xef\xac\x81t of our theoretical study in the problem of jointly learning\nT different tasks \xc2\xb5t, introducing a neural network architecture for which our bounds hold.'
<EOS>
b'Following\nour theoretical framework, the network we propose extracts representations wt from inputs xt for each\ntask \xc2\xb5t, mapping them to common features in a set of shared layers h, specializing the learning of\neach task in respective separated layers ft, and \xef\xac\x81nally computing the output yt'
<EOS>
b'ft  h  wtxt \nfthwtxt Figure 1a.'
<EOS>
b'The idea behind this architecture is not new in the literature.'
<EOS>
b'For\ninstance, similar ideas have already been used in DQN variants to improve exploration on the same\ntask via bootstrapping Osband et al., 2016 and to perform MTRL Liu et al., 2016.'
<EOS>
b'The intuitive and desirable property of this architecture is the exploitation of the regularization effect\nintroduced by the shared representation of the jointly learned tasks.'
<EOS>
b'Indeed, unlike learning a single\ntask that may end up in over\xef\xac\x81tting, forcing the model to compute a shared representation of the tasks\nhelps the regression process to extract more general features, with a consequent reduction in the\nvariance of the learned function.'
<EOS>
b'This intuitive justi\xef\xac\x81cation for our approach, joins the theoretical\nbene\xef\xac\x81t proven in Section 3.'
<EOS>
b'Note that our architecture can be used in any MTRL problem involving a\nregression process indeed, it can be easily used in value-based methods as a Q-function regressor,\nor in policy search as a policy regressor.'
<EOS>
b'In both cases, the targets are learned for each task \xc2\xb5t\nin its respective output block ft.'
<EOS>
b'Remarkably, as we show in the experimental Section 5, it is\nstraightforward to extend RL algorithms to their multi-task variants only through the use of the\nproposed network architecture, without major changes to the algorithms themselves.'
<EOS>
b'5 EXPERIMENTAL RESULTS'
<EOS>
b'To empirically evince the effect described by our bounds, we propose an extension of FQI Ernst\net al.,'
<EOS>
b'2005 Riedmiller, 2005, that we call MFQI, for which our AVI bounds apply.'
<EOS>
b'Then, to\nempirically evaluate our approach in challenging RL problems, we introduce multi-task variants\nof two well-known DRL algorithms DQN Mnih et al., 2015 and DDPG Lillicrap et al., 2015,'
<EOS>
b'which we call Multi Deep Q-Network MDQN and Multi Deep Deterministic Policy Gradient\nMDDPG respectively.'
<EOS>
b'Note that for these methodologies, our AVI and API bounds hold only with\n\n6'
<EOS>
b'hhw1w1w2w2wTwTf1f1f2f2fTfTInputOutputx1x2xTy1y2yT........'
<EOS>
b'02550 Iterations0.150.200.250.300.350.400.450.50QQKFQIMULTI02550 Iterations0.050.000.050.100.15Performance02550 Iterations0.150.200.250.300.350.400.450.50QQK1248\x0cPublished as a conference paper at ICLR 2020'
<EOS>
b'a Multi-task\n\nb'
<EOS>
b'Transfer\n\nFigure'
<EOS>
b'2 Discounted cumulative reward averaged over 100 experiments of DQN and MDQN for\neach task and for transfer learning in the Acrobot problem.'
<EOS>
b'An epoch consists of 1, 000 steps, after\nwhich the greedy policy is evaluated for 2, 000 steps.'
<EOS>
b'The 95 con\xef\xac\x81dence intervals are shown.'
<EOS>
b'the simplifying assumption that the samples are i.i.d'
<EOS>
b'.'
<EOS>
b'nevertheless they are useful to show the bene\xef\xac\x81t\nof our method also in complex scenarios,'
<EOS>
b'e.g. MuJoCo Todorov et al., 2012.'
<EOS>
b'We remark that in\nthese experiments we are only interested in showing the bene\xef\xac\x81t of learning multiple tasks with a\nshared representation'
<EOS>
b'w.r.t.'
<EOS>
b'learning a single task therefore, we only compare our methods with\nthe single task counterparts, ignoring other works on MTRL in literature.'
<EOS>
b'Experiments have been\ndeveloped using the MushroomRL library DEramo et al., 2020, and run on an NVIDIA Rcid13'
<EOS>
b'DGX\nStationTM and Intel Rcid13 AI DevCloud.'
<EOS>
b'Refer to Appendix B for all the details and our motivations\nabout the experimental settings.'
<EOS>
b'5.1 MULTI FITTED Q-ITERATION'
<EOS>
b'As a \xef\xac\x81rst empirical evaluation, we consider FQI, as an example of an AVI algorithm, to show the\neffect described by our theoretical AVI bounds in experiments.'
<EOS>
b'We consider the Car-On-Hill problem\nas described in Ernst et al. 2005, and select four different tasks from it changing the mass of the\ncar and the value of the actions details in Appendix B.'
<EOS>
b'Then, we run separate instances of FQI\nwith a single task network for each task respectively, and one of MFQI considering all the tasks\nsimultaneously.'
<EOS>
b'Figure 1b shows the L1-norm of the difference between Q and Q\xcf\x80K averaged\nover all the tasks.'
<EOS>
b'It is clear how MFQI is able to get much closer to the optimal Q-function, thus\ngiving an empirical evidence of the AVI bounds in Theorem 2.'
<EOS>
b'For completeness, we also show the\nadvantage of MFQI w.r.t. FQI in performance.'
<EOS>
b'Then, in Figure 1c we provide an empirical evidence\nof the bene\xef\xac\x81t of increasing the number of tasks in MFQI in terms of both quality and stability.'
<EOS>
b'5.2 MULTI'
<EOS>
b'DEEP Q-NETWORK'
<EOS>
b'As in Liu et al.'
<EOS>
b'2016, our MDQN uses separate replay memories for each task and'
<EOS>
b'the batch\nused in each training step is built picking the same number of samples from each replay memory.'
<EOS>
b'Furthermore, a step of the algorithm consists of exactly one step in each task.'
<EOS>
b'These are the only\nminor changes to the vanilla DQN algorithm we introduce, while all other aspects, such as the use of\nthe target network, are not modi\xef\xac\x81ed.'
<EOS>
b'Thus, the time complexity of MDQN is considerably lower than\nvanilla DQN'
<EOS>
b'thanks to the learning of T tasks with a single model, but at the cost of a higher memory\ncomplexity for the collection of samples for each task.'
<EOS>
b'We consider \xef\xac\x81ve problems with similar\nstate spaces, sparse rewards and discrete actions Cart-Pole, Acrobot, Mountain-Car, Car-On-Hill,\nand Inverted-Pendulum.'
<EOS>
b'The implementation of the \xef\xac\x81rst three problems is the one provided by the\nOpenAI Gym library Brockman et al. 2016, while Car-On-Hill is described in Ernst et al. 2005\nand Inverted-Pendulum in Lagoudakis  Parr 2003.'
<EOS>
b'Figure 2a shows the performance of MDQN w.r.t. to vanilla DQN that uses a single-task network\nstructured as the multi-task one in the case with T  1.'
<EOS>
b'The \xef\xac\x81rst three plots from the left show good\nperformance of MDQN, which is both higher and more stable than DQN.'
<EOS>
b'In Car-On-Hill, MDQN is\nslightly slower than DQN to reach the best performance, but eventually manages to be more stable.'
<EOS>
b'Finally, the Inverted-Pendulum experiment is clearly too easy to solve for both approaches, but it is\nstill useful for the shared feature extraction in MDQN.'
<EOS>
b'The described results provide important hints\nabout the better quality of the features extracted by MDQN w.r.t. DQN.'
<EOS>
b'To further demonstrate this,\nwe evaluate the performance of DQN on Acrobot, arguably the hardest of the \xef\xac\x81ve problems, using\na single-task network with the shared parameters in h initialized with the weights of a multi-task\n\n7\n\n02550Epochs20406080PerformanceCart-Pole02550Epochs10090807060Acrobot02550Epochs10095908580757065Mountain-Car02550Epochs0.00.10.20.30.4Car-On-Hill02550Epochs0.60.40.20.0Inverted-PendulumDQNMULTI02550Epochs10090807060PerformanceAcrobotNo initializationUnfreeze-0Unfreeze-10No unfreeze\x0cPublished as a conference paper at ICLR 2020'
<EOS>
b'a Multi-task for pendulums'
<EOS>
b'b Transfer for pendulums'
<EOS>
b'c Multi-task for walkers\n\nd Transfer for walkers'
<EOS>
b'Figure'
<EOS>
b'3 Discounted cumulative reward averaged over 40 experiments of DDPG and MDDPG for\neach task and for transfer learning in the Inverted-Double-Pendulum and Hopper problems.'
<EOS>
b'An\nepoch consists of 10, 000 steps, after which the greedy policy is evaluated for 5, 000 steps.'
<EOS>
b'The 95\ncon\xef\xac\x81dence intervals are shown.'
<EOS>
b'network trained with MDQN on the other four problems.'
<EOS>
b'Arbitrarily, the pre-trained weights can be\nadjusted during the learning of the new task or can be kept \xef\xac\x81xed and'
<EOS>
b'only the remaining randomly\ninitialized parameters in w and f are trained.'
<EOS>
b'From Figure 2b, the advantages of initializing the\nweights are clear.'
<EOS>
b'In particular, we compare the performance of DQN without initialization'
<EOS>
b'w.r.t.'
<EOS>
b'DQN with initialization in three settings in Unfreeze-0'
<EOS>
b'the initialized weights are adjusted, in No-'
<EOS>
b'Unfreeze they are kept \xef\xac\x81xed, and in Unfreeze-10 they are kept \xef\xac\x81xed until epoch 10 after which they\nstart to be optimized.'
<EOS>
b'Interestingly, keeping the shared weights \xef\xac\x81xed shows a signi\xef\xac\x81cant performance\nimprovement in the earliest epochs, but ceases to improve soon.'
<EOS>
b'On the other hand, the adjustment of\nweights from the earliest epochs shows improvements only compared to the uninitialized network\nin the intermediate stages of learning.'
<EOS>
b'The best results are achieved by starting to adjust the shared\nweights after epoch 10, which is approximately the point at which the improvement given by the\n\xef\xac\x81xed initialization starts to lessen.'
<EOS>
b'5.3 MULTI DEEP DETERMINISTIC POLICY GRADIENT'
<EOS>
b'In order to show how the \xef\xac\x82exibility of our approach easily allows to perform MTRL in policy search'
<EOS>
b'algorithms, we propose MDDPG as a multi-task variant of DDPG.'
<EOS>
b'As an actor-critic method, DDPG\nrequires an actor network and a critic network.'
<EOS>
b'Intuitively, to obtain MDDPG both the actor and critic'
<EOS>
b'networks should be built following our proposed structure.'
<EOS>
b'We perform separate experiments on two\nsets of MuJoCo'
<EOS>
b'Todorov'
<EOS>
b'et al.'
<EOS>
b'2012 problems with similar continuous state and action spaces'
<EOS>
b'the\n\xef\xac\x81rst set includes Inverted-Pendulum, Inverted-Double-Pendulum, and Inverted-Pendulum-Swingup as\nimplemented in the pybullet library, whereas the second set includes Hopper-Stand, Walker-Walk,\nand Half-Cheetah-Run as implemented in the DeepMind Control SuiteTassa'
<EOS>
b'et al. 2018.'
<EOS>
b'Figure 3a\nshows a relevant improvement of MDDPG w.r.t. DDPG in the pendulum tasks.'
<EOS>
b'Indeed, while in\nInverted-Pendulum, which is the easiest problem among the three, the performance of MDDPG is\nonly slightly better than DDPG, the difference in the other two problems is signi\xef\xac\x81cant.'
<EOS>
b'The advantage\nof MDDPG is con\xef\xac\x81rmed in Figure 3c where it performs better than DDPG in Hopper and equally\ngood in the other two tasks.'
<EOS>
b'Again, we perform a TL evaluation of DDPG in the problems where\nit suffers the most, by initializing the shared weights of a single-task network with the ones of a\nmulti-task network trained with MDDPG on the other problems.'
<EOS>
b'Figures'
<EOS>
b'3b and 3d show evident\nadvantages of pre-training the shared weights and a signi\xef\xac\x81cant difference between keeping them \xef\xac\x81xed\nor not.'
<EOS>
b'8'
<EOS>
b'050100Epochs2030405060708090100PerformanceInverted'
<EOS>
b'-PendulumDDPGMULTI050100Epochs100200300400500600700800Inverted-Double'
<EOS>
b'-Pendulum050100Epochs100806040200Inverted-Pendulum-Swingup050100Epochs200400600800PerformanceInverted-'
<EOS>
b'Double-PendulumNo initializationUnfreeze-0No unfreeze050100Epochs05101520253035PerformanceHopper050100Epochs010203040506070Walker050100Epochs0510152025303540Half-CheetahDDPGMULTI050100Epochs010203040PerformanceHopperNo initializationUnfreeze-0No unfreeze\x0cPublished as a conference paper at ICLR 2020'
<EOS>
b'6 RELATED WORKS'
<EOS>
b'Our work is inspired from both theoretical and empirical studies in MTL and MTRL literature.'
<EOS>
b'In\nparticular, the theoretical analysis we provide follows previous results about the theoretical properties\nof multi-task algorithms.'
<EOS>
b'For instance, Cavallanti et al.'
<EOS>
b'2010 and Maurer 2006 prove the theoretical\nadvantages of MTL based on linear approximation.'
<EOS>
b'More in detail, Maurer 2006 derives bounds on\nMTL when a linear approximator is used to extract a shared representation among tasks.'
<EOS>
b'Then, Maurer\net al. 2016, which we considered in this work, describes similar results that extend to the use of\nnon-linear approximators.'
<EOS>
b'Similar studies have been conducted in the context of MTRL.'
<EOS>
b'Among the\nothers, Lazaric  Restelli 2011 and Brunskill  Li 2013 give theoretical proofs of the advantage\nof learning from multiple MDPs and introduces new algorithms to empirically support their claims,\nas done in this work.'
<EOS>
b'Generally, contributions in MTRL assume that properties of different tasks, e.g. dynamics and reward\nfunction, are generated from a common generative model.'
<EOS>
b'About this, interesting analyses consider'
<EOS>
b'Bayesian approaches for instance'
<EOS>
b'Wilson et al. 2007 assumes that the tasks are generated from a\nhierarchical Bayesian model, and likewise Lazaric  Ghavamzadeh 2010 considers the case when\nthe value functions are generated from a common prior distribution.'
<EOS>
b'Similar considerations, which\nhowever does not use a Bayesian approach, are implicitly made in Taylor et al. 2007,'
<EOS>
b'Lazaric et al.\n2008, and also in this work.'
<EOS>
b'In recent years, the advantages of MTRL have been empirically evinced also in DRL, especially\nexploiting the powerful representational capacity of deep neural networks.'
<EOS>
b'For instance, Parisotto\net al. 2015 and Rusu et al.'
<EOS>
b'2015 propose to derive a multi-task policy from the policies learned by\nDQN experts trained separately on different tasks.'
<EOS>
b'Rusu et al. 2015 compares to a therein introduced\nvariant of DQN, which is very similar to our MDQN and the one in Liu et al. 2016, showing how\ntheir method overcomes it in the Atari benchmark Bellemare'
<EOS>
b'et al. 2013.'
<EOS>
b'Further developments,\nextend the analysis to policy search Yang et al., 2017 Teh et al., 2017, and to multi-goal RL Schaul\net al., 2015 Andrychowicz et al., 2017.'
<EOS>
b'Finally, Hessel et al. 2018 addresses the problem of\nbalancing the learning of multiple tasks with a single deep neural network proposing a method that\nuniformly adapts the impact of each task on the training updates of the agent.'
<EOS>
b'7 CONCLUSION'
<EOS>
b'We have theoretically proved the advantage in RL of using a shared representation to learn multiple\ntasks'
<EOS>
b'w.r.t.'
<EOS>
b'learning a single task.'
<EOS>
b'We have derived our results extending the AVIAPI bounds Farah-'
<EOS>
b'mand, 2011 to MTRL, leveraging the upper bounds on the approximation error in MTL provided\nin Maurer'
<EOS>
b'et al.'
<EOS>
b'2016.'
<EOS>
b'The results of this analysis show that the error propagation during the\nAVIAPI iterations is reduced according to the number of tasks.'
<EOS>
b'Then, we proposed a practical way of\nexploiting this theoretical bene\xef\xac\x81t which consists in an effective way of extracting shared representa-'
<EOS>
b'tions of multiple tasks by means of deep neural networks.'
<EOS>
b'To empirically show the advantages of our\nmethod, we carried out experiments on challenging RL problems with the introduction of multi-task\nextensions of FQI, DQN, and DDPG based on the neural network structure we proposed.'
<EOS>
b'As desired,\nthe favorable empirical results con\xef\xac\x81rm the theoretical bene\xef\xac\x81t we described.'
<EOS>
b'9'
<EOS>
b'Published as a conference paper at ICLR 2020'
<EOS>
b'ACKNOWLEDGMENTS'
<EOS>
b'This project has received funding from the European Unions Horizon 2020 research and innovation\nprogramme under grant agreement'
<EOS>
b'No. 640554'
<EOS>
b'SKILLS4ROBOTS'
<EOS>
b'and No.'
<EOS>
b'713010 GOAL-'
<EOS>
b'Robots.'
<EOS>
b'This project has also been supported by grants from NVIDIA, the NVIDIA DGX Station,\nand the Intel Rcid13 AI DevCloud.'
<EOS>
b'The authors thank Alberto Maria Metelli, Andrea Tirinzoni and Matteo\nPapini for their helpful insights during the development of the project.'
<EOS>
b'REFERENCES'
<EOS>
b'Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob\nMcGrew, Josh Tobin, OpenAI Pieter Abbeel, and Wojciech Zaremba.'
<EOS>
b'Hindsight experience replay.'
<EOS>
b'In Advances in Neural Information Processing Systems, pp.'
<EOS>
b'50485058, 2017.'
<EOS>
b'Jonathan Baxter.'
<EOS>
b'A model of inductive bias learning.'
<EOS>
b'Journal of Arti\xef\xac\x81cial Intelligence Research, 12\n\n149198, 2000.'
<EOS>
b'Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling.'
<EOS>
b'The arcade learning environ-'
<EOS>
b'ment An evaluation platform for general agents.'
<EOS>
b'Journal of Arti\xef\xac\x81cial Intelligence Research, 47\n253279, 2013.'
<EOS>
b'Richard Bellman.'
<EOS>
b'The theory of dynamic programming.'
<EOS>
b'Technical report, RAND Corp Santa Monica\n\nCA, 1954.'
<EOS>
b'Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and\n\nWojciech Zaremba.'
<EOS>
b'Openai gym, 2016.'
<EOS>
b'Emma Brunskill and Lihong Li.'
<EOS>
b'Sample complexity of multi-task reinforcement learning.'
<EOS>
b'Proceedings of the Twenty-Ninth Conference on Uncertainty in Arti\xef\xac\x81cial Intelligence, 2013.'
<EOS>
b'In\n\nRich Caruana.'
<EOS>
b'Multitask learning.'
<EOS>
b'Machine learning, 2814175, 1997.'
<EOS>
b'Giovanni Cavallanti, Nicolo Cesa-Bianchi, and Claudio Gentile.'
<EOS>
b'Linear algorithms for online\n\nmultitask classi\xef\xac\x81cation.'
<EOS>
b'Journal of Machine Learning Research, 11Oct29012934, 2010.'
<EOS>
b'Carlo DEramo,'
<EOS>
b'Davide Tateo, Andrea Bonarini, Marcello Restelli, and Jan Peters.'
<EOS>
b'Mushroomrl'
<EOS>
b'Simplifying reinforcement learning research.'
<EOS>
b'arXiv2001.01102, 2020.'
<EOS>
b'Damien Ernst, Pierre Geurts, and Louis Wehenkel.'
<EOS>
b'Tree-based batch mode reinforcement learning.'
<EOS>
b'Journal of Machine Learning Research, 6Apr503556, 2005.'
<EOS>
b'Amir-massoud Farahmand.'
<EOS>
b'Regularization in reinforcement learning.'
<EOS>
b'2011.'
<EOS>
b'Matteo Hessel, Hubert Soyer, Lasse Espeholt, Wojciech Czarnecki, Simon Schmitt, and Hado van'
<EOS>
b'Hasselt.'
<EOS>
b'Multi-task deep reinforcement learning with popart.'
<EOS>
b'arXiv1809.04474, 2018.'
<EOS>
b'Irina Higgins, Arka Pal, Andrei Rusu, Loic Matthey, Christopher Burgess, Alexander Pritzel, Matthew\nBotvinick, Charles Blundell, and Alexander Lerchner.'
<EOS>
b'Darla Improving zero-shot transfer in\nreinforcement learning.'
<EOS>
b'In International Conference on Machine Learning, pp.'
<EOS>
b'14801490, 2017.'
<EOS>
b'Michail G Lagoudakis and Ronald Parr.'
<EOS>
b'Least-squares policy iteration.'
<EOS>
b'Journal of machine learning'
<EOS>
b'research, 4Dec11071149, 2003.'
<EOS>
b'Alessandro Lazaric.'
<EOS>
b'Transfer in reinforcement learning a framework and a survey.'
<EOS>
b'In Reinforcement\n\nLearning, pp. 143173.'
<EOS>
b'Springer, 2012.'
<EOS>
b'Alessandro Lazaric and Mohammad Ghavamzadeh.'
<EOS>
b'Bayesian multi-task reinforcement learning.'
<EOS>
b'In\n\nICML-27th International Conference on Machine Learning, pp. 599606.'
<EOS>
b'Omnipress, 2010.'
<EOS>
b'Alessandro Lazaric and Marcello Restelli.'
<EOS>
b'Transfer from multiple mdps.'
<EOS>
b'In Advances in Neural\n\nInformation Processing Systems, pp. 17461754, 2011.'
<EOS>
b'Alessandro Lazaric, Marcello Restelli, and Andrea Bonarini.'
<EOS>
b'Transfer of samples in batch rein-'
<EOS>
b'forcement learning.'
<EOS>
b'In Proceedings of the 25th international conference on Machine learning, pp.\n544551.'
<EOS>
b'ACM, 2008.'
<EOS>
b'10'
<EOS>
b'Published as a conference paper at ICLR 2020'
<EOS>
b'Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,\nDavid Silver, and Daan Wierstra.'
<EOS>
b'Continuous control with deep reinforcement learning.'
<EOS>
b'arXiv'
<EOS>
b'preprint arXiv1509.02971, 2015.'
<EOS>
b'Lydia Liu, Urun Dogan, and Katja Hofmann.'
<EOS>
b'Decoding multitask dqn in the world of minecraft.'
<EOS>
b'In\n\nEuropean Workshop on Reinforcement Learning, 2016.'
<EOS>
b'Andreas Maurer.'
<EOS>
b'Bounds for linear multi-task learning.'
<EOS>
b'Journal of Machine Learning Research, 7\n\nJan117139, 2006.'
<EOS>
b'Science, 650109122, 2016.'
<EOS>
b'Andreas Maurer.'
<EOS>
b'A chain rule for the expected suprema of gaussian processes.'
<EOS>
b'Theoretical Computer'
<EOS>
b'Andreas Maurer, Massimiliano Pontil, and Bernardino Romera-Paredes.'
<EOS>
b'The bene\xef\xac\x81t of multitask\n\nrepresentation learning.'
<EOS>
b'The Journal of Machine Learning Research, 17128532884, 2016.'
<EOS>
b'Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare,\nAlex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al.'
<EOS>
b'Human-level control\nthrough deep reinforcement learning.'
<EOS>
b'Nature, 5187540529, 2015.'
<EOS>
b'Ian Osband, Charles Blundell, Alexander Pritzel, and Benjamin Van Roy.'
<EOS>
b'Deep exploration via\nbootstrapped dqn.'
<EOS>
b'In Advances in neural information processing systems, pp.'
<EOS>
b'40264034, 2016.'
<EOS>
b'Emilio Parisotto, Jimmy Lei Ba, and Ruslan Salakhutdinov.'
<EOS>
b'Actor-mimic Deep multitask and\n\ntransfer reinforcement learning.'
<EOS>
b'arXiv preprint arXiv1511.06342, 2015.'
<EOS>
b'Martin Riedmiller.'
<EOS>
b'Neural \xef\xac\x81tted q iteration\xef\xac\x81rst experiences with a data ef\xef\xac\x81cient neural reinforcement\n\nlearning method.'
<EOS>
b'In European Conference on Machine Learning, pp. 317328.'
<EOS>
b'Springer, 2005.'
<EOS>
b'Andrei'
<EOS>
b'A Rusu, Sergio Gomez Colmenarejo, Caglar Gulcehre, Guillaume Desjardins, James Kirk-\npatrick, Razvan Pascanu, Volodymyr Mnih, Koray Kavukcuoglu, and Raia Hadsell.'
<EOS>
b'Policy\ndistillation.'
<EOS>
b'arXiv preprint arXiv1511.06295, 2015.'
<EOS>
b'Tom Schaul, Daniel Horgan, Karol Gregor, and David Silver.'
<EOS>
b'Universal value function approximators.'
<EOS>
b'In International Conference on Machine Learning, pp. 13121320, 2015.'
<EOS>
b'Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de Las Casas, David Budden,'
<EOS>
b'Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq, Timothy P. Lillicrap, and Martin A. Riedmiller.'
<EOS>
b'Deepmind control suite.'
<EOS>
b'CoRR, abs1801.00690, 2018.'
<EOS>
b'Matthew E Taylor and Peter Stone.'
<EOS>
b'Transfer learning for reinforcement learning domains A survey.'
<EOS>
b'Journal of Machine Learning Research, 10Jul16331685, 2009.'
<EOS>
b'Matthew E Taylor, Peter Stone, and Yaxin Liu.'
<EOS>
b'Transfer learning via inter-task mappings for temporal\n\ndifference learning.'
<EOS>
b'Journal of Machine Learning Research, 8Sep21252167, 2007.'
<EOS>
b'Yee Teh, Victor Bapst, Wojciech M Czarnecki, John Quan, James Kirkpatrick, Raia Hadsell, Nicolas'
<EOS>
b'Heess, and Razvan Pascanu.'
<EOS>
b'Distral Robust multitask reinforcement learning.'
<EOS>
b'In Advances in\nNeural Information Processing Systems, pp. 44964506, 2017.'
<EOS>
b'Sebastian Thrun and Lorien Pratt.'
<EOS>
b'Learning to learn.'
<EOS>
b'Springer Science  Business Media, 2012.'
<EOS>
b'Emanuel Todorov, Tom Erez, and Yuval Tassa.'
<EOS>
b'Mujoco A physics engine for model-based control.'
<EOS>
b'In 2012 IEEERSJ International Conference on Intelligent Robots and Systems.'
<EOS>
b'IEEE, 2012.'
<EOS>
b'Aaron Wilson, Alan Fern, Soumya Ray, and Prasad Tadepalli.'
<EOS>
b'Multi-task reinforcement learning a\nhierarchical bayesian approach.'
<EOS>
b'In Proceedings of the 24th international conference on Machine\nlearning, pp. 10151022.'
<EOS>
b'ACM, 2007.'
<EOS>
b'Markus Wulfmeier, Abbas Abdolmaleki, Roland Hafner, Jost Tobias Springenberg, Michael Neunert,'
<EOS>
b'Tim Hertweck, Thomas Lampe, Noah Siegel, Nicolas Heess, and Martin Riedmiller.'
<EOS>
b'Regularized\nhierarchical policies for compositional transfer in robotics.'
<EOS>
b'arXiv1906.11228, 2019.'
<EOS>
b'Zhaoyang Yang, Kathryn E Merrick, Hussein A Abbass, and Lianwen Jin.'
<EOS>
b'Multi-task deep reinforce-'
<EOS>
b'ment learning for continuous action control.'
<EOS>
b'In IJCAI, pp.'
<EOS>
b'33013307, 2017.'
<EOS>
b'11'
<EOS>
b'Published as a conference paper at ICLR 2020'
<EOS>
b'A PROOFS\n\nA.1 APPROXIMATED VALUE-ITERATION BOUNDS\n\nProof of Theorem 2.'
<EOS>
b'We compute the average expected loss across tasks\n\n1'
<EOS>
b'T\n\nT'
<EOS>
b'cid88'
<EOS>
b't1'
<EOS>
b'cid107Q'
<EOS>
b't'
<EOS>
b'Q\xcf\x80K'
<EOS>
b't'
<EOS>
b'cid1071,\xcf\x81\n\n1\nT\n\nT'
<EOS>
b'cid88'
<EOS>
b't1'
<EOS>
b'2\xce\xb3'
<EOS>
b'1'
<EOS>
b'\xce\xb32\n\n1'
<EOS>
b'\xce\xb32\n\n1  \xce\xb32'
<EOS>
b'2\xce\xb3'
<EOS>
b'2\xce\xb3'
<EOS>
b'2\xce\xb3'
<EOS>
b'2\xce\xb3t'
<EOS>
b'cid20\n\n1'
<EOS>
b'\xce\xb3t2\n\ninf'
<EOS>
b'r0,1'
<EOS>
b'T'
<EOS>
b'cid88'
<EOS>
b'cid20'
<EOS>
b't1'
<EOS>
b'inf'
<EOS>
b'r0,1'
<EOS>
b'1'
<EOS>
b'T\n\nT'
<EOS>
b'cid88'
<EOS>
b'cid18'
<EOS>
b't1'
<EOS>
b'inf'
<EOS>
b'r0,1'
<EOS>
b'1'
<EOS>
b'T\n\ninf'
<EOS>
b'r0,1'
<EOS>
b'T'
<EOS>
b'cid88'
<EOS>
b'cid16'
<EOS>
b't1'
<EOS>
b'1'
<EOS>
b'T'
<EOS>
b'cid34'
<EOS>
b'cid34'
<EOS>
b'cid34'
<EOS>
b'1'
<EOS>
b'2'
<EOS>
b'1'
<EOS>
b'2'
<EOS>
b'1'
<EOS>
b'2'
<EOS>
b'1'
<EOS>
b'2'
<EOS>
b'1'
<EOS>
b'T'
<EOS>
b'C'
<EOS>
b'VI,\xcf\x81,'
<EOS>
b'\xce\xbdK'
<EOS>
b't, rE'
<EOS>
b'2'
<EOS>
b'\xce\xb5t,0, . . . , \xce\xb5t,K1 t, r'
<EOS>
b'C'
<EOS>
b'VI,\xcf\x81,'
<EOS>
b'\xce\xbdK'
<EOS>
b't, rE'
<EOS>
b'2'
<EOS>
b'\xce\xb5t,0, . . . , \xce\xb5t,K1 t, r'
<EOS>
b'C'
<EOS>
b'VI,\xcf\x81,'
<EOS>
b'\xce\xbdK'
<EOS>
b't, rE'
<EOS>
b'2'
<EOS>
b'\xce\xb5t,0, . . . , \xce\xb5t,K1 t, r\n\n\n\nC'
<EOS>
b'VI,\xcf\x81,'
<EOS>
b'\xce\xbdK'
<EOS>
b't, rE'
<EOS>
b'2'
<EOS>
b'\xce\xb5t,0, . . . , \xce\xb5t,K1 t, r\n\n\n\n1\n\n1\n\n1\n\n1\n\n2\n\n1  \xce\xb3t'
<EOS>
b'\xce\xb3K\nt Rmax,t'
<EOS>
b'2'
<EOS>
b'1'
<EOS>
b'\xce\xb3t\n\n\xce\xb3K'
<EOS>
b't Rmax,t'
<EOS>
b'cid21'
<EOS>
b'cid21'
<EOS>
b'cid19'
<EOS>
b'2'
<EOS>
b'1'
<EOS>
b'\xce\xb3'
<EOS>
b'\xce\xb3KRmax,avg'
<EOS>
b'cid17'
<EOS>
b'2'
<EOS>
b'1'
<EOS>
b'\xce\xb3'
<EOS>
b'\xce\xb3KRmax,avg'
<EOS>
b'cid35'
<EOS>
b'cid35'
<EOS>
b'cid35\n\n1'
<EOS>
b'\xce\xb32\n\ninf'
<EOS>
b'r0,1\n\n1'
<EOS>
b'2'
<EOS>
b'C'
<EOS>
b'VIK r'
<EOS>
b'T'
<EOS>
b'cid88'
<EOS>
b'cid16\n\n1'
<EOS>
b't1'
<EOS>
b'E'
<EOS>
b'2'
<EOS>
b'\xce\xb5t,0, . . .'
<EOS>
b', \xce\xb5t,K1 t, r\n\n\n\n\xce\xb3KRmax,avg\n\n11\n\ncid17'
<EOS>
b'2'
<EOS>
b'1'
<EOS>
b'\xce\xb3'
<EOS>
b'with \xce\xb3  max\n\n\xce\xb3t, C'
<EOS>
b'VIK r'
<EOS>
b'max'
<EOS>
b't1,'
<EOS>
b'...'
<EOS>
b',T'
<EOS>
b't1,'
<EOS>
b'...'
<EOS>
b',T'
<EOS>
b'1'
<EOS>
b'2'
<EOS>
b'1'
<EOS>
b'2'
<EOS>
b'VI,\xcf\x81,'
<EOS>
b'\xce\xbdK'
<EOS>
b't, r, and Rmax,'
<EOS>
b'avg'
<EOS>
b'1T cid80T'
<EOS>
b'C'
<EOS>
b't1'
<EOS>
b'Rmax,t.'
<EOS>
b'Considering the term 1T cid80T\n\ncid104'
<EOS>
b'E 1'
<EOS>
b't1\n\n2'
<EOS>
b'\xce\xb5t,0, . . . , \xce\xb5t,K1 t, r'
<EOS>
b'cid105\n\n 1T cid80T'
<EOS>
b't1'
<EOS>
b'cid16cid80K1'
<EOS>
b'k0'
<EOS>
b'\xce\xb12r\n\nt,k\xce\xb5t,k\n\ncid17 1'
<EOS>
b'2'
<EOS>
b'let\n\n\xce\xb1k'
<EOS>
b'cid40'
<EOS>
b'1\xce\xb3\xce\xb3Kk1'
<EOS>
b'1\xce\xb3K1'
<EOS>
b'1\xce\xb3\xce\xb3K'
<EOS>
b'1\xce\xb3K1\n\n0'
<EOS>
b'k'
<EOS>
b'K,\n\n,\n\nk'
<EOS>
b'K'
<EOS>
b'T'
<EOS>
b'cid88'
<EOS>
b'cid32K1'
<EOS>
b'cid88'
<EOS>
b't1'
<EOS>
b'k0'
<EOS>
b'\xce\xb12r\n\nt,k\xce\xb5t,k\n\ncid33 1'
<EOS>
b'2'
<EOS>
b'1'
<EOS>
b'T\n\nT'
<EOS>
b'cid88'
<EOS>
b'cid32K1'
<EOS>
b'cid88'
<EOS>
b't1'
<EOS>
b'k0'
<EOS>
b'cid33 1\n\n2'
<EOS>
b'\xce\xb12r'
<EOS>
b'k \xce\xb5t,k\n\n.'
<EOS>
b'T'
<EOS>
b'cid88'
<EOS>
b'cid32K1'
<EOS>
b'cid88'
<EOS>
b't1'
<EOS>
b'k0'
<EOS>
b'cid33 1\n\n2'
<EOS>
b'\xce\xb12r'
<EOS>
b'k \xce\xb5t,k'
<EOS>
b'cid32K1'
<EOS>
b'cid88'
<EOS>
b'k0'
<EOS>
b'\xce\xb12r'
<EOS>
b'k\n\n1'
<EOS>
b'T\n\nT'
<EOS>
b'cid88'
<EOS>
b't1'
<EOS>
b'cid33 1'
<EOS>
b'2'
<EOS>
b'\xce\xb5t,k\n\n.'
<EOS>
b'then we bound\n\nUsing Jensens inequality\n\n1'
<EOS>
b'T\n\n1'
<EOS>
b'T'
<EOS>
b'So, now we can write 11 as\n\n1'
<EOS>
b'T\n\nT'
<EOS>
b'cid88'
<EOS>
b't1'
<EOS>
b'cid107Q'
<EOS>
b't'
<EOS>
b'Q\xcf\x80K'
<EOS>
b't'
<EOS>
b'cid1071,\xcf\x81'
<EOS>
b'2\xce\xb3'
<EOS>
b'cid20\n\n1'
<EOS>
b'\xce\xb32\n\n1\n2'
<EOS>
b'inf'
<EOS>
b'r0,1'
<EOS>
b'C'
<EOS>
b'VIK'
<EOS>
b'rE'
<EOS>
b'avg\xce\xb5avg,0, . . .'
<EOS>
b', \xce\xb5avg,K1 r\n\n1\n2'
<EOS>
b'cid21'
<EOS>
b'2'
<EOS>
b'1'
<EOS>
b'\xce\xb3'
<EOS>
b'\xce\xb3KRmax,avg\n\n,\n\nwith \xce\xb5avg,k  1T cid80T\n\nt1 \xce\xb5t,k and Eavg\xce\xb5avg,0, . . .'
<EOS>
b', \xce\xb5avg,K1 r  cid80K1\n\nk0'
<EOS>
b'\xce\xb12r'
<EOS>
b'k \xce\xb5avg,k.'
<EOS>
b'Proof of Lemma 4.'
<EOS>
b'Let us start from the de\xef\xac\x81nition of optimal task-averaged risk'
<EOS>
b'\xce\xb5'
<EOS>
b'avg,k'
<EOS>
b'cid107Q'
<EOS>
b't,k1  T'
<EOS>
b't Qt,kcid1072'
<EOS>
b'\xce\xbd,'
<EOS>
b'1'
<EOS>
b'T\n\nT'
<EOS>
b'cid88'
<EOS>
b't1'
<EOS>
b'12'
<EOS>
b'Published as a conference paper at ICLR 2020'
<EOS>
b'where Q\n\nt,k, with t  1, T , are the minimizers of \xce\xb5avg,k.'
<EOS>
b'Consider the task \xcb\x87t'
<EOS>
b'such that\n\nwe can write the following inequality'
<EOS>
b'\xcb\x87tk'
<EOS>
b'arg max'
<EOS>
b'cid107Q'
<EOS>
b't,k1  T'
<EOS>
b't Qt,kcid1072'
<EOS>
b'\xce\xbd,\n\nt1,...'
<EOS>
b',T'
<EOS>
b'cid113'
<EOS>
b'avg,k  cid107Q'
<EOS>
b'\xce\xb5'
<EOS>
b'\xcb\x87tk,k1  T'
<EOS>
b'\xcb\x87t Q\xcb\x87tk,kcid107\xce\xbd.'
<EOS>
b'By the application of Theorem 5.3 by Farahmand 2011 to the right hand side, and de\xef\xac\x81ning\nbk,i  cid107Q\xcb\x87tk,i1  T'
<EOS>
b'\xcb\x87t Q\xcb\x87tk,icid107\xce\xbd'
<EOS>
b', we obtain'
<EOS>
b'cid113'
<EOS>
b'avg,k  cid107Q'
<EOS>
b'\xce\xb5'
<EOS>
b'\xcb\x87tk,k1'
<EOS>
b'T'
<EOS>
b'\xcb\x87t k1Q\xcb\x87tk,0cid107\xce\xbd'
<EOS>
b'\xce\xb3\xcb\x87tk CAE\xce\xbd \xcb\x87tk, P i1bk,k1i.'
<EOS>
b'Squaring both sides yields the result'
<EOS>
b'cid32'
<EOS>
b'\xce\xb5'
<EOS>
b'avg,k'
<EOS>
b'cid107Q'
<EOS>
b'\xcb\x87tk,k1'
<EOS>
b'T'
<EOS>
b'\xcb\x87t k1Q\xcb\x87tk,0cid107\xce\xbd'
<EOS>
b'\xce\xb3\xcb\x87tk CAE\xce\xbd \xcb\x87tk, P i1bk,'
<EOS>
b'k1i\n\n.'
<EOS>
b'cid332'
<EOS>
b'k1'
<EOS>
b'cid88'
<EOS>
b'i0'
<EOS>
b'k1'
<EOS>
b'cid88'
<EOS>
b'i0'
<EOS>
b'A.2 APPROXIMATED POLICY-ITERATION BOUNDS'
<EOS>
b'We start by considering the bound for the API framework'
<EOS>
b'Theorem 5.'
<EOS>
b'Theorem 3.2 of Farahmand 2011 Let K be a positive integer, and Qmax  Rmax\nfor any sequence QkK1'
<EOS>
b'k0  BS'
<EOS>
b'A, Qmax and the corresponding sequence \xce\xb5kK1'
<EOS>
b'\xce\xb5k'
<EOS>
b'cid107Qk'
<EOS>
b'Q\xcf\x80k'
<EOS>
b'cid1072'
<EOS>
b'1\xce\xb3 .'
<EOS>
b'Then\nk0 , where\n\n1'
<EOS>
b'2'
<EOS>
b'C'
<EOS>
b'PI,\xcf\x81,'
<EOS>
b'\xce\xbdK'
<EOS>
b'rE\n\n1'
<EOS>
b'2 \xce\xb50, . . .'
<EOS>
b', \xce\xb5K1 r'
<EOS>
b'\xce\xb3K1Rmax\n\n,\n\n12'
<EOS>
b'cid21'
<EOS>
b'cid107Q  Q\xcf\x80K'
<EOS>
b'cid1071,\xcf\x81 \n\n\xce\xbd'
<EOS>
b', we have'
<EOS>
b'2\xce\xb3\n\n1  \xce\xb32'
<EOS>
b'cid20'
<EOS>
b'inf'
<EOS>
b'r0,1'
<EOS>
b'where\n\nCPI,\xcf\x81,\xce\xbdK'
<EOS>
b'r'
<EOS>
b'cid18 1'
<EOS>
b'\xce\xb3'
<EOS>
b'cid192\n\n2'
<EOS>
b'sup'
<EOS>
b'0,...'
<EOS>
b',\xcf\x80cid48'
<EOS>
b'\xcf\x80cid48'
<EOS>
b'K'
<EOS>
b'K1'
<EOS>
b'cid88'
<EOS>
b'k0'
<EOS>
b'a21r'
<EOS>
b'k'
<EOS>
b'cid88'
<EOS>
b'm0'
<EOS>
b'cid88\n\nm1\n\n\xce\xb3mcPI1,\xcf\x81,'
<EOS>
b'\xce\xbdK'
<EOS>
b'k  1, m'
<EOS>
b'1 \xcf\x80cid48'
<EOS>
b'k1'
<EOS>
b'\xce\xb3mcPI2,\xcf\x81,'
<EOS>
b'\xce\xbdK'
<EOS>
b'k  1, m \xcf\x80cid48\n\nk1,'
<EOS>
b'\xcf\x80cid48'
<EOS>
b'k'
<EOS>
b'cPI3,\xcf\x81,\xce\xbd'
<EOS>
b'2'
<EOS>
b'13'
<EOS>
b'with E\xce\xb50, . . .'
<EOS>
b', \xce\xb5K1 r  cid80K1\nbutions \xcf\x81 and \xce\xbd, and the series \xce\xb1k are de\xef\xac\x81ned as in Farahmand 2011.'
<EOS>
b'k0 \xce\xb12r'
<EOS>
b'k \xce\xb5k, the three coef\xef\xac\x81cients cPI1,\xcf\x81,\xce\xbd, cPI2,\xcf\x81,\xce\xbd, cPI3,\xcf\x81,\xce\xbd, the distri-'
<EOS>
b'From Theorem 5, by computing the average loss across tasks and pushing inside the average using\nJensens inequality, we derive the API bounds averaged on multiple tasks.'
<EOS>
b'Theorem 6.'
<EOS>
b'Let K be a positive integer, and Qmax  Rmax'
<EOS>
b'A, Qmax and the corresponding sequence \xce\xb5avg,kK1\nhave\n\n1\xce\xb3 .'
<EOS>
b'Then for any sequence QkK1'
<EOS>
b'k0 , where \xce\xb5avg,k \n\nt1cid107Qt,k'
<EOS>
b'Q\xcf\x80k'
<EOS>
b'k0'
<EOS>
b'BS'
<EOS>
b'\xce\xbd,'
<EOS>
b'we'
<EOS>
b't'
<EOS>
b'cid1072'
<EOS>
b'cid80'
<EOS>
b'T\n\n1'
<EOS>
b'T\n\n1'
<EOS>
b'T\n\nT'
<EOS>
b'cid88'
<EOS>
b't1'
<EOS>
b'cid107Q'
<EOS>
b't'
<EOS>
b'Q\xcf\x80K'
<EOS>
b't'
<EOS>
b'cid1071,\xcf\x81'
<EOS>
b'2\xce\xb3'
<EOS>
b'cid20\n\n1'
<EOS>
b'\xce\xb32\n\ninf'
<EOS>
b'r0,1'
<EOS>
b'1'
<EOS>
b'2'
<EOS>
b'1'
<EOS>
b'2'
<EOS>
b'C'
<EOS>
b'PIK'
<EOS>
b'rE'
<EOS>
b'avg\xce\xb5avg,0, . . .'
<EOS>
b', \xce\xb5avg,K1 r'
<EOS>
b'\xce\xb3K1Rmax,'
<EOS>
b'avg\n\ncid3 ,\n\n14'
<EOS>
b'13'
<EOS>
b'Published as a conference paper at ICLR 2020\n\nwith Eavg  cid80K1'
<EOS>
b'k0'
<EOS>
b'\xce\xb12r'
<EOS>
b'k \xce\xb5avg,k, \xce\xb3'
<EOS>
b'max'
<EOS>
b'cid40'
<EOS>
b'1\xce\xb3\xce\xb3Kk1'
<EOS>
b't1,'
<EOS>
b'...'
<EOS>
b',T'
<EOS>
b'1'
<EOS>
b'T'
<EOS>
b'cid80'
<EOS>
b'T'
<EOS>
b't1'
<EOS>
b'Rmax,t and \xce\xb1k'
<EOS>
b'1\xce\xb3K1'
<EOS>
b'1\xce\xb3\xce\xb3K'
<EOS>
b'1\xce\xb3K1\n\n0'
<EOS>
b'k'
<EOS>
b'K,\n\n.'
<EOS>
b'k'
<EOS>
b'K\n\n1\n2\n\n1'
<EOS>
b'2'
<EOS>
b'\xce\xb3t, C'
<EOS>
b'PIK r'
<EOS>
b'max'
<EOS>
b'C'
<EOS>
b'PI,\xcf\x81,\xce\xbdK'
<EOS>
b't, r, Rmax,'
<EOS>
b'avg \n\nt1,'
<EOS>
b'...'
<EOS>
b',T'
<EOS>
b'Proof of Theorem 6.'
<EOS>
b'The proof is very similar to the one for AVI.'
<EOS>
b'We compute the average expected'
<EOS>
b'loss across tasks'
<EOS>
b'1'
<EOS>
b'T\n\nT'
<EOS>
b'cid88'
<EOS>
b't1'
<EOS>
b'cid107Q'
<EOS>
b't'
<EOS>
b'Q\xcf\x80K'
<EOS>
b't'
<EOS>
b'cid1071,\xcf\x81\n\n1\nT\n\nT'
<EOS>
b'cid88'
<EOS>
b't1'
<EOS>
b'2\xce\xb3'
<EOS>
b'1'
<EOS>
b'\xce\xb32\n\n1'
<EOS>
b'\xce\xb32\n\n1  \xce\xb32'
<EOS>
b'2\xce\xb3'
<EOS>
b'2\xce\xb3'
<EOS>
b'2\xce\xb3'
<EOS>
b'2\xce\xb3t'
<EOS>
b'cid20\n\n1'
<EOS>
b'\xce\xb3t2\n\ninf'
<EOS>
b'r0,1'
<EOS>
b'T'
<EOS>
b'cid88'
<EOS>
b'cid20'
<EOS>
b't1'
<EOS>
b'inf'
<EOS>
b'r0,1'
<EOS>
b'1'
<EOS>
b'T\n\nT'
<EOS>
b'cid88'
<EOS>
b'cid18'
<EOS>
b't1'
<EOS>
b'inf'
<EOS>
b'r0,1'
<EOS>
b'1'
<EOS>
b'T\n\ninf'
<EOS>
b'r0,1'
<EOS>
b'T'
<EOS>
b'cid88'
<EOS>
b'cid16'
<EOS>
b't1'
<EOS>
b'1'
<EOS>
b'T'
<EOS>
b'cid34'
<EOS>
b'cid34'
<EOS>
b'cid34'
<EOS>
b'1'
<EOS>
b'2'
<EOS>
b'1'
<EOS>
b'2'
<EOS>
b'1'
<EOS>
b'T\n\n1\n2'
<EOS>
b'C'
<EOS>
b'PI,\xcf\x81,'
<EOS>
b'\xce\xbdK'
<EOS>
b't,'
<EOS>
b'rE\n\n1'
<EOS>
b'2'
<EOS>
b'\xce\xb5t,0, . . . , \xce\xb5t,K1 t, r'
<EOS>
b'\xce\xb3K1'
<EOS>
b'Rmax,t'
<EOS>
b't'
<EOS>
b'1'
<EOS>
b'2'
<EOS>
b'C'
<EOS>
b'PI,\xcf\x81,'
<EOS>
b'\xce\xbdK'
<EOS>
b't,'
<EOS>
b'rE\n\n1'
<EOS>
b'2'
<EOS>
b'\xce\xb5t,0, . . . , \xce\xb5t,K1 t, r'
<EOS>
b'\xce\xb3K1'
<EOS>
b'Rmax,t'
<EOS>
b't'
<EOS>
b'C'
<EOS>
b'PI,\xcf\x81,\xce\xbdK'
<EOS>
b't, rE'
<EOS>
b'2'
<EOS>
b'\xce\xb5t,0, . . . , \xce\xb5t,K1 t, r\n\n \xce\xb3K1Rmax,avg'
<EOS>
b'C'
<EOS>
b'PI,\xcf\x81,'
<EOS>
b'\xce\xbdK'
<EOS>
b't, rE'
<EOS>
b'2'
<EOS>
b'\xce\xb5t,0, . . . , \xce\xb5t,K1 t, r\n\n \xce\xb3K1Rmax,avg\n\n1\n\n1'
<EOS>
b'cid21'
<EOS>
b'cid21'
<EOS>
b'cid35'
<EOS>
b'cid35'
<EOS>
b'cid35\n\n1'
<EOS>
b'\xce\xb32\n\ninf'
<EOS>
b'r0,1\n\n1'
<EOS>
b'2'
<EOS>
b'C'
<EOS>
b'PIK r'
<EOS>
b'T'
<EOS>
b'cid88'
<EOS>
b'cid16\n\n1'
<EOS>
b't1'
<EOS>
b'E'
<EOS>
b'2'
<EOS>
b'\xce\xb5t,0, . . . , \xce\xb5t,K1 t, r\n\n \xce\xb3K1Rmax,avg\n\n.'
<EOS>
b'15'
<EOS>
b'cid17'
<EOS>
b'Using Jensens inequality as in the AVI scenario, we can write 15 as\n\n1'
<EOS>
b'T\n\nT'
<EOS>
b'cid88'
<EOS>
b't1'
<EOS>
b'cid107Q'
<EOS>
b't'
<EOS>
b'Q\xcf\x80K'
<EOS>
b't'
<EOS>
b'cid1071,\xcf\x81'
<EOS>
b'2\xce\xb3'
<EOS>
b'cid20\n\n1'
<EOS>
b'\xce\xb32\n\ninf'
<EOS>
b'r0,1'
<EOS>
b'1'
<EOS>
b'2'
<EOS>
b'1'
<EOS>
b'2'
<EOS>
b'C'
<EOS>
b'PIK'
<EOS>
b'rE'
<EOS>
b'avg\xce\xb5avg,0, . . .'
<EOS>
b', \xce\xb5avg,K1 r'
<EOS>
b'\xce\xb3K1Rmax,'
<EOS>
b'avg\n\ncid3 ,\n\n16'
<EOS>
b'with \xce\xb5avg,k  1T cid80T\n\nt1 \xce\xb5t,k and Eavg\xce\xb5avg,0, . . .'
<EOS>
b', \xce\xb5avg,K1 r  cid80K1\n\nk0'
<EOS>
b'\xce\xb12r'
<EOS>
b'k \xce\xb5avg,k.'
<EOS>
b'A.3'
<EOS>
b'APPROXIMATION'
<EOS>
b'BOUNDS\n\n1,'
<EOS>
b'.'
<EOS>
b'. .'
<EOS>
b', w'
<EOS>
b'Proof of Theorem 3.'
<EOS>
b'Let w'
<EOS>
b'cid32'
<EOS>
b'T , h and f \n\n1 , . . .'
<EOS>
b', f \n\nT be the minimizers of \xce\xb5'
<EOS>
b'avg'
<EOS>
b', then\n\n\xce\xb5avg \xcb\x86w, \xcb\x86h, \xcb\x86f   \xce\xb5\n\navg'
<EOS>
b'\xce\xb5avg \xcb\x86w, \xcb\x86h, \xcb\x86f  \n\ncid96 \xcb\x86ft\xcb\x86h \xcb\x86wtXti, Yti\n\ncid19'
<EOS>
b'cid17'
<EOS>
b'cid33'
<EOS>
b'cid125'
<EOS>
b'cid96 \xcb\x86ft\xcb\x86h \xcb\x86wtXti, Yti \n\ncid96f'
<EOS>
b't'
<EOS>
b'hw'
<EOS>
b't Xti, Yti\n\ncid124'
<EOS>
b'cid32'
<EOS>
b'cid124'
<EOS>
b'cid32'
<EOS>
b'cid124'
<EOS>
b'1'
<EOS>
b'nT'
<EOS>
b'cid88'
<EOS>
b'ti\n\n1'
<EOS>
b'nT'
<EOS>
b'cid88'
<EOS>
b'ti'
<EOS>
b'cid88\n\n1'
<EOS>
b'nT'
<EOS>
b'cid123cid122'
<EOS>
b'B'
<EOS>
b'ti'
<EOS>
b'cid33'
<EOS>
b'cid125\n\ncid96f \n\nt'
<EOS>
b'hw'
<EOS>
b't Xti,'
<EOS>
b'Yti  \xce\xb5'
<EOS>
b'avg\n\n.'
<EOS>
b'cid33'
<EOS>
b'cid125'
<EOS>
b'17'
<EOS>
b'We proceed to bound the three components individually'
<EOS>
b'C can be bounded using Hoeffdings inequality, with probability 1  \xce\xb42 by cid112ln2\xce\xb42nT ,\n\nas it contains only nT'
<EOS>
b'random variables bounded in the interval 0, 1'
<EOS>
b'cid88\n\n1'
<EOS>
b'nT'
<EOS>
b'ti'
<EOS>
b'cid123cid122'
<EOS>
b'A\n\ncid123cid122'
<EOS>
b'C'
<EOS>
b'14'
<EOS>
b'Published as a conference paper at ICLR 2020\n\n B can be bounded by 0, by de\xef\xac\x81nition of \xcb\x86w, \xcb\x86h and \xcb\x86f , as they are the minimizers of Equa-'
<EOS>
b'tion 3'
<EOS>
b'the bounding of A is less straightforward and is described in the following.'
<EOS>
b'We de\xef\xac\x81ne the following auxiliary function spaces'
<EOS>
b'W'
<EOS>
b'cid48'
<EOS>
b'x  X  wtxti'
<EOS>
b'w1, . . .'
<EOS>
b','
<EOS>
b'wT'
<EOS>
b'W T ,\n F cid48'
<EOS>
b'cid8y  RKT n'
<EOS>
b'ftyti'
<EOS>
b'f1, . . .'
<EOS>
b','
<EOS>
b'fT   F T cid9,\n\nand the following auxiliary sets\n\n S  cid8cid96fthwtXti, Yti  f'
<EOS>
b'F T , h  H, w'
<EOS>
b'W T cid9'
<EOS>
b'RT n,\n Scid48  F cid48HW'
<EOS>
b'cid48'
<EOS>
b'X'
<EOS>
b'cid8fthwtXti  f'
<EOS>
b'F T , h  H, w'
<EOS>
b'W T cid9'
<EOS>
b'RT n,\n Scid48cid48'
<EOS>
b'HW cid48 X'
<EOS>
b'cid8hwtXti  h  H, w  W T cid9'
<EOS>
b'RKT n,\n\nwhich will be useful in our proof.'
<EOS>
b'Using Theorem 9 by Maurer et al.'
<EOS>
b'2016, we can write'
<EOS>
b'\xce\xb5avg \xcb\x86w, \xcb\x86h, \xcb\x86f'
<EOS>
b'cid96 \xcb\x86ft\xcb\x86h \xcb\x86wtXti,'
<EOS>
b'Yti\n\n1'
<EOS>
b'nT'
<EOS>
b'cid88'
<EOS>
b'ti'
<EOS>
b'sup'
<EOS>
b'\xce\xb5avgw, h, f  \n\ncid96fthwtXti,'
<EOS>
b'Yti\n\n1'
<EOS>
b'nT'
<EOS>
b'cid88'
<EOS>
b'ti'
<EOS>
b'cid32'
<EOS>
b'wW T ,hH,f F T'
<EOS>
b'cid115'
<EOS>
b'2\xcf\x80GS'
<EOS>
b'nT'
<EOS>
b'9 ln 2'
<EOS>
b'\xce\xb4'
<EOS>
b'2nT'
<EOS>
b',\n\nthen by Lipschitz property of the loss function cid96 and the contraction lemma Corollary 11 Maurer et al.'
<EOS>
b'1 and ccid48\n2016'
<EOS>
b'GS  GScid48.'
<EOS>
b'By Theorem 12 by Maurer et al. 2016, for universal constants ccid48'
<EOS>
b'2'
<EOS>
b'GScid48'
<EOS>
b'ccid48'
<EOS>
b'1LF cid48GScid48cid48'
<EOS>
b'ccid48'
<EOS>
b'2DScid48cid48OF'
<EOS>
b'cid48'
<EOS>
b'min'
<EOS>
b'yY'
<EOS>
b'GFy,\n\nwhere LF cid48 is the largest value for the Lipschitz constants in the function space F cid48, and DScid48cid48 is\nthe Euclidean diameter of the set Scid48cid48.'
<EOS>
b'Using Theorem 12 by Maurer et al. 2016 again, for universal constants ccid48cid48\n\n1 and ccid48cid48\n2 \n\nGScid48cid48  ccid48cid48'
<EOS>
b'1 LHGW'
<EOS>
b'cid48 X'
<EOS>
b'ccid48cid48'
<EOS>
b'2 DW'
<EOS>
b'cid48 XOH'
<EOS>
b'min'
<EOS>
b'GHp.'
<EOS>
b'20'
<EOS>
b'Putting 19 and 20 together'
<EOS>
b'cid18'
<EOS>
b'GScid48'
<EOS>
b'ccid48'
<EOS>
b'1LF'
<EOS>
b'cid48'
<EOS>
b'1 LHGW'
<EOS>
b'cid48 X  ccid48cid48'
<EOS>
b'ccid48cid48'
<EOS>
b'2 DW'
<EOS>
b'cid48 XOH'
<EOS>
b'min'
<EOS>
b'GHp'
<EOS>
b'pP'
<EOS>
b'pP'
<EOS>
b'cid19'
<EOS>
b'ccid48\n\n ccid48'
<EOS>
b'1ccid48cid48\n\n ccid48'
<EOS>
b'GFy'
<EOS>
b'2DScid48cid48OF'
<EOS>
b'cid48'
<EOS>
b'min'
<EOS>
b'yY'
<EOS>
b'1'
<EOS>
b'LF cid48LHGW cid48 X  ccid48'
<EOS>
b'2DScid48cid48OF'
<EOS>
b'cid48  min'
<EOS>
b'yY'
<EOS>
b'GFy.'
<EOS>
b'1ccid48cid48'
<EOS>
b'2 LF cid48DW cid48'
<EOS>
b'XOH'
<EOS>
b'ccid48'
<EOS>
b'1LF'
<EOS>
b'cid48'
<EOS>
b'min'
<EOS>
b'pP'
<EOS>
b'GHp'
<EOS>
b'At this point, we have to bound the individual terms in the right hand side of 21, following the same\nprocedure proposed by Maurer et al. 2016.'
<EOS>
b'15'
<EOS>
b'cid33'
<EOS>
b'18'
<EOS>
b'19'
<EOS>
b'21'
<EOS>
b'Published as a conference paper at ICLR 2020'
<EOS>
b'Firstly, to bound LF cid48, let y, ycid48'
<EOS>
b'RKT n, where y  yti with yti  RK and ycid48  ycid48'
<EOS>
b'ti'
<EOS>
b'RK.'
<EOS>
b'We can write the following'
<EOS>
b'ycid48'
<EOS>
b'ti with\n\ncid107f'
<EOS>
b'y'
<EOS>
b'f'
<EOS>
b'ycid48cid1072'
<EOS>
b'ftyti'
<EOS>
b'ftycid48'
<EOS>
b'ti2'
<EOS>
b'cid88'
<EOS>
b'ti'
<EOS>
b'LF2'
<EOS>
b'cid88'
<EOS>
b'cid107yti  ycid48'
<EOS>
b'ticid1072'
<EOS>
b'ti'
<EOS>
b'LF2cid107y'
<EOS>
b'ycid48cid1072,\n\n22'
<EOS>
b'23'
<EOS>
b'24'
<EOS>
b'whence LF cid48  LF.'
<EOS>
b'Then, we bound\n\nGW'
<EOS>
b'cid48'
<EOS>
b'X'
<EOS>
b'E'
<EOS>
b'cid34'
<EOS>
b'sup'
<EOS>
b'wW'
<EOS>
b'T'
<EOS>
b'cid12\ncid12\ncid12'
<EOS>
b'\xce\xb3ktiwtkXti\ncid12'
<EOS>
b'cid12'
<EOS>
b'cid88'
<EOS>
b'kti'
<EOS>
b'cid35'
<EOS>
b'cid88'
<EOS>
b'Xti'
<EOS>
b'cid34'
<EOS>
b'E'
<EOS>
b'cid88\n\ncid12'
<EOS>
b'cid12'
<EOS>
b'cid12'
<EOS>
b'\xce\xb3kliwkXli\ncid12'
<EOS>
b'cid12'
<EOS>
b'cid35'
<EOS>
b'Xli'
<EOS>
b'sup'
<EOS>
b'l1,...'
<EOS>
b',T'
<EOS>
b'sup'
<EOS>
b'wW'
<EOS>
b't'
<EOS>
b'T'
<EOS>
b'sup'
<EOS>
b'l1,...'
<EOS>
b',T'
<EOS>
b'ki'
<EOS>
b'GWXl.'
<EOS>
b'Then, since it is possible to bound the Euclidean diameter using the norm of the supremum value in\nthe set, we bound DScid48cid48  2 suph,wcid107hw Xcid107 and DW cid48 X  2'
<EOS>
b'supwW T cid107w Xcid107.'
<EOS>
b'Also, we bound OF cid48\n\ncid21'
<EOS>
b'cid104\xce\xb3, gy  gycid48cid105\n\n E'
<EOS>
b'cid34'
<EOS>
b'cid20'
<EOS>
b'E'
<EOS>
b'sup'
<EOS>
b'gF'
<EOS>
b'cid48\n\n\xce\xb3ti ftyti'
<EOS>
b'ftycid48'
<EOS>
b'ti'
<EOS>
b'\xce\xb3i f'
<EOS>
b'yti'
<EOS>
b'f'
<EOS>
b'ycid48'
<EOS>
b'cid35'
<EOS>
b'cid35'
<EOS>
b'ti'
<EOS>
b'cid88'
<EOS>
b'ti'
<EOS>
b'sup'
<EOS>
b'f F T'
<EOS>
b'cid34'
<EOS>
b'sup'
<EOS>
b'f F'
<EOS>
b'cid88'
<EOS>
b'i'
<EOS>
b'cid34'
<EOS>
b'cid88'
<EOS>
b'E'
<EOS>
b't'
<EOS>
b'sup'
<EOS>
b'f F'
<EOS>
b'cid88'
<EOS>
b'i'
<EOS>
b'\xce\xb3i f yti  f'
<EOS>
b'ycid48'
<EOS>
b'ti'
<EOS>
b'cid88'
<EOS>
b'OF2 cid88'
<EOS>
b'cid107yti  ycid48'
<EOS>
b'ticid1072'
<EOS>
b'cid33 1\n\n2'
<EOS>
b'cid88'
<EOS>
b'E'
<EOS>
b't\n\n\n\n\n\n\n\nT'
<EOS>
b'cid32'
<EOS>
b'T\n\n\n\nt'
<EOS>
b'i'
<EOS>
b'T'
<EOS>
b'OFcid107y  ycid48cid107,\n\n\n\n1'
<EOS>
b'2'
<EOS>
b'cid352\n\n\nwhence OF'
<EOS>
b'cid48 \n\nT OF.'
<EOS>
b'To minimize the last term, it is possible to choose y0  0, as f 0  0, f  F, resulting in\nminyY GFy  GF0  0.'
<EOS>
b'Then, substituting in 21, and recalling that GS  GScid48\n\nGS  ccid48'
<EOS>
b'1ccid48cid48'
<EOS>
b'1 LFLHT'
<EOS>
b'sup'
<EOS>
b'GWXl'
<EOS>
b'2ccid48'
<EOS>
b'l1,...'
<EOS>
b',T'
<EOS>
b'1ccid48cid48'
<EOS>
b'2 LF sup'
<EOS>
b'wW'
<EOS>
b'T'
<EOS>
b'cid107w Xcid107OH\n\n ccid48'
<EOS>
b'1LF'
<EOS>
b'min'
<EOS>
b'pP'
<EOS>
b'GHp  2ccid48'
<EOS>
b'cid107hw Xcid107'
<EOS>
b'T OF.'
<EOS>
b'25'
<EOS>
b'2 sup'
<EOS>
b'h,w\n\n16'
<EOS>
b'Published as a conference paper at ICLR 2020'
<EOS>
b'Now, the \xef\xac\x81rst term A of 17 can be bounded substituting 25 in 18\n\n\xce\xb5avg \xcb\x86w, \xcb\x86h, \xcb\x86f  \n\n\n1\nnT'
<EOS>
b'cid88'
<EOS>
b'ti'
<EOS>
b'cid96 \xcb\x86ft\xcb\x86h \xcb\x86wtXti,'
<EOS>
b'Yti\n\n\n\ncid16\n\n2\xcf\x80'
<EOS>
b'nT'
<EOS>
b'ccid48'
<EOS>
b'1ccid48cid48'
<EOS>
b'1 LFLHT'
<EOS>
b'sup'
<EOS>
b'GWXl'
<EOS>
b'2ccid48'
<EOS>
b'1ccid48cid48'
<EOS>
b'l1,...'
<EOS>
b',T'
<EOS>
b'cid107w'
<EOS>
b'Xcid107OH\n\n ccid48'
<EOS>
b'1LF'
<EOS>
b'min'
<EOS>
b'pP'
<EOS>
b'GHp'
<EOS>
b'2ccid48'
<EOS>
b'cid107hw'
<EOS>
b'Xcid107'
<EOS>
b'2 sup'
<EOS>
b'h,'
<EOS>
b'w\n\nLFLH supl1,...'
<EOS>
b',T  GWXl\n\n c1'
<EOS>
b'n'
<EOS>
b'LF'
<EOS>
b'minpP'
<EOS>
b'GHp\n\n c3\n\n c4'
<EOS>
b'nT'
<EOS>
b'2 LF sup'
<EOS>
b'wW'
<EOS>
b'T'
<EOS>
b'cid115'
<EOS>
b'cid17'
<EOS>
b'T OF\n\n9 ln 2'
<EOS>
b'\xce\xb4'
<EOS>
b'2nT\nsupwcid107w'
<EOS>
b'Xcid107LFOH\n\n\n\n c2'
<EOS>
b'nT'
<EOS>
b'suph,wcid107hw'
<EOS>
b'Xcid107OF'
<EOS>
b'cid115\n\n\n\n9 ln 2'
<EOS>
b'\xce\xb4'
<EOS>
b'2nT\n\n.'
<EOS>
b'A union bound between A, B and C of 17 completes the proof\n\n\xce\xb5avg \xcb\x86w, \xcb\x86h, \xcb\x86f'
<EOS>
b'\xce\xb5'
<EOS>
b'avg'
<EOS>
b'c1'
<EOS>
b'LFLH supl1,...'
<EOS>
b',T  GWXl'
<EOS>
b'n'
<EOS>
b'T'
<EOS>
b'n'
<EOS>
b'c2\n\n c3\n\n c4'
<EOS>
b'cid115'
<EOS>
b'supwcid107w'
<EOS>
b'Xcid107LFOH'
<EOS>
b'LF'
<EOS>
b'minpP GHp\n\nsuph,wcid107hw'
<EOS>
b'Xcid107OF'
<EOS>
b'nT'
<EOS>
b'nT'
<EOS>
b'n'
<EOS>
b'T\n\n8'
<EOS>
b'ln 3'
<EOS>
b'\xce\xb4'
<EOS>
b'nT\n\n.'
<EOS>
b'B'
<EOS>
b'ADDITIONAL DETAILS OF EMPIRICAL EVALUATION'
<EOS>
b'B.1 MULTI FITTED Q-ITERATION'
<EOS>
b'We consider Car-On-Hill problem with discount factor 0.95 and horizon 100.'
<EOS>
b'Running Adam\noptimizer with learning rate 0.001 and using a mean squared loss, we train a neural network composed\nof 2 shared layers of 30 neurons each, with sigmoidal activation function, as described in Riedmiller\n2005.'
<EOS>
b'We select 8 tasks for the problem changing the mass of the car m and the value of the\ndiscrete actions a Table 1.'
<EOS>
b'Figure 1b is computed considering the \xef\xac\x81rst four tasks, while Figure 1c\nconsiders task 1 in the result with 1 task, tasks 1 and 2 for the result with 2 tasks, tasks 1, 2, 3, and 4\nfor the result with 4 tasks, and all the tasks for the result with 8 tasks.'
<EOS>
b'To run FQI and MFQI, for each\ntask we collect transitions running an extra-tree trained following the procedure and setting in Ernst'
<EOS>
b'et al. 2005, using an cid15-greedy policy with cid15  0.1, to obtain a small, but representative dataset.'
<EOS>
b'The\noptimal Q-function for each task is computed by tree-search3 for 100 states uniformly picked from\nthe state space, and the 2 discrete actions, for a total of 200 state-action tuples.'
<EOS>
b'B.2 MULTI DEEP Q-NETWORK'
<EOS>
b'The \xef\xac\x81ve problems we consider for this experiment are Cart-Pole, Acrobot, Mountain-Car, Car-'
<EOS>
b'On-'
<EOS>
b'Hill, and Inverted-Pendulum4.'
<EOS>
b'The discount factors are respectively 0.99, 0.99, 0.99, 0.95, and 0.95.'
<EOS>
b'The horizons are respectively 500, 1, 000, 1, 000, 100, and 3, 000.'
<EOS>
b'The network we use consists of 80\nReLu units for each wt,'
<EOS>
b't'
<EOS>
b'1, . . .'
<EOS>
b', T  block, with T  5.'
<EOS>
b'Then, the shared block h consists of one'
<EOS>
b'3We follow the method described in Ernst et al. 2005.'
<EOS>
b'4The IDs of the problems in the OpenAI Gym library are CartPole-v0, Acrobot-v1, and MountainCar-v0.'
<EOS>
b'17'
<EOS>
b'Published as a conference paper at ICLR 2020'
<EOS>
b'Task Mass'
<EOS>
b'1.0\n0.8'
<EOS>
b'1.0'
<EOS>
b'1.2'
<EOS>
b'1.0'
<EOS>
b'1.0'
<EOS>
b'0.8'
<EOS>
b'0.85'
<EOS>
b'1'
<EOS>
b'2'
<EOS>
b'3'
<EOS>
b'4'
<EOS>
b'5'
<EOS>
b'6'
<EOS>
b'7'
<EOS>
b'8'
<EOS>
b'Action set\n4.0 4.0'
<EOS>
b'4.0 4.0'
<EOS>
b'4.5 4.5'
<EOS>
b'4.5 4.5'
<EOS>
b'4.125 4.125\n\n4.25 4.25\n\n4.375 4.375'
<EOS>
b'4.0 4.0\n\nTable 1 Different values of the mass of the car and available actions chosen for the Car-On-Hill tasks\nin the MFQI empirical evaluation.'
<EOS>
b'i   yts, at\n\ni   fthwts, at\n\nlayer with 80 ReLu units and another one with 80 sigmoid units.'
<EOS>
b'Eventually, each ft has a number of\nlinear units equal to the number of discrete actions at\n,'
<EOS>
b'i  1, . . .'
<EOS>
b', At of task \xc2\xb5t which outputs'
<EOS>
b'i'
<EOS>
b'the action-value Qts, at'
<EOS>
b'i , s  S'
<EOS>
b't.'
<EOS>
b'The use of sigmoid units\nin the second layer of h is due to our choice to extract meaningful shared features bounded between 0\nand 1 to be used as input of the last linear layer, as in most RL approaches.'
<EOS>
b'In practice, we have also\nfound that sigmoid units help to reduce task interference in multi-task networks, where instead the\nlinear response of ReLu units cause a problematic increase in the feature values.'
<EOS>
b'Furthermore, the use\nof a bounded feature space reduces the suph,wcid107hw'
<EOS>
b'Xcid107 term in the upper bound of Theorem 3,'
<EOS>
b'corresponding to the upper bound of the diameter of the feature space, as shown in Appendix A.'
<EOS>
b'The initial replay memory size for each task is 100 and the maximum size is 5, 000.'
<EOS>
b'We use Huber\nloss with Adam optimizer using learning rate 103 and batch size of 100 samples for each task.'
<EOS>
b'The\ntarget network is updated every 100 steps.'
<EOS>
b'The exploration is \xce\xb5-greedy with \xce\xb5 linearly decaying from\n1 to 0.01 in the \xef\xac\x81rst 5, 000 steps.'
<EOS>
b'B.3 MULTI DEEP DETERMINISTIC POLICY GRADIENT'
<EOS>
b'The two set of problems we consider for this experiment are one including Inverted-Pendulum,\nInverted-Double-Pendulum, and Inverted-Pendulum-Swingup, and another one including Hopper-'
<EOS>
b'Stand, Walker-Walk, and Half-Cheetah-Run5.'
<EOS>
b'The discount factors are 0.99 and the horizons are\n1, 000 for all problems.'
<EOS>
b'The actor network is composed of 600 ReLu units for each wt, t  1, . . .'
<EOS>
b', T \nblock, with T  3.'
<EOS>
b'The shared block h has 500 units with ReLu activation function as for MDQN.'
<EOS>
b'Finally, each ft has a number of tanh units equal to the number of dimensions of the continuous\nactions at  At of task \xc2\xb5t which outputs the policy \xcf\x80ts'
<EOS>
b'yts  fthwts, s'
<EOS>
b'S t.'
<EOS>
b'On the other hand, the critic network consists of the same wt units of the actor, except for the use of\nsigmoidal units in the h layer, as in MDQN.'
<EOS>
b'In addition to this, the actions at are given as input to\nh.'
<EOS>
b'Finally, each ft has a single linear unit Qts, at  yts, at  fthwts, at, s'
<EOS>
b'S t.'
<EOS>
b'The initial replay memory size for each task is 64 and the maximum size is 50, 000.'
<EOS>
b'We use Huber\nloss to update the critic network and the policy gradient to update the actor network.'
<EOS>
b'In both cases\nthe optimization is performed with Adam optimizer and batch size of 64 samples for each task.'
<EOS>
b'The\nlearning rate of the actor is 104 and the learning rate of the critic is 103.'
<EOS>
b'Moreover, we apply\ncid962-penalization to the critic network using a regularization coef\xef\xac\x81cient of 0.01.'
<EOS>
b'The target networks are\nupdated with soft-updates using \xcf\x84  103.'
<EOS>
b'The exploration is performed using the action computed\nby the actor network adding a noise generated with an Ornstein-Uhlenbeck process with \xce\xb8  0.15\nand \xcf\x83  0.2.'
<EOS>
b'Note that most of these values are taken from the original DDPG paper'
<EOS>
b'Lillicrap'
<EOS>
b'et al.'
<EOS>
b'2015, which optimizes them for the single-task scenario.'
<EOS>
b'5The'
<EOS>
b'IDs of\n\nthe problems\n\nInvertedPendulumBulletEnv-v0,\nInvertedDoublePendulumBulletEnv-v0, and InvertedPendulumSwingupBulletEnv-v0.'
<EOS>
b'The names of the\ndomain and the task of the problems in the DeepMind Control Suite are hopper-stand, walker-walk, and\ncheetah-run.'
<EOS>
b'in the pybullet\n\nlibrary are\n\n18'
<EOS>
