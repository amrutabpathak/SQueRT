0 2 0 2    r p A 9 1         ]  V Cs c [      1 v 5 0 7 0 1  4 0 0 2 : v i X r a  A Committee of Convolutional Neural Networks  for Image Classification in the Concurrent  Presence of Feature and Label Noise  Stanis(cid:32)law Ka´zmierczak((cid:66))[0000−0002−8981−1592] and  Jacek Ma´ndziuk[0000−0003−0947−028X]  Faculty of Mathematics and Information Science  Warsaw University of Technology   {s.kazmierczak mandziuk}@mini.pw.edu.pl  Warsaw  Poland  Abstract.Image classification has become a ubiquitous task.Models trained on good quality data achieve accuracy which in some applica- tion domains is already above human-level performance.
<EOS>
Unfortunately  real-world data are quite often degenerated by the noise existing in fea- tures and/or labels.There are quite many papers that handle the prob- lem of either feature or label noise  separately.However  to the best of our knowledge  this piece of research is the first attempt to address the problem of concurrent occurrence of both types of noise.
<EOS>
Basing on the MNIST  CIFAR-10 and CIFAR-100 datasets  we experimentally proved that the diﬀerence by which committees beat single models increases along with noise level  no matter it is an attribute or label disruption.Thus  it makes ensembles legitimate to be applied to noisy images with noisy labels.The aforementioned committees’ advantage over single mod- els is positively correlated with dataset diﬃculty level as well.
<EOS>
We propose three committee selection algorithms that outperform a strong baseline algorithm which relies on an ensemble of individual (nonassociated) best models. Keywords: Committee of classifiers  Ensemble learning  Label noise  Feature noise  Convolutional neural networks. 1  Introduction  Standard image classification task consists in assigning a correct label to an input sample picture.
<EOS>
In the most widely-used supervised learning approach  one trains a model to recognize the correct class by providing input-output image-label pairs (training set).In many cases  the achieved accuracy is very high [6 38]  close to or above human-level performance [7 15]. The quality of real-world images is not perfect.
<EOS>
Data may contain some noise defined as anything that blurs the relationship between the attributes of an instance and its class [16].There are mainly two types of noise considered in the literature: feature (attribute) noise and class (label) noise [11 27 36]. 2  S. Ka´zmierczak and J. Ma´ndziuk  Despite the fact that machine algorithms (especially those based on deep architectures) perform on par with humans or even better on high-quality pic- tures  their performance on distorted images is noticeably worse [10].
<EOS>
Similarly  label noise may potentially result in many negative consequences  e.g. deterio- ration of prediction accuracy along with an increase of model’s complexity  size of a training set  or length of a training process [11].Hence  it is necessary to devise methods that reduce noise or are able to perform well in its presence.The problem is furthermore important considering the fact that the acquisition of accurately labeled data is usually time-consuming  expensive and often requires a substantial engagement of human experts [2].
<EOS>
 There are many papers in the literature which tackle the problem of label noise.Likewise  a lot of works have been dedicated to studying attribute noise.However  to the best of our knowledge  there are no papers that consider the problem of feature and label noise occurring simultaneously.
<EOS>
In this paper  we present the method that successfully deals with the concurrent presence of at- tribute noise and class noise. 1.1 The main contribution  Encouraged by the promising results of ensemble models applied to label noise and CNN-based architectures utilized to handle noisy images we examine how a committee of CNN classifiers (each trained on the whole dataset) deal with noisy images marked with noisy labels.With regard to the common taxonomy  there are four groups of ensemble methods [22].
<EOS>
The first one relates to data selection mechanisms aiming to provide diﬀerent subset for every single classifier to be trained on.The second one refers to the feature level.Methods among this group select features that each model uses.
<EOS>
The third one  the classifier level group  comprises algorithms that have to determine the base model  the number of classifiers  other types of classifiers  etc.The final one refers to the combination of classifiers level where an algorithm has to decide how to combine models’ individual decisions to make a final prediction.In this study  we assume having a set of well-trained CNNs which make the ultimate decision by means of soft voting (averaging) scheme [26].
<EOS>
We concentrate on the task of finding an optimal or near-optimal model committee that deals with concurrent presence of attribute and label noise in the image classification problem.In summary  the main contribution of this work is threefold:  – addressing the problem of simultaneously occurring feature and label noise  which  to the best of our knowledge  is a novel unexplored setting;  – designing three methods of building committees of classifiers which outper- form a strong baseline algorithm that employs a set of individually best models;  – proving empirically that a margin of ensembles gain over the best single model rises along with an increase of both noise types  as well as dataset diﬃculty  which makes proposed approaches specifically well-suited to the case of noisy images with noisy labels. Concurrent Presence of Feature and Label Noise  3  The remainder of this paper is arranged as follows.
<EOS>
Section 2 provides a litera- ture review  with considerable emphasis on methods addressing label noise and distorted images.Section 3 introduces the proposed novel algorithms for classi- fiers’ selection.Sections 4 and 5 describe the experimental setup and analysis of results  respectively.
<EOS>
Finally  brief conclusions and directions for further research are presented in the last section. 2 Related literature  Many possible sources of label noise have been identified in the literature [11]  e.g. insuﬃcient information provided to the expert [11 3]  expert (human or machine) mistakes [25 32]  the subjectivity of the task [17] or communication problems [36 3].Generally speaking  there are three main approaches to dealing with label noise [11].
<EOS>
The first one is based on algorithms that are naturally robust to class noise.This includes ensemble methods like bagging and boosting.It has been shown in [9] that bagging performs generally better than boosting in this task.
<EOS>
The second group of methods relies on data cleansing.In this approach  corrupted instances are identified before the training process starts oﬀ and some kind of filter (e.g. voting or partition filter [4 37] which is deemed easy  cheap and relatively solid) is applied to them.Ultimately  the third group consists of methods that directly model label noise during the learning phase or were specifically designed to take label noise into consideration [11].
<EOS>
 In terms of images  the key reasons behind feature noise are faults in sen- sor devices  analog-to-digital converter errors [30] or electromechanical interfer- ences during the image capturing process [14].State-of-the-art approaches to deal with feature noise are founded on deep architectures.In [24] CNNs (LeNet-5 for MNIST and an architecture similar to the base model C of [33] for CIFAR-10 and SVHN datasets) were used to handle noisy images.
<EOS>
Application of a denoising procedure (Non-Local Means [5]) before the training phase improves classifica- tion accuracy for some types and levels of noise.In [28] several combinations of denoising autoencoder (DAE) and CNNs were proposed  e.g. DAE-CNN  DAE- DAE-CNN  etc.The paper states that properly combined DAE and CNN achieve better results than individual models and other popular methods like SMV [35]  sparse rectifier neural network [13] or deep belief network [1].
<EOS>
 3 Proposed algorithms  As mentioned earlier  classification results are obtained via a soft voting scheme.More specifically  probabilities of particular classes from single CNNs are summed up and a class with the highest cumulative value is ultimately selected (see Fig 1). Many state-of-the-art results in image classification tasks are achieved by an ensemble of well-trained networks that were not selected in any way [7 18 21].
<EOS>
In [31] the authors went further and noticed that limiting ensemble size just to two best-performing models increased accuracy in their case.We adopted that  4  S. Ka´zmierczak and J. Ma´ndziuk  Fig 1.The concept of the soft voting approach for the 10-class problem.
<EOS>
 idea to the algorithm called (for the purpose of this study) top-n  which serves as a benchmark in our experiments.First  all models are sorted in descending order according to their accuracies.Then  ensembles constituted by k best networks where k ranges from 1 to the number of available models are created.
<EOS>
Finally  the committee with the best score on the validation dataset is chosen.Algorithm 1 summarizes the procedure. The first algorithm proposed in this paper  called 2-opt-c  was inspired by the local search technique commonly applied to solving the Traveling Salesman Problem (TSP) [8].
<EOS>
The original 2-opt formulation looks for any two nodes of the current salesman’s route which  if swapped  would shorten the route length.It works until no improvement is made within a certain number of sampling trials.The 2-opt-c algorithm receives an initial committee as an input and in each step modifies it by adding/subtracting/exchanging one or two elements in the way that maximizes accuracy on the validation set.
<EOS>
Thus  there are eight possible atomic operations listed in Algorithm 2.The procedure operates until neither of the operations improves performance.The 1-opt-c works in a very similar way but is limited to three operations that modify only one element in a committee (adding  removal and swap).
<EOS>
The top-n-2-opt-c and top-n-1-opt-c  Concurrent Presence of Feature and Label Noise  5  operate likewise besides being initialized with the output of the top-n procedure  not an empty committee. Algorithm 1: top-n  // sort models descending by accuracy  7  8  9  10  5  6  7  8  9  10  11  12  13  14  15  input : Mall–all available models output: Cbest–selected committee  1 Cbest ← ∅; 2 Ccurr ← ∅; 3 accbest = 0; 4 Msorted = sort(Mall); 5 for i ← 1 to size(Msorted) do Ccurr ← Ccurr ∪ Msorted[i]; acccurr ← accuracy(Ccurr); if acccurr > accbest then  6  accbest ← acccurr; Cbest ← Ccurr;  end  11 12 end  Algorithm 2: 2-opt-c  input : Mall–all available models  C0–initial committee output: Cbest–selected committee  1 Cbest ← C0; 2 accbest ← accuracy(Cbest); 3 while accbest rises do 4  acccurr = 0; acccurr  Ccurr ← add(Cbest  Mall  acccurr); acccurr  Ccurr ← remove(Cbest  acccurr); acccurr  Ccurr ← swap(Cbest  Mall  acccurr); acccurr  Ccurr ← addT wo(Cbest  Mall  acccurr); acccurr  Ccurr ← removeT wo(Cbest  acccurr); acccurr  Ccurr ← addAndSwap(Cbest  Mall  acccurr); acccurr  Ccurr ← removeAndSwap(Cbest  Mall  acccurr); acccurr  Ccurr ← swapT wice(Cbest  Mall  acccurr); if acccurr > accbest then  accbest ← acccurr; Cbest ← Ccurr;  end  16 17 end  Another algorithm  called stochastic  relies on the fact that the performance of the entire ensemble depends on diversity among individual component classi- fiers  on the one hand  and the predictive performance of single models  on the other hand [29].The pseudocode of the algorithm is presented in Algorithm 3.
<EOS>
In the ith epoch  a committee size ranges from i to i + range with the expected value equal to i + rangeThis property is assured by the formula in line 7 which  2  6  S. Ka´zmierczak and J. Ma´ndziuk  increases the probability of adding a new model along with decreasing ensemble size and vice versa.Figure 2 illustrates this relationship.In each step  one model is either added or removed.
<EOS>
In the first scenario a model with the best individ- ual performance is appended to the committee with probability ta (lines 11-14) or a model which minimizes the maximum correlation between any model from the committee and itself is added with probability 1 − ta (lines 15-18).Anal- ogously  if the algorithm decides to decrease a committee size it removes the weakest model with probability tr (lines 22-25) or a model which minimizes the highest correlation between any two models in the committee with probability 1 − tr (lines 26-29).In each epoch  the algorithm performs Ni iterations to ex- plore the solution space.
<EOS>
A correlation between two models is measured by the Pearson correlation coeﬃcient calculated on probability vectors obtained from predictions on the validation set. 4 Experimental setup  4.1 MNIST  CIFAR-10 and CIFAR-100 datasets  As a benchmark  we selected three datasets with a diversified diﬃculty level.MNIST database contains a large set of 28x28 grayscale images of handwritten digits (10 classes) and is commonly used in machine learning experiments [23].
<EOS>
The training set and the test set are composed of 60 000 and 10 000 images  respectively. CIFAR-10 [20] is another popular image dataset broadly used to assess ma- chine learning/computer vision algorithms.It contains 60 000 32x32 color images in 10 diﬀerent classes.
<EOS>
The training set includes 50 000 pictures  while the test set – 10 000 ones. The CIFAR-100 dataset is similar to CIFAR-10.It comprises 60 000 images with the same resolution and three color channels as well.
<EOS>
The only diﬀerence is the number of classes–CIFAR-100 has 100 of them  thus yielding 600 pictures per class. 4.2 CNN architectures  Individual classifiers are in the form of a convolutional neural network composed of VGG blocks (i.e. a sequence of convolutional layers  followed by a max pooling layer) [31]  additionally enhanced by adding dropout [34] and batch normaliza- tion [18].All convolutional layers and hidden dense layer have ReLU as an acti- vation function and their weights were initialized with He normal initializer [15].
<EOS>
Softmax was applied in the output layer while the initial weights were drawn from the Glorot uniform distribution [12].Table 1 summarizes the CNNs archi- tectures.Please note that slight diﬀerences in architectures for MNIST  CIFAR- 10 and CIFAR-100 are caused by distinct image sizes and numbers of classes in the datasets.
<EOS>
Without any attribute or label noise  a single CNN achieved ap- proximately 99%  83% and 53% accuracy on MNIST  CIFAR-10 and CIFAR-100  respectively. Concurrent Presence of Feature and Label Noise  7  Algorithm 3: Stochastic algorithm  input : Mall–all available models  N –number of epochs  Ni–number of  iterations within an epoch  ta–probability threshold below which the strongest model is added  tr–probability threshold below which the weakest model is removed  r–range of possible committee sizes in an epoch  output: Cbest–selected committee  1 Ccurr ← ∅; 2 Cbest ← ∅; 3 accbest ← 0; 4 Mlef t ← Mall; 5 for i ← 0 to N − 1 do for j ← 1 to Ni do  6  pa ← 1 − (size(Ccurr) − i)/r; u ← generate from the uniform distribution U(0  1); if u < pa then  ua ← generate from the uniform distribution U(0  1); if size(Ccurr) == 0 or ua < ta then ma ← getStrongestM odel(Mlef t); Ccurr ← Ccurr ∪ ma; Mlef t ← Mlef t \ ma;  else  // select model from Mlef t which minimizes maximum // correlation between any model from Ccurr and itself ma ← getM arginallyCorrelatedM odel(Ccurr  Mlef t); Ccurr ← Ccurr ∪ ma; Mlef t ← Mlef t \ ma;  else  end ur ← generate from the uniform distribution U(0  1); if size(Ccurr) == 1 or ur < tr then mr ← getW eakestM odel(Ccurr); Ccurr ← Ccurr \ mr; Mlef t ← Mlef t ∪ mr;  else  // select model from Mcurr which minimizes maximum // correlation between any two models in Ccurr mr ← getM aximallyCorrelatedM odel(Ccurr); Ccurr ← Ccurr \ mr; Mlef t ← Mlef t ∪ mr;  end  end acccurr ← accuracy(Ccurr); if acccurr > accbest then  accbest ← acccurr; Cbest ← Ccurr;  end  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  end  37 38 end  8  S. Ka´zmierczak and J. Ma´ndziuk  Table 1.CNN architectures used for MNIST  CIFAR-10 and CIFAR-100.
<EOS>
 Layer Type 1  convolutional  #maps & neurons 32 maps of 32x32 neurons (CIFAR) 32 maps of 28x28 neurons (MNIST)  kernel/pool size 3x3  2 3  4 5 6 7  8 9  10 11 12 13  14 15  16 17 18 19 20 21 22  batch normalization convolutional  batch normalization max pooling dropout (20%) convolutional  batch normalization convolutional  batch normalization max pooling dropout (20%) convolutional  batch normalization convolutional  batch normalization max pooling dropout (20%) dense batch normalization dropout (20%) dense  32 maps of 32x32 neurons (CIFAR) 32 maps of 28x28 neurons (MNIST)  3x3  64 maps of 16x16 neurons (CIFAR) 64 maps of 14x14 neurons (MNIST)  2x2  3x3  64 maps of 16x16 neurons (CIFAR) 64 maps of 14x14 neurons (MNIST)  3x3  128 maps of 8x8 neurons (CIFAR) 128 maps of 7x7 neurons (MNIST)  2x2  3x3  128 maps of 8x8 neurons (CIFAR) 128 maps of 7x7 neurons (MNIST)  3x3  2x2  128 neurons  10 neurons (MNIST  CIFAR-10) 100 neurons (CIFAR-100)  4.3 Training protocol  The following procedure was applied to all three datasets.Partition of a dataset into training and testing subsets was predefined as described in Section 4.1. At the very beginning  all features (RGB values) were divided by 255 to fit [0  1] range.Images were neither preprocessed nor formatted in any other way.
<EOS>
From the training part  we set aside 5 000 samples as a validation set for a single mod- els training and another 5 000 samples for a committee performance comparison.From now on when referring to the training set we would mean all training sam- ples excluding the above-mentioned 10 000 samples used for validation purposes.To create noisy versions of the datasets we degraded features of the three copies of each dataset by adding the Gaussian noise with standard deviation σ = 0.1  0.2  0.3  respectively.
<EOS>
The above distortion was applied to the training set  two validation sets and the test set.All aﬀected values were then clipped to  Concurrent Presence of Feature and Label Noise  9  [0  1] range.Next  for the original datasets and each of the three copies inﬂuenced by the Gaussian noise  another three copies were created and their training set labels were altered with probability p = 0.1  0.2  0.3  respectively.
<EOS>
If a label was selected to be modified  a new value was chosen from the discrete uniform distribution U{0  9}.If the new label value equaled the initial value  then a new label was drawn again  until the sampled label was diﬀerent from the original one.Hence  we ended up with 16 diﬀerent versions of each dataset in total (no feature noise plus three degrees of feature noise multiplied by analogous four options regarding the label noise).
<EOS>
 The second step was to train CNNs on each of the above-mentioned dataset versions.We set the maximum number of epochs to 30 and batch size to 32.In the case of four consecutive epochs with no improvement on the validation set  training was stopped and weights from the best epoch were restored.
<EOS>
Adam optimizer [19] was used to optimize the cross-entropy loss function:  −(cid:80)K  c=1 yo c log(po c) where K is the number of classes  yo c – a binary indicator whether c is a correct class for observation o  and po c – a predicted probability that o is from class c. The learning rate was fixed to 0.001. 4.4 Algorithms parametrization  The stochastic algorithm was run with the following parameters: the number of available models – 25  the number of epochs – 16  the number of iterations within an epoch – 1000  probability threshold below which the strongest model is added – 0.5  probability threshold below which the weakest model is removed – 0.5  range of possible committee sizes in each epoch – 10.Please note that the above parametrization allows the algorithm to consider any possible committee size .
<EOS>
Other analyzed algorithms are parameterless. 5 Experimental results  This section presents experimental results of testing various ensemble selection algorithms.For each pair (σ  p) ∈ {0  0.1  0.2  0.3} × {0  0.1  0.2  0.3} 50 CNNs were independently trained from which 25 were drawn to create one instance of experiment.
<EOS>
Each experiment was repeated 20 times to obtain reliable results.In the whole study  we assume not having any knowledge regarding either the type or the level of noise the datasets are aﬀected by. Figure 3 depicts the relative accuracy margin that committees gained over the top-1 algorithm which selects the best individual model from the whole li- brary of models.
<EOS>
Scores are averaged over top-n  2-opt-c  1-opt-c  top-n-2-opt-c  top-n-1-opt-c and stochastic algorithms.For example  if the best individual model achieves 80% accuracy while the mean accuracy of ensembles found by analyzed algorithms equals 88% then the relative margin of ensembles over top-1 is equal to 10%.Attribute curves refer to computations where all scores within particular attribute noise level are averaged over label noise (four values for every attribute noise level).
<EOS>
Label curves are created analogously–for particular label  10  S. Ka´zmierczak and J. Ma´ndziuk  noise all scores within specific label noise are averaged over attribute noise.Both curves concern increasing noise level concurrently on both attributes and labels by the same amount (i.e. with σ = p).For example  0.2 value on the x-axis refers to σ = 0.2 attribute noise and p = 0.2 label noise.
<EOS>
 Two main conclusions can be drawn from the plots.First  a committee margin rises along with an increase of both noise types (separately and jointly as well).Secondly  a diﬀerence increases further for more demanding datasets.
<EOS>
In case of MNIST  the diﬀerence is less than 1% while when concerning CIFAR-100 the margin amounts to more than 20% for 0.1 and 0.2 noise level and around 30% for 0.3 noise level.Figure 4 illustrates in a concise  aggregated way how algorithms perform in comparison with top-n for various noise levels.Values on the y-axis indicate how much (percentage-wise) the margin achieved by top-n over top-1 is better/worse than the margin attained by the rest of the algorithms.
<EOS>
For example  if accuracies of top-1  top-n and stochastic methods are 80%  85% and 85.5%  respectively then the value for stochastic algorithm amounts to 10% in that case since 0.5% constitutes 10% of 5%.Line y = 0 refers to top-n.For each dataset the leftmost plot  for the given level of attribute noise  presents scores averaged over the label noise (four values for each level).
<EOS>
Likewise  in the middle plot  for the given level of label noise  the scores averaged over the four values of attribute noise are depicted.In the third plot  the scores are not averaged since x-values refer to both attribute and label noise.From the first row of plots  which refers to the MNIST dataset  it stems that there are huge relative diﬀerences in results achieved by the algorithms which  furthermore  vary a lot between noise levels.
<EOS>
This phenomenon is caused by the fact that all errors for all noise levels are below 1% in MNIST.Thus  even very little absolute diﬀerence between scores may be reﬂected in high relative value (one instance constitutes 0.01% of test set size).Therefore  it is hard to draw any vital conclusions for this dataset other than a general observation that for a relatively easy dataset the results of all algorithms are close to each other.
<EOS>
 From the plots related to CIFAR-10 and CIFAR-100  one can see that three of our algorithms noticeably surpassed the top-n one.The stochastic method achieved better results on all noise levels.The only yellow dot below zero refers to no noise case on either attributes and labels.
<EOS>
Both top-n-2-opt-c and top-n-1-opt-c also beat top-n in most of the cases.Another observation is that our algorithms are positively correlated with a noise level in the sense that the attained margin rises along with increasing noise. We have also analyzed 35-sized libraries of the models.
<EOS>
The relationships be- tween results achieved by the algorithms remain similar to those with 25 models  only the absolute accuracy values are slightly higher.It is not surprising since algorithms have a wider choice of models and may keep more of them in a com- mittee.As the last remark  we noticed that 2-opt-c and 1-opt-c obtained very high accuracy on validation sets (greater than top-n-2-opt-c and top-n-1-opt-c  respectively) however it was not reﬂected on test sets.
<EOS>
This observation suggests that one has to be careful when dealing with methods whose performance is  Concurrent Presence of Feature and Label Noise  11  measured solely on the validation set with neglecting models’ diversity  as such committees tend to overfit. 6 Conclusions and future work  The main goal of this paper is to address the problem of concurrently occurring feature and label noise in the image classification task which  to the best of our knowledge  has not been considered in the existing literature.To this end  we propose five novel ensemble selection algorithms among which four are inspired by the local optimization algorithm derived from the TSP and one employs a stochastic search.
<EOS>
Three out of five methods outperform the strong baseline ref- erence algorithm that applies a set of individually selected best models (top-n).We have also empirically proven that a margin gained by the committees over the best single model rises along with an increase of both types of noise as well as with raising dataset diﬃculty  thus making proposed ensembles specifically well-suited to noisy images with noisy labels. There is a couple of lines of inquiry worth pursuing in the future.
<EOS>
Firstly  one may experiment with the parametrization of the stochastic algorithm (a range of possible committee sizes in an epoch  a tradeoﬀ between individual performance and ensemble diversity  etc.).Analysis of other correlation measures could be insightful as well.Secondly  all our algorithms operate on probability vectors  which allows us to assume that they would achieve similar results in other do- mains and are not limited to noisy images only.
<EOS>
