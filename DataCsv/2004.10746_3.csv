Chip Placement with Deep Reinforcement Learning  Azalia Mirhoseini * Anna Goldie * Mustafa Yazgan Joe Jiang Ebrahim Songhori Shen Wang Young-Joon Lee  {azalia  agoldie  mustafay  wenjiej  esonghori  shenwang  youngjoonlee}@google.com  Eric Johnson Omkar Pathak Sungmin Bae Azade Nazi Jiwoo Pak Andy Tong Kavya Srinivasa  William Hang Emre Tuncer Anand Babu Quoc Le James Laudon Richard Ho Roger Carpenter Jeff Dean  0 2 0 2    r p A 2 2         ]  G Ls c [      1 v 6 4 7 0 1  4 0 0 2 : v i X r a  Abstract  In this work  we present a learning-based ap- proach to chip placement  one of the most com- plex and time-consuming stages of the chip de- sign process.Unlike prior methods  our approach has the ability to learn from past experience and improve over time.In particular  as we train over a greater number of chip blocks  our method becomes better at rapidly generating optimized placements for previously unseen chip blocks.
<EOS>
To achieve these results  we pose placement as a Reinforcement Learning (RL) problem and train an agent to place the nodes of a chip netlist onto a chip canvas.To enable our RL policy to gen- eralize to unseen blocks  we ground representa- tion learning in the supervised task of predicting placement quality.By designing a neural archi- tecture that can accurately predict reward across a wide variety of netlists and their placements  we are able to generate rich feature embeddings of the input netlists.
<EOS>
We then use this architec- ture as the encoder of our policy and value net- works to enable transfer learning.Our objec- tive is to minimize PPA (power  performance  and area)  and we show that  in under 6 hours  our method can generate placements that are su- perhuman or comparable on modern accelerator netlists  whereas existing baselines require hu- man experts in the loop and take several weeks. 1.
<EOS>
Introduction Rapid progress in AI has been enabled by remarkable ad- vances in computer systems and hardware  but with the end of Moores Law and Dennard scaling  the world is mov- ing toward specialized hardware to meet AIs exponentially growing demand for compute.However  todays chips take  *Equal contribution  order determined by coin ﬂip. years to design  leaving us with the speculative task of opti- mizing them for the machine learning (ML) models of 2-5 years from now.
<EOS>
Dramatically shortening the chip design cycle would allow hardware to better adapt to the rapidly advancing field of AI.We believe that it is AI itself that will provide the means to shorten the chip design cycle  creating a symbiotic relationship between hardware and AI with each fueling advances in the other.In this work  we present a learning-based approach to chip placement  one of the most complex and time-consuming stages of the chip design process.
<EOS>
The objective is to place a netlist graph of macros (e.g.  SRAMs) and standard cells (logic gates  such as NAND  NOR  and XOR) onto a chip canvas  such that power  performance  and area (PPA) are optimized  while adhering to constraints on placement den- sity and routing congestion (described in Sections 3.3.6 and 3.3.5).Despite decades of research on this problem  it is still necessary for human experts to iterate for weeks with the existing placement tools  in order to produce solu- tions that meet multi-faceted design criteria.The problem’s complexity arises from the sizes of the netlist graphs (mil- lions to billions of nodes)  the granularity of the grids onto which these graphs must be placed  and the exorbitant cost of computing the true target metrics (many hours and some- times over a day for industry-standard electronic design au- tomation (EDA) tools to evaluate a single design).
<EOS>
Even after breaking the problem into more manageable subprob- lems (e.g.  grouping the nodes into a few thousand clusters and reducing the granularity of the grid)  the state space is still orders of magnitude larger than recent problems on which learning-based methods have shown success.To address this challenge  we pose chip placement as a Reinforcement Learning (RL) problem  where we train an agent (e.g.  RL policy network) to optimize the place- ments.In each iteration of training  all of the macros of the chip block are sequentially placed by the RL agent  af- ter which the standard cells are placed by a force-directed method (Hanan & Kurtzberg  1972; Tao Luo & Pan  2008; Bo Hu & Marek-Sadowska  2005; Obermeier et al  2005; Spindler et al  2008; Viswanathan et al  2007b;a).
<EOS>
Train-  Chip Placement with Deep Reinforcement Learning  ing is guided by a fast-but-approximate reward signal for each of the agent’s chip placements.To our knowledge  the proposed method is the first place- ment approach with the ability to generalize  meaning that it can leverage what it has learned from placing previous netlists to generate placements for new unseen netlists.In particular  we show that  as our agent is exposed to a greater volume and variety of chips  it becomes both faster and bet- ter at generating optimized placements for new chip blocks  bringing us closer to a future in which chip designers are assisted by artificial agents with vast chip placement expe- rience.
<EOS>
We believe that the ability of our approach to learn from ex- perience and improve over time unlocks new possibilities for chip designers.We show that we can achieve superior PPA on real AI accelerator chips (Google TPUs)  as com- pared to state-of-the-art baselines.Furthermore  our meth- ods generate placements that are superior or comparable to human expert chip designers in under 6 hours  whereas the highest-performing alternatives require human experts in the loop and take several weeks for each of the dozens of blocks in a modern chip.
<EOS>
Although we evaluate primarily on AI accelerator chips  our proposed method is broadly applicable to any chip placement optimization. 2.Related Work Global placement is a longstanding challenge in chip design  requiring multi-objective optimization over cir- cuits of ever-growing complexity.
<EOS>
Since the 1960s  many approaches have been proposed  so far falling into three broad categories: 1) partitioning-based methods  2) stochastic/hill-climbing methods  and 3) analytic solvers.Starting in the 1960s  industry and academic labs took a partitioning-based approach to the global placement prob- lem  proposing (Breuer  1977; Kernighan  1985; Fiduccia & Mattheyses  1982)  as well as resistive-network based methods (Chung-Kuan Cheng & Kuh  1984; Ren-Song Tsay et al  1988).These methods are characterized by a divide-and-conquer approach; the netlist and the chip canvas are recursively partitioned until sufficiently small sub-problems emerge  at which point the sub-netlists are placed onto the sub-regions using optimal solvers.
<EOS>
Such approaches are quite fast to execute and their hierarchi- cal nature allows them to scale to arbitrarily large netlists.However  by optimizing each sub-problem in isolation  partitioning-based methods sacrifice quality of the global solution  especially routing congestion.Furthermore  a poor early partition may result in an unsalvageable end placement.
<EOS>
In the 1980s  analytic approaches emerged  but were quickly overtaken by stochastic / hill-climbing algorithms   particularly simulated annealing (Kirkpatrick et al  1983; Sechen & Sangiovanni-Vincentelli  1986; Sarrafzadeh et al  2003).Simulated annealing (SA) is named for its analogy to metallurgy  in which metals are first heated and then gradually cooled to induce  or anneal  energy- optimal crystalline surfaces.SA applies random perturba- tions to a given placement (e.g.  shifts  swaps  or rotations of macros)  and then measures their effect on the objec- tive function (e.g.  half-perimeter wirelength described in Section 3.3.1).
<EOS>
If the perturbation is an improvement  it is applied; if not  it is still applied with some probability  referred to as temperature.Temperature is initialized to a particular value and is then gradually annealed to a lower value.Although SA generates high-quality solutions  it is very slow and difficult to parallelize  thereby failing to scale to the increasingly large and complex circuits of the 1990s and beyond.
<EOS>
The 1990s-2000s were characterized by multi-level parti- tioning methods (Agnihotri et al  2005; Roy et al  2007)  as well as the resurgence of analytic techniques  such as force-directed methods (Tao Luo & Pan  2008; Bo Hu & Marek-Sadowska  2005; Obermeier et al  2005; Spindler et al  2008; Viswanathan et al  2007b;a) and non-linear optimizers (Kahng et al  2005; Chen et al  2006).The re- newed success of quadratic methods was due in part to al- gorithmic advances  but also to the large size of modern cir- cuits (10-100 million nodes)  which justified approximat- ing the placement problem as that of placing nodes with zero area.However  despite the computational efficiency of quadratic methods  they are generally less reliable and produce lower quality solutions than their non-linear coun- terparts.
<EOS>
Non-linear optimization approximates cost using smooth mathematical functions  such as log-sum-exp  and weighted-average  mod- els for wirelength  as well as Gaussian  and Helmholtz models for density.These functions are then combined into a single objective function using a La- grange penalty or relaxation.Due to the higher complexity of these models  it is necessary to take a hierarchical ap- proach  placing clusters rather than individual nodes  an ap- proximation which degrades the quality of the placement.
<EOS>
The last decade has seen the rise of modern analytic tech- niques  including more advanced quadratic methods (Kim et al  2010; 2012b; Kim & Markov  2012; Brenner et al  2008; Lin et al  2013)  and more recently  electrostatics- based methods like ePlace  and RePlAce .Modeling netlist placement as an electrostatic system  ePlace  proposed a new formulation of the density penalty where each node (macro or standard cell) of the netlist is analogous to a pos- itively charged particle whose area corresponds to its elec-  Chip Placement with Deep Reinforcement Learning  tric charge.In this setting  nodes repel each other with a force proportional to their charge (area)  and the density function and gradient correspond to the system’s poten- tial energy.
<EOS>
Variations of this electrostatics-based approach have been proposed to address standard-cell placement  and mixed-size placement (Lu et al  2015; Lu et al  2016).RePlAce  is a recent state- of-the-art mixed-size placement technique that further opti- mizes ePlace’s density function by introducing a local den- sity function  which tailors the penalty factor for each indi- vidual bin size.Section 5 compares the performance of the state-of-the-art RePlAce algorithm against our approach.
<EOS>
Recent work  proposes training a model to predict the number of Design Rule Check (DRC) vi- olations for a given macro placement.DRCs are rules that ensure that the placed and routed netlist adheres to tape-out requirements.To generate macro placements with fewer DRCs   use the predictions from this trained model as the evaluation function in simulated annealing.
<EOS>
While this work represents an interesting di- rection  it reports results on netlists with no more than 6 macros  far fewer than any modern block  and the approach does not include any optimization during the place and the route steps.Due to the optimization  the placement and the routing can change dramatically  and the actual DRC will change accordingly  invalidating the model prediction.In addition  although adhering to the DRC criteria is a neces- sary condition  the primary objective of macro placement is to optimize for wirelength  timing (e.g. Worst Negative Slack (WNS) and Total Negative Slack (TNS))  power  and area  and this work does not even consider these metrics.
<EOS>
To address this classic problem  we propose a new category of approach: end-to-end learning-based methods.This type of approach is most closely related to analytic solvers  par- ticularly non-linear ones  in that all of these methods op- timize an objective function via gradient updates.How- ever  our approach differs from prior approaches in its abil- ity to learn from past experience to generate higher-quality placements on new chips.
<EOS>
Unlike existing methods that optimize the placement for each new chip from scratch  our work leverages knowledge gained from placing prior chips to become better over time.In addition  our method enables direct optimization of the target metrics  such as wirelength  density  and congestion  without having to de- fine convex approximations of those functions as is done in other approaches (Cheng et al  2019; Lu et al  2015).Not only does our formulation make it easy to incorporate new cost functions as they become available  but it also al- lows us to weight their relative importance according to the needs of a given chip block (e.g.  timing-critical or power- constrained).
<EOS>
Domain adaptation is the problem of training policies that  can learn across multiple experiences and transfer the ac- quired knowledge to perform better on new unseen exam- ples.In the context of chip placement  domain adaptation involves training a policy across a set of chip netlists and applying that trained policy to a new unseen netlist.Re- cently  domain adaptation for combinatorial optimization has emerged as a trend (Zhou et al  2019; Paliwal et al  2019; Addanki et al  2019).
<EOS>
While the focus in prior work has been on using domain knowledge learned from previ- ous examples of an optimization problem to speed up pol- icy training on new problems  we propose an approach that  for the first time  enables the generation of higher quality results by leveraging past experience.Not only does our novel domain adaptation produce better results  it also re- duces the training time 8-fold compared to training the pol- icy from scratch. 3.
<EOS>
Methods 3.1. Problem Statement  In this work  we target the chip placement optimization problem  in which the objective is to map the nodes of a netlist (the graph describing the chip) onto a chip canvas (a bounded 2D space)  such that final power  performance  and area (PPA) is optimized.In this section  we describe an overview of how we formulate the problem as a rein- forcement learning (RL) problem  followed by a detailed description of the reward function  action and state repre- sentations  policy architecture  and policy updates. 3.2. Overview of Our Approach  We take a deep reinforcement learning approach to the placement problem  where an RL agent (policy network) sequentially places the macros; once all macros are placed  a force-directed method is used to produce a rough place- ment of the standard cells  as shown in Figure 1.
<EOS>
RL problems can be formulated as Markov Decision Processes (MDPs)  consisting of four key elements:  • states: the set of possible states of the world (e.g.  in our case  every possible partial placement of the netlist onto the chip canvas). • actions:  the set of actions that can be taken by the agent (e.g.  given the current macro to place  the avail- able actions are the set of all the locations in the dis- crete canvas space (grid cells) onto which that macro can be placed without violating any hard constraints on density or blockages). • state transition: given a state and an action  this is the  probability distribution over next states.
<EOS>
 • reward: the reward for taking an action in a state. (e.g.   Chip Placement with Deep Reinforcement Learning  Figure 1.The RL agent (i.e.  the policy network) places macros one at a time.Once all macros are placed  the standard cells are placed using a force-directed method.
<EOS>
The reward  a linear combination of the approximate wirelength and congestion  are calculated and passed to the agent to optimize its parameters for the next iteration. in our case  the reward is 0 for all actions except the last action where the reward is a negative weighted sum of proxy wirelength and congestion  subject to density constraints as described in Section 3.3). In our setting  at the initial state  s0  we have an empty chip canvas and an unplaced netlist.
<EOS>
The final state sT corre- sponds to a completely placed netlist.At each step  one macro is placed.Thus  T is equal to the total number of macros in the netlist.
<EOS>
At each time step t  the agent be- gins in state (st)  takes an action (at)  arrives at a new state (st+1)  and receives a reward (rt) from the environment (0 for t < T and negative proxy cost for t = T ).We define st to be a concatenation of features represent- ing the state at time t  including a graph embedding of the netlist (including both placed and unplaced nodes)  a node embedding of the current macro to place  metadata about the netlist (Section 4)  and a mask representing the feasi- bility of placing the current node onto each cell of the grid (Section 3.3.6).The action space is all valid placements of the tth macro  which is a function of the density mask described in sec- tion 3.3.6. Action at is the cell placement of the tth macro that was chosen by the RL policy network. st+1 is the next state  which includes an updated repre- sentation containing information about the newly placed macro  an updated density mask  and an embedding for the next node to be placed.
<EOS>
In our formulation  rt is 0 for every time step except for the final rT   where it is a weighted sum of approximate wirelength and congestion as described in Section 3.3. Through repeated episodes (sequences of states  actions  and rewards)  the policy network learns to take actions that will maximize cumulative reward.We use Proximal Pol- icy Optimization (PPO)  to update the parameters of the policy network  given the cumulative reward for each placement.In this section  we define the reward r  state s  actions a   policy network architecture πθ(a|s) parameterized by θ  and finally the optimization method we use to train those parameters.
<EOS>
 3.3. Reward  Our goal in this work is to minimize power  performance and area  subject to constraints on routing congestion and density.Our true reward is the output of a commercial EDA tool  including wirelength  routing congestion  den- sity  power  timing  and area.However  RL policies require 100 000s of examples to learn effectively  so it is critical that the reward function be fast to evaluate  ideally running in a few milliseconds.
<EOS>
In order to be effective  these ap- proximate reward functions must also be positively corre- lated with the true reward.Therefore  a component of our cost is wirelength  because it is not only much cheaper to evaluate  but also correlates with power and performance (timing).We define approximate cost functions for both wirelength and congestion  as described in Section 3.3.1 and Section 3.3.5  respectively.
<EOS>
To combine multiple objectives into a single reward func- tion  we take the weighted sum of proxy wirelength and congestion where the weight can be used to explore the trade-off between the two metrics.While we treat congestion as a soft constraint (i.e.  lower congestion improves the reward function)  we treat density as a hard constraint  masking out actions (grid cells to place nodes onto) whose density exceeds the target density  as described further in section 3.3.6. To keep the runtime per iteration small  we apply several approximations to the calculation of the reward function:  1.We group millions of standard cells into a few thou- sand clusters using hMETIS   a partitioning technique based on the normal- ized minimum cut objective.
<EOS>
Once all the macros are placed  we use force-directed methods to place the standard cell clusters  as described in section 3.3.4. Doing so enables us to achieve an approximate but fast  (cid:68)(cid:74)(cid:72)(cid:81)(cid:87)(cid:68)(cid:74)(cid:72)(cid:81)(cid:87)(cid:68)(cid:74)(cid:72)(cid:81)(cid:87)(cid:68)(cid:74)(cid:72)(cid:81)(cid:87)(cid:41)(cid:82)(cid:85)(cid:70)(cid:72)(cid:16)(cid:39)(cid:76)(cid:85)(cid:72)(cid:70)(cid:87)(cid:72)(cid:71)(cid:3)(cid:48)(cid:72)(cid:87)(cid:75)(cid:82)(cid:71)(cid:3)(cid:51)(cid:79)(cid:68)(cid:70)(cid:72)(cid:86)(cid:3)(cid:54)(cid:87)(cid:68)(cid:81)(cid:71)(cid:68)(cid:85)(cid:71)(cid:3)(cid:38)(cid:72)(cid:79)(cid:79)(cid:86)(cid:86)(cid:19)(cid:86)(cid:20)(cid:86)(cid:21)(cid:86)(cid:55)(cid:68)(cid:19)(cid:3)(cid:68)(cid:20)(cid:3)(cid:68)(cid:55)(cid:16)(cid:20)(cid:3)(cid:85)(cid:20)(cid:32)(cid:19)(cid:85)(cid:21)(cid:32)(cid:19)(cid:17)(cid:17)(cid:17)(cid:17)(cid:85)(cid:55)(cid:32)(cid:16)(cid:43)(cid:51)(cid:58)(cid:47)(cid:16)(cid:70)(cid:13)(cid:38)(cid:82)(cid:81)(cid:74)(cid:72)(cid:86)(cid:87)(cid:76)(cid:82)(cid:81)(cid:85)(cid:19)(cid:32)(cid:19)(cid:53)(cid:47)(cid:3)(cid:36)(cid:74)(cid:72)(cid:81)(cid:87)(cid:3)(cid:51)(cid:79)(cid:68)(cid:70)(cid:72)(cid:86)(cid:3)(cid:48)(cid:68)(cid:70)(cid:85)(cid:82)(cid:86)(cid:3)(cid:50)(cid:81)(cid:72)(cid:3)(cid:68)(cid:87)(cid:3)(cid:68)(cid:3)(cid:55)(cid:76)(cid:80)(cid:72)(cid:38)(cid:75)(cid:76)(cid:83)(cid:3)(cid:70)(cid:68)(cid:81)(cid:89)(cid:68)(cid:86)Chip Placement with Deep Reinforcement Learning  standard cell placement that facilitates policy network optimization. 2.We discretize the grid to a few thousand grid cells and place the center of macros and standard cell clusters onto the center of the grid cells.
<EOS>
 3.When calculating wirelength  we make the simplify- ing assumption that all wires leaving a standard cell cluster originate at the center of the cluster. 4.
<EOS>
To calculate routing congestion cost  we only consider the average congestion of the top 10% most congested grid cells  as described in Section 3.3.5.  3.3.1. WIRELENGTH  Following the literature   we employ half-perimeter wirelength (HPWL)  the most commonly used approximation for wirelength.HPWL is defined as the half-perimeter of the bounding boxes for all nodes in the netlist.The HPWL for a given net (edge) i is shown in the equation below:  HP W L(i) = (M AXb∈i{xb} − M INb∈i{xb} + 1) (1)  + (M AXb∈i{yb} − M INb∈i{yb} + 1)  Here xb and yb show the x and y coordinates of the end points of net i. The overall HPWL cost is then calculated by taking the normalized sum of all half-perimeter bound- ing boxes  as shown in Equation 2. q(i) is a normaliza- tion factor which improves the accuracy of the estimate by increasing the wirelength cost as the number of nodes in- creases  where Nnetlist is the number of nets.
<EOS>
 HP W L(netlist) =  Nnetlist(cid:88)  i=1  q(i) ∗ HP W L(i)  (2)  Intuitively  the HPWL for a given placement is roughly the length of its Steiner tree   which is a lower bound on routing cost.Wirelength also has the advantage of correlating with other important metrics  such as power and timing.Although we don’t optimize directly for these other metrics  we ob- serve high performance in power and timing (as shown in Table 2).
<EOS>
 3.3.2. SELECTION OF GRID ROWS AND COLUMNS  Given the dimensions of the chip canvas  there are many choices to discretize the 2D canvas into grid cells.This de- cision impacts the difficulty of optimization and the quality  of the final placement.We limit the maximum number of rows and columns to 128.
<EOS>
We treat choosing the optimal number of rows and columns as a bin-packing problem and rank different combinations of rows and columns by the amount of wasted space they incur.We use an average of 30 rows and columns in the experiments described in Sec- tion 5. 3.3.3. SELECTION OF MACRO ORDER  To select the order in which the macros are placed  we sort macros by descending size and break ties using a topolog- ical sort.
<EOS>
By placing larger macros first  we reduce the chance of there being no feasible placement for a later macro.The topological sort can help the policy network learn to place connected nodes close to one another.An- other potential approach would be to learn to jointly op- timize the ordering of macros and their placement  mak- ing the choice of which node to place next part of the ac- tion space.
<EOS>
However  this enlarged action space would sig- nificantly increase the complexity of the problem  and we found that this heuristic worked in practice. 3.3.4. STANDARD CELL PLACEMENT  To place standard cell clusters  we use an approach similar to classic force-directed methods .We represent the netlist as a system of springs that apply force to each node  according to the weight × distance formula  causing tightly connected nodes to be attracted to one another.
<EOS>
We also introduce a repulsive force between overlapping nodes to reduce place- ment density.After applying all forces  we move nodes in the direction of the force vector.To reduce oscillations  we set a maximum distance for each move.
<EOS>
 3.3.5. ROUTING CONGESTION  We also followed convention in calculating proxy conges- tion (Kim et al  2012a)  using a simple deterministic rout- ing based on the locations of the driver and loads on the net.The routed net occupies a certain amount of available routing resources (determined by the underlying semicon- ductor fabrication technology) for each grid cell which it passes through.We keep track of vertical and horizontal allocations in each grid cell separately.
<EOS>
To smoothe the congestion estimate  we run 5 × 1 convolutional filters in both the vertical and horizontal direction.After all nets are routed  we take the average of the top 10% conges- tion values  drawing inspiration from the ABA10 metric in MAPLE (Kim et al  2012a).The congestion cost in Equa- tion 4 is the top 10% average congestion calculated by this process.
<EOS>
 Chip Placement with Deep Reinforcement Learning  3.3.6. DENSITY  We treat density as a hard constraint  disallowing the pol- icy network from placing macros in locations which would cause density to exceed the target (maxdensity) or which would result in infeasible macro overlap.This approach has two benefits: (1) it reduces the number of invalid place- ments generated by the policy network  and (2) it reduces the search space of the optimization problem  making it more computationally tractable.A feasible standard cell cluster placement should meet the following criterion: the density of placed items in each grid cell should not exceed a given target density threshold (maxdensity).
<EOS>
We set this threshold to be 0.6 in our exper- iments.To meet this constraint  during each RL step  we calculate the current density mask  a binary m × n matrix that represents grid cells onto which we can place the center of the current node without violating the density threshold criteria.Before choosing an action from the policy net- work output  we first take the dot product of the mask and the policy network output and then take the argmax over feasible locations.
<EOS>
This approach prevents the policy net- work from generating placements with overlapping macros or dense standard cell areas.We also enable blockage-aware placements (such as clock straps) by setting the density function of the blocked areas to 1. 3.3.7. POSTPROCESSING  To prepare the placements for evaluation by commercial EDA tools  we perform a greedy legalization step to snap macros onto the nearest legal position while honoring the minimum spacing constraints.
<EOS>
We then fix the macro place- ments and use an EDA tool to place the standard cells and evaluate the placement. 3.4. Action Representation  For policy optimization purposes  we convert the canvas into a m × n grid.Thus  for any given state  the action space (or the output of the policy network) is the probabil- ity distribution of placements of the current macro over the m × n grid.
<EOS>
The action is the argmax of this probability distribution. 3.5. State Representation  Our state contains information about the netlist graph (ad- jacency matrix)  its node features (width  height  type  etc.)  edge features (number of connections)  current node (macro) to be placed  and metadata of the netlist and the un- derlying technology (e.g.  routing allocations  total number of wires  macros  and standard cell clusters  etc.).In the  following section  we discuss how we process these fea- tures to learn effective representations for the chip place- ment problem.
<EOS>
 4.Domain Transfer: Learning Better Chip  Placements from Experience  Our goal is to develop RL agents that can generate higher quality results as they gain experience placing chips.We can formally define the placement objective function as fol- lows:  (cid:88)  g∼G  J(θ  G) =  1 K  Eg p∼πθ [Rp g]  (3)  Here J(θ  G) is the cost function.
<EOS>
The agent is parameter- ized by θ.The dataset of netlist graphs of size K is denoted by G with each individual netlist in the dataset written as g. Rp g is the episode reward of a placement p drawn from the policy network applied to netlist g.  Rp g = −W irelength(p  g) − λ Congestion(p  g) S.t. density(p  g) ≤ maxdensity  (4)  Equation 4 shows the reward that we used for policy net- work optimization  which is the negative weighted aver- age of wirelength and congestion  subject to density con- straints.The reward is explained in detail in Section 3.3. In our experiments  congestion weight λ is set to 0.01 and the max density threshold is set to 0.6.  4.1. A Supervised Approach to Enable Transfer  Learning  We propose a novel neural architecture that enables us to train domain-adaptive policies for chip placement.
<EOS>
Train- ing such a policy network is a challenging task since the state space encompassing all possible placements of all possible chips is immense.Furthermore  different netlists and grid sizes can have very different properties  including differing numbers of nodes  macro sizes  graph topologies  and canvas widths and heights.To address this challenge  we first focused on learning rich representations of the state space.
<EOS>
Our intuition was that a policy network architecture capable of transferring placement optimization across chips should also be able to encode the state associated with a new unseen chip into a meaningful signal at inference time.We therefore proposed training a neural network architec- ture capable of predicting reward on new netlists  with the ultimate goal of using this architecture as the encoder layer of our policy network. Chip Placement with Deep Reinforcement Learning  Figure 2.
<EOS>
Policy and value network architecture.An embedding layer encodes information about the netlist adjacency  node features  and the current macro to be placed.The policy and value networks then output a probability distribution over available placement locations and an estimate of the expected reward for the current placement  respectively.
<EOS>
 To train this supervised model  we needed a large dataset of chip placements and their corresponding reward labels.We therefore created a dataset of 10 000 chip placements where the input is the state associated with a given placement and the label is the reward for that placement (wirelength and congestion).We built this dataset by first picking 5 dif- ferent accelerator netlists and then generating 2 000 place- ments for each netlist.
<EOS>
To create diverse placements for each netlist  we trained a vanilla policy network at vari- ous congestion weights (ranging from 0 to 1) and random seeds  and collected snapshots of each placement during the course of policy training.An untrained policy network starts off with random weights and the generated place- ments are of low quality  but as the policy network trains  the quality of generated placements improves  allowing us to collect a diverse dataset with placements of varying qual- ity.To train a supervised model that can accurately predict wirelength and congestion labels and generalize to unseen data  we developed a novel graph neural network architec- ture that embeds information about the netlist.
<EOS>
The role of graph neural networks is to distill information about the type and connectivity of a node within a large graph into low-dimensional vector representations which can be used in downstream tasks.Some examples of such down- stream tasks are node classification   de- vice placement   link prediction   and Design Rule Violations (DRCs) pre- diction (Zhiyao Xie Duke Univeristy  2018).We create a vector representation of each node by con- catenating the node features.
<EOS>
The node features include node type  width  height  and x and y coordinates.We also pass node adjacency information as input to our algo-  rithm.We then repeatedly perform the following updates: 1) each edge updates its representation by applying a fully connected network to an aggregated representation of in- termediate node embeddings  and 2) each node updates its representation by taking the mean of adjacent edge embed- dings.
<EOS>
The node and edge updates are shown in Equation 5. eij = f c1(concat(f c0(vi)|f c0(vj)|we vi =  meanj∈N (vi)(eij)  ij))  (5)  Node embeddings are denoted by vis for 1 <= i <= N  where N is the total number of macros and standard cell clusters.Vectorized edges connecting nodes vi and vj are represented as eij.
<EOS>
Both edge (eij) and node (vi) embed- dings are randomly initialized and are 32-dimensional. f c0 is a 32× 32  f c1 is a 65× 32 feedforward network and we ijs are learnable 1x1 weights corresponding to edges.N (vi) shows the neighbors of vi.The outputs of the algorithm are the node and edge embeddings.
<EOS>
Our supervised model consists of: (1) The graph neural network described above that embeds information about node types and the netlist adjacency matrix. (2) A fully connected feedforward network that embeds the metadata  including information about the underlying semiconduc- tor technology (horizontal and vertical routing capacity)  the total number of nets (edges)  macros  and standard cell clusters  canvas size and number of rows and columns in the grid. (3) A fully connected feedforward network (the prediction layer) whose input is a concatenation of the netlist graph and metadata embedding and whose output is the reward prediction.The netlist graph embedding is  (cid:59)(cid:248)(cid:212)(cid:345)(cid:350)(cid:334)(cid:248)(cid:3)(cid:41)(cid:299)(cid:237)(cid:248)(cid:244)(cid:244)(cid:276)(cid:300)(cid:268)(cid:338)(cid:90)(cid:212)(cid:238)(cid:334)(cid:306)(cid:3)(cid:41)(cid:299)(cid:237)(cid:248)(cid:244)(cid:244)(cid:276)(cid:300)(cid:268)(cid:338)(cid:91)(cid:248)(cid:345)(cid:294)(cid:276)(cid:338)(cid:345)(cid:3)(cid:90)(cid:248)(cid:345)(cid:212)(cid:244)(cid:212)(cid:345)(cid:212)(cid:90)(cid:212)(cid:238)(cid:334)(cid:306)(cid:3)(cid:59)(cid:248)(cid:212)(cid:345)(cid:350)(cid:334)(cid:248)(cid:338)(cid:91)(cid:248)(cid:345)(cid:294)(cid:276)(cid:338)(cid:345)(cid:3)(cid:60)(cid:334)(cid:212)(cid:331)(cid:273)(cid:31)(cid:350)(cid:334)(cid:334)(cid:248)(cid:300)(cid:345)(cid:3)(cid:90)(cid:212)(cid:238)(cid:334)(cid:306)(cid:3)(cid:68)(cid:244)(cid:41)(cid:244)(cid:268)(cid:248)(cid:3)(cid:41)(cid:299)(cid:237)(cid:248)(cid:244)(cid:244)(cid:276)(cid:300)(cid:268)(cid:338)(cid:125)(cid:248)(cid:244)(cid:350)(cid:238)(cid:248)(cid:3)(cid:299)(cid:248)(cid:212)(cid:300)(cid:60)(cid:334)(cid:212)(cid:331)(cid:273)(cid:3)(cid:41)(cid:299)(cid:237)(cid:248)(cid:244)(cid:244)(cid:276)(cid:300)(cid:268)(cid:31)(cid:350)(cid:334)(cid:334)(cid:248)(cid:300)(cid:345)(cid:3)(cid:90)(cid:212)(cid:238)(cid:334)(cid:306)(cid:3)(cid:41)(cid:299)(cid:237)(cid:248)(cid:244)(cid:244)(cid:276)(cid:300)(cid:268)(cid:91)(cid:248)(cid:345)(cid:294)(cid:276)(cid:338)(cid:345)(cid:3)(cid:90)(cid:248)(cid:345)(cid:212)(cid:244)(cid:212)(cid:345)(cid:212)(cid:3)(cid:41)(cid:299)(cid:237)(cid:248)(cid:244)(cid:244)(cid:276)(cid:300)(cid:268)(cid:60)(cid:334)(cid:212)(cid:331)(cid:273)(cid:3)(cid:31)(cid:306)(cid:300)(cid:369)(cid:68)(cid:300)(cid:244)(cid:248)(cid:375)(cid:73)(cid:70)(cid:73)(cid:70)(cid:24)(cid:92)(cid:24)(cid:92)(cid:23)(cid:22)(cid:28)(cid:92)(cid:28)(cid:92)(cid:21)(cid:26)(cid:21)(cid:26)(cid:92)(cid:21)(cid:26)(cid:92)(cid:28)(cid:23)(cid:22)(cid:92)(cid:23)(cid:22)(cid:92)(cid:24)(cid:58)(cid:69)(cid:80)(cid:89)(cid:73)(cid:50)(cid:73)(cid:88)(cid:52)(cid:83)(cid:80)(cid:77)(cid:71)(cid:93)(cid:50)(cid:73)(cid:88)(cid:26)(cid:24)(cid:92)(cid:26)(cid:24)(cid:92)(cid:22)(cid:73)(cid:70)(cid:122)(cid:306)(cid:294)(cid:276)(cid:238)(cid:376)(cid:3)(cid:212)(cid:300)(cid:244)(cid:3)(cid:159)(cid:212)(cid:294)(cid:350)(cid:248)(cid:3)(cid:91)(cid:248)(cid:345)(cid:370)(cid:306)(cid:334)(cid:291)(cid:338)(cid:73)(cid:70)(cid:39)(cid:72)(cid:16)(cid:70)(cid:82)(cid:81)(cid:89)(cid:21)(cid:22)(cid:28)(cid:92)(cid:21)(cid:22)(cid:28)(cid:92)(cid:21)(cid:39)(cid:72)(cid:16)(cid:70)(cid:82)(cid:81)(cid:89)(cid:39)(cid:72)(cid:16)(cid:70)(cid:82)(cid:81)(cid:89)(cid:39)(cid:72)(cid:16)(cid:70)(cid:82)(cid:81)(cid:89)(cid:39)(cid:72)(cid:16)(cid:70)(cid:82)(cid:81)(cid:89)(cid:21)(cid:22)(cid:28)(cid:92)(cid:21)(cid:22)(cid:28)(cid:92)(cid:21)(cid:49)(cid:69)(cid:87)(cid:79)(cid:73)(cid:72)(cid:4)(cid:52)(cid:83)(cid:80)(cid:77)(cid:71)(cid:93)(cid:21)(cid:22)(cid:28)(cid:92)(cid:21)(cid:22)(cid:28)(cid:92)(cid:21)(cid:90)(cid:212)(cid:338)(cid:291)Chip Placement with Deep Reinforcement Learning  created by applying a reduce mean function on the edge embeddings.The supervised model is trained via regres- sion to minimize the weighted sum of the mean squared loss of wirelength and congestion.
<EOS>
This supervised task allowed us to find the features and ar- chitecture necessary to generalize reward prediction across netlists.To incorporate this architecture into our policy net- work  we removed the prediction layer and then used it as the encoder component of the policy network as shown in Figure 2.To handle different grid sizes corresponding to different choices of rows and columns  we set the grid size to 128 × 128  and mask the unused L-shaped section for grid sizes smaller than 128 rows and columns.
<EOS>
To place a new test netlist at inference time  we load the pre-trained weights of the policy network and apply it to the new netlist.We refer to placements generated by a pre-trained policy network with no finetuning as zero-shot placements.Such a placement can be generated in less than a second  because it only requires a single inference step of the pre-trained policy network.
<EOS>
We can further optimize placement quality by finetuning the policy network.Do- ing so gives us the ﬂexibility to either use the pre-trained weights (that have learned a rich representation of the in- put state) or further finetune these weights to optimize for the properties of a particular chip netlist. 4.2. Policy Network Architecture  Figure 2 depicts an overview of the policy network (mod- eled by πθ in Equation 3) and the value network architec- ture that we developed for chip placement.
<EOS>
The inputs to these networks are the netlist graph (graph adjacency ma- trix and node features)  the id of the current node to be placed  and the metadata of the netlist and the semiconduc- tor technology.The netlist graph is passed through our pro- posed graph neural network architecture as described ear- lier.This graph neural network generates embeddings of (1) the partially placed graph and (2) the current node.
<EOS>
We use a simple feedforward network to embed (3) the meta- data.These three embedding vectors are then concatenated to form the state embedding  which is passed to a feedfor- ward neural network.The output of the feedforward net- work is then fed into the policy network (composed of 5 deconvolutions 1 and Batch Normalization layers) to gen- erate a probability distribution over actions and passed to a value network (composed of a feedforward network) to predict the value of the input state.
<EOS>
 1The deconvolutions layers have a 3x3 kernel size with stride  2 and 16  8  4  2  and 1 filter channels respectively. 4.3. Policy Network Update: Training Parameters θ  In Equation 3  the objective is to train a policy network πθ that maximizes the expected value (E) of the reward (Rp g) over the policy network’s placement distribution.To opti- mize the parameters of the policy network  we use Prox- imal Policy Optimization (PPO)  with a clipped objective as shown below: LCLIP (θ) = ˆEt[min(rt(θ) ˆAt  clip(rt(θ)  1 −   1 + ) ˆAt)] where ˆEt represents the expected value at timestep t  rt is the ratio of the new policy and the old policy  and ˆAt is the estimated advantage at timestep t.  5.
<EOS>
Results In this section  we evaluate our method and answer the fol- lowing questions: Does our method enable domain transfer and learning from experience?What is the impact of us- ing pre-trained policies on the quality of result?How does the quality of the generated placements compare to state- of-the-art baselines?
<EOS>
We also inspect the visual appearance of the generated placements and provide some insights into why our policy network made those decisions. 5.1. Transfer Learning Results  Figure 3 compares the quality of placements generated us- ing pre-trained policies to those generated by training the policy network from scratch.Zero-shot means that we ap- plied a pre-trained policy network to a new netlist with no finetuning  yielding a placement in less than one second.
<EOS>
We also show results where we finetune the pre-trained pol- icy network on the details of a particular design for 2 and 12 hours.The policy network trained from scratch takes much longer to converge  and even after 24 hours  the results are worse than what the finetuned policy network achieves af- ter 12 hours  demonstrating that the learned weights and ex- posure to many different designs are helping us to achieve higher quality placements for new designs in less time.Figure 4 shows the convergence plots for training from scratch vs. training from a pre-trained policy network for Ariane RISC-V CPU.
<EOS>
The pre-trained policy network starts with a lower placement cost at the beginning of the finetun- ing process.Furthermore  the pre-trained policy network converges to a lower placement cost and does so more than 30 hours faster than the policy network that was trained from scratch. 5.2. Learning from Larger Datasets  As we train on more chip blocks  we are able to speed up the training process and generate higher quality results faster.
<EOS>
Figure 5 (left) shows the impact of a larger training set on performance.The training dataset is created from  Chip Placement with Deep Reinforcement Learning  Figure 3.Domain adaptation results.
<EOS>
For each block  the zero-shot results  as well as the finetuned results after 2 and 6 hours of training are shown.We also include results for policies trained from scratch.As can be seen in the table  the pre-trained policy network consistently outperforms the policy network that was trained from scratch  demonstrating the effectiveness of learning from training data ofﬂine.
<EOS>
 Figure 4.Convergence plots for training a policy network from scratch vs. finetuning a pre-trained policy network for a block of Ariane. internal TPU blocks.
<EOS>
The training data consists of a variety of blocks including memory subsystems  compute units  and control logic.As we increase the training set from 2 blocks to 5 blocks and finally to 20 blocks  the policy net- work generates better placements both at zero-shot and af- ter being finetuned for the same number of hours.Figure 5 (right) shows the placement cost on the test data  as the policy network is being (pre-)trained.
<EOS>
We can see that for the small training dataset  the policy network quickly over- fits to the training data and performance on the test data degrades  whereas it takes longer for the policy network to overfit on largest dataset and the policy network pre-trained on this larger dataset yields better results on the test data.This plot suggests that as we expose the policy network to a greater variety of distinct blocks  while it might take longer for the policy network to pre-train  the policy network be- comes less prone to overfitting and better at finding opti-  mized placements for new unseen blocks. 5.3. Visualization Insights  Figure 6 show the placement results for the Ariane RISC- V CPU.
<EOS>
On the left  placements from the zero-shot policy network and on the right  placements from the finetuned policy network are shown.The zero-shot placements are generated at inference time on a previously unseen chip.The zero-shot policy network places the standard cells in the center of the canvas surrounded by macros  which is already quite close to the optimal arrangement.
<EOS>
After fine- tuning  the placements of macros become more regularized and the standard cell area in the center becomes less con- gested.Figure 7 shows the visualized placements: on the left  re-  Chip Placement with Deep Reinforcement Learning  Figure 5.We pre-train the policy network on three different training datasets (the small dataset is a subset of the medium one  and the medium dataset is a subset of the large one).
<EOS>
We then finetune this pre-trained policy network on the same test block and report cost at various training durations (shown on the left of the figure).As the dataset size increases  both the quality of generated placements and time to convergence on the test block improve.The right figure shows evaluation curves for policies trained on each dataset (each dot in the right figure shows the cost of the placement generated by the policy under training).
<EOS>
 Figure 6.Visualization of placements.On the left  zero-shot placements from the pre-trained policy and on the right  placements from the finetuned policy are shown.
<EOS>
The zero-shot policy placements are generated at inference time on a previously unseen chip.The pre-trained policy network (with no fine-tuning) places the standard cells in the center of the canvas surrounded by macros  which is already quite close to the optimal arrangement and in line with the intuitions of physical design experts. sults from a manual placement  and on the right  results from our approach are shown.
<EOS>
The white area shows the macro placements and the green area shows the standard cell placements.Our method creates donut-shaped place- ments of macros  surrounding standard cells  which results in a reduction in the total wirelength. 5.4. Comparing with Baseline Methods  In this section  we compare our method with 3 baselines methods: Simulated Annealing  RePlAce  and human ex- pert baselines.
<EOS>
For our method  we use a policy pre-trained on the largest dataset (of 20 TPU blocks) and then fine- tune it on 5 target unseen blocks denoted by Blocks 1 to 5.Our dataset consists a variety of blocks including memory subsystems  compute units  and control logic.Due to con- fidentiality  we cannot disclose the details of these blocks   but to give an idea of the scale  each block contains up to a few hundred macros and millions of standard cells.
<EOS>
Comparisons with Simulated Annealing: Simulated An- nealing (SA)  is known to be a powerful  but slow  opti- mization method.However  like RL  simulated annealing is capable of optimizing arbitrary non-differentiable cost functions.To show the relative sample efficiency of RL  we ran experiments in which we replaced it with a sim- ulated annealing based optimizer.
<EOS>
In these experiments  we use the same inputs and cost function as before  but in each episode  the simulated annealing optimizer places all macros  followed by an FD step to place the standard cell clusters.Each macro placement is accepted according to the SA update rule using an exponential decay annealing schedule .SA takes 18 hours to converge  whereas our method takes no more than 6 hours.
<EOS>
 Chip Placement with Deep Reinforcement Learning  Figure 7.Human-expert placements are shown on the left and results from our approach are shown on the right.The white area represents macros and the green area represents standard cells.
<EOS>
The figures are intentionally blurred as the designs are proprietary. Table 1.Experiments to evaluate sample efficiency of Deep RL compared to Simulated Annealing (SA).
<EOS>
We replaced our RL policy network with SA and ran 128 different SA experiments for each block  sweeping different hyper-parameters  including min and max temperature  seed  and max step size.The results from the run with minimum cost is reported.The results show proxy wirelength and congestion values for each block.
<EOS>
Note that because these proxy metrics are relative  comparisons are only valid for different placements of the same block. Replacing Deep RL with SA in our framework Wirelength  Congestion  Block 1 Block 2 Block 3 Block 4 Block 5  0.048 0.045 0.044 0.030 0.045  1.21 1.11 1.14 0.87 1.29  Ours  Wirelength Congestion  0.047 0.041 0.034 0.024 0.038  0.87 0.93 0.96 0.78 0.88  To make comparisons fair  we ran multiple SA experiments that sweep different hyper-parameters  including min and max temperature  seed  and max SA episodes  such that SA and RL spend the same amount of CPU-hours in sim- ulation and search a similar number of states.The results from the experiment with minimum cost are reported in Ta- ble 1.
<EOS>
As shown in the table  even with additional time  SA struggles to produce high-quality placements compared to our approach  and produces placements with 14.4% higher wirelength and 24.1% higher congestion on average.Comparisons with RePlAce  and manual baselines: Table 2 compares our results with the state-of-the-art method RePlAce  and manual baselines.The manual baseline is generated by a production chip design team  and involved many itera- tions of placement optimization  guided by feedback from a commercial EDA tool over a period of several weeks.
<EOS>
With respect to RePlAce  we share the same optimization goals  namely to optimize global placement in chip design  but we use different objective functions.Thus  rather than comparing results from different cost functions  we treat the output of a commercial EDA tool as ground truth.To perform this comparison  we fix the macro placements gen- erated by our method and by RePlAce and allow a commer- cial EDA tool to further optimize the standard cell place-  ments  using the tool’s default settings.
<EOS>
We then report total wirelength  timing (worst (WNS) and total (TNS) negative slack)  area  and power metrics.As shown in Table 2  our method outperforms RePLAce in generating placements that meet the design requirements.Given constraints im- posed by the underlying semiconductor technology  place- ments of these blocks will not be able to meet timing con- straints in the later stage of the design ﬂow if the WNS is significantly above 100 ps or if the horizontal or verti- cal congestion is over 1%  rendering some RePlAce place- ments (Blocks 1  2  3) unusable.
<EOS>
These results demonstrate that our congestion-aware approach is effective in generat- ing high-quality placements that meet design criteria.RePlAce is faster than our method as it converges in 1 to 3.5 hours  whereas our results were achieved in 3 to 6 hours.However  some of the fundamental advantages of our ap- proach are 1) our method can readily optimize for various non-differentiable cost functions  without the need to for- mulate closed form or differentiable equivalents of those cost functions.
<EOS>
For example  while it is straightforward to model wirelength as a convex function  this is not true for routing congestion or timing.2) our method has the abil- ity to improve over time as the policy is exposed to more chip blocks  and 3) our method is able to adhere to various design constraints  such as blockages of differing shapes. Chip Placement with Deep Reinforcement Learning  Table 2.
<EOS>
Comparing our method with the state-of-the-art (RePlAce ) method and manual expert placements using an industry standard electronic design automation (EDA) tool.For all metrics in this table  lower is better.For placements which violate constraints on timing (WNS significantly greater than 100 ps) or congestion (horizontal or vertical congestion greater than 1%)  we render their metrics in gray to indicate that these placements are infeasible.
<EOS>
 Name  Method  Timing  Area  Power  Wirelength Congestion  WNS (ps)  TNS (ns)  Total (µm2)  Total (W)  Block 1 RePlAce Manual Ours  Block 2 RePlAce Manual Ours  Block 3 RePlAce Manual Ours  Block 4 RePlAce Manual Ours  Block 5 RePlAce Manual Ours  374 136 84 97 75 59 193 18 11 58 58 52 156 107 68  233.7 47.6 23.3 6.6 98.1 170 3.9 0.2 2.2 11.2 17.9 0.7 254.6 97.2 141.0  1693139 1680790 1681767 785655 830470 694757 867390 869779 868101 944211 947766 942867 1477283 1480881 1472302  3.70 3.74 3.59 3.52 3.56 3.13 1.36 1.42 1.38 2.21 2.17 2.21 3.24 3.23 3.28  (m) 52.14 51.12 51.29 61.07 62.92 59.11 18.84 20.74 20.80 27.37 29.16 28.50 31.83 37.99 36.59  H (%) V (%) 0.06 1.82 0.03 0.13 0.34 0.03 0.06 1.58 0.04 0.23 0.03 0.45 0.19 0.05 0.07 0.22 0.04 0.04 0.03 0.03 0.00 0.01 0.02 0.03 0.03 0.04 0.01 0.00 0.01 0.03  Table 2 also shows the results generated by human ex- pert chip designers.Both our method and human ex- perts consistently generate viable placements  meaning that they meet the timing and congestion design criteria.We also outperform or match manual placements in WNS  area  power  and wirelength.
<EOS>
Furthermore  our end-to-end learning-based approach takes less than 6 hours  whereas the manual baseline involves a slow iterative optimization process with experts in the loop and can take multiple weeks. 5.5. Discussions Opportunities for further optimization of our ap- proach: There are multiple opportunities to further im- prove the quality of our method.For example  the pro- cess of standard cell partitioning  row and column selec- tion  as well as selecting the order in which the macros are placed all can be further optimized.
<EOS>
In addition  we would also benefit from a more optimized approach to stan- dard cell placement.Currently  we use a force-directed method to place standard cells due to its fast runtime.How- ever  we believe that more advanced techniques for stan- dard cell placement such as RePlAce  and DREAMPlace  can yield more accu- rate standard cell placements to guide the policy network training.
<EOS>
This is helpful because if the policy network has a clearer signal on how its macro placements affect standard cell placement and final metrics  it can learn to make more optimal macro placement decisions. Implications for a broader class of problems: This work is just one example of domain-adaptive policies for opti- mization and can be extended to other stages of the chip design process  such as architecture and logic design  syn- thesis  and design verification  with the goal of training ML models that improve as they encounter more instances of the problem.A learning based method also enables fur- ther design space exploration and co-optimization within the cascade of tasks that compose the chip design process.
<EOS>
 6.Conclusion In this work  we target the complex and impactful prob- lem of chip placement.We propose an RL-based approach that enables transfer learning  meaning that the RL agent becomes faster and better at chip placement as it gains ex- perience on a greater number of chip netlists.
<EOS>
We show that our method outperforms state-of-the-art baselines and can generate placements that are superior or comparable to human experts on modern accelerators.Our method is end- to-end and generates placements in under 6 hours  whereas the strongest baselines require human experts in the loop and take several weeks. 7.
<EOS>
