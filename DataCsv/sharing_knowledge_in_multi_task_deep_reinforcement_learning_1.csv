Published as a conference paper at ICLR 2020  SHARING KNOWLEDGE IN MULTI-TASK DEEP REINFORCEMENT LEARNING  Carlo D’Eramo & Davide Tateo Department of Computer Science TU Darmstadt, IAS Hochschulstraße 10, 64289, Darmstadt, Germany {carlo.deramo,davide.tateo}@tu-darmstadt.de  Andrea Bonarini & Marcello Restelli Politecnico di Milano, DEIB Piazza Leonardo da Vinci 32, 20133, Milano {andrea.bonarini,marcello.restelli}@polimi.it  Jan Peters TU Darmstadt, IAS Hochschulstraße 10, 64289, Darmstadt, Germany Max Planck Institute for Intelligent Systems Max-Planck-Ring 4, 72076, Tübingen, Germany jan.peters@tu-darmstadt.de  ABSTRACT  We study the benefit of sharing representations among tasks to enable the effective use of deep neural networks in Multi-Task Reinforcement Learning.
<EOS>
We leverage the assumption that learning from different tasks, sharing common properties, is helpful to generalize the knowledge of them resulting in a more effective feature ex- traction compared to learning a single task.
<EOS>
Intuitively, the resulting set of features offers performance benefits when used by Reinforcement Learning algorithms.
<EOS>
We prove this by providing theoretical guarantees that highlight the conditions for which is convenient to share representations among tasks, extending the well- known finite-time bounds of Approximate Value-Iteration to the multi-task setting.
<EOS>
In addition, we complement our analysis by proposing multi-task extensions of three Reinforcement Learning algorithms that we empirically evaluate on widely used Reinforcement Learning benchmarks showing significant improvements over the single-task counterparts in terms of sample efficiency and performance.
<EOS>
 1  INTRODUCTION  Multi-Task Learning (MTL) ambitiously aims to learn multiple tasks jointly instead of learning them separately, leveraging the assumption that the considered tasks have common properties which can be exploited by Machine Learning (ML) models to generalize the learning of each of them.
<EOS>
For instance, the features extracted in the hidden layers of a neural network trained on multiple tasks have the advantage of being a general representation of structures common to each other.
<EOS>
This translates into an effective way of learning multiple tasks at the same time, but it can also improve the learning of each individual task compared to learning them separately (Caruana, 1997).
<EOS>
Furthermore, the learned representation can be used to perform Transfer Learning (TL), i.e. using it as a preliminary knowledge to learn a new similar task resulting in a more effective and faster learning than learning the new task from scratch (Baxter, 2000; Thrun & Pratt, 2012).
<EOS>
 The same benefits of extraction and exploitation of common features among the tasks achieved in MTL, can be obtained in Multi-Task Reinforcement Learning (MTRL) when training a single agent on multiple Reinforcement Learning (RL) problems with common structures (Taylor & Stone, 2009; Lazaric, 2012).
<EOS>
In particular, in MTRL an agent can be trained on multiple tasks in the same  1  Published as a conference paper at ICLR 2020  domain, e.g. riding a bicycle or cycling while going towards a goal, or on different but similar domains, e.g. balancing a pendulum or balancing a double pendulum1.
<EOS>
Considering recent advances in Deep Reinforcement Learning (DRL) and the resulting increase in the complexity of experimental benchmarks, the use of Deep Learning (DL) models, e.g. deep neural networks, has become a popular and effective way to extract common features among tasks in MTRL algorithms (Rusu et al, 2015; Liu et al, 2016; Higgins et al, 2017).
<EOS>
However, despite the high representational capacity of DL models, the extraction of good features remains challenging.
<EOS>
For instance, the performance of the learning process can degrade when unrelated tasks are used together (Caruana, 1997; Baxter, 2000); another detrimental issue may occur when the training of a single model is not balanced properly among multiple tasks (Hessel et al, 2018).
<EOS>
 Recent developments in MTRL achieve significant results in feature extraction by means of algorithms specifically developed to address these issues.
<EOS>
While some of these works rely on a single deep neural network to model the multi-task agent (Liu et al, 2016; Yang et al, 2017; Hessel et al, 2018; Wulfmeier et al, 2019), others use multiple deep neural networks, e.g. one for each task and another for the multi-task agent (Rusu et al, 2015; Parisotto et al, 2015; Higgins et al, 2017; Teh et al, 2017).
<EOS>
Intuitively, achieving good results in MTRL with a single deep neural network is more desirable than using many of them, since the training time is likely much less and the whole architecture is easier to implement.
<EOS>
In this paper we study the benefits of shared representations among tasks.
<EOS>
We theoretically motivate the intuitive effectiveness of our method, deriving theoretical guarantees that exploit the theoretical framework provided by Maurer et al (2016), in which the authors present upper bounds on the quality of learning in MTL when extracting features for multiple tasks in a single shared representation.
<EOS>
The significancy of this result is that the cost of learning the shared representation decreases with a factor O(1/ T ), where T is the number of tasks for many function approximator hypothesis classes.
<EOS>
The main contribution of this work is twofold.
<EOS>
 √  1.
<EOS>
We derive upper confidence bounds for Approximate Value-Iteration (AVI) and Approximate Policy-Iteration (API)2 (Farahmand, 2011) in the MTRL setting, and we extend the approx- imation error bounds in Maurer et al (2016) to the case of multiple tasks with different dimensionalities.
<EOS>
Then, we show how to combine these results resulting in, to the best of our knowledge, the first proposed extension of the finite-time bounds of AVI/API to MTRL.
<EOS>
Despite being an extension of previous works, we derive these results to justify our approach showing how the error propagation in AVI/API can theoretically benefit from learning multiple tasks jointly.
<EOS>
 2.
<EOS>
We leverage these results proposing a neural network architecture, for which these bounds hold with minor assumptions, that allow us to learn multiple tasks with a single regressor extracting a common representation.
<EOS>
We show an empirical evidence of the consequence of our bounds by means of a variant of Fitted Q-Iteration (FQI) (Ernst et al, 2005), based on our shared network and for which our bounds apply, that we call Multi Fitted Q-Iteration (MFQI).
<EOS>
Then, we perform an empirical evaluation in challenging RL problems proposing multi- task variants of the Deep Q-Network (DQN) (Mnih et al, 2015) and Deep Deterministic Policy Gradient (DDPG) (Lillicrap et al, 2015) algorithms.
<EOS>
These algorithms are practical implementations of the more general AVI/API framework, designed to solve complex problems.
<EOS>
In this case, the bounds apply to these algorithms only with some assumptions, e.g. stationary sampling distribution.
<EOS>
The outcome of the empirical analysis joins the theoretical results, showing significant performance improvements compared to the single- task version of the algorithms in various RL problems, including several MuJoCo (Todorov et al, 2012) domains.
<EOS>
 2 PRELIMINARIES  Let B(X ) be the space of bounded measurable functions w.r.t. the σ-algebra σX , and similarly B(X , L) be the same bounded by L < ∞.
<EOS>
 A Markov Decision Process (MDP) is defined as a 5-tuple M =< S, A, P, R, γ >, where S is the state space, A is the action space, P : S × A → S is the transition distribution where P(s(cid:48)|s, a)  1For simplicity, in this paper we refer to the concepts of task and domain interchangeably.
<EOS>
2All proofs and the theorem for API are in Appendix A.2.  2  Published as a conference paper at ICLR 2020  is the probability of reaching state s(cid:48) when performing action a in state s, R : S × A × S → R is the reward function, and γ ∈ (0, 1] is the discount factor.
<EOS>
A deterministic policy π maps, for each state, the action to perform: π : S → A. Given a policy π, the value of an action a in a state s represents the expected discounted cumulative reward obtained by performing a in s and following π thereafter: Qπ(s, a) (cid:44) E[(cid:80)∞ k=0 γkri+k+1|si = s, ai = a, π], where ri+1 is the reward obtained after the i-th transition.
<EOS>
The expected discounted cumulative reward is maximized by following the optimal policy π∗ which is the one that determines the optimal action values, i.e., the ones that satisfy the Bellman optimality equation (Bellman, 1954): Q∗(s, a) (cid:44) (cid:82) S P(s(cid:48)|s, a) [R(s, a, s(cid:48)) + γ maxa(cid:48) Q∗(s(cid:48), a(cid:48))] ds(cid:48).
<EOS>
The solution of the Bellman optimality equation is the fixed point of the optimal Bellman operator T ∗ : B(S × A) → B(S × A) defined as (T ∗Q)(s, a) (cid:44) (cid:82) S P(s(cid:48)|s, a)[R(s, a, s(cid:48)) + γ maxa(cid:48) Q(s(cid:48), a(cid:48))]ds(cid:48).
<EOS>
In the MTRL setting, there are multiple MDPs M(t) =< S (t), A(t), P (t), R(t), γ(t) > where t ∈ {1,., T } and T is the number of MDPs.
<EOS>
For each MDP M(t), a deterministic policy πt : S (t) → A(t) induces an action-value function Qπt i+k+1|si = s(t), ai = a(t), πt].
<EOS>
In this setting, the goal is to maximize the sum of the expected cumulative discounted reward of each task.
<EOS>
 t (s(t), a(t)) = E[(cid:80)∞  k=0 γkr(t)  In our theoretical analysis of the MTRL problem, the complexity of representation plays a central role.
<EOS>
As done in Maurer et al (2016), we consider the Gaussian complexity, a variant of the well-known Rademacher complexity, to measure the complexity of the representation.
<EOS>
Given a set ¯X ∈ X T n of n input samples for each task t ∈ {1,., T }, and a class H composed of k ∈ {1,., K} functions, the Gaussian complexity of a random set H( ¯X) = {(hk(Xti)) : h ∈ H} ⊆ RKT n is defined as follows:  G(H( ¯X)) = E  γtkihk(Xti)  (cid:34)  sup h∈H  (cid:88)  tki  (cid:35)  ,  (cid:12) (cid:12) (cid:12) Xti (cid:12) (cid:12)  where γtki are independent standard normal variables.
<EOS>
We also need to define the following quantity, taken from Maurer (2016): let γ be a vector of m random standard normal variables, and f ∈ F : Y → Rm, with Y ⊆ Rn, we define  O(F) =  sup  y,y(cid:48)∈Y,y(cid:54)=y(cid:48)  (cid:34)  E  sup f ∈F  (cid:104)γ, f (y) − f (y(cid:48))(cid:105)  (cid:107)y − y(cid:48)(cid:107)  (cid:35)  (1)  (2)  Equation 2 can be viewed as a Gaussian average of Lipschitz quotients, and appears in the bounds provided in this work.
<EOS>
Finally, we define L(F) as the upper bound of the Lipschitz constant of all the functions f in the function class F.  3 THEORETICAL ANALYSIS  The following theoretical study starts from the derivation of theoretical guarantees for MTRL in the AVI framework, extending the results of Farahmand (2011) in the MTRL scenario.
<EOS>
Then, to bound the approximation error term in the AVI bound, we extend the result described in Maurer (2006) to MTRL.
<EOS>
As we discuss, the resulting bounds described in this section clearly show the benefit of sharing representation in MTRL.
<EOS>
To the best of our knowledge, this is the first general result for MTRL; previous works have focused on finite MDPs (Brunskill & Li, 2013) or linear models (Lazaric & Restelli, 2011).
<EOS>
 3.1 MULTI-TASK REPRESENTATION LEARNING  The multi-task representation learning problem consists in learning simultaneously a set of T tasks µt, modeled as probability measures over the space of the possible input-output pairs (x, y), with x ∈ X and y ∈ R, being X the input space.
<EOS>
Let w ∈ W : X → RJ , h ∈ H : RJ → RK and f ∈ F : RK → R be functions chosen from their respective hypothesis classes.
<EOS>
The functions in the hypothesis classes must be Lipschitz continuous functions.
<EOS>
Let ¯Z = (Z1,., ZT ) be the multi-sample over the set of tasks µ = (µ1,., µT ), where Zt = (Zt1,., Ztn) ∼ µn t and Zti = (Xti, Yti) ∼ µt.
<EOS>
We can formalize our regression problem as the following minimization  3  Published as a conference paper at ICLR 2020  problem:  min  (cid:40)  1 nT  T (cid:88)  N (cid:88)  t=1  i=1  (cid:96)(ft(h(wt(Xti))), Yti) : f ∈ F T , h ∈ H, w ∈ W T  ,  (3)  (cid:41)  where we use f = (f1,., fT ), w = (w1,., wT ), and define the minimizers of Equation (3) as ˆw, ˆh, and ˆfWe assume that the loss function (cid:96) : R × R → [0, 1] is 1-Lipschitz in the first argument for every value of the second argument.
<EOS>
While this assumption may seem restrictive, the result obtained can be easily scaled to the general case.
<EOS>
To use the principal result of this section, for a generic loss function (cid:96)(cid:48), it is possible to use (cid:96)(·) = (cid:96)(cid:48)(·)/(cid:15)max, where (cid:15)max is the maximum value of (cid:96)(cid:48).
<EOS>
The expected loss over the tasks, given w, h and f is the task-averaged risk:  εavg(w, h, f ) =  E [(cid:96)(ft(h(wt(X))), Y )]  (4)  1 T  T (cid:88)  t=1  The minimum task-averaged risk, given the set of tasks µ and the hypothesis classes W, H and F is ε∗ avg, and the corresponding minimizers are w∗, h∗ and f ∗.
<EOS>
 3.2 MULTI-TASK APPROXIMATE VALUE ITERATION BOUND  We start by considering the bound for the AVI framework which applies for the single-task scenario.
<EOS>
Theorem 1. (Theorem 3.4 of Farahmand (2011)) Let K be a positive integer, and Qmax ≤ Rmax 1−γThen k=0 ⊂ B(S × A, Qmax) and the corresponding sequence (εk)K−1 for any sequence (Qk)K k=0 , where εk = (cid:107)Qk+1 − T ∗Qk(cid:107)2 ν, we have:  (cid:107)Q∗ − QπK (cid:107)1,ρ ≤  2γ  (cid:20)  (1 − γ)2  1 2  inf  r∈[0,1]  C  VI,ρ,ν(K; r)E  2 (ε0,., εK−1; r) +  (cid:21)  γKRmax  ,  (5)  2  1 − γ  where  CVI,ρ,ν(K; r) =  (cid:18) 1 − γ  (cid:19)2  2  sup 1,...,π(cid:48) π(cid:48) K  K−1 (cid:88)  k=0  a2(1−r) k  (cid:88)  γm(cid:16)  m≥0  cVI1,ρ,ν(m, K − k; π(cid:48)  K)  1      +cVI2,ρ,ν(m + 1; π(cid:48)  k+1,., π(cid:48)  K)    ,  (6)   2  (cid:17)  with E(ε0,., εK−1; r) = (cid:80)K−1 and ν, and the series αk are defined as in Farahmand (2011).
<EOS>
 k=0 α2r  k εk, the two coefficients cVI1,ρ,ν, cVI2,ρ,ν, the distributions ρ  In the multi-task scenario, let the average approximation error across tasks be:  εavg,k( ˆwk, ˆhk, ˆfk) =  (cid:107)Qt,k+1 − T ∗  t Qt,k(cid:107)2 ν,  (7)  1 T  T (cid:88)  t=1  is the optimal Bellman operator of task t.  where Qt,k+1 = ˆft,k ◦ ˆhk ◦ ˆwt,k, and T ∗ t In the following, we extend the AVI bound of Theorem 1 to the multi-task scenario, by computing the average loss across tasks and pushing inside the average using Jensen’s inequality.
<EOS>
Theorem 2.
<EOS>
Let K be a positive integer, and Qmax ≤ Rmax 1−γThen for any sequence (Qk)K A, Qmax) and the corresponding sequence (εavg,k)K−1 we have:  k=0 ⊂ B(S × t Qt,k(cid:107)2 ν,  k=0 , where εavg,k =  t=1(cid:107)Qt,k+1−T ∗  (cid:80)T  1 T  1 T  T (cid:88)  t=1  (cid:107)Q∗  t − QπK  t (cid:107)1,ρ ≤  2γ  (cid:20)  (1 − γ)2  inf  r∈[0,1]  1 2  1 2  C  VI(K; r)E  avg(εavg,0,., εavg,K−1; r) +  2γKRmax,avg  (cid:21)  1 − γ  (8)  with Eavg = (cid:80)K−1  k=0 α2r  k εavg,k, γ = max (cid:40) (1−γ)γK−k−1  t∈{1,...,T }  1 T  (cid:80)T  t=1 Rmax,t and αk =  1−γK+1  (1−γ)γK 1−γK+1  0 ≤ k < K,  k = K  γt, C  VI(K; r) = max  C  VI,ρ,ν(K; t, r), Rmax,avg =  t∈{1,...,T }  1 2  1 2  4  Published as a conference paper at ICLR 2020  Remarks Theorem 2 retains most of the properties of Theorem 3.4 of Farahmand (2011), except that the regression error in the bound is now task-averaged.
<EOS>
Interestingly, the second term of the sum in Equation (8) depends on the average maximum reward for each task.
<EOS>
In order to obtain this result we use an overly pessimistic bound on γ and the concentrability coefficients, however this approximation is not too loose if the MDPs are sufficiently similar.
<EOS>
 3.3 MULTI-TASK APPROXIMATION ERROR BOUND  We bound the task-averaged approximation error εavg at each AVI iteration k involved in (8) following a derivation similar to the one proposed by Maurer et al (2016), obtaining: Theorem 3.
<EOS>
Let µ, W, H and F be defined as above and assume 0 ∈ H and f (0) = 0, ∀f ∈ F. Then for δ > 0 with probability at least 1 − δ in the draw of ¯Z ∼ (cid:81)T  t=1 µn  t we have that  εavg( ˆw, ˆh, ˆf ) ≤ L(F)  c1  (cid:18)  L(H) supl∈{1,...,T } G(W(Xl))  supw(cid:107)w( ¯X)(cid:107)O(H)  minp∈P G(H(p))  (cid:19)  suph,w(cid:107)h(w( ¯X))(cid:107)O(F)  +c3  nT  n  + c4  + c2  √  n  T  nT  (cid:115)  +  8 ln( 3 δ ) nT  + ε∗  avg.
<EOS>
 (9)  √  √  Remarks The assumptions 0 ∈ H and f (0) = 0 for all f ∈ F are not essential for the proof and are only needed to simplify the result.
<EOS>
For reasonable function classes, the Gaussian complexity n).
<EOS>
If supw(cid:107)w( ¯X)(cid:107) and suph,w(cid:107)h(w( ¯X))(cid:107) can be uniformly bounded, then G(W(Xl)) is O( they are O( nT ).
<EOS>
For some function classes, the Gaussian average of Lipschitz quotients O(·) can be bounded independently from the number of samples.
<EOS>
Given these assumptions, the first and the fourth term of the right hand side of Equation (9), which represent respectively the cost of learning the meta-state space w and the task-specific f mappings, are both O(1/√ n).
<EOS>
The second term represents the cost of learning the multi-task representation h and is O(1/ nT ), thus vanishing in the multi-task limit T → ∞.
<EOS>
The third term can be removed if ∀h ∈ H, ∃p0 ∈ P : h(p) = 0; even when this assumption does not hold, this term can be ignored for many classes of interest, e.g. neural networks, as it can be arbitrarily small.
<EOS>
The last term to be bounded in (9) is the minimum average approximation error ε∗ avg at each AVI iteration k. Recalling that the task-averaged approximation error is defined as in (7), applying Theorem 5.3 by Farahmand (2011) we obtain: Lemma 4.
<EOS>
Let Q∗ T ∗ t Qt,k(cid:107)2  t,k, ∀t ∈ {1,., T } be the minimizers of ε∗  avg,k, ˇtk = arg maxt∈{1,...,T }(cid:107)Q∗  ν, and bk,i = (cid:107)Qˇtk,i+1 − T ∗  ˇt Qˇtk,i(cid:107)ν, then:  t,k+1 −  √  (cid:32)  (cid:33)2  ε∗ avg,k ≤  (cid:107)Q∗  ˇtk,k+1 − (T ∗  ˇt )k+1Qˇtk,0(cid:107)ν +  (γˇtk CAE(ν; ˇtk, P ))i+1bk,k−1−i  ,  (10)  k−1 (cid:88)  i=0  with CAE defined as in Farahmand (2011).
<EOS>
 Final remarks The bound for MTRL is derived by composing the results in Theorems 2 and 3, and Lemma 4.
<EOS>
The results above highlight the advantage of learning a shared representation.
<EOS>
The bound in Theorem 2 shows that a small approximation error is critical to improve the convergence towards the optimal action-value function, and the bound in Theorem 3 shows that the cost of learning the shared representation at each AVI iteration is mitigated by using multiple tasks.
<EOS>
This is particularly beneficial when the feature representation is complex, e.g. deep neural networks.
<EOS>
 3.4 DISCUSSION  As stated in the remarks of Equation (9), the benefit of MTRL is evinced by the second component of the bound, i.e. the cost of learning h, which vanishes with the increase of the number of tasks.
<EOS>
Obviously, adding more tasks require the shared representation to be large enough to include all of them, undesirably causing the term suph,w(cid:107)h(w( ¯X))(cid:107) in the fourth component of the bound to increase.
<EOS>
This introduces a tradeoff between the number of features and number of tasks; however, for  5  Published as a conference paper at ICLR 2020  (a) Shared network  (b) FQI vs MFQI  (c) #Task analysis  Figure 1: (a) The architecture of the neural network we propose to learn T tasks simultaneously.
<EOS>
The wt block maps each input xt from task µt to a shared set of layers h which extracts a common representation of the tasks.
<EOS>
Eventually, the shared representation is specialized in block ft and the output yt of the network is computed.
<EOS>
Note that each block can be composed of arbitrarily many layers. (b) Results of FQI and MFQI averaged over 4 tasks in Car-On-Hill, showing (cid:107)Q∗ − QπK (cid:107) on the left, and the discounted cumulative reward on the right. (c) Results of MFQI showing (cid:107)Q∗ −QπK (cid:107) for increasing number of tasks.
<EOS>
Both results in (b) and (c) are averaged over 100 experiments, and show the 95% confidence intervals.
<EOS>
 a reasonable number of tasks the number of features used in the single-task case is enough to handle them, as we show in some experiments in Section 5.
<EOS>
Notably, since the AVI/API framework provided by Farahmand (2011) provides an easy way to include the approximation error of a generic function approximator, it is easy to show the benefit in MTRL of the bound in Equation (9).
<EOS>
Despite being just multi-task extensions of previous works, our results are the first one to theoretically show the benefit of sharing representation in MTRL.
<EOS>
Moreover, they serve as a significant theoretical motivation, besides to the intuitive ones, of the practical algorithms that we describe in the following sections.
<EOS>
 4 SHARING REPRESENTATIONS  We want to empirically evaluate the benefit of our theoretical study in the problem of jointly learning T different tasks µt, introducing a neural network architecture for which our bounds hold.
<EOS>
Following our theoretical framework, the network we propose extracts representations wt from inputs xt for each task µt, mapping them to common features in a set of shared layers h, specializing the learning of each task in respective separated layers ft, and finally computing the output yt = (ft ◦ h ◦ wt)(xt) = ft(h(wt(xt))) (Figure 1(a)).
<EOS>
The idea behind this architecture is not new in the literature.
<EOS>
For instance, similar ideas have already been used in DQN variants to improve exploration on the same task via bootstrapping (Osband et al, 2016) and to perform MTRL (Liu et al, 2016).
<EOS>
 The intuitive and desirable property of this architecture is the exploitation of the regularization effect introduced by the shared representation of the jointly learned tasks.
<EOS>
Indeed, unlike learning a single task that may end up in overfitting, forcing the model to compute a shared representation of the tasks helps the regression process to extract more general features, with a consequent reduction in the variance of the learned function.
<EOS>
This intuitive justification for our approach, joins the theoretical benefit proven in Section 3.
<EOS>
Note that our architecture can be used in any MTRL problem involving a regression process; indeed, it can be easily used in value-based methods as a Q-function regressor, or in policy search as a policy regressor.
<EOS>
In both cases, the targets are learned for each task µt in its respective output block ft.
<EOS>
Remarkably, as we show in the experimental Section 5, it is straightforward to extend RL algorithms to their multi-task variants only through the use of the proposed network architecture, without major changes to the algorithms themselves.
<EOS>
 5 EXPERIMENTAL RESULTS  To empirically evince the effect described by our bounds, we propose an extension of FQI (Ernst et al, 2005; Riedmiller, 2005), that we call MFQI, for which our AVI bounds apply.
<EOS>
Then, to empirically evaluate our approach in challenging RL problems, we introduce multi-task variants of two well-known DRL algorithms: DQN (Mnih et al, 2015) and DDPG (Lillicrap et al, 2015), which we call Multi Deep Q-Network (MDQN) and Multi Deep Deterministic Policy Gradient (MDDPG) respectively.
<EOS>
Note that for these methodologies, our AVI and API bounds hold only with  6  hhw1w1w2w2wTwTf1f1f2f2fTfTInputOutputx1x2xTy1y2yT........02550# Iterations0.150.200.250.300.350.400.450.50Q*QKFQIMULTI02550# Iterations0.050.000.050.100.15Performance02550# Iterations0.150.200.250.300.350.400.450.50Q*QK1248Published as a conference paper at ICLR 2020  (a) Multi-task  (b) Transfer  Figure 2: Discounted cumulative reward averaged over 100 experiments of DQN and MDQN for each task and for transfer learning in the Acrobot problem.
<EOS>
An epoch consists of 1, 000 steps, after which the greedy policy is evaluated for 2, 000 steps.
<EOS>
The 95% confidence intervals are shown.
<EOS>
 the simplifying assumption that the samples are i.i.d.; nevertheless they are useful to show the benefit of our method also in complex scenarios, e.g. MuJoCo (Todorov et al, 2012).
<EOS>
We remark that in these experiments we are only interested in showing the benefit of learning multiple tasks with a shared representation w.r.t. learning a single task; therefore, we only compare our methods with the single task counterparts, ignoring other works on MTRL in literature.
<EOS>
Experiments have been developed using the MushroomRL library (D’Eramo et al, 2020), and run on an NVIDIA R(cid:13) DGX StationTM and Intel R(cid:13) AI DevCloud.
<EOS>
Refer to Appendix B for all the details and our motivations about the experimental settings.
<EOS>
 5.1 MULTI FITTED Q-ITERATION  As a first empirical evaluation, we consider FQI, as an example of an AVI algorithm, to show the effect described by our theoretical AVI bounds in experiments.
<EOS>
We consider the Car-On-Hill problem as described in Ernst et al (2005), and select four different tasks from it changing the mass of the car and the value of the actions (details in Appendix B).
<EOS>
Then, we run separate instances of FQI with a single task network for each task respectively, and one of MFQI considering all the tasks simultaneously.
<EOS>
Figure 1(b) shows the L1-norm of the difference between Q∗ and QπK averaged over all the tasks.
<EOS>
It is clear how MFQI is able to get much closer to the optimal Q-function, thus giving an empirical evidence of the AVI bounds in Theorem 2.
<EOS>
For completeness, we also show the advantage of MFQI w.r.t. FQI in performance.
<EOS>
Then, in Figure 1(c) we provide an empirical evidence of the benefit of increasing the number of tasks in MFQI in terms of both quality and stability.
<EOS>
 5.2 MULTI DEEP Q-NETWORK  As in Liu et al (2016), our MDQN uses separate replay memories for each task and the batch used in each training step is built picking the same number of samples from each replay memory.
<EOS>
Furthermore, a step of the algorithm consists of exactly one step in each task.
<EOS>
These are the only minor changes to the vanilla DQN algorithm we introduce, while all other aspects, such as the use of the target network, are not modified.
<EOS>
Thus, the time complexity of MDQN is considerably lower than vanilla DQN thanks to the learning of T tasks with a single model, but at the cost of a higher memory complexity for the collection of samples for each task.
<EOS>
We consider five problems with similar state spaces, sparse rewards and discrete actions: Cart-Pole, Acrobot, Mountain-Car, Car-On-Hill, and Inverted-Pendulum.
<EOS>
The implementation of the first three problems is the one provided by the OpenAI Gym library Brockman et al (2016), while Car-On-Hill is described in Ernst et al (2005) and Inverted-Pendulum in Lagoudakis & Parr (2003).
<EOS>
 Figure 2(a) shows the performance of MDQN w.r.t. to vanilla DQN that uses a single-task network structured as the multi-task one in the case with T = 1.
<EOS>
The first three plots from the left show good performance of MDQN, which is both higher and more stable than DQN.
<EOS>
In Car-On-Hill, MDQN is slightly slower than DQN to reach the best performance, but eventually manages to be more stable.
<EOS>
Finally, the Inverted-Pendulum experiment is clearly too easy to solve for both approaches, but it is still useful for the shared feature extraction in MDQN.
<EOS>
The described results provide important hints about the better quality of the features extracted by MDQN w.r.t. DQN.
<EOS>
To further demonstrate this, we evaluate the performance of DQN on Acrobot, arguably the hardest of the five problems, using a single-task network with the shared parameters in h initialized with the weights of a multi-task  7  02550#Epochs20406080PerformanceCart-Pole02550#Epochs10090807060Acrobot02550#Epochs10095908580757065Mountain-Car02550#Epochs0.00.10.20.30.4Car-On-Hill02550#Epochs0.60.40.20.0Inverted-PendulumDQNMULTI02550#Epochs10090807060PerformanceAcrobotNo initializationUnfreeze-0Unfreeze-10No unfreezePublished as a conference paper at ICLR 2020  (a) Multi-task for pendulums  (b) Transfer for pendulums  (c) Multi-task for walkers  (d) Transfer for walkers  Figure 3: Discounted cumulative reward averaged over 40 experiments of DDPG and MDDPG for each task and for transfer learning in the Inverted-Double-Pendulum and Hopper problems.
<EOS>
An epoch consists of 10, 000 steps, after which the greedy policy is evaluated for 5, 000 steps.
<EOS>
The 95% confidence intervals are shown.
<EOS>
 network trained with MDQN on the other four problems.
<EOS>
Arbitrarily, the pre-trained weights can be adjusted during the learning of the new task or can be kept fixed and only the remaining randomly initialized parameters in w and f are trained.
<EOS>
From Figure 2(b), the advantages of initializing the weights are clear.
<EOS>
In particular, we compare the performance of DQN without initialization w.r.t. DQN with initialization in three settings: in Unfreeze-0 the initialized weights are adjusted, in No- Unfreeze they are kept fixed, and in Unfreeze-10 they are kept fixed until epoch 10 after which they start to be optimized.
<EOS>
Interestingly, keeping the shared weights fixed shows a significant performance improvement in the earliest epochs, but ceases to improve soon.
<EOS>
On the other hand, the adjustment of weights from the earliest epochs shows improvements only compared to the uninitialized network in the intermediate stages of learning.
<EOS>
The best results are achieved by starting to adjust the shared weights after epoch 10, which is approximately the point at which the improvement given by the fixed initialization starts to lessen.
<EOS>
 5.3 MULTI DEEP DETERMINISTIC POLICY GRADIENT  In order to show how the ﬂexibility of our approach easily allows to perform MTRL in policy search algorithms, we propose MDDPG as a multi-task variant of DDPG.
<EOS>
As an actor-critic method, DDPG requires an actor network and a critic network.
<EOS>
Intuitively, to obtain MDDPG both the actor and critic networks should be built following our proposed structure.
<EOS>
We perform separate experiments on two sets of MuJoCo Todorov et al (2012) problems with similar continuous state and action spaces: the first set includes Inverted-Pendulum, Inverted-Double-Pendulum, and Inverted-Pendulum-Swingup as implemented in the pybullet library, whereas the second set includes Hopper-Stand, Walker-Walk, and Half-Cheetah-Run as implemented in the DeepMind Control SuiteTassa et al (2018).
<EOS>
Figure 3(a) shows a relevant improvement of MDDPG w.r.t. DDPG in the pendulum tasks.
<EOS>
Indeed, while in Inverted-Pendulum, which is the easiest problem among the three, the performance of MDDPG is only slightly better than DDPG, the difference in the other two problems is significant.
<EOS>
The advantage of MDDPG is confirmed in Figure 3(c) where it performs better than DDPG in Hopper and equally good in the other two tasks.
<EOS>
Again, we perform a TL evaluation of DDPG in the problems where it suffers the most, by initializing the shared weights of a single-task network with the ones of a multi-task network trained with MDDPG on the other problems.
<EOS>
Figures 3(b) and 3(d) show evident advantages of pre-training the shared weights and a significant difference between keeping them fixed or not.
<EOS>
 8  050100#Epochs2030405060708090100PerformanceInverted-PendulumDDPGMULTI050100#Epochs100200300400500600700800Inverted-Double-Pendulum050100#Epochs100806040200Inverted-Pendulum-Swingup050100#Epochs200400600800PerformanceInverted-Double-PendulumNo initializationUnfreeze-0No unfreeze050100#Epochs05101520253035PerformanceHopper050100#Epochs010203040506070Walker050100#Epochs0510152025303540Half-CheetahDDPGMULTI050100#Epochs010203040PerformanceHopperNo initializationUnfreeze-0No unfreezePublished as a conference paper at ICLR 2020  6 RELATED WORKS  Our work is inspired from both theoretical and empirical studies in MTL and MTRL literature.
<EOS>
In particular, the theoretical analysis we provide follows previous results about the theoretical properties of multi-task algorithms.
<EOS>
For instance, Cavallanti et al (2010) and Maurer (2006) prove the theoretical advantages of MTL based on linear approximation.
<EOS>
More in detail, Maurer (2006) derives bounds on MTL when a linear approximator is used to extract a shared representation among tasks.
<EOS>
Then, Maurer et al (2016), which we considered in this work, describes similar results that extend to the use of non-linear approximators.
<EOS>
Similar studies have been conducted in the context of MTRL.
<EOS>
Among the others, Lazaric & Restelli (2011) and Brunskill & Li (2013) give theoretical proofs of the advantage of learning from multiple MDPs and introduces new algorithms to empirically support their claims, as done in this work.
<EOS>
 Generally, contributions in MTRL assume that properties of different tasks, e.g. dynamics and reward function, are generated from a common generative model.
<EOS>
About this, interesting analyses consider Bayesian approaches; for instance Wilson et al (2007) assumes that the tasks are generated from a hierarchical Bayesian model, and likewise Lazaric & Ghavamzadeh (2010) considers the case when the value functions are generated from a common prior distribution.
<EOS>
Similar considerations, which however does not use a Bayesian approach, are implicitly made in Taylor et al (2007), Lazaric et al (2008), and also in this work.
<EOS>
 In recent years, the advantages of MTRL have been empirically evinced also in DRL, especially exploiting the powerful representational capacity of deep neural networks.
<EOS>
For instance, Parisotto et al (2015) and Rusu et al (2015) propose to derive a multi-task policy from the policies learned by DQN experts trained separately on different tasks.
<EOS>
Rusu et al (2015) compares to a therein introduced variant of DQN, which is very similar to our MDQN and the one in Liu et al (2016), showing how their method overcomes it in the Atari benchmark Bellemare et al (2013).
<EOS>
Further developments, extend the analysis to policy search (Yang et al, 2017; Teh et al, 2017), and to multi-goal RL (Schaul et al, 2015; Andrychowicz et al, 2017).
<EOS>
Finally, Hessel et al (2018) addresses the problem of balancing the learning of multiple tasks with a single deep neural network proposing a method that uniformly adapts the impact of each task on the training updates of the agent.
<EOS>
 7 CONCLUSION  We have theoretically proved the advantage in RL of using a shared representation to learn multiple tasks w.r.t. learning a single task.
<EOS>
We have derived our results extending the AVI/API bounds (Farah- mand, 2011) to MTRL, leveraging the upper bounds on the approximation error in MTL provided in Maurer et al (2016).
<EOS>
The results of this analysis show that the error propagation during the AVI/API iterations is reduced according to the number of tasks.
<EOS>
Then, we proposed a practical way of exploiting this theoretical benefit which consists in an effective way of extracting shared representa- tions of multiple tasks by means of deep neural networks.
<EOS>
To empirically show the advantages of our method, we carried out experiments on challenging RL problems with the introduction of multi-task extensions of FQI, DQN, and DDPG based on the neural network structure we proposed.
<EOS>
As desired, the favorable empirical results confirm the theoretical benefit we described.
<EOS>
 9  Published as a conference paper at ICLR 2020  ACKNOWLEDGMENTS  This project has received funding from the European Union’s Horizon 2020 research and innovation programme under grant agreement No. #640554 (SKILLS4ROBOTS) and No. #713010 (GOAL- Robots).
<EOS>
This project has also been supported by grants from NVIDIA, the NVIDIA DGX Station, and the Intel R(cid:13) AI DevCloud.
<EOS>
The authors thank Alberto Maria Metelli, Andrea Tirinzoni and Matteo Papini for their helpful insights during the development of the project.
<EOS>
 REFERENCES  Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob McGrew, Josh Tobin, OpenAI Pieter Abbeel, and Wojciech Zaremba.
<EOS>
Hindsight experience replay.
<EOS>
In Advances in Neural Information Processing Systems, pp 5048–5058, 2017.
<EOS>
 Jonathan Baxter.
<EOS>
A model of inductive bias learning.
<EOS>
Journal of Artificial Intelligence Research, 12:  149–198, 2000.
<EOS>
 Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling.
<EOS>
The arcade learning environ- ment: An evaluation platform for general agents.
<EOS>
Journal of Artificial Intelligence Research, 47: 253–279, 2013.
<EOS>
 Richard Bellman.
<EOS>
The theory of dynamic programming.
<EOS>
Technical report, RAND Corp Santa Monica  CA, 1954.
<EOS>
 Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and  Wojciech Zaremba.
<EOS>
Openai gym, 2016.
<EOS>
 Emma Brunskill and Lihong Li. Sample complexity of multi-task reinforcement learning.
<EOS>
Proceedings of the Twenty-Ninth Conference on Uncertainty in Artificial Intelligence, 2013.
<EOS>
 In  Rich Caruana.
<EOS>
Multitask learning.
<EOS>
Machine learning, 28(1):41–75, 1997.
<EOS>
 Giovanni Cavallanti, Nicolo Cesa-Bianchi, and Claudio Gentile.
<EOS>
Linear algorithms for online  multitask classification.
<EOS>
Journal of Machine Learning Research, 11(Oct):2901–2934, 2010.
<EOS>
 Carlo D’Eramo, Davide Tateo, Andrea Bonarini, Marcello Restelli, and Jan Peters.
<EOS>
Mushroomrl:  Simplifying reinforcement learning research. arXiv:2001.01102, 2020.
<EOS>
 Damien Ernst, Pierre Geurts, and Louis Wehenkel.
<EOS>
Tree-based batch mode reinforcement learning.
<EOS>
 Journal of Machine Learning Research, 6(Apr):503–556, 2005.
<EOS>
 Amir-massoud Farahmand.
<EOS>
Regularization in reinforcement learning.
<EOS>
2011.
<EOS>
 Matteo Hessel, Hubert Soyer, Lasse Espeholt, Wojciech Czarnecki, Simon Schmitt, and Hado van  Hasselt.
<EOS>
Multi-task deep reinforcement learning with popart. arXiv:1809.04474, 2018.
<EOS>
 Irina Higgins, Arka Pal, Andrei Rusu, Loic Matthey, Christopher Burgess, Alexander Pritzel, Matthew Botvinick, Charles Blundell, and Alexander Lerchner.
<EOS>
Darla: Improving zero-shot transfer in reinforcement learning.
<EOS>
In International Conference on Machine Learning, pp 1480–1490, 2017.
<EOS>
 Michail G Lagoudakis and Ronald Parr.
<EOS>
Least-squares policy iteration.
<EOS>
Journal of machine learning  research, 4(Dec):1107–1149, 2003.
<EOS>
 Alessandro Lazaric.
<EOS>
Transfer in reinforcement learning: a framework and a survey.
<EOS>
In Reinforcement  Learning, pp 143–173.
<EOS>
Springer, 2012.
<EOS>
 Alessandro Lazaric and Mohammad Ghavamzadeh.
<EOS>
Bayesian multi-task reinforcement learning.
<EOS>
In  ICML-27th International Conference on Machine Learning, pp 599–606.
<EOS>
Omnipress, 2010.
<EOS>
 Alessandro Lazaric and Marcello Restelli.
<EOS>
Transfer from multiple mdps.
<EOS>
In Advances in Neural  Information Processing Systems, pp 1746–1754, 2011.
<EOS>
 Alessandro Lazaric, Marcello Restelli, and Andrea Bonarini.
<EOS>
Transfer of samples in batch rein- forcement learning.
<EOS>
In Proceedings of the 25th international conference on Machine learning, pp 544–551.
<EOS>
ACM, 2008.
<EOS>
 10  Published as a conference paper at ICLR 2020  Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra.
<EOS>
Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015.
<EOS>
 Lydia Liu, Urun Dogan, and Katja Hofmann.
<EOS>
Decoding multitask dqn in the world of minecraft.
<EOS>
In  European Workshop on Reinforcement Learning, 2016.
<EOS>
 Andreas Maurer.
<EOS>
Bounds for linear multi-task learning.
<EOS>
Journal of Machine Learning Research, 7  (Jan):117–139, 2006.
<EOS>
 Science, 650:109–122, 2016.
<EOS>
 Andreas Maurer.
<EOS>
A chain rule for the expected suprema of gaussian processes.
<EOS>
Theoretical Computer  Andreas Maurer, Massimiliano Pontil, and Bernardino Romera-Paredes.
<EOS>
The benefit of multitask  representation learning.
<EOS>
The Journal of Machine Learning Research, 17(1):2853–2884, 2016.
<EOS>
 Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al Human-level control through deep reinforcement learning.
<EOS>
Nature, 518(7540):529, 2015.
<EOS>
 Ian Osband, Charles Blundell, Alexander Pritzel, and Benjamin Van Roy.
<EOS>
Deep exploration via bootstrapped dqn.
<EOS>
In Advances in neural information processing systems, pp 4026–4034, 2016.
<EOS>
 Emilio Parisotto, Jimmy Lei Ba, and Ruslan Salakhutdinov.
<EOS>
Actor-mimic: Deep multitask and  transfer reinforcement learning. arXiv preprint arXiv:1511.06342, 2015.
<EOS>
 Martin Riedmiller.
<EOS>
Neural fitted q iteration–first experiences with a data efficient neural reinforcement  learning method.
<EOS>
In European Conference on Machine Learning, pp 317–328.
<EOS>
Springer, 2005.
<EOS>
 Andrei A Rusu, Sergio Gomez Colmenarejo, Caglar Gulcehre, Guillaume Desjardins, James Kirk- patrick, Razvan Pascanu, Volodymyr Mnih, Koray Kavukcuoglu, and Raia Hadsell.
<EOS>
Policy distillation. arXiv preprint arXiv:1511.06295, 2015.
<EOS>
 Tom Schaul, Daniel Horgan, Karol Gregor, and David Silver.
<EOS>
Universal value function approximators.
<EOS>
 In International Conference on Machine Learning, pp 1312–1320, 2015.
<EOS>
 Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de Las Casas, David Budden, Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq, Timothy P. Lillicrap, and Martin A. Riedmiller.
<EOS>
Deepmind control suite.
<EOS>
CoRR, abs/1801.00690, 2018.
<EOS>
 Matthew E Taylor and Peter Stone.
<EOS>
Transfer learning for reinforcement learning domains: A survey.
<EOS>
 Journal of Machine Learning Research, 10(Jul):1633–1685, 2009.
<EOS>
 Matthew E Taylor, Peter Stone, and Yaxin Liu.
<EOS>
Transfer learning via inter-task mappings for temporal  difference learning.
<EOS>
Journal of Machine Learning Research, 8(Sep):2125–2167, 2007.
<EOS>
 Yee Teh, Victor Bapst, Wojciech M Czarnecki, John Quan, James Kirkpatrick, Raia Hadsell, Nicolas Heess, and Razvan Pascanu.
<EOS>
Distral: Robust multitask reinforcement learning.
<EOS>
In Advances in Neural Information Processing Systems, pp 4496–4506, 2017.
<EOS>
 Sebastian Thrun and Lorien Pratt.
<EOS>
Learning to learn.
<EOS>
Springer Science & Business Media, 2012.
<EOS>
 Emanuel Todorov, Tom Erez, and Yuval Tassa.
<EOS>
Mujoco: A physics engine for model-based control.
<EOS>
 In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems.
<EOS>
IEEE, 2012.
<EOS>
 Aaron Wilson, Alan Fern, Soumya Ray, and Prasad Tadepalli.
<EOS>
Multi-task reinforcement learning: a hierarchical bayesian approach.
<EOS>
In Proceedings of the 24th international conference on Machine learning, pp 1015–1022.
<EOS>
ACM, 2007.
<EOS>
 Markus Wulfmeier, Abbas Abdolmaleki, Roland Hafner, Jost Tobias Springenberg, Michael Neunert, Tim Hertweck, Thomas Lampe, Noah Siegel, Nicolas Heess, and Martin Riedmiller.
<EOS>
Regularized hierarchical policies for compositional transfer in robotics. arXiv:1906.11228, 2019.
<EOS>
 Zhaoyang Yang, Kathryn E Merrick, Hussein A Abbass, and Lianwen Jin.
<EOS>
Multi-task deep reinforce-  ment learning for continuous action control.
<EOS>
In IJCAI, pp 3301–3307, 2017.
<EOS>
 11  Published as a conference paper at ICLR 2020  A PROOFS  A.1 APPROXIMATED VALUE-ITERATION BOUNDS  Proof of Theorem 2.
<EOS>
We compute the average expected loss across tasks:  1 T  T (cid:88)  t=1  (cid:107)Q∗  t − QπK  t (cid:107)1,ρ  1 T  T (cid:88)  t=1  2γ  (1 − γ)2  (1 − γ)2  (1 − γ)2  2γ  2γ  2γ  ≤  ≤  ≤  ≤  ≤  2γt  (cid:20)  (1 − γt)2  inf  r∈[0,1]  T (cid:88)  (cid:20)  t=1  inf  r∈[0,1]  1 T  T (cid:88)  (cid:18)  t=1  inf  r∈[0,1]  1 T  inf  r∈[0,1]  T (cid:88)  (cid:16)  t=1  1 T  (cid:34)  (cid:34)  (cid:34)  1 2  1 2  1 2  1 2  1 T  C  VI,ρ,ν(K; t, r)E  2 (εt,0,., εt,K−1; t, r) +  C  VI,ρ,ν(K; t, r)E  2 (εt,0,., εt,K−1; t, r) +  C  VI,ρ,ν(K; t, r)E  2 (εt,0,., εt,K−1; t, r)  +  C  VI,ρ,ν(K; t, r)E  2 (εt,0,., εt,K−1; t, r)  +  1  1  1  1  2  1 − γt  γK t Rmax,t  2  1 − γt  γK t Rmax,t  (cid:21)  (cid:21)  (cid:19)  2  1 − γ  γKRmax,avg  (cid:17)  2  1 − γ  γKRmax,avg  (cid:35)  (cid:35)  (cid:35)  (1 − γ)2  inf  r∈[0,1]  1 2  C  VI(K; r)  T (cid:88)  (cid:16)  1  t=1  E  2 (εt,0,., εt,K−1; t, r)  +  γKRmax,avg  (11)  (cid:17)  2  1 − γ  with γ = max  γt, C  VI(K; r) = max  t∈{1,...,T }  t∈{1,...,T }  1 2  1 2  VI,ρ,ν(K; t, r), and Rmax,avg = 1/T (cid:80)T C  t=1 Rmax,t.
<EOS>
 Considering the term 1/T (cid:80)T  (cid:104)  E 1  t=1  2 (εt,0,., εt,K−1; t, r)  (cid:105)  = 1/T (cid:80)T  t=1  (cid:16)(cid:80)K−1  k=0 α2r  t,kεt,k  (cid:17) 1  2  let  αk =  (cid:40) (1−γ)γK−k−1  1−γK+1  (1−γ)γK 1−γK+1  0 ≤ k < K,  ,  k = K  T (cid:88)  (cid:32)K−1 (cid:88)  t=1  k=0  α2r  t,kεt,k  (cid:33) 1  2  ≤  1 T  T (cid:88)  (cid:32)K−1 (cid:88)  t=1  k=0  (cid:33) 1  2  α2r  k εt,k  T (cid:88)  (cid:32)K−1 (cid:88)  t=1  k=0  (cid:33) 1  2  α2r  k εt,k  ≤  (cid:32)K−1 (cid:88)  k=0  α2r k  1 T  T (cid:88)  t=1  (cid:33) 1  2  εt,k  then we bound  Using Jensen’s inequality:  1 T  1 T  So, now we can write (11) as  1 T  T (cid:88)  t=1  (cid:107)Q∗  t − QπK  t (cid:107)1,ρ ≤  2γ  (cid:20)  (1 − γ)2  1 2  inf  r∈[0,1]  C  VI(K; r)E  avg(εavg,0,., εavg,K−1; r)  1 2  (cid:21)  2  +  1 − γ  γKRmax,avg  ,  with εavg,k = 1/T (cid:80)T  t=1 εt,k and Eavg(εavg,0,., εavg,K−1; r) = (cid:80)K−1  k=0 α2r  k εavg,k.
<EOS>
 Proof of Lemma 4.
<EOS>
Let us start from the definition of optimal task-averaged risk:  ε∗ avg,k =  (cid:107)Q∗  t,k+1 − T ∗  t Qt,k(cid:107)2 ν,  1 T  T (cid:88)  t=1  12  Published as a conference paper at ICLR 2020  where Q∗  t,k, with t ∈ [1, T ], are the minimizers of εavg,k.
<EOS>
 Consider the task ˇt such that  we can write the following inequality:  ˇtk = arg max  (cid:107)Q∗  t,k+1 − T ∗  t Qt,k(cid:107)2 ν,  t∈{1,...,T }  (cid:113)  avg,k ≤ (cid:107)Q∗ ε∗  ˇtk,k+1 − T ∗  ˇt Qˇtk,k(cid:107)ν.
<EOS>
 By the application of Theorem 5.3 by Farahmand (2011) to the right hand side, and defining bk,i = (cid:107)Qˇtk,i+1 − T ∗  ˇt Qˇtk,i(cid:107)ν, we obtain:  (cid:113)  avg,k ≤ (cid:107)Q∗ ε∗  ˇtk,k+1 − (T ∗  ˇt )k+1Qˇtk,0(cid:107)ν +  (γˇtk CAE(ν; ˇtk, P ))i+1bk,k−1−i.
<EOS>
 Squaring both sides yields the result:  (cid:32)  ε∗ avg,k ≤  (cid:107)Q∗  ˇtk,k+1 − (T ∗  ˇt )k+1Qˇtk,0(cid:107)ν +  (γˇtk CAE(ν; ˇtk, P ))i+1bk,k−1−i  (cid:33)2  k−1 (cid:88)  i=0  k−1 (cid:88)  i=0  A.2 APPROXIMATED POLICY-ITERATION BOUNDS  We start by considering the bound for the API framework: Theorem 5. (Theorem 3.2 of Farahmand (2011)) Let K be a positive integer, and Qmax ≤ Rmax for any sequence (Qk)K−1 k=0 ⊂ B(S × A, Qmax) and the corresponding sequence (εk)K−1 εk = (cid:107)Qk − Qπk (cid:107)2  1−γThen k=0 , where  1 2  C  PI,ρ,ν(K; r)E  1  2 (ε0,., εK−1; r) + γK−1Rmax  ,  (12)  (cid:21)  (cid:107)Q∗ − QπK (cid:107)1,ρ ≤  ν, we have: 2γ  (1 − γ)2  (cid:20)  inf  r∈[0,1]  where  CPI,ρ,ν(K; r) =  (cid:18) 1 − γ  (cid:19)2  2  sup 0,...,π(cid:48) π(cid:48) K  K−1 (cid:88)  k=0  a2(1−r) k      (cid:88)  m≥0  (cid:88)  m≥1  γmcPI1,ρ,ν(K − k − 1, m + 1; π(cid:48)  k+1)+  γmcPI2,ρ,ν(K − k − 1, m; π(cid:48)  k+1, π(cid:48)  k) + cPI3,ρ,ν    ;   2  (13)  with E(ε0,., εK−1; r) = (cid:80)K−1 butions ρ and ν, and the series αk are defined as in Farahmand (2011).
<EOS>
 k=0 α2r  k εk, the three coefficients cPI1,ρ,ν, cPI2,ρ,ν, cPI3,ρ,ν, the distri-  From Theorem 5, by computing the average loss across tasks and pushing inside the average using Jensen’s inequality, we derive the API bounds averaged on multiple tasks.
<EOS>
Theorem 6.
<EOS>
Let K be a positive integer, and Qmax ≤ Rmax A, Qmax) and the corresponding sequence (εavg,k)K−1 have:  1−γThen for any sequence (Qk)K−1  k=0 , where εavg,k =  t=1(cid:107)Qt,k − Qπk  k=0 ⊂ B(S ×  ν, we  t (cid:107)2  (cid:80)T  1 T  1 T  T (cid:88)  t=1  (cid:107)Q∗  t − QπK  t (cid:107)1,ρ ≤  2γ  (cid:20)  (1 − γ)2  inf  r∈[0,1]  1 2  1 2  C  PI(K; r)E  avg(εavg,0,., εavg,K−1; r)  +γK−1Rmax,avg  (cid:3) ,  (14)  13  Published as a conference paper at ICLR 2020  with Eavg = (cid:80)K−1  k=0 α2r  k εavg,k, γ = max (cid:40) (1−γ)γK−k−1  t∈{1,...,T }  1 T  (cid:80)T  t=1 Rmax,t and αk =  1−γK+1  (1−γ)γK 1−γK+1  0 ≤ k < K,  k = K  1 2  1 2  γt, C  PI(K; r) = max  C  PI,ρ,ν(K; t, r), Rmax,avg =  t∈{1,...,T }  Proof of Theorem 6.
<EOS>
The proof is very similar to the one for AVI.
<EOS>
We compute the average expected loss across tasks:  1 T  T (cid:88)  t=1  (cid:107)Q∗  t − QπK  t (cid:107)1,ρ  1 T  T (cid:88)  t=1  2γ  (1 − γ)2  (1 − γ)2  (1 − γ)2  2γ  2γ  2γ  ≤  ≤  ≤  ≤  ≤  2γt  (cid:20)  (1 − γt)2  inf  r∈[0,1]  T (cid:88)  (cid:20)  t=1  inf  r∈[0,1]  1 T  T (cid:88)  (cid:18)  t=1  inf  r∈[0,1]  1 T  inf  r∈[0,1]  T (cid:88)  (cid:16)  t=1  1 T  (cid:34)  (cid:34)  (cid:34)  1 2  1 2  1 T  1 2  C  PI,ρ,ν(K; t, r)E  1  2 (εt,0,., εt,K−1; t, r) + γK−1  Rmax,t  t  1 2  C  PI,ρ,ν(K; t, r)E  1  2 (εt,0,., εt,K−1; t, r) + γK−1  Rmax,t  t  C  PI,ρ,ν(K; t, r)E  2 (εt,0,., εt,K−1; t, r)  + γK−1Rmax,avg  C  PI,ρ,ν(K; t, r)E  2 (εt,0,., εt,K−1; t, r)  + γK−1Rmax,avg  1  1  (cid:21)  (cid:21)  (cid:35)  (cid:35)  (cid:35)  (1 − γ)2  inf  r∈[0,1]  1 2  C  PI(K; r)  T (cid:88)  (cid:16)  1  t=1  E  2 (εt,0,., εt,K−1; t, r)  + γK−1Rmax,avg  (15)  (cid:17)  Using Jensen’s inequality as in the AVI scenario, we can write (15) as:  1 T  T (cid:88)  t=1  (cid:107)Q∗  t − QπK  t (cid:107)1,ρ ≤  2γ  (cid:20)  (1 − γ)2  inf  r∈[0,1]  1 2  1 2  C  PI(K; r)E  avg(εavg,0,., εavg,K−1; r)  +γK−1Rmax,avg  (cid:3) ,  (16)  with εavg,k = 1/T (cid:80)T  t=1 εt,k and Eavg(εavg,0,., εavg,K−1; r) = (cid:80)K−1  k=0 α2r  k εavg,k.
<EOS>
 A.3 APPROXIMATION BOUNDS  1,., w∗ Proof of Theorem 3.
<EOS>
Let w∗ (cid:32)  T , h∗ and f ∗  1 ,., f ∗  T be the minimizers of ε∗  avg, then:  εavg( ˆw, ˆh, ˆf ) − ε∗  avg =  εavg( ˆw, ˆh, ˆf ) −  (cid:96)( ˆft(ˆh( ˆwt(Xti))), Yti)  (cid:19)  (cid:17)  (cid:33)  (cid:125)  (cid:96)( ˆft(ˆh( ˆwt(Xti))), Yti) −  (cid:96)(f ∗  t (h∗(w∗  t (Xti))), Yti)  (cid:124) (cid:32)  (cid:124) (cid:32)  (cid:124)  +  +  1 nT  (cid:88)  ti  1 nT  (cid:88)  ti  (cid:88)  1 nT  (cid:123)(cid:122) B  ti  (cid:33)  (cid:125)  (cid:96)(f ∗  t (h∗(w∗  t (Xti))), Yti) − ε∗ avg  (cid:33)  (cid:125)  (17)  We proceed to bound the three components individually:  • C can be bounded using Hoeffding’s inequality, with probability 1 − δ/2 by (cid:112)ln(2/δ)/(2nT ),  as it contains only nT random variables bounded in the interval [0, 1];  (cid:88)  1 nT  ti (cid:123)(cid:122) A  (cid:123)(cid:122) C  14  Published as a conference paper at ICLR 2020  • B can be bounded by 0, by definition of ˆw, ˆh and ˆf , as they are the minimizers of Equa-  tion (3);  • the bounding of A is less straightforward and is described in the following.
<EOS>
 We define the following auxiliary function spaces:  • W (cid:48) = {x ∈ X → (wt(xti)) : (w1,., wT ) ∈ W T }, • F (cid:48) = (cid:8)y ∈ RKT n → (ft(yti)) : (f1,., fT ) ∈ F T (cid:9),  and the following auxiliary sets:  • S = (cid:8)((cid:96)(ft(h(wt(Xti))), Yti)) : f ∈ F T , h ∈ H, w ∈ W T (cid:9) ⊆ RT n, • S(cid:48) = F (cid:48)(H(W (cid:48)( ¯X))) = (cid:8)(ft(h(wt(Xti)))) : f ∈ F T , h ∈ H, w ∈ W T (cid:9) ⊆ RT n, • S(cid:48)(cid:48) = H(W (cid:48)( ¯X)) = (cid:8)(h(wt(Xti))) : h ∈ H, w ∈ W T (cid:9) ⊆ RKT n,  which will be useful in our proof.
<EOS>
 Using Theorem 9 by Maurer et al (2016), we can write:  εavg( ˆw, ˆh, ˆf ) −  (cid:96)( ˆft(ˆh( ˆwt(Xti))), Yti)  1 nT  (cid:88)  ti  sup  εavg(w, h, f ) −  (cid:96)(ft(h(wt(Xti))), Yti)  1 nT  (cid:88)  ti  (cid:32)  ≤  ≤  w∈W T ,h∈H,f ∈F T (cid:115) √  2πG(S)  +  nT  9 ln( 2 δ ) 2nT  ,  then by Lipschitz property of the loss function (cid:96) and the contraction lemma Corollary 11 Maurer et al 1 and c(cid:48) (2016): G(S) ≤ G(S(cid:48)).
<EOS>
By Theorem 12 by Maurer et al (2016), for universal constants c(cid:48) 2:  G(S(cid:48)) ≤ c(cid:48)  1L(F (cid:48))G(S(cid:48)(cid:48)) + c(cid:48)  2D(S(cid:48)(cid:48))O(F (cid:48)) + min y∈Y  G(F(y)),  where L(F (cid:48)) is the largest value for the Lipschitz constants in the function space F (cid:48), and D(S(cid:48)(cid:48)) is the Euclidean diameter of the set S(cid:48)(cid:48).
<EOS>
Using Theorem 12 by Maurer et al (2016) again, for universal constants c(cid:48)(cid:48)  1 and c(cid:48)(cid:48) 2 :  G(S(cid:48)(cid:48)) ≤ c(cid:48)(cid:48)  1 L(H)G(W (cid:48)( ¯X)) + c(cid:48)(cid:48)  2 D(W (cid:48)( ¯X))O(H) + min  G(H(p)).
<EOS>
 (20)  Putting (19) and (20) together:  (cid:18)  G(S(cid:48)) ≤ c(cid:48)  1L(F (cid:48))  1 L(H)G(W (cid:48)( ¯X)) + c(cid:48)(cid:48) c(cid:48)(cid:48)  2 D(W (cid:48)( ¯X))O(H) + min  G(H(p))  p∈P  p∈P  (cid:19)  + c(cid:48)  = c(cid:48)  1c(cid:48)(cid:48)  + c(cid:48)  G(F(y))  2D(S(cid:48)(cid:48))O(F (cid:48)) + min y∈Y 1 L(F (cid:48))L(H)G(W (cid:48)( ¯X)) + c(cid:48) 2D(S(cid:48)(cid:48))O(F (cid:48)) + min y∈Y  G(F(y)).
<EOS>
 1c(cid:48)(cid:48)  2 L(F (cid:48))D(W (cid:48)( ¯X))O(H) + c(cid:48)  1L(F (cid:48)) min p∈P  G(H(p))  At this point, we have to bound the individual terms in the right hand side of (21), following the same procedure proposed by Maurer et al (2016).
<EOS>
 15  (cid:33)  (18)  (19)  (21)  Published as a conference paper at ICLR 2020  Firstly, to bound L(F (cid:48)), let y, y(cid:48) ∈ RKT n, where y = (yti) with yti ∈ RK and y(cid:48) = (y(cid:48) ti ∈ RK.
<EOS>
We can write the following: y(cid:48)  ti) with  (cid:107)f (y) − f (y(cid:48))(cid:107)2 =  (ft(yti) − ft(y(cid:48)  ti))2  (cid:88)  ti  ≤ L(F)2 (cid:88)  (cid:107)yti − y(cid:48)  ti(cid:107)2  ti  = L(F)2(cid:107)y − y(cid:48)(cid:107)2,  (22)  (23)  (24)  whence L(F (cid:48)) ≤ L(F).
<EOS>
 Then, we bound:  G(W (cid:48)( ¯X)) = E  (cid:34)  sup w∈W T  (cid:12) (cid:12) (cid:12) γktiwtk(Xti) (cid:12) (cid:12)  (cid:88)  kti  (cid:35)  (cid:88)  Xti  ≤  (cid:34)  E  (cid:88)  (cid:12) (cid:12) (cid:12) γkliwk(Xli) (cid:12) (cid:12)  (cid:35)  Xli  sup  l∈{1,...,T }  sup w∈W  t = T  sup  l∈{1,...,T }  ki G(W(Xl)).
<EOS>
 Then, since it is possible to bound the Euclidean diameter using the norm of the supremum value in the set, we bound D(S(cid:48)(cid:48)) ≤ 2 suph,w(cid:107)h(w( ¯X))(cid:107) and D(W (cid:48)( ¯X)) ≤ 2 supw∈W T (cid:107)w( ¯X)(cid:107).
<EOS>
Also, we bound O(F (cid:48)):  (cid:21) (cid:104)γ, g(y) − g(y(cid:48))(cid:105)  = E  (cid:34)  (cid:20)  E  sup g∈F (cid:48)  γti (ft(yti) − ft(y(cid:48)  ti))  γi (f (yti) − f (y(cid:48)  (cid:35)  (cid:35) ti))  (cid:88)  ti  sup f ∈F T  (cid:34)  sup f ∈F  (cid:88)  i  (cid:34)  (cid:88)  E  t  sup f ∈F  (cid:88)  i  γi (f (yti) − f (y(cid:48)  ti))  (cid:88)  O(F)2 (cid:88)  (cid:107)yti − y(cid:48)  ti(cid:107)2  (cid:33) 1  2  (cid:88)  E  =  t  √    ≤  T    (cid:32)  √  ≤  T  √  t  i T O(F)(cid:107)y − y(cid:48)(cid:107),  =  1 2  (cid:35)2   whence O(F (cid:48)) ≤  T O(F).
<EOS>
 √  To minimize the last term, it is possible to choose y0 = 0, as f (0) = 0, ∀f ∈ F, resulting in miny∈Y G(F(y)) = G(F(0)) = 0.
<EOS>
Then, substituting in (21), and recalling that G(S) ≤ G(S(cid:48)):  G(S) ≤ c(cid:48)  1c(cid:48)(cid:48)  1 L(F)L(H)T  sup  G(W(Xl)) + 2c(cid:48)  l∈{1,...,T }  1c(cid:48)(cid:48) 2 L(F) sup w∈W T √  (cid:107)w( ¯X)(cid:107)O(H)  + c(cid:48)  1L(F) min p∈P  G(H(p)) + 2c(cid:48)  (cid:107)h(w( ¯X))(cid:107)  T O(F).
<EOS>
 (25)  2 sup h,w  16  Published as a conference paper at ICLR 2020  Now, the first term A of (17) can be bounded substituting (25) in (18):  εavg( ˆw, ˆh, ˆf ) − √  1 nT  (cid:88)  ti  (cid:96)( ˆft(ˆh( ˆwt(Xti))), Yti)  ≤  (cid:16)  2π nT  c(cid:48) 1c(cid:48)(cid:48)  1 L(F)L(H)T  sup  G(W(Xl)) + 2c(cid:48)  1c(cid:48)(cid:48)  l∈{1,...,T }  (cid:107)w( ¯X)(cid:107)O(H)  + c(cid:48)  1L(F) min p∈P  G(H(p)) + 2c(cid:48)  (cid:107)h(w( ¯X))(cid:107)  2 sup h,w  L(F)L(H) supl∈{1,...,T } G(W(Xl))  = c1  n  L(F) minp∈P G(H(p))  + c3  + c4  nT  2 L(F) sup w∈W T (cid:115)  (cid:17)  √  T O(F)  9 ln( 2 δ ) 2nT supw(cid:107)w( ¯X)(cid:107)L(F)O(H)  +  + c2  nT suph,w(cid:107)h(w( ¯X))(cid:107)O(F)  (cid:115)  +  9 ln( 2 δ ) 2nT  A union bound between A, B and C of (17) completes the proof:  εavg( ˆw, ˆh, ˆf ) − ε∗  avg ≤ c1  L(F)L(H) supl∈{1,...,T } G(W(Xl))  √  n  T  n  + c2  + c3  + c4  (cid:115)  +  supw(cid:107)w( ¯X)(cid:107)L(F)O(H)  L(F) minp∈P G(H(p))  suph,w(cid:107)h(w( ¯X))(cid:107)O(F)  nT  nT  √  n  T  8 ln( 3 δ ) nT  B ADDITIONAL DETAILS OF EMPIRICAL EVALUATION  B.1 MULTI FITTED Q-ITERATION  We consider Car-On-Hill problem with discount factor 0.95 and horizon 100.
<EOS>
Running Adam optimizer with learning rate 0.001 and using a mean squared loss, we train a neural network composed of 2 shared layers of 30 neurons each, with sigmoidal activation function, as described in Riedmiller (2005).
<EOS>
We select 8 tasks for the problem changing the mass of the car m and the value of the discrete actions a (Table 1).
<EOS>
Figure 1(b) is computed considering the first four tasks, while Figure 1(c) considers task 1 in the result with 1 task, tasks 1 and 2 for the result with 2 tasks, tasks 1, 2, 3, and 4 for the result with 4 tasks, and all the tasks for the result with 8 tasks.
<EOS>
To run FQI and MFQI, for each task we collect transitions running an extra-tree trained following the procedure and setting in Ernst et al (2005), using an (cid:15)-greedy policy with (cid:15) = 0.1, to obtain a small, but representative dataset.
<EOS>
The optimal Q-function for each task is computed by tree-search3 for 100 states uniformly picked from the state space, and the 2 discrete actions, for a total of 200 state-action tuples.
<EOS>
 B.2 MULTI DEEP Q-NETWORK  The five problems we consider for this experiment are: Cart-Pole, Acrobot, Mountain-Car, Car-On- Hill, and Inverted-Pendulum4.
<EOS>
The discount factors are respectively 0.99, 0.99, 0.99, 0.95, and 0.95.
<EOS>
The horizons are respectively 500, 1, 000, 1, 000, 100, and 3, 000.
<EOS>
The network we use consists of 80 ReLu units for each wt, t ∈ {1,., T } block, with T = 5.
<EOS>
Then, the shared block h consists of one  3We follow the method described in Ernst et al (2005).
<EOS>
4The IDs of the problems in the OpenAI Gym library are: CartPole-v0, Acrobot-v1, and MountainCar-v0.
<EOS>
 17  Published as a conference paper at ICLR 2020  Task Mass 1.0 0.8 1.0 1.2 1.0 1.0 0.8 0.85  1 2 3 4 5 6 7 8  Action set {−4.0; 4.0} {−4.0; 4.0} {−4.5; 4.5} {−4.5; 4.5}  {−4.125; 4.125}  {−4.25; 4.25}  {−4.375; 4.375}  {−4.0; 4.0}  Table 1: Different values of the mass of the car and available actions chosen for the Car-On-Hill tasks in the MFQI empirical evaluation.
<EOS>
 i ) = yt(s, a(t)  i ) = ft(h(wt(s)), a(t)  layer with 80 ReLu units and another one with 80 sigmoid units.
<EOS>
Eventually, each ft has a number of linear units equal to the number of discrete actions a(t) , i ∈ {1,., #A(t)} of task µt which outputs i the action-value Qt(s, a(t) i ), ∀s ∈ S (t).
<EOS>
The use of sigmoid units in the second layer of h is due to our choice to extract meaningful shared features bounded between 0 and 1 to be used as input of the last linear layer, as in most RL approaches.
<EOS>
In practice, we have also found that sigmoid units help to reduce task interference in multi-task networks, where instead the linear response of ReLu units cause a problematic increase in the feature values.
<EOS>
Furthermore, the use of a bounded feature space reduces the suph,w(cid:107)h(w( ¯X))(cid:107) term in the upper bound of Theorem 3, corresponding to the upper bound of the diameter of the feature space, as shown in Appendix A. The initial replay memory size for each task is 100 and the maximum size is 5, 000.
<EOS>
We use Huber loss with Adam optimizer using learning rate 10−3 and batch size of 100 samples for each task.
<EOS>
The target network is updated every 100 steps.
<EOS>
The exploration is ε-greedy with ε linearly decaying from 1 to 0.01 in the first 5, 000 steps.
<EOS>
 B.3 MULTI DEEP DETERMINISTIC POLICY GRADIENT  The two set of problems we consider for this experiment are: one including Inverted-Pendulum, Inverted-Double-Pendulum, and Inverted-Pendulum-Swingup, and another one including Hopper- Stand, Walker-Walk, and Half-Cheetah-Run5.
<EOS>
The discount factors are 0.99 and the horizons are 1, 000 for all problems.
<EOS>
The actor network is composed of 600 ReLu units for each wt, t ∈ {1,., T } block, with T = 3.
<EOS>
The shared block h has 500 units with ReLu activation function as for MDQN.
<EOS>
Finally, each ft has a number of tanh units equal to the number of dimensions of the continuous actions a(t) ∈ A(t) of task µt which outputs the policy πt(s) = yt(s) = ft(h(wt(s))), ∀s ∈ S (t).
<EOS>
On the other hand, the critic network consists of the same wt units of the actor, except for the use of sigmoidal units in the h layer, as in MDQN.
<EOS>
In addition to this, the actions a(t) are given as input to h. Finally, each ft has a single linear unit Qt(s, a(t)) = yt(s, a(t)) = ft(h(wt(s), a(t))), ∀s ∈ S (t).
<EOS>
The initial replay memory size for each task is 64 and the maximum size is 50, 000.
<EOS>
We use Huber loss to update the critic network and the policy gradient to update the actor network.
<EOS>
In both cases the optimization is performed with Adam optimizer and batch size of 64 samples for each task.
<EOS>
The learning rate of the actor is 10−4 and the learning rate of the critic is 10−3.
<EOS>
Moreover, we apply (cid:96)2-penalization to the critic network using a regularization coefficient of 0.01.
<EOS>
The target networks are updated with soft-updates using τ = 10−3.
<EOS>
The exploration is performed using the action computed by the actor network adding a noise generated with an Ornstein-Uhlenbeck process with θ = 0.15 and σ = 0.2. Note that most of these values are taken from the original DDPG paper Lillicrap et al (2015), which optimizes them for the single-task scenario.
<EOS>
 5The  IDs of  the problems  InvertedPendulumBulletEnv-v0, InvertedDoublePendulumBulletEnv-v0, and InvertedPendulumSwingupBulletEnv-v0.
<EOS>
The names of the domain and the task of the problems in the DeepMind Control Suite are: hopper-stand, walker-walk, and cheetah-run.
<EOS>
 in the pybullet  library are:  18  
<EOS>
