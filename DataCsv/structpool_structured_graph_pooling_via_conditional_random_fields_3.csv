b'Published as a conference paper at ICLR 2020\n\nSTRUCTPOOL: STRUCTURED GRAPHPOOLING VIACONDITIONAL'
<EOS>
b'RANDOM FIELDSHao Yuan\nDepartment of Computer Science & EngineeringTexas A&M University'
<EOS>
b'College Station, TX 77843, USA\nhao.yuan@tamu.eduShuiwang JiDepartment of Computer Science & Engineering'
<EOS>
b'Texas A&M UniversityCollege Station, TX 77843, USA\nsji@tamu.edu\n\nABSTRACT'
<EOS>
b'Learning high-level representations for graphs is of great importance for graph\nanalysis tasks.In addition to graph convolution, graph pooling is an important\nbut less explored research area.In particular, most of existing graph pooling\ntechniques do not consider the graph structural information explicitly.'
<EOS>
b'We argue\nthat such information is important and develop a novel graph pooling technique,know as the STRUCTPOOL, in this work.We consider the graph pooling as a\nnode clustering problem, which requires the learning of a cluster assignment ma-\ntrix.'
<EOS>
b'We propose to formulate it as a structured prediction problem and employ\nconditional random \xef\xac\x81elds to capture the relationships among the assignments of\ndifferent nodes.We also generalize our method to incorporate graph topologi-cal information in designing the Gibbs energy function.'
<EOS>
b'Experimental results on\nmultiple datasets demonstrate the effectiveness of our proposed STRUCTPOOL.1INTRODUCTION'
<EOS>
b'Graph neural networks have achieved the state-of-the-art results for multiple graph tasks, such as\nnode classi\xef\xac\x81cation (Veli\xcb\x87ckovi\xc2\xb4c et al., 2018; Gao & Ji, 2019b; Gao et al., 2018) and link predic-\ntion (Zhang & Chen, 2018; Cai & Ji, 2020).These results demonstrate the effectiveness of graph\nneural networks to learn node representations.However, graph classi\xef\xac\x81cation tasks also require learn-'
<EOS>
b'ing good graph-level representations.Since pooling operations are shown to be effective in many\nimage and NLP tasks, it is natural to investigate pooling techniques for graph data (Yu & Koltun,\n2016; Springenberg et al., 2014).Recent work extends the global sum/average pooling operations\nto graph models by simply summing or averaging all node features (Atwood & Towsley, 2016;\nSimonovsky & Komodakis, 2017).'
<EOS>
b'However, these trivial global pooling operations may lose im-portant features and ignore structural information.Furthermore, global pooling are not hierarchical\nso that we cannot apply them where multiple pooling operations are required, such as Graph U-'
<EOS>
b'Net (Gao & Ji, 2019a).Several advanced graph pooling methods, such as SORTPOOL (Zhang\net al., 2018), TOPKPOOL (Gao & Ji, 2019a), DIFFPOOL(Ying et al., 2018), and SAGPOOL'
<EOS>
b'(Leeet al., 2019) , are recently proposed and achieve promising performance on graph classi\xef\xac\x81cation tasks.However, none of them explicitly models the relationships among different nodes and thus may ig-\nnore important structural information.'
<EOS>
b'We argue that such information is important and should be\nexplicitly captured in graph pooling.In this work, we propose a novel graph pooling technique, known as the STRUCTPOOL, that formu-\nlates graph pooling as a structured prediction problem.Following DIFFPOOL'
<EOS>
b'(Ying et al., 2018),\nwe consider graph pooling as a node clustering problem, and each cluster corresponds to a node\nin the new graph after pooling.Intuitively, two nodes with similar features should have a higher\nprobability of being assigned to the same cluster.'
<EOS>
b'Hence, the assignment of a given node should\ndepend on both the input node features and the assignments of other nodes.We formulate this as a\nstructured prediction problem and employ conditional random \xef\xac\x81elds (CRFs)(Lafferty et al., 2001)'
<EOS>
b'to capture such high-order structural relationships among the assignments of different nodes.In\naddition, we generalize our method by incorporating the graph topological information so that our\nmethod can control the clique set in our CRFs.We employ the mean \xef\xac\x81eld approximation to compute\nthe assignments and describe how to incorporate it in graph networks.'
<EOS>
b'Then the networks can be\n\n1Published as a conference paper at ICLR 2020trained in an end-to-end fashion.'
<EOS>
b'Experiments show that our proposed STRUCTPOOL outperforms\nexisting methods signi\xef\xac\x81cantly and consistently.We also show that STRUCTPOOL incurs acceptable\ncomputational cost given its superior performance.2 BACKGROUND AND RELATED WORK'
<EOS>
b'2.1 GRAPH CONVOLUTIONAL NETWORKSA graph can be represented by its adjacency matrix and node features.Formally, for a graph\nG consisting of n nodes, its topology information can be represented by an adjacency matrix'
<EOS>
b'A \xe2\x88\x88 {0, 1}n\xc3\x97n, and the node features can be represented as X \xe2\x88\x88 Rn\xc3\x97c assuming each nodehas a c-dimensional feature vector.Deep graph neural networks (GNNs) learn feature representa-'
<EOS>
b'tions for different nodes using these matrices (Gilmer et al., 2017).Several approaches are pro-\nposed to investigate deep GNNs, and they generally follow a neighborhood information aggregation\nscheme (Gilmer et al., 2017; Xu et al., 2019; Hamilton et al., 2017; Kipf & Welling, 2017; Veli\xcb\x87ckovi\xc2\xb4c\net al., 2018).In each step, the representation of a node is updated by aggregating the representations\nof its neighbors.'
<EOS>
b'Graph Convolutional Networks (GCNs) are popular variants of GNNs and inspired\nby the \xef\xac\x81rst order graph Laplacian methods (Kipf & Welling, 2017).The graph convolution operation\nis formally de\xef\xac\x81ned as:\n\nXi+1 = f (D\xe2\x88\x92 1\n\n2 \xcb\x86AD\xe2\x88\x92 1(1)\nwhere \xcb\x86A ='
<EOS>
b'A+ I is used to add self-loops to the adjacency matrix, D denotes the diagonal node\ndegree matrix to normalize \xcb\x86A,Xi \xe2\x88\x88 Rn\xc3\x97ci are the node features after ith graph convolution layer,'
<EOS>
b'Pi \xe2\x88\x88 Rci\xc3\x97ci+1 is a trainable matrix to perform feature transformation, and f (\xc2\xb7) denotes a non-linear\nactivation function.Then Xi \xe2\x88\x88 Rn\xc3\x97ci is transformed to Xi+1 \xe2\x88\x88Rn\xc3\x97ci+1 where the number of\nnodes remains the same.'
<EOS>
b'A similar form of GCNs proposed in (Zhang et al., 2018) can be expressed\nas:\n\n2 XiPi),(2)It differs from the GCNs in Equation (1) by performing different normalization and is a theoretically\ncloser approximation to the Weisfeiler-Lehman algorithm (Weisfeiler & Lehman, 1968).'
<EOS>
b'Hence, in\nour models, we use the latter version of GCNs in Equation (2).Xi+1= f (D\xe2\x88\x921 \xcb\x86AXiPi).'
<EOS>
b'2.2 GRAPH POOLINGSeveral advanced pooling techniques are proposed recently for graph models, such as SORTPOOL,\nTOPKPOOL, DIFFPOOL, and SAGPOOL, and achieve great performance on multiple benchmark\ndatasets.All of SORTPOOL (Zhang et al., 2018), TOPKPOOL (Gao & Ji, 2019a), and SAG-\nPOOL (Lee et al., 2019) learn to select important nodes from the original graph and use these nodes\nto build a new graph.'
<EOS>
b'They share the similar idea to learn a sorting vector based on node representa-\ntions using GCNs, which indicates the importance of different nodes.Then only the top k important\nnodes are selected to form a new graph while the other nodes are ignored.However, the ignored\nnodes may contain important features and this information is lost during pooling.'
<EOS>
b'DIFFPOOL (Ying\net al., 2018) treats the graph pooling as a node clustering problem.A cluster of nodes from the orig-\ninal graph are merged to form a new node in the new graph.DIFFPOOL proposes to perform GCNs\non node features to obtain node clustering assignment matrix.'
<EOS>
b'Intuitively, the cluster assignment\nof a given node should depend on the cluster assignments of other nodes.However, DIFFPOOL\ndoes not explicitly consider such high-order structural relationships, which we believe are important\nfor graph pooling.In this work, we propose a novel structured graph pooling technique, known as\nthe STRUCTPOOL, for effectively learning high-level graph representations.'
<EOS>
b'Different from exist-ing methods, our method explicitly captures high-order structural relationships between different\nnodes via conditional random \xef\xac\x81elds.'
<EOS>
b'In addition, our method is generalized by incorporating graph\ntopological information A to control which node pairs are included in our CRFs.2.3INTEGRATING CRFS WITH GNNS'
<EOS>
b'Recent work (Gao et al., 2019; Qu et al., 2019;Ma et al., 2019) investigates how to combine CRFs\nwith GNNs.The CGNF (Ma et al., 2019) is a GNN architecture for graph node classi\xef\xac\x81cation which\nexplicitly models a joint probability of the entire set of node labels via CRFs and performs inference'
<EOS>
b'2Published as a conference paper at ICLR 2020\n\nvia dynamic programming.In addition, the GMNN (Qu et al., 2019) focuses on semi-supervised\nobject classi\xef\xac\x81cation tasks and models the joint distribution of object labels conditioned on object'
<EOS>
b'attributes using CRFs.It proposes a pseudolikelihood variational EM framework for model learning\nand inference.Recent work (Gao et al., 2019) integrates CRFs with GNNs by proposing a CRF\nlayer to encourage similar nodes to have similar hidden features so that similarity information can\nbe preserved explicitly.'
<EOS>
b'All these methods are proposed for node classi\xef\xac\x81cation tasks and the CRFs\nare incorporated in different ways.Different from existing work, our STRUCTPOOL is proposed for\ngraph pooling operation and the energy is optimized via mean \xef\xac\x81eld approximation.All operations\nin our STRUCTPOOL can be realized by GNN operations so that our STRUCTPOOL can be easily\nused in any GNNs and trained in an end-to-end fashion.'
<EOS>
b'3STRUCTURED GRAPH POOLING3.1 GRAPH POOLING VIA NODE CLUSTERING'
<EOS>
b'Even though pooling techniques are shown to facilitate the training of deep models and improve\ntheir performance signi\xef\xac\x81cantly in many image and NLP tasks (Yu & Koltun, 2016; Springenberg\net al., 2014), local pooling operations cannot be directly applied to graph tasks.The reason is there\nis no spatial locality information among graph nodes.Global max/average pooling operations can be\nemployed for graph tasks but they may lead to information loss, due to largely reducing the size of\nrepresentations trivially.'
<EOS>
b'A graph G with n nodes can be represented by a feature matrix X \xe2\x88\x88 Rn\xc3\x97c\nand an adjacent matrix A \xe2\x88\x88 {0, 1}n\xc3\x97n.Graph pooling operations aim at reducing the number of\ngraph nodes and learning new representations.Suppose that graph pooling generates a new graph\n\xcb\x9cG with k nodes.'
<EOS>
b'The representation matrices of \xcb\x9cG are denoted as \xcb\x9cX \xe2\x88\x88Rk\xc3\x97\xcb\x9cc and \xcb\x9cA \xe2\x88\x88 {0, 1}k\xc3\x97k.The goal of graph pooling is to learn relationships between X, A and \xcb\x9cX, \xcb\x9cA.'
<EOS>
b'In this work, we\nconsider graph pooling via node clustering.In particular, the nodes of the original graph G are\nassigned to k different clusters.Then each cluster is transformed to a new node in the new graph'
<EOS>
b'\xcb\x9cG.The clustering assignments can be represented as an assignment matrix M \xe2\x88\x88 Rn\xc3\x97k.For hard\nassignments, mi,j \xe2\x88\x88 {0, 1} denotes if node i in graph G belongs to cluster j. For soft assignments,\nmi,j \xe2\x88\x88'
<EOS>
b'[0, 1] denotes the probability that node i in graph G belongs to cluster j and (cid:80)j mi,j =1.'
<EOS>
b'Then the new graph \xcb\x9cG can be computed as\n\n\xcb\x9cX =M T X, \xcb\x9cA = g(M T AM ),\n\n(3)\n\nwhere g(\xc2\xb7) is a function that g(\xcb\x9cai,j) = 1if \xcb\x9cai,'
<EOS>
b'j > 0 and g(\xcb\x9cai,j) =0otherwise.'
<EOS>
b'3.2 LEARNING CLUSTERINGASSIGNMENTS VIACONDITIONAL RANDOM FIELDS'
<EOS>
b'Intuitively, node features describe the properties of different nodes.Then nodes with similar features\nshould have a higher chance to be assigned to the same cluster.That is, for any node in the original\ngraph G, its cluster assignment should not only depend on node feature matrix X but also condition\non the cluster assignments of the other nodes.'
<EOS>
b'We believe such high-order structural information is\nuseful for graph pooling and should be explicitly captured while learning clustering assignments.Tothis end, we propose a novel structured graph pooling technique, known as STRUCTPOOL, which\ngenerates the assignment matrix by considering the feature matrix X and the relationships between\nthe assignments of different nodes.'
<EOS>
b'We propose to formulate this as a conditional random \xef\xac\x81eld(CRF) problem.The CRFs model a set of random variables with a Markov Random Field (MRF),\nconditioned on a global observation (Lafferty et al., 2001).'
<EOS>
b'We formally de\xef\xac\x81ne Y = {Y1, \xc2\xb7 \xc2\xb7 \xc2\xb7 , Yn}\nas a random \xef\xac\x81eld where Yi \xe2\x88\x88 {1, \xc2\xb7 \xc2\xb7 \xc2\xb7 , k} is a random variable.Each Yi indicates to which clusterthe node i is assigned.'
<EOS>
b'Here the feature representation X is treated as global observation.We build\na graphical model on Y , which is de\xef\xac\x81ned as G(cid:48).Then the pair (Y, X) can be de\xef\xac\x81ned as a CRF,\ncharacterized by the Gibbs distribution as\n\nP (Y |X) ='
<EOS>
b'exp\xef\xa3\xad\xe2\x88\x92\xcf\x88c(Yc|X)'
<EOS>
b'\xef\xa3\xb8 ,\n\n(4)\n\n1Z(X)(cid:88)\n\nc\xe2\x88\x88CG(cid:48)'
<EOS>
b'\xef\xa3\xb6where c denotes a clique, CG(cid:48) is a set of cliques in G(cid:48), Z(X)is the partition function, and \xcf\x88c(\xc2\xb7) is a\npotential function induced by c (Kr\xc2\xa8ahenb\xc2\xa8uhl & Koltun, 2011; Lafferty et al., 2001).'
<EOS>
b'Then the Gibbs\n\n\xef\xa3\xab3Published as a conference paper at ICLR 2020'
<EOS>
b'Figure 1:Illustrations of our proposed STRUCTPOOL.Given a graph with 6 nodes, the color of each\nnode represents its features.'
<EOS>
b'We perform graph pooling to obtain a new graph with k = 4 nodes.The unary energy matrix can be obtained by multiple GCN layers using X and A.The pairwise'
<EOS>
b'energy is measured by attention matrix using node feature X and topology information A. Then by\nperforming iterative updating,the mean \xef\xac\x81eld approximation yields the most probable assignment\nmatrix.Finally, we obtain the new graph with 4 nodes, represented by \xcb\x9cX and \xcb\x9cA.'
<EOS>
b'(5)\n\n(6)\n\nenergy function for an assignment y = {y1, \xc2\xb7 \xc2\xb7 \xc2\xb7 , yn} for all variables can be written as\n\nE(y|X) =\n\n\xcf\x88c(yc|X).(cid:88)\n\nc\xe2\x88\x88CG(cid:48)Finding the optimal assignment is equivalent to maximizing P (Y |X), which can also be interpreted\nas minimizing the Gibbs energy.'
<EOS>
b'3.3 GIBBS ENERGY WITH TOPOLOGY INFORMATIONNow we de\xef\xac\x81ne the clique set CG(cid:48) in G(cid:48).Similar to the existing CRF model (Kr\xc2\xa8ahenb\xc2\xa8uhl & Koltun,\n2011), we include all unary cliques in CG(cid:48) since we need to measure the energy for assigning\neach node.'
<EOS>
b'For pairwise cliques, we generalize our method to control the pairwise clique set by\nincorporating the graph topological information A. We consider (cid:96)-hop connectivity based on A\nto de\xef\xac\x81ne the pairwise cliques, which builds pairwise relationships between different nodes.LetA(cid:96) \xe2\x88\x88 {0, 1}n\xc3\x97n represent the (cid:96)-hop connectivity of graph G'
<EOS>
b'where a(cid:96)\ni,j = 1 indicates node i andnode j are reachable in G within (cid:96) hops.Then we include all pairwise cliques (i, j) in CG(cid:48)'
<EOS>
b'if\na(cid:96)i,j = 1.Altogether, the Gibbs energy for a cluster assignment y can be written as\n\nE(y) ='
<EOS>
b'\xcf\x88u(yi) +\xcf\x88p(yi, yj)a(cid:96)i,j,\n\n(cid:88)'
<EOS>
b'i(cid:88)\n\ni(cid:54)=jwhere \xcf\x88u(yi) represents the unary energy for node i to be assigned to cluster yi.'
<EOS>
b'In addition,\n\xcf\x88p(yi, yj) is the pairwise energy, which indicates the energy of assigningnodei, j to cluster yi, yj\nrespectively.'
<EOS>
b'Note that we drop the condition information in Equation (6) for simplicity.If (cid:96) is\nlarge enough, our CRF is equivalent to the dense CRFs.If (cid:96) is equal to 1, we have A(cid:96)'
<EOS>
b'=A sothat only 1-hop information in the adjacent matrix is considered.'
<EOS>
b'These two types of energy can be\nobtained directly by neural networks (Zheng et al., 2015).Given the global observations X and the\ntopology information A, we employ multiple graph convolution layers to obtain the unary energy\xce\xa8u \xe2\x88\x88'
<EOS>
b'Rn\xc3\x97k.Existing work on image tasks (Kr\xc2\xa8ahenb\xc2\xa8uhl & Koltun, 2011) proposes to employ Gaus-sian kernels to measure the pairwise energy.'
<EOS>
b'However, due to computational inef\xef\xac\x81ciency, we cannot\ndirectly apply it to our CRF model.The pairwise energy proposed in (Kr\xc2\xa8ahenb\xc2\xa8uhl & Koltun, 2011)\ncan be written as\n\n\xcf\x88p(yi, yj) =\xc2\xb5(yi, yj)'
<EOS>
b'w(m)k(m)(xi, xj),(7)\n\nwhere k(m)(\xc2\xb7, \xc2\xb7) represents the mth Gaussian kernel, xi is the feature vector for node'
<EOS>
b'i in X, w(m)denotes learnable weights, and \xc2\xb5(yi, yj) is a compatibility function that models the compatibilityK\n(cid:88)'
<EOS>
b'm=1\n\n41234561234Original GraphNew GraphGCNsAttention'
<EOS>
b'Iteratively UpdateUnary EnergyAssignment MatrixSoftmaxPairwise EnergyPublished as a conference paper at ICLR 2020Algorithm'
<EOS>
b'1 STRUCTPOOL1: Given a graph G with n nodes represented by X \xe2\x88\x88 Rn\xc3\x97c and A \xe2\x88\x88 {0, 1}n\xc3\x97n, the goal is to\nobtain \xcb\x9cG with k nodes'
<EOS>
b'that \xcb\x9cX \xe2\x88\x88Rk\xc3\x97\xcb\x9cc and \xcb\x9cA \xe2\x88\x88 {0, 1}k\xc3\x97k.The (cid:96)-hop connectivity matrix A(cid:96)\ncan be easily obtained from A.\n\n2:'
<EOS>
b'Perform GCNs to obtain unary energy matrix\xce\xa8u \xe2\x88\x88 Rn\xc3\x97k.3: Initialize that Q(i, j)'
<EOS>
b'=1Zi'
<EOS>
b'4: while not converged do\n\nexp (\xce\xa8u(i, j)) for all 0 \xe2\x89\xa4 i \xe2\x89\xa4 n and 0 \xe2\x89\xa4 j \xe2\x89\xa4 k.xTi xj'
<EOS>
b'm(cid:54)=ixT(cid:80)\n\n6:\n\n5:'
<EOS>
b'i xmm(cid:54)=iwi,mQ(m, j).'
<EOS>
b'Calculate attention map W that wi,j =Message passing that \xcb\x9cQ(i, j) =(cid:80)'
<EOS>
b'Compatibility transform that \xcb\x86Q(i, j) =(cid:80)Local update that \xc2\xafQ(i, j) = \xce\xa8u(i, j) \xe2\x88\x92 \xcb\x86Q(i, j).'
<EOS>
b'Perform normalizationthat Q(i, j) =1'
<EOS>
b'Zi\n\n7:\n8:\n9:\n10: endwhile\n11'
<EOS>
b': For soft assignments, the assignment matrix is M = softmax(Q).12: For hard assignments, the assignment matrix is M = argmax(Q) for each row.13'
<EOS>
b': Obtain new graph \xcb\x9cQ that \xcb\x9cX = M T X, \xcb\x9cA = g(M T AM ).exp (cid:0) \xc2\xafQ(i, j)(cid:1)for all i and j.'
<EOS>
b'm \xc2\xb5(m, j)\xcb\x9cQ(i, m).a(cid:96)\ni,j for all i (cid:54)= j and 0 \xe2\x89\xa4'
<EOS>
b'i, j \xe2\x89\xa4n.between different assignment pairs.'
<EOS>
b'However, it is computationally inef\xef\xac\x81cient to accurately com-pute the outputs of Gaussian kernels, especially for graph data when the feature vectors are high-dimensional.'
<EOS>
b'Hence, in this work, we propose to employ the attention matrix as the measurement\nof pairwise energy.Intuitively, Gaussian kernels indicate how strongly different feature vectors are\nconnected with each other.Similarly, the attention matrix re\xef\xac\x82ects similarities between different fea-'
<EOS>
b'ture vectors but with a signi\xef\xac\x81cantly less computational cost.Speci\xef\xac\x81cally, each feature vector xi is\nattended to any other feature vector xj if the pair (i, j) is existing in clique set CG(cid:48).Hence, the\npairwise energy can be obtained by\n\n\xcf\x88p(yi, yj) ='
<EOS>
b'\xc2\xb5(yi, yj)xTi xj'
<EOS>
b'k(cid:54)=ixTi xk\n\n,'
<EOS>
b'(cid:80)(8)It can be ef\xef\xac\x81ciently computed by matrix multiplication and normalization.'
<EOS>
b'Minimizing the Gibbs en-\nergy in Equation (6) results in the most probable cluster assignments fora given graph G. However,\nsuch minimization is intractable, and hence a mean \xef\xac\x81eld approximation is proposed (Kr\xc2\xa8ahenb\xc2\xa8uhl &\nKoltun, 2011), which is an iterative updating algorithm.We follow the mean-\xef\xac\x81eld approximation\nto obtain the most probable cluster assignments.'
<EOS>
b'Altogether, the steps of our proposed STRUCT-\nPOOL are shown in Algorithm 1.All operations in our proposed STRUCTPOOL can be implemented\nas GNN operations, and hence the STRUCTPOOL can be employed in any deep graph model and\ntrained in an end-to-end fashion.The unary energy matrix can be obtained by stacking several\nGCN layers, and the normalization operations (step 3&9 in Algorithm 1) are equivalent to softmax\noperations.'
<EOS>
b'All other steps can be computed by matrix computations.It is noteworthy that the com-\npatibility function \xc2\xb5(yi, yj) can be implemented as a trainable matrix N \xe2\x88\x88 Rk\xc3\x97k, and automatically\nlearned during training.Hence, no prior domain knowledge is required for designing the compatibil-\nity function.'
<EOS>
b'We illustrate our proposed STRUCTPOOL in Figure 1 where we perform STRUCTPOOL\non a graph G with 6 nodes, and obtain a new graph \xcb\x9cG with 4 nodes.3.4 COMPUTATIONAL COMPLEXITY ANALYSISWe theoretically analyze the computational ef\xef\xac\x81ciency of our proposed STRUCTPOOL.'
<EOS>
b'Since\ncomputational ef\xef\xac\x81ciency is especially important for large-scale graph datasets, we assume that\nn > k, c, \xcb\x9cc.The computational complexity of one GCN layer is O(n3 + n2c + nc\xcb\x9cc) \xe2\x89\x88 O(n3).Assuming we employ i layers of GCNs to obtain the unary energy, its computational cost is\nO(in3).'
<EOS>
b'Assuming there are m iterations in our updating algorithm, the computational com-\nplexity is O(m(n2c + n2k + nk2))\xe2\x89\x88 O(mn3).The \xef\xac\x81nal step for computing \xcb\x9cA and \xcb\x9cX takes'
<EOS>
b'O(nkc+ n2k+ nk2)'
<EOS>
b'\xe2\x89\x88O(n3) computational complexity.Altogether, the complexity STRUCT-\nPOOL is O((m + i)n3), which is close to the complexity of stacking m'
<EOS>
b'+ i layers of GCNs.5Published as a conference paper at ICLR 2020'
<EOS>
b'Table 1:Classi\xef\xac\x81cation results for six benchmark datasets.Note that none of these deep methods\ncan outperform the traditional method WL on COLLAB.'
<EOS>
b'We believe the reason is the graphs in\nCOLLAB only have single-layer structures while deep models are too complex to capture them.MethodDataset'
<EOS>
b'ENZYMES D&D COLLABPROTEINS\n\nIMDB-B IMDB-MGRAPHLET\nSHORTEST-PATH'
<EOS>
b'WL\n\nPATCHYSANDCNNDGK'
<EOS>
b'ECCGRAPHSAGESET2SET'
<EOS>
b'DGCNNDIFFPOOLSTRUCTPOOL'
<EOS>
b'41.03\n42.3253.43\n\n--\n-\n\n53.50\n54.25\n60.15'
<EOS>
b'57.1262.5363.83'
<EOS>
b'74.8578.86\n78.3476.27\n58.09'
<EOS>
b'-72.5475.42'
<EOS>
b'78.12\n79.3780.6484.19'
<EOS>
b'64.6659.1078.61'
<EOS>
b'72.6052.1173.09'
<EOS>
b'67.7968.2571.75'
<EOS>
b'73.7675.4874.22'
<EOS>
b'72.9176.4374.68'
<EOS>
b'75.0061.2971.68\n72.65'
<EOS>
b'70.4874.29\n75.54\n76.2580.36\n\n-'
<EOS>
b'-\n-\n\n-\n-\n-\n\n-\n\n71.0049.0666.96'
<EOS>
b'45.23\n33.4944.55\n\n70.0347.83'
<EOS>
b'74.7052.47\n\n--\n-\n\n-\n-\n-\n\n-\n\n3.5 DEEP GRAPH NETWORKS FOR GRAPH CLASSIFICATION'
<EOS>
b'In this section, we investigate graph classi\xef\xac\x81cation tasks which require both good node-level and\ngraph-level representations.For most state-of-the-art deep graph classi\xef\xac\x81cation models, they share\na similar pipeline that \xef\xac\x81rst produces node representations using GNNs, then performs pooling op-erations to obtain high-level representations, and \xef\xac\x81nally employs fully-connected layers to perform\nclassi\xef\xac\x81cation.'
<EOS>
b'Note that the high-level representations can be either a vector or a group of k vectors.For a set of graphs with different node numbers, with a pre-de\xef\xac\x81ned k, our proposed STRUCTPOOL\ncan produce k vectors for each graphs.Hence, our method can be easily generalized and coupled\nto any deep graph classi\xef\xac\x81cation model.'
<EOS>
b'Specially, our model for graph classi\xef\xac\x81cation is developed\nbased on DGCNN (Zhang et al., 2018).Given any input graph, our model \xef\xac\x81rst employs several\nlayers of GCNs (Equation (2)) to aggregate features from neighbors and learn representations for\nnodes.Next, we perform one STRUCTPOOL layer to obtain k vectors for each graph.'
<EOS>
b'Finally, 1D\nconvolutional layers and fully-connected layers are used to classify the graph.4 EXPERIMENTAL STUDIES\n\n4.1 DATASETS AND EXPERIMENTAL SETTINGSWe evaluate our proposed STRUCTPOOL on eight benchmark datasets, including \xef\xac\x81ve bioinformatics'
<EOS>
b'protein datasets: ENZYMES, PTC, MUTAG,PROTEINS (Borgwardt et al., 2005), D&D (Dobson\n& Doig, 2003), and three social network datasets: COLLAB (Yanardag & Vishwanathan, 2015b),\nIMDB-B, IMDB-M (Yanardag & Vishwanathan, 2015a).'
<EOS>
b'Most of them are relatively large-scale and\nhence suitable for evaluating deep graph models.We report the statistics and properties of them in\nSupplementary Table 6.Please see the Supplementary Section A for experimental settings.'
<EOS>
b'We compare our method with several state-of-the-art deep GNN methods.PATCHYSAN (Niepert\net al., 2016) learns node representations and a canonical node ordering to perform classi\xef\xac\x81cation.DCNN (Atwood & Towsley, 2016) learns multi-scale substructure features by diffusion graph con-\nvolutions and performs global sum pooling.'
<EOS>
b'DGK (Yanardag & Vishwanathan, 2015a) models latentrepresentations for sub-structures in graphs, which is similar to learn word embeddings.ECC (Si-\nmonovsky & Komodakis, 2017) performs GCNs conditioning on both node features and edge in-\nformation and uses global sum pooling before the \xef\xac\x81nal classi\xef\xac\x81er.'
<EOS>
b'GRAPHSAGE (Hamilton et al.,\n2017) is an inductive framework which generates node embeddings by sampling and aggregating\nfeatures from local neighbors, and it employs global mean pooling.SET2SET(Vinyals et al., 2015)\nproposes an aggregation method to replace the global pooling operations in deep graph networks.'
<EOS>
b'DGCNN (Zhang et al., 2018) proposes a pooling strategy named SORTPOOL which sorts all nodes6Published as a conference paper at ICLR 2020'
<EOS>
b'Table 2: Comparisons between different pooling techniques under the same framework.MethodDataset'
<EOS>
b'ENZYMES D&D COLLABPROTEINS\n\nIMDB-B IMDB-MSUM POOL'
<EOS>
b'SORTPOOLTOPK POOLDIFFPOOL'
<EOS>
b'SAGPOOLSTRUCTPOOL\n\n47.33\n52.83\n53.6760.33\n64.17'
<EOS>
b'63.8378.72\n80.6081.71\n80.94'
<EOS>
b'81.0384.1969.45'
<EOS>
b'73.9273.34\n71.78\n73.2874.22'
<EOS>
b'76.2676.83\n77.47\n77.7478.82'
<EOS>
b'80.3651.69\n70.00\n72.80\n72.4073.40'
<EOS>
b'74.7042.76\n46.2649.00'
<EOS>
b'50.1351.1352.47'
<EOS>
b'by learning and selects the \xef\xac\x81rst k nodes to form a new graph.DIFFPOOL (Ying et al., 2018)'
<EOS>
b'is\nbuilt based on GRAPHSAGE architecture but with their proposed differentiable pooling.Note that\nfor most of these methods, pooling operations are employed to obtain graph-level representations\nbefore the \xef\xac\x81nal classi\xef\xac\x81er.In addition, we compare our STRUCTPOOL with three graph kernels:\nGraphlet (Shervashidze et al., 2009), Shortest-path (Borgwardt & Kriegel, 2005), and Weisfeiler-'
<EOS>
b'Lehman subtree kernel (WL) (Weisfeiler & Lehman, 1968).4.2 CLASSIFICATION RESULTSWe evaluate our proposed method on six benchmark datasets and compare with several state-of-the-\nart approaches.'
<EOS>
b'The results are reported in Table 1 where the best results are shown in bold and the\nsecond best results are shown with underlines.For our STRUCTPOOL, we perform 10-fold cross\nvalidations and report the average accuracy for each dataset.The 10-fold splitting is the same as\nDGCNN.'
<EOS>
b'For all comparing methods, the results are taken from existing work (Ying et al., 2018;\nZhang et al., 2018).We can observe that our STRUCTPOOL obtains the best performance on 5 out of\n6 benchmark datasets.'
<EOS>
b'For these 5 datasets, the classi\xef\xac\x81cation results of our method are signi\xef\xac\x81cantly\nbetter than all comparing methods, including advanced models DGCNN and DIFFPOOL.Notably,\nour model outperforms the second-best performance by an average of 3.58% on these 5 datasets.In addition, the graph kernel method WL obtains the best performance on COLLAB dataset and'
<EOS>
b'none of these deep models can achieve similar performance.Our model can obtain competitive\nperformance compared with the second best model.This is because many graphs in COLLAB only\nhave simple structures and deep models may be too complex to capture them.'
<EOS>
b'4.3 COMPARISONS OF DIFFERENT POOLING METHODSTo demonstrate the effectiveness of our proposed pooling technique, we compare different poolingtechniques under the same network framework.'
<EOS>
b'Speci\xef\xac\x81cally, we compare our STRUCTPOOL with\nthe global sum pool, SORTPOOL, TOPKPOOL, DIFFPOOL, and SAGPOOL.All pooling methods\nare employed in the network framework introduced in Section 3.5.In addition, the same 10-fold\ncross validations from DGCNN are used for all pooling methods.'
<EOS>
b'We report the results in Table 2\nand the best results are shown in bold.Obviously, our method achieves the best performance on \xef\xac\x81ve\nof six datasets, and signi\xef\xac\x81cantly outperforms all comparing pooling techniques.For the dataset EN-\nZYMES, our obtained result is competitive since SAGPOOL only slightly outperforms our proposed\nmethod by 0.34%.'
<EOS>
b'Such observations demonstrate the structural information in graphs is useful for\ngraph pooling and the relationships between different nodes should be explicitly modeled.4.4 STUDY OF COMPUTATIONAL COMPLEXITYAs mentioned in Section 3.4, our pro-\nposed STRUCTPOOL yields O((m'
<EOS>
b'+i)n3) computational complexity.The\ncomplexity of DIFFPOOL is O(jn3)'
<EOS>
b'if\nwe assume it employs j layers of GCNs to\nobtain the assignment matrix.In our ex-periments, i is usually set to 2 or 3 which\n\nTable 3:'
<EOS>
b'The prediction accuracy with different iterationnumber m.Dataset\n\nm'
<EOS>
b'=1 m =3 m'
<EOS>
b'= 5 m = 10ENZYMESD&D'
<EOS>
b'PROTEINS\n\n62.6782.8280.09'
<EOS>
b'63.0083.08\n80.0063.83'
<EOS>
b'83.5980.1863.50\n84.19\n80.18'
<EOS>
b'7Published as a conference paper at ICLR 2020is much smaller than n.'
<EOS>
b'We conduct experiments to show how different iteration number m affectsthe prediction accuracy and the results are reported in Table 3.Note that we employ the dense CRF\nform for all different m.'
<EOS>
b'We can observe that the performance generally increases with m increasing,\nespecially for large-scale datasetD&D. We also observe m = 5is a good trade-off between time\ncomplexity and prediction performance.'
<EOS>
b'Notably, our method can even outperform other approaches\nwhen m = 1.Furthermore, we evaluate the running time of our STRUCTPOOL and compare it with\nDIFFPOOL.For 500 graphs from large-scale dataset D&D, we set i'
<EOS>
b'= j = 3 and show the aver-\naging time cost to perform pooling for each graph.The time cost for DIFFPOOL is 0.042 second,\nwhile our STRUCTPOOL takes 0.049 second, 0.053 second and 0.058second for m = 1'
<EOS>
b', m = 3,m = 5 respectively.Even though our STRUCTPOOL has a relatively higher computational cost, it is\nstill reasonable and acceptable given its superior performance.'
<EOS>
b'4.5 EFFECTS OF TOPOLOGY INFORMATIONin\n\n(cid:96)= 5'
<EOS>
b'(cid:96) = 1(cid:96) =10'
<EOS>
b'DatasetTable 4:The prediction accuracy using different A(cid:96)\nSTRUCTPOOL.'
<EOS>
b'Next, we conduct experiments\nto show how the topology in-formation A(cid:96) affects the predic-\ntion performance.We evaluate'
<EOS>
b'our STRUCTPOOL with different (cid:96)\nvalues and report the results in Ta-\nble 4.Note that when (cid:96) is large\nenough, our STRUCTPOOL considers all pairwise relationships between all nodes, and it is equiva-\nlent to the dense CRF.For the datasets IMDB-M and PROTEINS, we can observe that the prediction\naccuracies are generally increasing with the increasing of (cid:96).'
<EOS>
b'With the increasing of (cid:96), more pairwise\nrelationships are considered by the model, and hence it is reasonable to obtain better performance.In addition, for the dataset IMDB-B, the results remain similar with different (cid:96), and even (cid:96) = 1yields competitive performance with dense CRF.'
<EOS>
b'It is possible that 1-hop pairwise relationships are\nenough to learn good embeddings for such graph types.Overall, dense CRF consistently produces\npromising results and is a proper choice in practice.IMDB-B\nIMDB-M\nPROTEINS\n\n74.70'
<EOS>
b'52.4780.1874.30'
<EOS>
b'52.0079.8374.40'
<EOS>
b'51.6779.6174.60'
<EOS>
b'51.5379.7374.70\n51.96\n80.36'
<EOS>
b'(cid:96) =15 DENSE\n\n4.6 GRAPH ISOMORPHISM NETWORKS WITH STRUCTPOOL\n\nPTCDataset\n\n64.60'
<EOS>
b'73.4675.1078.50'
<EOS>
b'GINSOURSIMDB-B MUTAG COLLAB'
<EOS>
b'Table 5: Comparisons with Graph Isomorphism Networks.Isomor-Recently, Graph\nphism Networks\n(GINs)\nare proposed and shown\nto be more powerful\nthan\ntraditional GNNs'
<EOS>
b'(Xu et al.,\n2019).To demonstrate the\neffectiveness of our STRUCTPOOL and show its generalizability, we build models based on GINs\nand evaluate their performance.Speci\xef\xac\x81cally, we employ GINs to learn node representations and\nperform one layer of the dense form of our STRUCTPOOL, followed by 1D convolutional layers\nand fully-connected layers as the classi\xef\xac\x81er.'
<EOS>
b'The results are reported in the Table 5, where we\nemploy the same 10-fold splitting as GINs (Xu et al., 2019) andthe GIN results are taken from\nits released results.These \xef\xac\x81ve datasets include both bioinformatic data and social media data, and\nboth small-scale data and large-scale data.'
<EOS>
b'Obviously, incorporating our proposed STRUCTPOOL in\nGINs consistently and signi\xef\xac\x81cantly improves the prediction performance.It leads to an average of\n4.52% prediction accuracy improvement, which is promising.89.40'
<EOS>
b'93.5980.2084.06'
<EOS>
b'52.3054.60IMDB-M\n\n5 CONCLUSIONS'
<EOS>
b'Graph pooling is an appealing way to learn good graph-level representations, and several advanedpooling techiques are proposed.However, none of existing graph pooling techniques explicitly\nconsiders the relationship between different nodes.'
<EOS>
b'We propose a novel graph pooling technique,\nknown as STRUCTPOOL, which is developed based on the conditional random \xef\xac\x81elds.We consider\nthe graph pooling as a node clustering problem and employ the CRF to build relationships between\nthe assignments of different nodes.In addition, we generalize our method by incorporating the graph\ntopological information so that our method can control the pairwise clique set in our CRFs.'
<EOS>
b'Finally,\n\n8Published as a conference paper at ICLR 2020we evaluate our proposed STRUCTPOOL on several benchmark datasets and our method can achieve\nnew state-of-the-art results on \xef\xac\x81ve out of six datasets.'
<EOS>
b'This work was supported in part by National Science Foundation grants DBI-1661289 and IIS-1908198.ACKNOWLEDGEMENT'
<EOS>
b'REFERENCESJames Atwood and Don Towsley.Diffusion-convolutional neural networks.'
<EOS>
b'In Advances in Neural\n\nInformation Processing Systems, pp. 1993\xe2\x80\x932001, 2016.Karsten M Borgwardt and Hans-Peter Kriegel.Shortest-path kernels on graphs.'
<EOS>
b'In Fifth IEEEinternational conference on data mining (ICDM\xe2\x80\x9905), pp.8\xe2\x80\x93pp.'
<EOS>
b'IEEE, 2005.Karsten M Borgwardt, Cheng Soon Ong, Stefan Sch\xc2\xa8onauer, SVN Vishwanathan, Alex J Smola, and\nHans-Peter Kriegel.Protein function prediction via graph kernels.'
<EOS>
b'Bioinformatics, 21(suppl 1):\ni47\xe2\x80\x93i56, 2005.Lei Cai and Shuiwang Ji.A multi-scale approach for graph link prediction.'
<EOS>
b'In Thirty-Fourth AAAI\n\nConference on Arti\xef\xac\x81cial Intelligence, 2020.Paul D Dobson and Andrew J Doig.Distinguishing enzyme structures from non-enzymes without\n\nalignments.'
<EOS>
b'Journal of molecular biology, 330(4):771\xe2\x80\x93783, 2003.Hongchang Gao, Jian Pei, and Heng Huang.Conditional random \xef\xac\x81eld enhanced graph convolu-'
<EOS>
b'tional neural networks.In Proceedings of the 25th ACM SIGKDD International Conference on\nKnowledge Discovery & Data Mining, pp. 276\xe2\x80\x93284.ACM, 2019.'
<EOS>
b'Hongyang Gao and Shuiwang Ji.Graph u-nets.In International Conference on Machine Learning,\n\npp. 2083\xe2\x80\x932092, 2019a.'
<EOS>
b'Hongyang Gao and Shuiwang Ji.Graph representation learning via hard and channel-wise attentionIn Proceedings of the 25th ACM SIGKDD International Conference on Knowledge\n\nnetworks.'
<EOS>
b'Discovery & Data Mining, pp.741\xe2\x80\x93749, 2019b.Hongyang Gao, Zhengyang Wang, and Shuiwang Ji.'
<EOS>
b'Large-scale learnable graph convolutionalIn Proceedings of the 24th ACM SIGKDD International Conference on Knowledge\n\nnetworks.Discovery & Data Mining, pp. 1416\xe2\x80\x931424, 2018.'
<EOS>
b'Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl.Neural\nmessage passing for quantum chemistry.In Proceedings of the 34th International Conference on\nMachine Learning-Volume 70, pp. 1263\xe2\x80\x931272.'
<EOS>
b'JMLR.org, 2017.Will Hamilton, Zhitao Ying, and Jure Leskovec.'
<EOS>
b'Inductive representation learning on large graphs.In Advances in Neural Information Processing Systems, pp. 1024\xe2\x80\x931034, 2017.Diederik P Kingma and Jimmy Ba.'
<EOS>
b'Adam:A method for stochastic optimization.In Proceedings of\n\nthe 3rd International Conference on Learning Representations, 2014.'
<EOS>
b'Thomas N Kipf and Max Welling.Semi-supervised classi\xef\xac\x81cation with graph convolutional net-\n\nworks.In Proceedings of the International Conference on Learning Representations, 2017.'
<EOS>
b'Philipp Kr\xc2\xa8ahenb\xc2\xa8uhl and Vladlen Koltun.Ef\xef\xac\x81cient inference in fully connected crfs with gaussianedge potentials.'
<EOS>
b'In Advances in neural information processing systems, pp. 109\xe2\x80\x93117, 2011.John Lafferty, Andrew McCallum, and Fernando CN Pereira.Conditional random \xef\xac\x81elds: Probabilis-'
<EOS>
b'tic models for segmenting and labeling sequence data.In International conference on machine\nlearning, pp. 282\xe2\x80\x93289, 2001.Junhyun Lee, Inyeop Lee, and Jaewoo Kang.'
<EOS>
b'Self-attention graph pooling.In International Confer-\n\nence on Machine Learning, pp. 3734\xe2\x80\x933743, 2019.9'
<EOS>
b'Published as a conference paper at ICLR 2020Tengfei Ma, Cao Xiao, Junyuan Shang, and Jimeng Sun.CGNF:'
<EOS>
b'Conditional graph neural \xef\xac\x81elds,\n\n2019.URL https://openreview.net/forum?id=ryxMX2R9YQ.Mathias Niepert, Mohamed Ahmed, and Konstantin Kutzkov.'
<EOS>
b'Learning convolutional neural net-\n\nworks for graphs.In International conference on machine learning, pp. 2014\xe2\x80\x932023, 2016.Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito,'
<EOS>
b'Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer.Automatic differentiation in\npytorch.In Proceedings of the International Conference on Learning Representations, 2017.'
<EOS>
b'Meng Qu, Yoshua Bengio, and Jian Tang.GMNN:Graph Markov neural networks.'
<EOS>
b'In Kamalika\nChaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International Conference on\nMachine Learning, volume 97 of Proceedings of Machine Learning Research, pp.5241\xe2\x80\x935250,\nLong Beach, California, USA, 09\xe2\x80\x9315 Jun 2019.PMLR.'
<EOS>
b'Nino Shervashidze, SVN Vishwanathan, Tobias Petri, Kurt Mehlhorn, and Karsten Borgwardt.Ef-\n\xef\xac\x81cient graphlet kernels for large graph comparison.In Arti\xef\xac\x81cial Intelligence and Statistics, pp.\n488\xe2\x80\x93495, 2009.'
<EOS>
b'Martin Simonovsky and Nikos Komodakis.Dynamic edge-conditioned \xef\xac\x81lters in convolutional neu-\nral networks on graphs.In Proceedings of the IEEE conference on computer vision and pattern\nrecognition, pp. 3693\xe2\x80\x933702, 2017.'
<EOS>
b'Jost Tobias Springenberg, Alexey Dosovitskiy, Thomas Brox, and Martin Riedmiller.Striving for\nsimplicity:The all convolutional net.'
<EOS>
b'In Proceedings of the International Conference on Learning\nRepresentations, 2014.Petar Veli\xcb\x87ckovi\xc2\xb4c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li`o, and Yoshua\nBengio.Graph attention networks.'
<EOS>
b'In International Conference on Learning Representations,\n2018.URL https://openreview.net/forum?id=rJXMpikCZ.Oriol Vinyals, Samy Bengio, and Manjunath Kudlur.'
<EOS>
b'Order matters:Sequence to sequence for sets.In International Conference on Learning Representations, 2015.'
<EOS>
b'Boris Weisfeiler and Andrei A Lehman.A reduction of a graph to a canonical form and an algebra\n\narising during this reduction.Nauchno-Technicheskaya Informatsia, 2(9):12\xe2\x80\x9316, 1968.'
<EOS>
b'Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka.How powerful are graph neuralIn International Conference on Learning Representations, 2019.'
<EOS>
b'URL https:\n\nnetworks?//openreview.net/forum?id=ryGs6iA5Km.'
<EOS>
b'Pinar Yanardag and SVN Vishwanathan.Deep graph kernels.In Proceedings of the 21th ACM\nSIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 1365\xe2\x80\x931374.'
<EOS>
b'ACM, 2015a.Pinar Yanardag and SVN Vishwanathan.A structural smoothing framework for robust graph com-\n\nparison.'
<EOS>
b'In Advances in neural information processing systems, pp. 2134\xe2\x80\x932142, 2015b.Zhitao Ying, Jiaxuan You, Christopher Morris, Xiang Ren, Will Hamilton, and Jure Leskovec.Hi-'
<EOS>
b'erarchical graph representation learning with differentiable pooling.In Advances in Neural Infor-mation Processing Systems, pp. 4800\xe2\x80\x934810, 2018.'
<EOS>
b'Fisher Yu and Vladlen Koltun.Multi-scale context aggregation by dilated convolutions.In Proceed-\n\nings of the International Conference on Learning Representations, 2016.'
<EOS>
b'Muhan Zhang and Yixin Chen.Link prediction based on graph neural networks.In Advances in\n\nNeural Information Processing Systems, pp.'
<EOS>
b'5165\xe2\x80\x935175, 2018.Muhan Zhang, Zhicheng Cui, Marion Neumann, and Yixin Chen.An end-to-end deep learning\n\narchitecture for graph classi\xef\xac\x81cation.'
<EOS>
b'In AAAI, pp.4438\xe2\x80\x934445, 2018.Shuai Zheng, Sadeep Jayasumana, Bernardino Romera-Paredes, Vibhav Vineet, Zhizhong Su, Da-'
<EOS>
b'long Du, Chang Huang, and Philip HS Torr.Conditional random \xef\xac\x81elds as recurrent neural net-\nworks.In Proceedings of the IEEE international conference on computer vision, pp. 1529\xe2\x80\x931537,\n2015.'
<EOS>
b'10Published as a conference paper at ICLR 2020A APPENDIX'
<EOS>
b'A.1 DATASETS AND EXPERIMENTAL SETTINGSTable 6: Statistics and properties of eight benchmark datasets.ENZYMES'
<EOS>
b'D&DCOLLABPROTEINS\n\n# of Edges (avg)'
<EOS>
b'# of Nodes (avg)\n# of Graphs\n# of Classes\n\n124.2032.63600'
<EOS>
b'6Dataset1431.3'
<EOS>
b'284.3211782'
<EOS>
b'2457.7874.495000'
<EOS>
b'3DatasetIMDB-B'
<EOS>
b'IMDB-M\n\n# of Edges (avg)# of Nodes (avg)\n# of Graphs\n# of Classes\n\n96.5319.77\n1000'
<EOS>
b'265.9413.00'
<EOS>
b'15003PTC\n\n14.69'
<EOS>
b'14.303442'
<EOS>
b'72.8239.061113'
<EOS>
b'2MUTAG19.79\n17.93'
<EOS>
b'1882We report the statistics and properties of eight benchmark datasets in Supplementary Table 6.'
<EOS>
b'For\nour STRUCTPOOL, we implement our models using Pytorch (Paszke et al., 2017) and conduct exper-\niments on one GeForce GTX 1080 Ti GPU.The model is trained using Stochastic gradient descent(SGD) with the ADAM optimizer (Kingma & Ba, 2014).'
<EOS>
b'For the models built on DGCNN (Zhang\net al., 2018) in Section 4.2, 4.3, 4.4, 4.5, we employ GCNs to obtain the node features and the unary\nenergy matrix.All experiments in these sections perform 10-fold cross validations and we report the\naveraging results.The 10-fold splitting is exactly the same as DGCNN (Zhang et al., 2018).'
<EOS>
b'For the\nnon-linear function, we employ tanh for GCNs and relu for 1D convolution layers.For the models\nbuilt on GINs in Section 4.6, we employ GINs to learn node features and unary energy.Here the 10-\nfold splitting is exactly the same as GINs.'
<EOS>
b'We employ relu for all layers as the non-linear function.For all models, 1D convolutional layers and fully-connected layers are used after our STRUCTPOOL.Hard clustering assignments are employed in all experiments.'
<EOS>
b'A.2 EFFECTS OF PAIRWISE ENERGYTable 7:Comparison with the baseline which excludes pairwise energy.'
<EOS>
b'DatasetENZYMES D&D COLLABPROTEINS\n\nIMDB-B IMDB-M'
<EOS>
b'BASELINEOURS\n\n60.8363.83'
<EOS>
b'81.3084.1970.58'
<EOS>
b'74.22\n\n78.1880.36\n\n72.4074.70'
<EOS>
b'50.1352.47We conduct experiments to show the importance of the pairwise energy.'
<EOS>
b'If the pairwise energy is\nremoved, the relations between different node assignments are not explicitly considered.Then the\nmethod is similar to the DIFFPOOL.We compare our method with such a baseline that removes the\npairwise energy.'
<EOS>
b'Experimental results are reported in Table 7.The network framework is the same\nas introduced in Section 3.5 and the same 10-fold cross validations from DGCNN are used.Obvi-'
<EOS>
b'ously, our proposed method consistently and signi\xef\xac\x81cantly outperforms the baseline which excludespairwise energy.It indicates the importance and effectiveness of incorporating pairwise energy and'
<EOS>
b'considering high-order relationships between different node assignments.A.3 STUDY OF HIERARCHICAL NETWORK STRUCTURETo demonstrate how the network depth and multiple pooling layers affects the prediction perfor-'
<EOS>
b'mance, we conduct experiments to evaluate different hierarchical network structures.We \xef\xac\x81rst de\xef\xac\x81nea network block contains two GCN layers and one STRUCTPOOL layer.'
<EOS>
b'Then we compare three11Published as a conference paper at ICLR 2020'
<EOS>
b'Table 8: Comparison with different hierarchical network structures.Dataset\n\n1BLOCK'
<EOS>
b'2 BLOCKS\n\n3 BLOCKSPROTEINSD&D\n\n79.73'
<EOS>
b'81.8777.4283.59'
<EOS>
b'74.9581.63different network settings:'
<EOS>
b'1 block with the \xef\xac\x81nal classi\xef\xac\x81er, 2 blocks with the \xef\xac\x81nal classi\xef\xac\x81er, and\n3 blocks with the \xef\xac\x81nal classi\xef\xac\x81er.The results are reported in Table 8.For the dataset Proteins, we\nobserve that the network with one block can obtain better performance than deeper networks.'
<EOS>
b'We\nbelieve the main reason is datasetProteins is a small-scale dataset with an average number of nodes\nequal to 39.06.A relatively simpler network is powerful enough to learn its data distribution'
<EOS>
b'while\nstacking multiple GCN layers and pooling layers may lead to a serious over\xef\xac\x81tting problems.For\nthe dataset D&D, the network with 2 blocks performs better than the one with 1 block.Since D&D\nis relatively large scale, stacking 2 blocks increases the power of network and hence increases the\nperformance.'
<EOS>
b'However, going very deep, e.g., stacking 3 blocks, will cause the over\xef\xac\x81tting problem.A.4 STUDY OF GRAPH POOLING RATETable 9: Comparison with different pooling rates.'
<EOS>
b'r= 0.1r = 0.3'
<EOS>
b'r = 0.5r = 0.7r ='
<EOS>
b'0.9kACC'
<EOS>
b'9180.77160'
<EOS>
b'81.5324181.53'
<EOS>
b'33181.97503'
<EOS>
b'80.68We follow the DGCNN (Zhang et al., 2018) to select the number of clustersk. Speci\xef\xac\x81cally, we use\na pooling rate r \xe2\x88\x88 (0, 1) to control k.'
<EOS>
b'Then k is set to an integer so that r \xc3\x97 100% of graphs havenodes less than this integer in the current dataset.As suggested in DGCNN, generally, r = 0.9\nis a proper choice for bioinformatics datasets and r = 0.6'
<EOS>
b'is good for social network datasets.In\naddition, we conduct experiments to show the performance with the respect to different r values.We set r = 0.1, 0.3, 0.5, 0.7, 0.9 to evaluate the performance on a large-scale social network dataset'
<EOS>
b'D&D.The average number of nodes in dataset D&D is 284.32 and the maximum number of nodes\nis 5748.The results are reported in Table 9 where the \xef\xac\x81rst row shows different pooling rates, the\nsecond row reports the corresponding k values and the \xef\xac\x81nal row shows the results.'
<EOS>
b'For simplicity,\nwe employ the network structure with 1 block and a \xef\xac\x81nal classi\xef\xac\x81er (as de\xef\xac\x81ned in Section A.3).We\ncan observe that the performance drops when r, k is relatively large or small.In addition, the model\ncan obtain competitive performance when r is set to a proper range, for example, r \xe2\x88\x88 [0.3, 0.7] for\ndataset D&D.\n\n12'
<EOS>
