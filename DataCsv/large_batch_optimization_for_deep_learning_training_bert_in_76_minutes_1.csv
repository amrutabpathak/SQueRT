b'Published as a conference paper at ICLR 2020'
<EOS>
b'LARGE BATCH OPTIMIZATION FOR DEEP LEARNING:'
<EOS>
b'TRAINING BERT IN 76 MINUTES'
<EOS>
b'Yang You2, Jing Li1,'
<EOS>
b'Sashank Reddi1, Jonathan Hseu1, Sanjiv Kumar1, Srinadh Bhojanapalli1'
<EOS>
b'Xiaodan Song1, James Demmel2, Kurt Keutzer2,'
<EOS>
b'Cho-Jui Hsieh1,3'
<EOS>
b'Yang'
<EOS>
b'You was a student researcher at Google Brain.'
<EOS>
b'This project was done when he was at Google Brain.'
<EOS>
b'Google1, UC Berkeley2, UCLA3\n{youyang, demmel, keutzer}@cs.berkeley.edu, {jingli, sashank, jhseu, sanjivk, bsrinadh, xiaodansong, chojui}@google.com'
<EOS>
b'ABSTRACT'
<EOS>
b'Training large deep neural networks on massive datasets is computationally very\nchallenging.'
<EOS>
b'There has been recent surge in interest in using large batch stochastic\noptimization methods to tackle this issue.'
<EOS>
b'The most prominent algorithm in this\nline of research is LARS, which by employing layerwise adaptive learning rates\ntrains RESNET on ImageNet in a few minutes.'
<EOS>
b'However, LARS performs poorly for\nattention models like BERT, indicating that its performance gains are not consistent\nacross tasks.'
<EOS>
b'In this paper, we \xef\xac\x81rst study a principled layerwise adaptation strategy\nto accelerate training of deep neural networks using large mini-batches.'
<EOS>
b'Using this\nstrategy, we develop a new layerwise adaptive large batch optimization technique\ncalled LAMB; we then provide convergence analysis of LAMB as well as LARS,\nshowing convergence to a stationary point in general nonconvex settings.'
<EOS>
b'Our\nempirical results demonstrate the superior performance of LAMB across various\ntasks such as BERT and RESNET-50 training with very little hyperparameter tuning.'
<EOS>
b'In particular, for BERT training, our optimizer enables use of very large batch sizes\nof 32868 without any degradation of performance.'
<EOS>
b'By increasing the batch size to\nthe memory limit of a TPUv3 Pod, BERT training time can be reduced from 3 days\nto just 76 minutes (Table 1).'
<EOS>
b'The LAMB implementation is available online1.'
<EOS>
b'1'
<EOS>
b'INTRODUCTION'
<EOS>
b'With the advent of large scale datasets, training large deep neural networks, even using computation-\nally ef\xef\xac\x81cient optimization methods like Stochastic gradient descent (SGD), has become particularly\nchallenging.'
<EOS>
b'For instance, training state-of-the-art deep learning models like BERT and ResNet-50\ntakes 3 days on 16 TPUv3 chips and 29 hours on 8 Tesla P100 gpus respectively (Devlin et al., 2018;'
<EOS>
b'He et al., 2016).'
<EOS>
b'Thus, there is a growing interest to develop optimization solutions to tackle this\ncritical issue.'
<EOS>
b'The goal of this paper is to investigate and develop optimization techniques to accelerate\ntraining large deep neural networks, mostly focusing on approaches based on variants of SGD.'
<EOS>
b'Methods based on SGD iteratively update the parameters of the model by moving them in a scaled\n(negative) direction of the gradient calculated on a minibatch.'
<EOS>
b'However, SGD\xe2\x80\x99s scalability is limited\nby its inherent sequential nature.'
<EOS>
b'Owing to this limitation, traditional approaches to improve SGD\ntraining time in the context of deep learning largely resort to distributed asynchronous setup (Dean\net al., 2012; Recht et al., 2011).'
<EOS>
b'However, the implicit staleness introduced due to the asynchrony\nlimits the parallelization of the approach, often leading to degraded performance.'
<EOS>
b'The feasibility of\ncomputing gradient on large minibatches in parallel due to recent hardware advances has seen the\nresurgence of simply using synchronous SGD with large minibatches as an alternative to asynchronous\nSGD.'
<EOS>
b'However, na\xc3\xafvely increasing the batch size typically results in degradation of generalization\nperformance and reduces computational bene\xef\xac\x81ts (Goyal et al., 2017).'
<EOS>
b'Synchronous SGD on large minibatches bene\xef\xac\x81ts from reduced variance of the stochastic gradients\nused in SGD.'
<EOS>
b'This allows one to use much larger learning rates in SGD, typically of the order square\nroot of the minibatch size.'
<EOS>
b'Surprisingly, recent works have demonstrated that up to certain minibatch'
<EOS>
b'sizes, linear scaling of the learning rate with minibatch size can be used to further speed up the\n\n1https://github.com/tensorflow/addons/blob/master/tensorflow_addons/'
<EOS>
b'optimizers/lamb.py\n\n1'
<EOS>
b'Published as a conference paper at ICLR 2020\n\ntraining Goyal et al.'
<EOS>
b'(2017).'
<EOS>
b'These works also elucidate two interesting aspects to enable the use of\nlinear scaling in large batch synchronous SGD: (i'
<EOS>
b') linear scaling of learning rate is harmful during the\ninitial phase; thus, a hand-tuned warmup strategy of slowly increasing the learning rate needs to be\nused initially, and (ii) linear scaling of learning rate can be detrimental beyond a certain batch size.'
<EOS>
b'Using these tricks, Goyal et al.'
<EOS>
b'(2017) was able to drastically reduce the training time of ResNet-50\nmodel from 29 hours to 1 hour using a batch size of 8192.'
<EOS>
b'While these works demonstrate the\nfeasibility of this strategy for reducing the wall time for training large deep neural networks, they\nalso highlight the need for an adaptive learning rate mechanism for large batch learning.'
<EOS>
b'Variants of SGD using layerwise adaptive learning rates have been recently proposed to address this\nproblem.'
<EOS>
b'The most successful in this line of research is the LARS algorithm ('
<EOS>
b'You et al., 2017), which\nwas initially proposed for training RESNET.'
<EOS>
b'Using LARS, ResNet-50 can be trained on ImageNet in\njust a few minutes!'
<EOS>
b'However, it has been observed that its performance gains are not consistent across\ntasks.'
<EOS>
b'For instance, LARS performs poorly for attention models like BERT.'
<EOS>
b'Furthermore, theoretical\nunderstanding of the adaptation employed in LARS is largely missing.'
<EOS>
b'To this end, we study and\ndevelop new approaches specially catered to the large batch setting of our interest.'
<EOS>
b'Contributions.'
<EOS>
b'More speci\xef\xac\x81cally, we make the following main contributions in this paper.'
<EOS>
b'\xe2\x80\xa2'
<EOS>
b'Inspired by LARS, we investigate a general adaptation strategy specially catered to large\n\nbatch learning and provide intuition for the strategy.'
<EOS>
b'\xe2\x80\xa2'
<EOS>
b'Based on the adaptation strategy, we develop a new optimization algorithm (LAMB) for\nachieving adaptivity of learning rate in SGD.'
<EOS>
b'Furthermore, we provide convergence analysis\nfor both LARS and LAMB to achieve a stationary point in nonconvex settings.'
<EOS>
b'We highlight'
<EOS>
b'the bene\xef\xac\x81ts of using these methods for large batch settings.'
<EOS>
b'\xe2\x80\xa2'
<EOS>
b'We demonstrate the strong empirical performance of LAMB across several challenging tasks.'
<EOS>
b'Using LAMB we scale the batch size in training BERT to more than 32k without degrading\nthe performance; thereby, cutting the time down from 3 days to 76 minutes.'
<EOS>
b'Ours is the \xef\xac\x81rst\nwork to reduce BERT training wall time to less than couple of hours.'
<EOS>
b'\xe2\x80\xa2'
<EOS>
b'We also demonstrate the ef\xef\xac\x81ciency of LAMB for training state-of-the-art image classi\xef\xac\x81cation\nmodels like RESNET.'
<EOS>
b'To the best of our knowledge, ours is \xef\xac\x81rst adaptive solver that can\nachieve state-of-the-art accuracy for RESNET-50 as adaptive solvers like Adam fail to obtain'
<EOS>
b'the accuracy of SGD with momentum for these tasks.'
<EOS>
b'1.1 RELATED WORK'
<EOS>
b'The literature on optimization for machine learning is vast and hence, we restrict our attention to the\nmost relevant works here.'
<EOS>
b'Earlier works on large batch optimization for machine learning mostly\nfocused on convex models, bene\xef\xac\x81ting by a factor of square root of batch size using appropriately large\nlearning rate.'
<EOS>
b'Similar results can be shown for nonconvex settings wherein using larger minibatches\nimproves the convergence to stationary points; albeit at the cost of extra computation.'
<EOS>
b'However,\nseveral important concerns were raised with respect to generalization and computational performance\nin large batch nonconvex settings.'
<EOS>
b'It was observed that training with extremely large batch was\ndif\xef\xac\x81cult (Keskar et al., 2016; Hoffer et al., 2017).'
<EOS>
b'Thus, several prior works carefully hand-tune\ntraining hyper-parameters, like learning rate and momentum, to avoid degradation of generalization\nperformance (Goyal et al., 2017; Li, 2017; You et al., 2018; Shallue et al., 2018).'
<EOS>
b'(Krizhevsky, 2014) empirically found that simply scaling the learning rate linearly with respect to\nbatch size works better up to certain batch sizes.'
<EOS>
b'To avoid optimization instability due to linear scaling\nof learning rate, Goyal et al.'
<EOS>
b'(2017) proposed a highly hand-tuned learning rate which involves a\nwarm-up strategy that gradually increases the LR to a larger value and then switching to the regular\nLR policy (e.g. exponential or polynomial decay).'
<EOS>
b'Using LR warm-up and linear scaling, Goyal et al.'
<EOS>
b'(2017) managed to train RESNET-50 with batch size 8192 without loss in generalization performance.'
<EOS>
b'However, empirical study (Shallue et al., 2018) shows that learning rate scaling heuristics with the\nbatch size do not hold across all problems or across all batch sizes.'
<EOS>
b'More recently, to reduce hand-tuning of hyperparameters, adaptive learning rates for large batch'
<EOS>
b'training garnered signi\xef\xac\x81cant interests.'
<EOS>
b'Several recent works successfully scaled the batch size to large\nvalues using adaptive learning rates without degrading the performance, thereby, \xef\xac\x81nishing RESNET-'
<EOS>
b'50 training on ImageNet in a few minutes'
<EOS>
b'(You et al., 2018; Iandola et al., 2016; Codreanu et al.,\n2017; Akiba et al., 2017; Jia et al., 2018; Smith et al., 2017; Martens & Grosse, 2015; Devarakonda\n\n2\n\n\x0cPublished as a conference paper at ICLR 2020\n\net al., 2017; Mikami et al., 2018; Osawa et al., 2018; You et al., 2019; Yamazaki et al., 2019).'
<EOS>
b'To the\nbest of our knowledge, the fastest training result for RESNET-50 on ImageNet is due to Ying et al.'
<EOS>
b'(2018), who achieve 76+% top-1 accuracy.'
<EOS>
b'By using the LARS optimizer and scaling the batch size to\n32K on a TPUv3 Pod, Ying et al.'
<EOS>
b'(2018) was able to train RESNET-50 on ImageNet in 2.2 minutes.'
<EOS>
b'However, it was empirically observed that none of these performance gains hold in other tasks such\nas BERT training'
<EOS>
b'(see Section 4).'
<EOS>
b'2 PRELIMINARIES'
<EOS>
b'Notation.'
<EOS>
b'For any vector xt \xe2\x88\x88'
<EOS>
b'Rd, either xt,j or [xt]j are used to denote its jth coordinate where\nj \xe2\x88\x88 [d].'
<EOS>
b'Let I be the d \xc3\x97 d identity matrix, and let I ='
<EOS>
b'[I1, I2, ..., Ih] be its decomposition into column'
<EOS>
b'submatrices'
<EOS>
b'Ii = d \xc3\x97 dh.'
<EOS>
b'For x \xe2\x88\x88'
<EOS>
b'Rd, let x(i) be the block of variables corresponding to the columns\nof Ii i.e., x(i)'
<EOS>
b'='
<EOS>
b'I(cid:62)'
<EOS>
b'i'
<EOS>
b'x \xe2\x88\x88'
<EOS>
b'Rdi for i = {1, 2, \xc2\xb7 \xc2\xb7 \xc2\xb7 , h}.'
<EOS>
b'For any function f :'
<EOS>
b'Rd \xe2\x86\x92 R, we use \xe2\x88\x87if (x) to\ndenote the gradient with respect to x(i).'
<EOS>
b'For any vectors u, v \xe2\x88\x88'
<EOS>
b'Rd, we use u2 and u/v to denote\nelementwise square and division operators respectively.'
<EOS>
b'We use (cid:107).(cid:107) and (cid:107).(cid:107)1 to denote l2-norm and\nl1-norm of a vector respectively.'
<EOS>
b'We start our discussion by formally stating the problem setup.'
<EOS>
b'In this paper, we study nonconvex'
<EOS>
b'stochastic optimization problems of the form'
<EOS>
b'f'
<EOS>
b'(x) :'
<EOS>
b'= Es\xe2\x88\xbcP[(cid:96)(x, s)]'
<EOS>
b'+'
<EOS>
b'(cid:107)x(cid:107)2,'
<EOS>
b'min'
<EOS>
b'x\xe2\x88\x88Rd'
<EOS>
b'\xce\xbb'
<EOS>
b'2'
<EOS>
b'(1)\n\nwhere (cid:96) is a smooth (possibly nonconvex) function and P'
<EOS>
b'is a probability distribution on the domain'
<EOS>
b'S'
<EOS>
b'\xe2\x8a\x82 Rk.'
<EOS>
b'Here, x corresponds to model parameters,'
<EOS>
b'(cid:96) is the loss function and P is an unknown data\ndistribution.'
<EOS>
b'We assume function (cid:96)(x) is Li-smooth with respect to x(i), i.e., there exists a constant Li'
<EOS>
b'such that'
<EOS>
b'(cid:107)\xe2\x88\x87i(cid:96)(x, s) \xe2\x88\x92 \xe2\x88\x87i(cid:96)(y, s)(cid:107) \xe2\x89\xa4 Li(cid:107)x(i)'
<EOS>
b'\xe2\x88\x92 y(i)(cid:107),'
<EOS>
b'\xe2\x88\x80 x, y \xe2\x88\x88'
<EOS>
b'Rd, and s \xe2\x88\x88 S,'
<EOS>
b'(2)\n\nfor all i \xe2\x88\x88 [h].'
<EOS>
b'We use L ='
<EOS>
b'(L1, \xc2\xb7 \xc2\xb7 \xc2\xb7 , Lh)(cid:62) to denote the h-dimensional vector of Lipschitz constants.'
<EOS>
b'We use L\xe2\x88\x9e and Lavg to denote maxi Li and (cid:80)'
<EOS>
b'Li\nh respectively.'
<EOS>
b'We assume the following bound'
<EOS>
b'i for all x \xe2\x88\x88'
<EOS>
b'Rd'
<EOS>
b'and i \xe2\x88\x88 [h].'
<EOS>
b'on the variance in stochastic gradients: E(cid:107)\xe2\x88\x87i(cid:96)(x, s) \xe2\x88\x92 \xe2\x88\x87if (x)(cid:107)2 \xe2\x89\xa4 \xcf\x832'
<EOS>
b'Furthermore, we also assume E(cid:107)[\xe2\x88\x87(cid:96)(x, s)]i \xe2\x88\x92 [\xe2\x88\x87f (x)]i(cid:107)2 \xe2\x89\xa4 \xcb\x9c\xcf\x832'
<EOS>
b'i for all x \xe2\x88\x88'
<EOS>
b'Rd'
<EOS>
b'and i \xe2\x88\x88 [d].'
<EOS>
b'We use\n\xcf\x83 = (\xcf\x831, \xc2\xb7 \xc2\xb7 \xc2\xb7 , \xcf\x83h)(cid:62) and \xcb\x9c\xcf\x83 ='
<EOS>
b'(\xcb\x9c\xcf\x831, \xc2\xb7 \xc2\xb7 \xc2\xb7 , \xcb\x9c\xcf\x83d)(cid:62) to denote the vectors of standard deviations of stochastic\ngradient per layer and per dimension respectively.'
<EOS>
b'Finally, we assume that the gradients are bounded'
<EOS>
b'i.e., [\xe2\x88\x87l(x, s)]j \xe2\x89\xa4 G for all i \xe2\x88\x88 [d], x \xe2\x88\x88'
<EOS>
b'Rd and s'
<EOS>
b'\xe2\x88\x88 S. Note that such assumptions are typical in the\nanalysis of stochastic \xef\xac\x81rst-order methods (cf.'
<EOS>
b'(Ghadimi & Lan, 2013a; Ghadimi et al., 2014)).'
<EOS>
b'i'
<EOS>
b'Stochastic gradient descent (SGD) is one of the simplest \xef\xac\x81rst-order algorithms for solving problem in\nEquation 1.'
<EOS>
b'The update at the tth iteration of SGD is of the following form:\n\nxt+1 ='
<EOS>
b'xt \xe2\x88\x92'
<EOS>
b'\xce\xb7t\n\n\xe2\x88\x87(cid:96)(xt, st)'
<EOS>
b'+ \xce\xbbxt,\n\n(SGD)\n\n1'
<EOS>
b'|St|'
<EOS>
b'(cid:88)'
<EOS>
b'st\xe2\x88\x88St'
<EOS>
b'where St is set of b random samples drawn from the distribution P.'
<EOS>
b'For very large batch settings, the\nfollowing is a well-known result for SGD.'
<EOS>
b'Theorem 1 ((Ghadimi & Lan, 2013b)).'
<EOS>
b'With large batch b = T and using appropriate learning rate,\nwe have the following for the iterates of SGD:'
<EOS>
b'E (cid:2)(cid:107)\xe2\x88\x87f (xa)(cid:107)2(cid:3) \xe2\x89\xa4'
<EOS>
b'O'
<EOS>
b'(cid:18)'
<EOS>
b'(f'
<EOS>
b'(x1) \xe2\x88\x92 f (x\xe2\x88\x97))L\xe2\x88\x9e'
<EOS>
b'(cid:107)\xcf\x83(cid:107)2'
<EOS>
b'(cid:19)'
<EOS>
b'T'
<EOS>
b'+'
<EOS>
b'T\n\n.'
<EOS>
b'where x\xe2\x88\x97 is an optimal solution to the problem in equation 1 and xa is an iterate'
<EOS>
b'uniformly randomly\nchosen from {x1, \xc2\xb7 \xc2\xb7 \xc2\xb7 , xT }.'
<EOS>
b'However, tuning the learning rate \xce\xb7t in SGD, especially in large batch settings, is dif\xef\xac\x81cult in practice.'
<EOS>
b'Furthermore, the dependence on L\xe2\x88\x9e (the maximum of smoothness across dimension) can lead to\nsigni\xef\xac\x81cantly slow convergence.'
<EOS>
b'In the next section, we discuss algorithms to circumvent this issue.'
<EOS>
b'3'
<EOS>
b'Published as a conference paper at ICLR 2020'
<EOS>
b'3 ALGORITHMS'
<EOS>
b'In this section, we \xef\xac\x81rst discuss a general strategy to adapt the learning rate in large batch settings.'
<EOS>
b'Using this strategy, we discuss two speci\xef\xac\x81c algorithms in the later part of the section.'
<EOS>
b'Since our\nprimary focus is on deep learning, our discussion is centered around training a h-layer neural network.'
<EOS>
b'General Strategy.'
<EOS>
b'Suppose we use an iterative base algorithm A (e.g. SGD or ADAM)'
<EOS>
b'in the small\nbatch setting with the following layerwise update rule:\n\nwhere ut is the update made by A at time step t.'
<EOS>
b'We propose the following two changes to the update\nfor large batch settings:\n\nxt+1'
<EOS>
b'= xt + \xce\xb7tut,\n\n1.'
<EOS>
b'The update is normalized to unit l2-norm.'
<EOS>
b'This is ensured by modifying the update to the\nform ut/(cid:107)ut(cid:107).'
<EOS>
b'Throughout this paper, such a normalization is done layerwise i.e., the update\nfor each layer is ensured to be unit l2-norm.'
<EOS>
b'2.'
<EOS>
b'The learning rate is scaled by \xcf\x86((cid:107)xt(cid:107))'
<EOS>
b'for some function \xcf\x86 : R+ \xe2\x86\x92 R+.'
<EOS>
b'Similar to the\n\nnormalization, such a scaling is done layerwise.'
<EOS>
b'Suppose the base algorithm'
<EOS>
b'A is SGD, then the modi\xef\xac\x81cation results in the following update rule:\n\nx(i)'
<EOS>
b't+1'
<EOS>
b'= x(i)'
<EOS>
b't \xe2\x88\x92'
<EOS>
b'\xce\xb7t'
<EOS>
b'\xcf\x86((cid:107)x(i)\nt (cid:107))\n(cid:107)g(i)\nt (cid:107)\n\ng(i)'
<EOS>
b't\n\n,'
<EOS>
b'(3)\n\nand g(i)\nt'
<EOS>
b'for all layers'
<EOS>
b'i \xe2\x88\x88'
<EOS>
b'[h] and where x(i)'
<EOS>
b'are the parameters and the gradients of the ith layer at\nt'
<EOS>
b'time step t.'
<EOS>
b'The normalization modi\xef\xac\x81cation is similar to one'
<EOS>
b'typically used in normalized gradient\ndescent except that it is done layerwise.'
<EOS>
b'Note that the modi\xef\xac\x81cation leads to a biased gradient update;\nhowever, in large-batch settings, it can be shown that this bias is small.'
<EOS>
b'It is intuitive that such a\nnormalization provides robustness to exploding gradients (where the gradient can be arbitrarily large)\nand plateaus (where the gradient can be arbitrarily small).'
<EOS>
b'Normalization of this form essentially\nignores the size of the gradient and is particularly useful in large batch settings where the direction of\nthe gradient is largely preserved.'
<EOS>
b'The scaling term involving \xcf\x86 ensures that the norm of the update is of the same order as that of\nthe parameter.'
<EOS>
b'We found that this typically ensures faster convergence in deep neural networks.'
<EOS>
b'In practice, we observed that a simple function of \xcf\x86(z) ='
<EOS>
b'min{max{z, \xce\xb3l}, \xce\xb3u} works well.'
<EOS>
b'It is\ninstructive to consider the case where \xcf\x86(z) ='
<EOS>
b'z.'
<EOS>
b'In this scenario, the overall change in the learning\nrate is (cid:107)x(i)\nt (cid:107)\n(cid:107)g(i)'
<EOS>
b't (cid:107)'
<EOS>
b'gradient'
<EOS>
b'(see equation 2).'
<EOS>
b'We now discuss different instantiations of the strategy discussed above.'
<EOS>
b'In\nparticular, we focus on two algorithms: LARS (3.1) and the proposed method, LAMB (3.2).'
<EOS>
b', which can also be interpreted as an estimate on the inverse of Lipschitz constant of the\n\n3.1 LARS ALGORITHM'
<EOS>
b'The \xef\xac\x81rst instantiation of the general strategy is LARS algorithm ('
<EOS>
b'You et al., 2017), which is obtained\nby using momentum optimizer as the base algorithm A in the framework.'
<EOS>
b'LARS was earlier proposed\nfor large batch learning for RESNET on ImageNet.'
<EOS>
b'In general, it is observed that the using (heavy-ball)\nmomentum, one can reduce the variance in the stochastic gradients at the cost of little bias.'
<EOS>
b'The\npseudocode for LARS is provide in Algorithm 1.'
<EOS>
b'We now provide convergence analysis for LARS in general nonconvex setting stated in this paper.'
<EOS>
b'For\nthe sake of simplicity, we analyze the case where \xce\xb21 = 0 and \xce\xbb = 0 in Algorithm 1.'
<EOS>
b'However, our\nanalysis should extend to the general case as well.'
<EOS>
b'We will defer all discussions about the convergence\nrate to the end of the section.'
<EOS>
b'Theorem 2.'
<EOS>
b'Let \xce\xb7t = \xce\xb7 ='
<EOS>
b'where \xce\xb1l, \xce\xb1u > 0.'
<EOS>
b'Then for xt generated using LARS (Algorithm 1), we have the following bound\n\nfor all t \xe2\x88\x88 [T ],'
<EOS>
b'b'
<EOS>
b'= T , \xce\xb1l \xe2\x89\xa4 \xcf\x86(v)'
<EOS>
b'\xe2\x89\xa4 \xce\xb1u for all v'
<EOS>
b'> 0'
<EOS>
b'u(cid:107)L(cid:107)1T\n\n\xce\xb12\n\n(cid:113)'
<EOS>
b'2(f'
<EOS>
b'(x1)\xe2\x88\x92f (x\xe2\x88\x97))'
<EOS>
b'(cid:32)'
<EOS>
b'(cid:34)'
<EOS>
b'E'
<EOS>
b'1'
<EOS>
b'\xe2\x88\x9a'
<EOS>
b'h'
<EOS>
b'h'
<EOS>
b'(cid:88)'
<EOS>
b'i=1'
<EOS>
b'(cid:35)(cid:33)2'
<EOS>
b'(cid:18)'
<EOS>
b'(f'
<EOS>
b'(x1) \xe2\x88\x92 f ('
<EOS>
b'x\xe2\x88\x97))Lavg'
<EOS>
b'(cid:107)\xe2\x88\x87if (xa)(cid:107)'
<EOS>
b'\xe2\x89\xa4'
<EOS>
b'O'
<EOS>
b'T'
<EOS>
b'+'
<EOS>
b'(cid:19)\n\n,\n\n(cid:107)\xcf\x83(cid:107)2'
<EOS>
b'1'
<EOS>
b'T h'
<EOS>
b'where x\xe2\x88\x97 is an optimal solution to the problem in equation 1 and xa is an iterate'
<EOS>
b'uniformly randomly\nchosen from {x1, \xc2\xb7 \xc2\xb7 \xc2\xb7 , xT }.'
<EOS>
b'4'
<EOS>
b'Published as a conference paper at ICLR 2020'
<EOS>
b'Algorithm 2 LAMB'
<EOS>
b'Input:'
<EOS>
b'x1 \xe2\x88\x88'
<EOS>
b'Rd, learning rate {\xce\xb7t}T\n0 < \xce\xb21, \xce\xb22 < 1, scaling function \xcf\x86, (cid:15)'
<EOS>
b'> 0'
<EOS>
b'Set m0 ='
<EOS>
b'0'
<EOS>
b', v0 = 0\nfor t'
<EOS>
b'= 1 to T'
<EOS>
b'do'
<EOS>
b't=1, parameters\n\n\xe2\x88\x87(cid:96)(xt, st).'
<EOS>
b'Algorithm'
<EOS>
b'1 LARS'
<EOS>
b'Input:'
<EOS>
b'x1 \xe2\x88\x88'
<EOS>
b'Rd, learning rate {\xce\xb7t}T\n0 < \xce\xb21 < 1, scaling function \xcf\x86, (cid:15)'
<EOS>
b'> 0'
<EOS>
b'Set m0'
<EOS>
b'= 0'
<EOS>
b'for t'
<EOS>
b'= 1 to T'
<EOS>
b'do'
<EOS>
b't=1, parameter'
<EOS>
b'(cid:80)'
<EOS>
b'Draw b samples St from P'
<EOS>
b'Compute gt = 1\n\xe2\x88\x87(cid:96)(xt, st)'
<EOS>
b'st\xe2\x88\x88St'
<EOS>
b'|St|'
<EOS>
b'mt = \xce\xb21mt\xe2\x88\x921'
<EOS>
b'+ (1 \xe2\x88\x92 \xce\xb21)(gt'
<EOS>
b'+ \xce\xbbxt)'
<EOS>
b'x(i)'
<EOS>
b't+1'
<EOS>
b'= x(i)'
<EOS>
b't \xe2\x88\x92'
<EOS>
b'\xce\xb7t'
<EOS>
b'm(i)'
<EOS>
b't'
<EOS>
b'\xcf\x86((cid:107)x'
<EOS>
b'(i)'
<EOS>
b't (cid:107))'
<EOS>
b'(i)'
<EOS>
b't (cid:107)'
<EOS>
b'(cid:107)m'
<EOS>
b'for all i \xe2\x88\x88'
<EOS>
b'[h]\n\nend for\n\n(cid:80)'
<EOS>
b'Draw b samples St from P.'
<EOS>
b'Compute gt = 1'
<EOS>
b'st\xe2\x88\x88St'
<EOS>
b'|St|'
<EOS>
b'mt = \xce\xb21mt\xe2\x88\x921'
<EOS>
b'+ (1 \xe2\x88\x92'
<EOS>
b'\xce\xb21)gt'
<EOS>
b'vt'
<EOS>
b'= \xce\xb22vt\xe2\x88\x921 + (1 \xe2\x88\x92'
<EOS>
b'\xce\xb22)g2\nt'
<EOS>
b'mt ='
<EOS>
b'mt/(1 \xe2\x88\x92 \xce\xb2t\n1)'
<EOS>
b'vt ='
<EOS>
b'vt/(1'
<EOS>
b'\xe2\x88\x92 \xce\xb2t'
<EOS>
b'2)\nCompute ratio rt ='
<EOS>
b'mt\xe2\x88\x9a\nx(i)'
<EOS>
b't+1'
<EOS>
b'= x(i)'
<EOS>
b't \xe2\x88\x92'
<EOS>
b'\xce\xb7t'
<EOS>
b'vt+(cid:15)'
<EOS>
b'(i)'
<EOS>
b't (cid:107))'
<EOS>
b'(i)'
<EOS>
b't (cid:107)\n\n\xcf\x86((cid:107)x'
<EOS>
b'(i)'
<EOS>
b't +'
<EOS>
b'\xce\xbbx'
<EOS>
b'(cid:107)r'
<EOS>
b'end for\n\n(r(i)'
<EOS>
b't + \xce\xbbx(i)\nt )'
<EOS>
b'3.2 LAMB ALGORITHM'
<EOS>
b'The second instantiation of the general strategy is obtained by using ADAM as the base algorithm A.\nADAM optimizer is popular in deep learning community and has shown to have good performance\nfor training state-of-the-art language models like BERT.'
<EOS>
b'Unlike LARS, the adaptivity of LAMB is\ntwo-fold: (i) per dimension normalization with respect to the square root of the second moment used\nin ADAM and (ii) layerwise normalization obtained due to layerwise adaptivity.'
<EOS>
b'The pseudocode for\nLAMB is provided in Algorithm 2.'
<EOS>
b'When \xce\xb21 = 0'
<EOS>
b'and'
<EOS>
b'\xce\xb22 = 0'
<EOS>
b', the algorithm reduces to be Sign SGD\nwhere the learning rate is scaled by square root of the layer dimension (Bernstein et al., 2018).'
<EOS>
b'The following result provides convergence rate for LAMB in general nonconvex settings.'
<EOS>
b'Similar to'
<EOS>
b'the previous case, we focus on the setting where \xce\xb21 = 0 and \xce\xbb = 0.'
<EOS>
b'As before, our analysis extends\nto the general case; however, the calculations become messy.'
<EOS>
b'(cid:113)'
<EOS>
b'2(f'
<EOS>
b'(x1)\xe2\x88\x92f (x\xe2\x88\x97))'
<EOS>
b'for all t \xe2\x88\x88 [T ], b'
<EOS>
b'= T , di = d'
<EOS>
b'/h for all'
<EOS>
b'i \xe2\x88\x88'
<EOS>
b'[h], and'
<EOS>
b'Theorem 3.'
<EOS>
b'Let \xce\xb7t = \xce\xb7 ='
<EOS>
b'\xce\xb1l \xe2\x89\xa4 \xcf\x86(v) \xe2\x89\xa4 \xce\xb1u for all v'
<EOS>
b'> 0'
<EOS>
b'where \xce\xb1l, \xce\xb1u > 0.'
<EOS>
b'Then for xt generated using LAMB ('
<EOS>
b'Algorithm 2),\nwe have the following bounds:\n\nu(cid:107)L(cid:107)1'
<EOS>
b'T'
<EOS>
b'\xce\xb12\n\n1.'
<EOS>
b'When \xce\xb22 = 0, we have\n\n2.'
<EOS>
b'When \xce\xb22 > 0, we have'
<EOS>
b'(cid:18)'
<EOS>
b'E'
<EOS>
b'(cid:20)'
<EOS>
b'1'
<EOS>
b'\xe2\x88\x9a'
<EOS>
b'd'
<EOS>
b'(cid:21)(cid:19)2'
<EOS>
b'(cid:18)'
<EOS>
b'(f'
<EOS>
b'(x1) \xe2\x88\x92 f ('
<EOS>
b'x\xe2\x88\x97))Lavg'
<EOS>
b'(cid:107)\xe2\x88\x87f ('
<EOS>
b'xa)(cid:107)1'
<EOS>
b'\xe2\x89\xa4'
<EOS>
b'O'
<EOS>
b'T'
<EOS>
b'+'
<EOS>
b'(cid:19)\n\n,\n\n(cid:107)\xcb\x9c\xcf\x83(cid:107)2'
<EOS>
b'1'
<EOS>
b'T h'
<EOS>
b'E[(cid:107)\xe2\x88\x87f (xa)(cid:107)2]'
<EOS>
b'\xe2\x89\xa4'
<EOS>
b'O'
<EOS>
b'(cid:32)(cid:115)\n\nG2d'
<EOS>
b'h(1 \xe2\x88\x92 \xce\xb22)'
<EOS>
b'(cid:34)(cid:114)\n\n\xc3\x97\n\n2(f'
<EOS>
b'(x1'
<EOS>
b') \xe2\x88\x92 f'
<EOS>
b'(x\xe2\x88\x97))(cid:107)L(cid:107)1'
<EOS>
b'T\n\n(cid:35)(cid:33)\n\n,\n\n+\n\n(cid:107)\xcb\x9c\xcf\x83(cid:107)1\xe2\x88\x9a\nT'
<EOS>
b'where x\xe2\x88\x97 is an optimal solution to the problem in equation 1 and xa is an iterate'
<EOS>
b'uniformly randomly\nchosen from {x1, \xc2\xb7 \xc2\xb7 \xc2\xb7 , xT }.'
<EOS>
b'Discussion on convergence rates.'
<EOS>
b'We \xef\xac\x81rst start our discussion with the comparison of convergence\nrate of LARS with that of SGD (Theorem 1).'
<EOS>
b'The convergence rates of LARS and SGD differ in\ntwo ways: (1) the convergence criterion is (E[(cid:80)h\ni=1'
<EOS>
b'(cid:107)\xe2\x88\x87if (cid:107)])2 as opposed to E[(cid:107)\xe2\x88\x87f (cid:107)2] in SGD and'
<EOS>
b'(2) the dependence on L and \xcf\x83 in the convergence rate.'
<EOS>
b'Brie\xef\xac\x82y, the convergence rate of LARS is\nbetter than SGD when the gradient is denser than curvature and stochasticity.'
<EOS>
b'This convergence rate\ncomparison is similar in spirit to the one obtained in (Bernstein et al., 2018).'
<EOS>
b'Assuming that the\nconvergence criterion in Theorem 1 and Theorem 2 is of similar order (which happens when gradients\nare fairly dense), convergence rate of LARS and LAMB depend on Lavg instead of L\xe2\x88\x9e and are thus,\nsigni\xef\xac\x81cantly better than that of SGD.'
<EOS>
b'A more quantitative comparison is provided in Section C of\nthe Appendix.'
<EOS>
b'The comparison of LAMB (with \xce\xb22 = 0) with SGD is along similar lines.'
<EOS>
b'We obtain\nslightly worse rates for the case where \xce\xb22 > 0; although, we believe that its behavior should be better\nthan the case'
<EOS>
b'\xce\xb22 = 0.'
<EOS>
b'We leave this investigation to future work.'
<EOS>
b'5'
<EOS>
b'Published as a conference paper at ICLR 2020'
<EOS>
b'4'
<EOS>
b'EXPERIMENTS'
<EOS>
b'We now present empirical results comparing LAMB with existing optimizers on two important\nlarge batch training tasks: BERT and RESNET-50 training.'
<EOS>
b'We also compare LAMB with existing\noptimizers for small batch size (< 1K) and small dataset (e.g. CIFAR, MNIST) (see Appendix).'
<EOS>
b'Experimental Setup.'
<EOS>
b'To demonstrate its robustness, we use very minimal hyperparameter tuning for\nthe LAMB optimizer.'
<EOS>
b'Thus, it is possible to achieve better results by further tuning the hyperparameters.'
<EOS>
b'The parameters \xce\xb21 and \xce\xb22 in Algorithm 2 are set to 0.9 and 0.999 respectively in all our experiments;\nwe only tune the learning rate.'
<EOS>
b'We use a polynomially decaying learning rate of \xce\xb7t = \xce\xb70 \xc3\x97(1\xe2\x88\x92t/T ) in\nAlgorithm 2), which is the same as in BERT baseline.'
<EOS>
b'This setting also works for all other applications\nin this paper.'
<EOS>
b'Furthermore, for BERT and RESNET-50 training, we did not tune the hyperparameters\nof LAMB while increasing the batch size.'
<EOS>
b'We use the square root of LR scaling rule to automatically\nadjust learning rate and linear-epoch warmup scheduling.'
<EOS>
b'We use TPUv3 in all the experiments.'
<EOS>
b'A\nTPUv3 Pod has 1024 chips and can provide more than 100 peta\xef\xac\x82ops performance for mixed precision\ncomputing.'
<EOS>
b'To make sure we are comparing with solid baselines, we use grid search to tune the\nhyper-parameters for ADAM, ADAGRAD, ADAMW (ADAM with weight decay), and LARS.'
<EOS>
b'We also\ntune weight decay for ADAMW.'
<EOS>
b'All the hyperparameter tuning settings are reported in the Appendix.'
<EOS>
b'Due to space constraints, several experimental details are relegated to the Appendix.'
<EOS>
b'4.1 BERT TRAINING'
<EOS>
b'We \xef\xac\x81rst discuss empirical results for speeding up BERT training.'
<EOS>
b'For this experiment, we use the same\ndataset as Devlin et al.'
<EOS>
b'(2018), which is a concatenation of Wikipedia and BooksCorpus with 2.5B\nand 800M words respectively.'
<EOS>
b'We speci\xef\xac\x81cally focus on the SQuAD task2 in this paper.'
<EOS>
b'The F1 score\non SQuAD-v1 is used as the accuracy metric in our experiments.'
<EOS>
b'All our comparisons are with respect\nto the baseline BERT model by Devlin et al.'
<EOS>
b'(2018).'
<EOS>
b'To train BERT, Devlin et al.'
<EOS>
b'(2018)'
<EOS>
b'\xef\xac\x81rst train the\nmodel for 900k iterations using a sequence length of 128 and then switch to a sequence length of\n512 for the last 100k iterations.'
<EOS>
b'This results in a training time of around 3 days on 16 TPUv3 chips.'
<EOS>
b'The baseline BERT model3 achieves a F1 score of 90.395.'
<EOS>
b'To ensure a fair comparison, we follow\nthe same SQuAD \xef\xac\x81ne-tune procedure of Devlin et al.'
<EOS>
b'(2018) without modifying any con\xef\xac\x81guration'
<EOS>
b'(including number of epochs and hyperparameters).'
<EOS>
b'As noted earlier, we could get even better results\nby changing the \xef\xac\x81ne-tune con\xef\xac\x81guration.'
<EOS>
b'For instance, by just slightly changing the learning rate in\nthe \xef\xac\x81ne-tune stage, we can obtain a higher F1 score of 91.688 for the batch size of 16K using LAMB.'
<EOS>
b'We report a F1 score of 91.345 in Table 1, which is the score obtained for the untuned version.'
<EOS>
b'Below\nwe describe two different training choices for training BERT and discuss the corresponding speedups.'
<EOS>
b'For the \xef\xac\x81rst choice, we maintain the same training procedure as the baseline except for changing the\ntraining optimizer to LAMB.'
<EOS>
b'We run with the same number of epochs as the baseline but with batch\nsize scaled from 512 to 32K.'
<EOS>
b'The choice of 32K batch size (with sequence length 512) is mainly\ndue to memory limits of TPU Pod.'
<EOS>
b'Our results are shown in Table 1.'
<EOS>
b'By using the LAMB optimizer,\nwe are able to achieve a F1 score of 91.460 in 15625 iterations for a batch size of 32768 ('
<EOS>
b'14063\niterations for sequence length 128 and 1562 iterations for sequence length 512).'
<EOS>
b'With 32K batch size,\nwe reduce BERT training time from 3 days to around 100 minutes.'
<EOS>
b'We achieved 49.1 times speedup\nby 64 times computational resources (76.7% ef\xef\xac\x81ciency).'
<EOS>
b'We consider the speedup is great because we\nuse the synchronous data-parallelism.'
<EOS>
b'There is a communication overhead coming from transferring\nof the gradients over the interconnect.'
<EOS>
b'For RESNET-50, researchers are able to achieve 90% scaling\nef\xef\xac\x81ciency because RESNET-50 has much fewer parameters (# parameters is equal to #gradients) than\nBERT (25 million versus 300 million).'
<EOS>
b'To obtain further improvements, we use the Mixed-Batch Training procedure with LAMB.'
<EOS>
b'Recall'
<EOS>
b'that BERT training involves two stages: the \xef\xac\x81rst 9/10 of the total epochs use a sequence length of 128,\nwhile the last 1/10 of the total epochs use a sequence length of 512.'
<EOS>
b'For the second stage training,\nwhich involves a longer sequence length, due to memory limits, a maximum batch size of only\n32768 can be used on a TPUv3 Pod.'
<EOS>
b'However, we can potentially use a larger batch size for the\n\xef\xac\x81rst stage because of a shorter sequence length.'
<EOS>
b'In particular, the batch size can be increased to\n131072 for the \xef\xac\x81rst stage.'
<EOS>
b'However, we did not observe any speedup by increasing the batch size from\n65536 to 131072 for the \xef\xac\x81rst stage, thus, we restrict the batch size to 65536 for this stage.'
<EOS>
b'By using\nthis strategy, we are able to make full utilization of the hardware resources throughout the training'
<EOS>
b'2https://rajpurkar.github.io/SQuAD-explorer/\n3Pre-trained BERT model can be downloaded from https://github.com/google-research/bert\n\n6\n\n\x0cPublished as a conference paper at ICLR 2020'
<EOS>
b'Table 1:'
<EOS>
b'We use the F1 score on SQuAD-v1 as the accuracy metric.'
<EOS>
b'The baseline F1 score is the\nscore obtained by the pre-trained model (BERT-Large) provided on BERT\xe2\x80\x99s public repository (as of\nFebruary 1st, 2019).'
<EOS>
b'We use TPUv3s in our experiments.'
<EOS>
b'We use the same setting as the baseline: the\n\xef\xac\x81rst 9/10 of the total epochs used a sequence length of 128 and the last 1/10 of the total epochs used\na sequence length of 512.'
<EOS>
b'All the experiments run the same number of epochs.'
<EOS>
b'Dev set means the test\ndata.'
<EOS>
b'It is worth noting that we can achieve better results by manually tuning the hyperparameters.'
<EOS>
b'The data in this table is collected from the untuned version.'
<EOS>
b'Solver'
<EOS>
b'batch size'
<EOS>
b'steps'
<EOS>
b'F1 score on dev set TPUs'
<EOS>
b'Time'
<EOS>
b'Baseline'
<EOS>
b'LAMB\nLAMB'
<EOS>
b'LAMB\nLAMB'
<EOS>
b'LAMB'
<EOS>
b'LAMB'
<EOS>
b'LAMB'
<EOS>
b'512'
<EOS>
b'512'
<EOS>
b'1k'
<EOS>
b'2k'
<EOS>
b'4k'
<EOS>
b'8k\n16k'
<EOS>
b'32k'
<EOS>
b'1000k'
<EOS>
b'1000k\n500k'
<EOS>
b'250k'
<EOS>
b'125k'
<EOS>
b'62500\n31250'
<EOS>
b'15625'
<EOS>
b'LAMB'
<EOS>
b'64k/32k'
<EOS>
b'8599'
<EOS>
b'90.395'
<EOS>
b'91.752'
<EOS>
b'91.761'
<EOS>
b'91.946'
<EOS>
b'91.137'
<EOS>
b'91.263'
<EOS>
b'91.345'
<EOS>
b'91.475'
<EOS>
b'90.584'
<EOS>
b'16'
<EOS>
b'16'
<EOS>
b'32'
<EOS>
b'64'
<EOS>
b'128\n256'
<EOS>
b'512'
<EOS>
b'1024'
<EOS>
b'1024'
<EOS>
b'81.4h'
<EOS>
b'82.8h'
<EOS>
b'43.2h'
<EOS>
b'21.4h'
<EOS>
b'693.6m'
<EOS>
b'390.5m'
<EOS>
b'200.0m'
<EOS>
b'101.2m'
<EOS>
b'76.19m\n\nprocedure.'
<EOS>
b'Increasing the batch size is able to warm-up and stabilize the optimization process (Smith\net al., 2017), but decreasing the batch size brings chaos to the optimization process and can cause\ndivergence.'
<EOS>
b'In our experiments, we found a technique that is useful to stabilize the second stage\noptimization.'
<EOS>
b'Because we switched to a different optimization problem, it is necessary to re-warm-up\nthe optimization.'
<EOS>
b'Instead of decaying the learning rate at the second stage, we ramp up the learning\nrate from zero again in the second stage (re-warm-up).'
<EOS>
b'As with the \xef\xac\x81rst stage, we decay the learning\nrate after the re-warm-up phase.'
<EOS>
b'With this method, we only need 8599 iterations and \xef\xac\x81nish BERT\ntraining in 76 minutes (100.2% ef\xef\xac\x81ciency).'
<EOS>
b'Comparison with ADAMW and LARS.'
<EOS>
b'To ensure that our approach is compared to a solid\nbaseline for the BERT training, we tried three different strategies for tuning ADAMW:'
<EOS>
b'(1) ADAMW\nwith default hyperparameters (see Devlin et al.'
<EOS>
b'(2018))'
<EOS>
b'(2) ADAMW with the same hyperparameters\nas LAMB, and'
<EOS>
b'(3) ADAMW with tuned hyperparameters.'
<EOS>
b'ADAMW stops scaling at the batch size of\n16K because it is not able to achieve the target F1 score (88.1 vs 90.4).'
<EOS>
b'The tuning information of\nADAMW is shown in the Appendix.'
<EOS>
b'For 64K/32K mixed-batch training, even after extensive tuning\nof the hyperparameters, we fail to get any reasonable result with ADAMW optimizer.'
<EOS>
b'We conclude\nthat ADAMW does not work well in large-batch BERT training or is at least hard to tune.'
<EOS>
b'We also\nobserve that LAMB performs better than LARS for all batch sizes (see Table 2).'
<EOS>
b'Table 2:'
<EOS>
b'LAMB achieves a higher performance (F1 score) than LARS for all the batch sizes.'
<EOS>
b'The\nbaseline achieves a F1 score of 90.390.'
<EOS>
b'Thus, LARS stops scaling at the batch size of 16K.\n\nBatch Size\n\n512'
<EOS>
b'1'
<EOS>
b'K\n\n2'
<EOS>
b'K\n\n4'
<EOS>
b'K\n\n8'
<EOS>
b'K\n\n16'
<EOS>
b'K\n\n32'
<EOS>
b'K'
<EOS>
b'LARS'
<EOS>
b'LAMB'
<EOS>
b'90.717'
<EOS>
b'91.752'
<EOS>
b'90.369'
<EOS>
b'91.761'
<EOS>
b'90.748'
<EOS>
b'91.946'
<EOS>
b'90.537'
<EOS>
b'91.137'
<EOS>
b'90.548'
<EOS>
b'91.263'
<EOS>
b'89.589'
<EOS>
b'91.345'
<EOS>
b'diverge'
<EOS>
b'91.475'
<EOS>
b'4.2'
<EOS>
b'IMAGENET TRAINING WITH RESNET-50.'
<EOS>
b'ImageNet training with ResNet-50 is an industry standard metric that is being used in MLPerf4.'
<EOS>
b'The baseline can get 76.3% top-1 accuracy in 90 epochs (Goyal et al., 2017).'
<EOS>
b'All the successful\nimplementations are based on momentum SGD (He et al., 2016; Goyal et al., 2017) or LARS optimizer'
<EOS>
b'(Ying et al., 2018; Jia et al., 2018; Mikami et al., 2018; You et al., 2018; Yamazaki et al., 2019).'
<EOS>
b'Before our study, we did not \xef\xac\x81nd any paper reporting a state-of-the-art accuracy achieved by ADAM,\n\n4https://mlperf.org/'
<EOS>
b'7'
<EOS>
b'Published as a conference paper at ICLR 2020\n\nADAGRAD, or ADAMW optimizer.'
<EOS>
b'In our experiments, even with comprehensive hyper-parameter\ntuning, ADAGRAD/ADAM/ADAMW (with batch size 16K) only achieves 55.38%/66.04%/67.27%'
<EOS>
b'top-1 accuracy.'
<EOS>
b'After adding learning rate scheme of Goyal et al.'
<EOS>
b'(2017), the top-1 accuracy of\nADAGRAD/ADAM/ADAMW was improved to 72.0%/73.48%/73.07%.'
<EOS>
b'However, they are still much\nlower than 76.3%.'
<EOS>
b'The details of the tuning information are in the Appendix.'
<EOS>
b'Table 3 shows that\nLAMB can achieve the target accuracy.'
<EOS>
b'Beyond a batch size of 8K, LAMB\xe2\x80\x99s accuracy is higher than\nthe momentum.'
<EOS>
b'LAMB\xe2\x80\x99s'
<EOS>
b'accuracy is also slightly better than LARS.'
<EOS>
b'At a batch size of 32K, LAMB\nachieves 76.4% top-1 accuracy while LARS achieves 76.3%.'
<EOS>
b'At a batch size of 2K, LAMB is able to\nachieve 77.11% top-1 accuracy while LARS achieves 76.6%.'
<EOS>
b'Table 3:'
<EOS>
b'Top-1'
<EOS>
b'validation accuracy of ImageNet/RESNET-50 training at the batch size of 16K (90\nepochs).'
<EOS>
b'The performance of momentum was reported by (Goyal et al., 2017).'
<EOS>
b'+ means adding the\nlearning rate scheme of Goyal et al.'
<EOS>
b'(2017) to the optimizer: (1) 5-epoch warmup to stablize the initial\nstage; and (2) multiply the learning rate by 0.1 at 30th, 60th, and 80th epoch.'
<EOS>
b'The target accuracy is\naround 0.763'
<EOS>
b'(Goyal et al., 2017).'
<EOS>
b'All the adaptive solvers were comprehensively tuned.'
<EOS>
b'The tuning\ninformation was in the Appendix.'
<EOS>
b'adagrad/adagrad+\n\nadamw/adamw+ momentum\n\nadam/'
<EOS>
b'adam+'
<EOS>
b'optimizer'
<EOS>
b'lamb'
<EOS>
b'Accuracy'
<EOS>
b'0.5538/0.7201'
<EOS>
b'0.6604/0.7348'
<EOS>
b'0.6727/0.7307'
<EOS>
b'0.7520'
<EOS>
b'0.7666'
<EOS>
b'4.3 HYPERPARAMETERS FOR SCALING'
<EOS>
b'THE BATCH SIZE'
<EOS>
b'For BERT and ImageNet training, we did not tune the hyperparameters of LAMB optimizer when\nincreasing the batch size.'
<EOS>
b'We use the square root LR scaling rule and linear-epoch warmup scheduling\nto automatically adjust learning rate.'
<EOS>
b'The details can be found in Tables 4 and 5\n\nTable 4: Untuned LAMB for BERT training across different batch sizes (\xef\xac\x81xed #epochs).'
<EOS>
b'We use\nsquare root LR scaling and linear-epoch warmup.'
<EOS>
b'For example, batch size 32K needs to \xef\xac\x81nish 15625\niterations.'
<EOS>
b'It uses 0.2\xc3\x9715625 = 3125 iterations for learning rate warmup.'
<EOS>
b'BERT'
<EOS>
b'\xe2\x80\x99s baseline achieved a\nF1 score of 90.395.'
<EOS>
b'We can achieve an even higher F1 score if we manually tune the hyperparameters.'
<EOS>
b'Batch Size'
<EOS>
b'512'
<EOS>
b'Learning Rate'
<EOS>
b'Warmup Ratio'
<EOS>
b'F1 score'
<EOS>
b'Exact Match'
<EOS>
b'5'
<EOS>
b'1'
<EOS>
b'320'
<EOS>
b'91.752'
<EOS>
b'85.090'
<EOS>
b'23.0\xc3\x97103'
<EOS>
b'22.5\xc3\x97103'
<EOS>
b'22.0\xc3\x97103'
<EOS>
b'21.5\xc3\x97103'
<EOS>
b'21.0\xc3\x97103'
<EOS>
b'20.5\xc3\x97103'
<EOS>
b'20.0\xc3\x97103'
<EOS>
b'1K'
<EOS>
b'5'
<EOS>
b'1'
<EOS>
b'160'
<EOS>
b'2K\n5'
<EOS>
b'1'
<EOS>
b'80'
<EOS>
b'4K'
<EOS>
b'5\n\n1'
<EOS>
b'40'
<EOS>
b'8K\n5'
<EOS>
b'1'
<EOS>
b'20'
<EOS>
b'16K\n\n32'
<EOS>
b'K\n\n5\n\n1'
<EOS>
b'10'
<EOS>
b'5'
<EOS>
b'1'
<EOS>
b'5\n\n91.761'
<EOS>
b'85.260\n\n91.946'
<EOS>
b'85.355'
<EOS>
b'91.137'
<EOS>
b'84.172'
<EOS>
b'91.263'
<EOS>
b'84.901'
<EOS>
b'91.345'
<EOS>
b'84.816'
<EOS>
b'91.475'
<EOS>
b'84.939'
<EOS>
b'Table 5: Untuned LAMB for ImageNet training with RESNET-50 for different batch sizes (90 epochs).'
<EOS>
b'We use square root LR scaling and linear-epoch warmup.'
<EOS>
b'The baseline Goyal et al.'
<EOS>
b'(2017) gets 76.3%'
<EOS>
b'top-1 accuracy in 90 epochs.'
<EOS>
b'Stanford DAWN Bench (Coleman et al., 2017) baseline achieves 93%'
<EOS>
b'top-5 accuracy.'
<EOS>
b'LAMB achieves both of them.'
<EOS>
b'LAMB can achieve an even higher accuracy if we\nmanually tune the hyperparameters.'
<EOS>
b'Batch Size'
<EOS>
b'Learning Rate'
<EOS>
b'Warmup Epochs'
<EOS>
b'Top-5 Accuracy'
<EOS>
b'Top-1 Accuracy'
<EOS>
b'512'
<EOS>
b'4'
<EOS>
b'23.0\xc3\x97100'
<EOS>
b'0.3125'
<EOS>
b'0.9335'
<EOS>
b'0.7696'
<EOS>
b'1K'
<EOS>
b'4'
<EOS>
b'22.5\xc3\x97100'
<EOS>
b'0.625'
<EOS>
b'0.9349'
<EOS>
b'0.7706'
<EOS>
b'2K'
<EOS>
b'4'
<EOS>
b'1.25'
<EOS>
b'0.9353'
<EOS>
b'0.7711\n\n22.0\xc3\x97100\n\n21.5\xc3\x97100'
<EOS>
b'21.0\xc3\x97100'
<EOS>
b'20.5\xc3\x97100'
<EOS>
b'20.0\xc3\x97100'
<EOS>
b'4K'
<EOS>
b'4\n\n2.5'
<EOS>
b'8K'
<EOS>
b'4'
<EOS>
b'5'
<EOS>
b'16'
<EOS>
b'K\n\n4\n\n10'
<EOS>
b'32'
<EOS>
b'K\n\n4\n\n20\n\n0.9332'
<EOS>
b'0.7692\n\n0.9331'
<EOS>
b'0.7689'
<EOS>
b'0.9322'
<EOS>
b'0.7666'
<EOS>
b'0.9308'
<EOS>
b'0.7642'
<EOS>
b'5 CONCLUSION'
<EOS>
b'Large batch techniques are critical to speeding up deep neural network training.'
<EOS>
b'In this paper, we\npropose the LAMB optimizer, which supports adaptive elementwise updating and layerwise learning'
<EOS>
b'8'
<EOS>
b'Published as a conference paper at ICLR 2020\n\nrates.'
<EOS>
b'Furthermore, LAMB is a general purpose optimizer that works for both small and large batches.'
<EOS>
b'We also provided theoretical analysis for the LAMB optimizer, highlighting the cases where it\nperforms better than standard SGD.'
<EOS>
b'LAMB achieves a better performance than existing optimizers for\na wide range of applications.'
<EOS>
b'By using LAMB, we are able to scale the batch size of BERT pre-training\nto 64K without losing accuracy, thereby, reducing the BERT training time from 3 days to around 76\nminutes.'
<EOS>
b'LAMB is also the \xef\xac\x81rst large batch adaptive solver that can achieve state-of-the-art accuracy\non ImageNet training with RESNET-50.'
<EOS>
b'6 ACKNOWLEDGEMENT'
<EOS>
b'We want to thank the comments from George Dahl and Jeff Dean.'
<EOS>
b'We want to thank Michael Ban\xef\xac\x81eld,\nDehao Chen, Youlong Cheng, Sameer Kumar, and Zak Stone for TPU Pod support.'
<EOS>
b'REFERENCES'
<EOS>
b'Takuya Akiba, Shuji Suzuki, and Keisuke Fukuda.'
<EOS>
b'Extremely large minibatch sgd:'
<EOS>
b'Training resnet-50\n\non imagenet in 15 minutes.'
<EOS>
b'arXiv preprint arXiv:1711.04325, 2017.'
<EOS>
b'Yoshua Bengio.'
<EOS>
b'Practical recommendations for gradient-based training of deep architectures.'
<EOS>
b'In\n\nNeural networks: Tricks of the trade, pp. 437\xe2\x80\x93478.'
<EOS>
b'Springer, 2012.'
<EOS>
b'Jeremy Bernstein, Yu-Xiang Wang, Kamyar Azizzadenesheli, and Anima Anandkumar.'
<EOS>
b'signsgd:'
<EOS>
b'compressed optimisation for non-convex problems.'
<EOS>
b'CoRR, abs/1802.04434, 2018.'
<EOS>
b'Valeriu Codreanu, Damian Podareanu, and Vikram Saletore.'
<EOS>
b'Scale out for large minibatch sgd:'
<EOS>
b'Residual network training on imagenet-1k with improved accuracy and reduced time to train.'
<EOS>
b'arXiv'
<EOS>
b'preprint arXiv:1711.04291, 2017.'
<EOS>
b'Cody Coleman, Deepak Narayanan, Daniel Kang, Tian Zhao, Jian Zhang, Luigi Nardi, Peter Bailis,\nKunle Olukotun, Chris R\xc3\xa9, and Matei Zaharia.'
<EOS>
b'Dawnbench:'
<EOS>
b'An end-to-end deep learning bench-\nmark and competition.'
<EOS>
b'Training, 100(101):102, 2017.'
<EOS>
b'Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao, Andrew Senior,\nPaul Tucker, Ke Yang, Quoc V Le, et al.'
<EOS>
b'Large scale distributed deep networks.'
<EOS>
b'In Advances in\nneural information processing systems, pp. 1223\xe2\x80\x931231, 2012.'
<EOS>
b'Aditya Devarakonda, Maxim Naumov, and Michael Garland.'
<EOS>
b'Adabatch:'
<EOS>
b'Adaptive batch sizes for\n\ntraining deep neural networks.'
<EOS>
b'arXiv preprint arXiv:1712.02029, 2017.'
<EOS>
b'Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.'
<EOS>
b'Bert: Pre-training of deep\n\nbidirectional transformers for language understanding.'
<EOS>
b'arXiv preprint arXiv:1810.04805, 2018.'
<EOS>
b'Timothy Dozat.'
<EOS>
b'Incorporating nesterov momentum into adam.'
<EOS>
b'2016.'
<EOS>
b'Saeed Ghadimi and Guanghui Lan.'
<EOS>
b'Stochastic \xef\xac\x81rst-'
<EOS>
b'and zeroth-order methods for nonconvex\nstochastic programming.'
<EOS>
b'SIAM Journal on Optimization, 23(4):2341\xe2\x80\x932368, 2013a.'
<EOS>
b'doi:'
<EOS>
b'10.1137/'
<EOS>
b'120880811.'
<EOS>
b'Saeed Ghadimi and Guanghui Lan.'
<EOS>
b'Stochastic \xef\xac\x81rst-and zeroth-order methods for nonconvex stochastic\n\nprogramming.'
<EOS>
b'SIAM Journal on Optimization, 23(4):2341\xe2\x80\x932368, 2013b.'
<EOS>
b'Saeed Ghadimi, Guanghui Lan, and Hongchao Zhang.'
<EOS>
b'Mini-batch stochastic approximation methods\nfor nonconvex stochastic composite optimization.'
<EOS>
b'Mathematical Programming, 155(1-2):267\xe2\x80\x93305,\n2014.'
<EOS>
b'Priya Goyal, Piotr Doll\xc3\xa1r, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola,\nAndrew Tulloch, Yangqing Jia, and Kaiming He.'
<EOS>
b'Accurate, large minibatch sgd:'
<EOS>
b'Training imagenet\nin 1 hour.'
<EOS>
b'arXiv preprint arXiv:1706.02677, 2017.'
<EOS>
b'Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.'
<EOS>
b'Deep residual learning for image\nrecognition.'
<EOS>
b'In Proceedings of the IEEE conference on computer vision and pattern recognition,\npp. 770\xe2\x80\x93778, 2016.'
<EOS>
b'Elad Hoffer, Itay Hubara, and Daniel Soudry.'
<EOS>
b'Train longer, generalize better: closing the generalization\n\ngap in large batch training of neural networks.'
<EOS>
b'arXiv preprint arXiv:1705.08741, 2017.'
<EOS>
b'9'
<EOS>
b'Published as a conference paper at ICLR 2020'
<EOS>
b'Forrest N Iandola, Matthew W Moskewicz, Khalid Ashraf, and Kurt Keutzer.'
<EOS>
b'Firecaffe: near-linear\nacceleration of deep neural network training on compute clusters.'
<EOS>
b'In Proceedings of the IEEE'
<EOS>
b'Conference on Computer Vision and Pattern Recognition, pp. 2592\xe2\x80\x932600, 2016.'
<EOS>
b'Xianyan Jia, Shutao Song,'
<EOS>
b'Wei He, Yangzihao Wang, Haidong Rong, Feihu Zhou, Liqiang Xie,\nZhenyu Guo, Yuanzhou Yang, Liwei Yu,'
<EOS>
b'et al.'
<EOS>
b'Highly scalable deep learning training system with\nmixed-precision: Training imagenet in four minutes.'
<EOS>
b'arXiv preprint arXiv:1807.11205, 2018.'
<EOS>
b'Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter'
<EOS>
b'Tang.'
<EOS>
b'On large-batch training for deep learning: Generalization gap and sharp minima.'
<EOS>
b'arXiv'
<EOS>
b'preprint arXiv:1609.04836, 2016.'
<EOS>
b'Alex Krizhevsky.'
<EOS>
b'One weird trick for parallelizing convolutional neural networks.'
<EOS>
b'arXiv preprint'
<EOS>
b'Mu Li.'
<EOS>
b'Scaling Distributed Machine Learning with System and Algorithm Co-design.'
<EOS>
b'PhD thesis,\n\narXiv:1404.5997, 2014.'
<EOS>
b'Intel, 2017.'
<EOS>
b'James Martens and Roger Grosse.'
<EOS>
b'Optimizing neural networks with kronecker-factored approximate\n\ncurvature.'
<EOS>
b'In International conference on machine learning, pp. 2408\xe2\x80\x932417, 2015.'
<EOS>
b'Hiroaki Mikami, Hisahiro Suganuma, Yoshiki Tanaka, Yuichi Kageyama, et al.'
<EOS>
b'Imagenet/resnet-50'
<EOS>
b'training in 224 seconds.'
<EOS>
b'arXiv preprint arXiv:1811.05233, 2018.'
<EOS>
b'Yurii E Nesterov.'
<EOS>
b'A method for solving the convex programming problem with convergence rate'
<EOS>
b'o'
<EOS>
b'(1/k\xcb\x86 2).'
<EOS>
b'In Dokl.'
<EOS>
b'akad.'
<EOS>
b'nauk Sssr, volume 269, pp. 543\xe2\x80\x93547, 1983.'
<EOS>
b'Kazuki Osawa, Yohei Tsuji, Yuichiro Ueno, Akira Naruse, Rio Yokota, and Satoshi Matsuoka.'
<EOS>
b'Second-order optimization method for large mini-batch:'
<EOS>
b'Training resnet-50 on imagenet in 35\nepochs.'
<EOS>
b'arXiv preprint arXiv:1811.12019, 2018.'
<EOS>
b'Benjamin Recht, Christopher Re, Stephen Wright, and Feng Niu.'
<EOS>
b'Hogwild:'
<EOS>
b'A lock-free approach to\nparallelizing stochastic gradient descent.'
<EOS>
b'In Advances in neural information processing systems,\npp. 693\xe2\x80\x93701, 2011.'
<EOS>
b'Christopher J Shallue, Jaehoon Lee, Joe Antognini, Jascha Sohl-Dickstein, Roy Frostig, and George E'
<EOS>
b'Dahl.'
<EOS>
b'Measuring the effects of data parallelism on neural network training.'
<EOS>
b'arXiv preprint'
<EOS>
b'arXiv:1811.03600, 2018.'
<EOS>
b'Samuel L Smith, Pieter-Jan Kindermans, and Quoc V Le.'
<EOS>
b'Don\xe2\x80\x99t decay the learning rate, increase the\n\nbatch size.'
<EOS>
b'arXiv preprint arXiv:1711.00489, 2017.'
<EOS>
b'Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton.'
<EOS>
b'On the importance of initialization\nand momentum in deep learning.'
<EOS>
b'In International conference on machine learning, pp. 1139\xe2\x80\x931147,\n2013.'
<EOS>
b'Masafumi Yamazaki, Akihiko Kasagi, Akihiro Tabuchi, Takumi Honda, Masahiro Miwa, Naoto'
<EOS>
b'Fukumoto, Tsuguchika Tabaru, Atsushi Ike, and Kohta Nakashima.'
<EOS>
b'Yet another accelerated sgd:'
<EOS>
b'Resnet-50 training on imagenet in 74.7 seconds.'
<EOS>
b'arXiv preprint arXiv:1903.12650, 2019.'
<EOS>
b'Chris Ying, Sameer Kumar, Dehao Chen, Tao Wang, and Youlong Cheng.'
<EOS>
b'Image classi\xef\xac\x81cation at\n\nsupercomputer scale.'
<EOS>
b'arXiv preprint arXiv:1811.06992, 2018.'
<EOS>
b'Yang You, Igor Gitman, and Boris Ginsburg.'
<EOS>
b'Scaling sgd batch size to 32k for imagenet training.'
<EOS>
b'arXiv preprint arXiv:1708.03888, 2017.'
<EOS>
b'Yang You, Zhao Zhang, Cho-Jui Hsieh, James Demmel, and Kurt Keutzer.'
<EOS>
b'Imagenet training in\nminutes.'
<EOS>
b'In Proceedings of the 47th International Conference on Parallel Processing, pp. 1.'
<EOS>
b'ACM,\n2018.'
<EOS>
b'Yang You, Jonathan Hseu, Chris Ying, James Demmel, Kurt Keutzer, and Cho-Jui Hsieh.'
<EOS>
b'Large-batch\n\ntraining for lstm and beyond.'
<EOS>
b'arXiv preprint arXiv:1901.08256, 2019.'
<EOS>
b'10'
<EOS>
b'Published as a conference paper at ICLR 2020'
<EOS>
b'APPENDIX'
<EOS>
b'A PROOF OF THEOREM 2\n\nProof.'
<EOS>
b'We analyze the convergence of LARS for general minibatch size here.'
<EOS>
b'Recall that the update\nof LARS is the following\n\nx(i)'
<EOS>
b't+1'
<EOS>
b'= x(i)'
<EOS>
b't \xe2\x88\x92 \xce\xb7t\xcf\x86((cid:107)x(i)\n\nt (cid:107))'
<EOS>
b'g(i)'
<EOS>
b't\n(cid:107)g(i)'
<EOS>
b't (cid:107)\n\n,\n\nfor all i \xe2\x88\x88 [h].'
<EOS>
b'For simplicity of notation, we reason the'
<EOS>
b'Since the function f is L-smooth, we have the following:\n\nf'
<EOS>
b'(xt+1) \xe2\x89\xa4 f (xt)'
<EOS>
b'+ (cid:104)\xe2\x88\x87if (xt), x(i)'
<EOS>
b't+1 \xe2\x88\x92 x(i)'
<EOS>
b't (cid:105)'
<EOS>
b'+'
<EOS>
b'(cid:107)x(i)'
<EOS>
b't+1 \xe2\x88\x92 x(i)'
<EOS>
b't (cid:107)2\n\n= f (xt) \xe2\x88\x92'
<EOS>
b'\xce\xb7t'
<EOS>
b'\xcf\x86((cid:107)x(i)\n\nt (cid:107))'
<EOS>
b'\xc3\x97'
<EOS>
b'[\xe2\x88\x87if ('
<EOS>
b'xt)]j'
<EOS>
b'\xc3\x97\n\n\xe2\x89\xa4 f (xt) \xe2\x88\x92'
<EOS>
b'\xce\xb7t'
<EOS>
b'\xcf\x86((cid:107)x(i)\n\nt (cid:107))'
<EOS>
b'\xc3\x97'
<EOS>
b'[\xe2\x88\x87if ('
<EOS>
b'xt)]j \xc3\x97'
<EOS>
b'(cid:32)'
<EOS>
b'(cid:32)'
<EOS>
b'h'
<EOS>
b'(cid:88)'
<EOS>
b'Li\xce\xb72'
<EOS>
b't \xcf\x862((cid:107)x(i)\n\nt (cid:107))'
<EOS>
b'+'
<EOS>
b'i=1'
<EOS>
b'2'
<EOS>
b'\xe2\x88\x92'
<EOS>
b'[\xe2\x88\x87if (xt)]j'
<EOS>
b'(cid:107)\xe2\x88\x87if (xt)(cid:107)\n\n+'
<EOS>
b'[\xe2\x88\x87if (xt)]j'
<EOS>
b'(cid:107)\xe2\x88\x87if (xt)(cid:107)\n\n(cid:33)(cid:33)\n\n+\n\nt \xce\xb12'
<EOS>
b'\xce\xb72'
<EOS>
b'u'
<EOS>
b'2'
<EOS>
b'(cid:107)L(cid:107)1\n\n= f (xt) \xe2\x88\x92'
<EOS>
b'\xce\xb7t'
<EOS>
b'\xcf\x86((cid:107)x(i)\n\nt (cid:107)) \xc3\x97 (cid:107)\xe2\x88\x87if (xt)(cid:107) \xe2\x88\x92 \xce\xb7t'
<EOS>
b'[\xe2\x88\x87if ('
<EOS>
b'xt)]j \xc3\x97'
<EOS>
b'(cid:32) g(i)'
<EOS>
b't,j'
<EOS>
b'(cid:107)g(i)'
<EOS>
b't (cid:107)'
<EOS>
b'\xe2\x88\x92'
<EOS>
b'[\xe2\x88\x87if (xt)]j'
<EOS>
b'(cid:107)\xe2\x88\x87if (xt)(cid:107)\n\n+\n\nt \xce\xb12'
<EOS>
b'\xce\xb72'
<EOS>
b'u'
<EOS>
b'2'
<EOS>
b'(cid:107)L(cid:107)1\n\n(cid:33)(cid:33)'
<EOS>
b'(4)\n\nh'
<EOS>
b'(cid:88)'
<EOS>
b'i=1'
<EOS>
b'Li'
<EOS>
b'2'
<EOS>
b'(cid:33)\n\ng(i)'
<EOS>
b't,j'
<EOS>
b'(cid:107)g(i)'
<EOS>
b't (cid:107)\n(cid:32) g(i)'
<EOS>
b't,j'
<EOS>
b'(cid:107)g(i)'
<EOS>
b't (cid:107)\n(cid:32)\ndi(cid:88)'
<EOS>
b'h'
<EOS>
b'(cid:88)'
<EOS>
b'i=1'
<EOS>
b'j=1'
<EOS>
b'The \xef\xac\x81rst inequality follows from the lipschitz continuous nature of the gradient.'
<EOS>
b'Let \xe2\x88\x86(i)'
<EOS>
b'\xe2\x88\x87if (xt).'
<EOS>
b'Then the above inequality can be rewritten in the following manner:'
<EOS>
b't = g(i)'
<EOS>
b't \xe2\x88\x92\n\nf'
<EOS>
b'(xt+1) \xe2\x89\xa4 f (xt) \xe2\x88\x92'
<EOS>
b'\xce\xb7t'
<EOS>
b'\xcf\x86((cid:107)x(i)\n\nt (cid:107))(cid:107)\xe2\x88\x87if (xt)(cid:107)\n\nh\n(cid:88)'
<EOS>
b'i=1'
<EOS>
b'\xe2\x88\x92'
<EOS>
b'\xce\xb7t'
<EOS>
b'h'
<EOS>
b'(cid:88)\n\ndi(cid:88)'
<EOS>
b'i=1'
<EOS>
b'j=1'
<EOS>
b'(cid:32)'
<EOS>
b'\xcf\x86((cid:107)x(i)\n\nt (cid:107))'
<EOS>
b'\xc3\x97'
<EOS>
b'[\xe2\x88\x87if ('
<EOS>
b'xt)]j \xc3\x97'
<EOS>
b'(cid:32)'
<EOS>
b'(\xe2\x88\x86(i)'
<EOS>
b'(cid:107)\xe2\x88\x86(i)'
<EOS>
b't,j + [\xe2\x88\x87if'
<EOS>
b'(xt)]j)\nt + \xe2\x88\x87if'
<EOS>
b'(xt)(cid:107)'
<EOS>
b'(cid:33)(cid:33)'
<EOS>
b'\xe2\x88\x92'
<EOS>
b'[\xe2\x88\x87if (xt)]j'
<EOS>
b'(cid:107)\xe2\x88\x87if (xt)(cid:107)\n\n+\n\nt \xce\xb12'
<EOS>
b'\xce\xb72'
<EOS>
b'u'
<EOS>
b'2'
<EOS>
b'(cid:107)L(cid:107)1\n\n= f (xt) \xe2\x88\x92'
<EOS>
b'\xce\xb7t'
<EOS>
b'\xcf\x86((cid:107)x(i)'
<EOS>
b't (cid:107))(cid:107)\xe2\x88\x87if (xt)(cid:107)\n\n\xe2\x88\x92'
<EOS>
b'\xce\xb7t'
<EOS>
b'\xcf\x86((cid:107)x(i)\n\nt (cid:107))'
<EOS>
b'\xc3\x97'
<EOS>
b'(cid:32)'
<EOS>
b'(cid:104)\xe2\x88\x86(i)'
<EOS>
b't + \xe2\x88\x87if (xt), \xe2\x88\x87if (xt)(cid:105)'
<EOS>
b'(cid:107)\xe2\x88\x86(i)'
<EOS>
b't + \xe2\x88\x87if'
<EOS>
b'(xt)(cid:107)\n\n\xe2\x88\x92 (cid:107)\xe2\x88\x87if (xt)(cid:107)\n\n+'
<EOS>
b'(cid:107)L(cid:107)1\n\n(cid:33)\n\n\xce\xb72\nt \xce\xb12'
<EOS>
b'u'
<EOS>
b'2'
<EOS>
b'= f (xt) \xe2\x88\x92'
<EOS>
b'\xce\xb7t'
<EOS>
b'\xcf\x86((cid:107)x(i)'
<EOS>
b't (cid:107))(cid:107)\xe2\x88\x87if (xt)(cid:107)\n\n+'
<EOS>
b'\xce\xb7t'
<EOS>
b'\xcf\x86((cid:107)x(i)\n\nt (cid:107))'
<EOS>
b'\xc3\x97'
<EOS>
b'(cid:32)'
<EOS>
b'(cid:107)\xe2\x88\x87if (xt)(cid:107)(cid:107)\xe2\x88\x86(i)'
<EOS>
b't + \xe2\x88\x87if (xt)(cid:107) \xe2\x88\x92 (cid:104)\xe2\x88\x86(i)'
<EOS>
b't + \xe2\x88\x87if'
<EOS>
b'(xt)(cid:107)'
<EOS>
b'(cid:107)\xe2\x88\x86(i)'
<EOS>
b't + \xe2\x88\x87if (xt), \xe2\x88\x87if (xt)(cid:105)'
<EOS>
b'(cid:33)'
<EOS>
b'+\n\n\xce\xb72'
<EOS>
b't \xce\xb12'
<EOS>
b'u'
<EOS>
b'2'
<EOS>
b'(cid:107)L(cid:107)1\n\n= f (xt) \xe2\x88\x92'
<EOS>
b'\xce\xb7t'
<EOS>
b'\xcf\x86((cid:107)x(i)\n\nt (cid:107))(cid:107)\xe2\x88\x87if (xt)(cid:107) +\n\n\xce\xb72'
<EOS>
b't \xce\xb12'
<EOS>
b'u'
<EOS>
b'2'
<EOS>
b'(cid:107)L(cid:107)1'
<EOS>
b'+ \xce\xb7t'
<EOS>
b'\xcf\x86((cid:107)x(i)\n\nt (cid:107))'
<EOS>
b'\xc3\x97'
<EOS>
b'h'
<EOS>
b'(cid:88)'
<EOS>
b'i=1'
<EOS>
b'(cid:32)'
<EOS>
b'(cid:107)\xe2\x88\x87if (xt)(cid:107)(cid:107)\xe2\x88\x86(i)'
<EOS>
b't + \xe2\x88\x87if'
<EOS>
b'(xt)(cid:107) \xe2\x88\x92'
<EOS>
b'(cid:107)\xe2\x88\x86(i)'
<EOS>
b't + \xe2\x88\x87if'
<EOS>
b'(xt)(cid:107)2'
<EOS>
b'+ (cid:104)\xe2\x88\x86(i)\n\nt\n\n, \xe2\x88\x86(i)'
<EOS>
b't + \xe2\x88\x87if (xt)(cid:105)'
<EOS>
b'(cid:33)\n\n.'
<EOS>
b'(cid:107)\xe2\x88\x86(i)'
<EOS>
b't + \xe2\x88\x87if'
<EOS>
b'(xt)(cid:107)'
<EOS>
b'(5)\n\nh\n(cid:88)\n\ndi(cid:88)'
<EOS>
b'i=1'
<EOS>
b'j=1'
<EOS>
b'h'
<EOS>
b'(cid:88)\n\ndi(cid:88)'
<EOS>
b'i=1'
<EOS>
b'j=1'
<EOS>
b'h'
<EOS>
b'(cid:88)'
<EOS>
b'i=1'
<EOS>
b'h'
<EOS>
b'(cid:88)'
<EOS>
b'i=1'
<EOS>
b'h'
<EOS>
b'(cid:88)'
<EOS>
b'i=1'
<EOS>
b'h'
<EOS>
b'(cid:88)'
<EOS>
b'i=1'
<EOS>
b'h'
<EOS>
b'(cid:88)'
<EOS>
b'i=1'
<EOS>
b'h'
<EOS>
b'(cid:88)'
<EOS>
b'i=1'
<EOS>
b'11'
<EOS>
b'Published as a conference paper at ICLR 2020\n\nUsing Cauchy-Schwarz inequality in the above inequality, we have:\n\nf'
<EOS>
b'(xt+1) \xe2\x89\xa4 f (xt) \xe2\x88\x92'
<EOS>
b'\xce\xb7t'
<EOS>
b'\xcf\x86((cid:107)x(i)\n\nt (cid:107))(cid:107)\xe2\x88\x87if (xt)(cid:107)\n\nh\n(cid:88)'
<EOS>
b'i=1'
<EOS>
b'h'
<EOS>
b'(cid:88)'
<EOS>
b'i=1'
<EOS>
b'h'
<EOS>
b'(cid:88)'
<EOS>
b'i=1'
<EOS>
b'h'
<EOS>
b'(cid:88)'
<EOS>
b'i=1'
<EOS>
b'h'
<EOS>
b'(cid:88)'
<EOS>
b'i=1'
<EOS>
b'h'
<EOS>
b'(cid:88)'
<EOS>
b'i=1'
<EOS>
b'h'
<EOS>
b'(cid:88)'
<EOS>
b'i=1'
<EOS>
b'+ \xce\xb7t'
<EOS>
b'\xcf\x86((cid:107)x(i)\n\nt (cid:107))'
<EOS>
b'\xc3\x97'
<EOS>
b'(cid:107)\xe2\x88\x87if (xt)(cid:107) \xe2\x88\x92'
<EOS>
b'(cid:107)\xe2\x88\x86(i)'
<EOS>
b't + \xe2\x88\x87if'
<EOS>
b'(xt)(cid:107)'
<EOS>
b'+ (cid:107)\xe2\x88\x86(i)'
<EOS>
b't (cid:107)\n\n(cid:16)'
<EOS>
b'(cid:17)'
<EOS>
b'+\n\n\xce\xb72'
<EOS>
b't \xce\xb12'
<EOS>
b'u'
<EOS>
b'2'
<EOS>
b'(cid:107)L(cid:107)1\n\n\xe2\x89\xa4 f (xt) \xe2\x88\x92 \xce\xb7t'
<EOS>
b'\xcf\x86((cid:107)x(i)'
<EOS>
b't (cid:107))(cid:107)\xe2\x88\x87if (xt)(cid:107) + 2\xce\xb7t'
<EOS>
b'\xcf\x86((cid:107)x(i)\n\nt (cid:107)) \xc3\x97 (cid:107)\xe2\x88\x86(i)\n\nt (cid:107) +\n\n\xce\xb72'
<EOS>
b't \xce\xb12'
<EOS>
b'u'
<EOS>
b'2'
<EOS>
b'(cid:107)L(cid:107)1\n\nTaking expectation, we obtain the following:'
<EOS>
b'E[f (xt+1)]'
<EOS>
b'\xe2\x89\xa4 f (xt) \xe2\x88\x92'
<EOS>
b'\xce\xb7t'
<EOS>
b'\xcf\x86((cid:107)x(i)'
<EOS>
b't (cid:107))(cid:107)\xe2\x88\x87if (xt)(cid:107) + 2\xce\xb7t'
<EOS>
b'\xcf\x86((cid:107)x(i)\n\nt (cid:107)) \xc3\x97'
<EOS>
b'E[(cid:107)\xe2\x88\x86(i)\n\nt (cid:107)]'
<EOS>
b'+\n\n\xe2\x89\xa4 f'
<EOS>
b'(xt) \xe2\x88\x92 \xce\xb7t\xce\xb1l'
<EOS>
b'(cid:107)\xe2\x88\x87if (xt)(cid:107) + 2\xce\xb7t\xce\xb1u'
<EOS>
b'(cid:107)\xcf\x83(cid:107)1\xe2\x88\x9a'
<EOS>
b'b'
<EOS>
b'+\n\nt \xce\xb12'
<EOS>
b'\xce\xb72'
<EOS>
b'u'
<EOS>
b'2'
<EOS>
b'(cid:107)L(cid:107)1.'
<EOS>
b'\xce\xb72'
<EOS>
b't \xce\xb12'
<EOS>
b'u'
<EOS>
b'2'
<EOS>
b'(cid:107)L(cid:107)1'
<EOS>
b'(6)\n\nSumming the above inequality for t'
<EOS>
b'= 1 to T and using telescoping sum'
<EOS>
b', we have the following\ninequality:\n\nE[f'
<EOS>
b'(xT +1)] \xe2\x89\xa4 f (x1) \xe2\x88\x92 \xce\xb7\xce\xb1l\n\nE[(cid:107)\xe2\x88\x87if'
<EOS>
b'(xt)(cid:107)]'
<EOS>
b'+ 2\xce\xb7T'
<EOS>
b'\xce\xb1u(cid:107)\xcf\x83(cid:107)1\xe2\x88\x9a'
<EOS>
b'+'
<EOS>
b'b'
<EOS>
b'\xce\xb72\xce\xb12\nuT'
<EOS>
b'2'
<EOS>
b'(cid:107)L(cid:107)1.'
<EOS>
b'T\n(cid:88)'
<EOS>
b'h'
<EOS>
b'(cid:88)'
<EOS>
b't=1'
<EOS>
b'i=1'
<EOS>
b'Rearranging the terms of the above inequality, and dividing by \xce\xb7T \xce\xb1l, we have:\n\n1'
<EOS>
b'T\n\nT\n(cid:88)'
<EOS>
b'h'
<EOS>
b'(cid:88)'
<EOS>
b't=1'
<EOS>
b'i=1'
<EOS>
b'E[(cid:107)\xe2\x88\x87if'
<EOS>
b'(xt)(cid:107)]'
<EOS>
b'\xe2\x89\xa4'
<EOS>
b'f (x1)'
<EOS>
b'\xe2\x88\x92'
<EOS>
b'E[f'
<EOS>
b'(xT +1)]'
<EOS>
b'T \xce\xb7\xce\xb1l'
<EOS>
b'2\xce\xb1u(cid:107)\xcf\x83(cid:107)1'
<EOS>
b'+'
<EOS>
b'\xe2\x88\x9a'
<EOS>
b'+'
<EOS>
b'b\xce\xb1l'
<EOS>
b'\xce\xb7\xce\xb12'
<EOS>
b'u'
<EOS>
b'2\xce\xb1l'
<EOS>
b'(cid:107)L(cid:107)1\n\nf'
<EOS>
b'(x1) \xe2\x88\x92 f (x\xe2\x88\x97)'
<EOS>
b'\xe2\x89\xa4'
<EOS>
b'T \xce\xb7\xce\xb1l'
<EOS>
b'+'
<EOS>
b'2\xce\xb1u(cid:107)\xcf\x83(cid:107)1'
<EOS>
b'\xe2\x88\x9a'
<EOS>
b'\xce\xb1l'
<EOS>
b'b'
<EOS>
b'+'
<EOS>
b'\xce\xb7\xce\xb12'
<EOS>
b'u'
<EOS>
b'2\xce\xb1l'
<EOS>
b'(cid:107)L(cid:107)1.'
<EOS>
b'B PROOF OF THEOREM 3'
<EOS>
b'Proof.'
<EOS>
b'We analyze the convergence of LAMB for general minibatch size here.'
<EOS>
b'Recall that the update\nof LAMB is the following'
<EOS>
b't+1 = x(i)'
<EOS>
b'x(i)'
<EOS>
b't \xe2\x88\x92 \xce\xb7t\xcf\x86((cid:107)x(i)'
<EOS>
b't (cid:107))\n\nr(i)'
<EOS>
b't'
<EOS>
b'(cid:107)r(i)'
<EOS>
b't (cid:107)\n\n,\n\nfor all i \xe2\x88\x88 [h].'
<EOS>
b'For simplicity of notation, we reason the'
<EOS>
b'Since the function f is L-smooth, we have the following:\n\nf'
<EOS>
b'(xt+1) \xe2\x89\xa4 f (xt)'
<EOS>
b'+ (cid:104)\xe2\x88\x87if (xt), x(i)'
<EOS>
b't+1 \xe2\x88\x92 x(i)'
<EOS>
b't (cid:105)'
<EOS>
b'+'
<EOS>
b'(cid:107)x(i)'
<EOS>
b't+1 \xe2\x88\x92 x(i)'
<EOS>
b't (cid:107)2'
<EOS>
b'= f (xt)'
<EOS>
b'\xe2\x88\x92\xce\xb7t'
<EOS>
b'\xcf\x86((cid:107)x(i)\n\nt (cid:107))'
<EOS>
b'\xc3\x97'
<EOS>
b'[\xe2\x88\x87if ('
<EOS>
b'xt)]j \xc3\x97'
<EOS>
b'h'
<EOS>
b'(cid:88)\n\ndi(cid:88)'
<EOS>
b'i=1'
<EOS>
b'j=1'
<EOS>
b'(cid:124)\n\nr(i)'
<EOS>
b't,j'
<EOS>
b'(cid:107)r(i)'
<EOS>
b't (cid:107)'
<EOS>
b'(cid:33)'
<EOS>
b'(cid:125)'
<EOS>
b'+\n\nh'
<EOS>
b'(cid:88)'
<EOS>
b'i=1'
<EOS>
b'Li\xce\xb12'
<EOS>
b'u\xce\xb72\nt'
<EOS>
b'2'
<EOS>
b'(7)\n\nh'
<EOS>
b'(cid:88)'
<EOS>
b'i=1'
<EOS>
b'Li\n2'
<EOS>
b'(cid:32)'
<EOS>
b'(cid:123)(cid:122)'
<EOS>
b'T1'
<EOS>
b'12'
<EOS>
b'Published as a conference paper at ICLR 2020'
<EOS>
b'The above inequality simply follows from the lipschitz continuous nature of the gradient.'
<EOS>
b'We bound\nterm T1 in the following manner:'
<EOS>
b'T1 \xe2\x89\xa4 \xe2\x88\x92\xce\xb7t'
<EOS>
b'\xcf\x86((cid:107)x(i)\n\nt (cid:107))'
<EOS>
b'\xc3\x97'
<EOS>
b'[\xe2\x88\x87if ('
<EOS>
b'xt)]j \xc3\x97'
<EOS>
b'(cid:32)'
<EOS>
b'(cid:33)\n\nr(i)'
<EOS>
b't,j'
<EOS>
b'(cid:107)r(i)'
<EOS>
b't (cid:107)\n\n\xe2\x89\xa4 \xe2\x88\x92\xce\xb7t'
<EOS>
b'(cid:114)'
<EOS>
b'1 \xe2\x88\x92 \xce\xb22'
<EOS>
b'G2di\n\n(cid:16)'
<EOS>
b'\xcf\x86((cid:107)x(i)\n\nt (cid:107))'
<EOS>
b'\xc3\x97'
<EOS>
b'[\xe2\x88\x87if (xt)]j \xc3\x97 g(i)\n\nt,j'
<EOS>
b'(cid:17)'
<EOS>
b'h'
<EOS>
b'(cid:88)\n\ndi(cid:88)'
<EOS>
b'i=1'
<EOS>
b'j=1'
<EOS>
b'h'
<EOS>
b'(cid:88)\n\ndi(cid:88)'
<EOS>
b'i=1'
<EOS>
b'j=1'
<EOS>
b'\xcf\x86((cid:107)x(i)\n\nt (cid:107))'
<EOS>
b'\xc3\x97'
<EOS>
b'[\xe2\x88\x87if ('
<EOS>
b'xt)]j \xc3\x97'
<EOS>
b'1(sign(\xe2\x88\x87if'
<EOS>
b'(xt)]j)'
<EOS>
b'(cid:54)= sign(r(i)\n\nt,j ))'
<EOS>
b'(cid:33)\n\nr(i)'
<EOS>
b't,j'
<EOS>
b'(cid:107)r(i)'
<EOS>
b't (cid:107)'
<EOS>
b'(cid:33)\n\nr(i)'
<EOS>
b't,j'
<EOS>
b'(cid:107)r(i)'
<EOS>
b't (cid:107)'
<EOS>
b'\xe2\x88\x92'
<EOS>
b'\xce\xb7t'
<EOS>
b'h'
<EOS>
b'(cid:88)\n\ndi(cid:88)'
<EOS>
b'(cid:32)'
<EOS>
b'i=1'
<EOS>
b'j=1'
<EOS>
b'h'
<EOS>
b'(cid:88)\n\ndi(cid:88)'
<EOS>
b'i=1'
<EOS>
b'j=1'
<EOS>
b'(cid:16)'
<EOS>
b'(cid:114)'
<EOS>
b'1'
<EOS>
b'di'
<EOS>
b'\xe2\x88\x92'
<EOS>
b'\xce\xb7t'
<EOS>
b'h'
<EOS>
b'(cid:88)\n\ndi(cid:88)'
<EOS>
b'(cid:32)'
<EOS>
b'i=1'
<EOS>
b'j=1'
<EOS>
b'This follows from the fact that (cid:107)r(i)\nas follows:\n\nt (cid:107) \xe2\x89\xa4'
<EOS>
b'(cid:113)'
<EOS>
b'di'
<EOS>
b'1\xe2\x88\x92\xce\xb22'
<EOS>
b'\xe2\x88\x9a'
<EOS>
b'and\n\nvt \xe2\x89\xa4 G.'
<EOS>
b'If \xce\xb22 = 0, then T1 can be bounded'
<EOS>
b'T1 \xe2\x89\xa4 \xe2\x88\x92\xce\xb7t'
<EOS>
b'\xcf\x86((cid:107)x(i)\n\nt (cid:107)) \xc3\x97 |[\xe2\x88\x87if'
<EOS>
b'(xt)]j|\n\n(cid:17)'
<EOS>
b'\xcf\x86((cid:107)x(i)\n\nt (cid:107))'
<EOS>
b'\xc3\x97'
<EOS>
b'[\xe2\x88\x87if ('
<EOS>
b'xt)]j \xc3\x97'
<EOS>
b'1(sign(\xe2\x88\x87if'
<EOS>
b'(xt)]j)'
<EOS>
b'(cid:54)= sign(r(i)\n\nt,j ))'
<EOS>
b'The rest of the proof for \xce\xb22 = 0 is similar to argument for the case \xce\xb22 > 0, which is shown below.'
<EOS>
b'Taking expectation, we have the following:\n\nE[T1] \xe2\x89\xa4 \xe2\x88\x92\xce\xb7t'
<EOS>
b'h'
<EOS>
b'(cid:88)\n\ndi(cid:88)'
<EOS>
b'i=1'
<EOS>
b'j=1'
<EOS>
b'(cid:114)'
<EOS>
b'1'
<EOS>
b'\xe2\x88\x92 \xce\xb22'
<EOS>
b'G2di'
<EOS>
b'(cid:104)'
<EOS>
b'\xcf\x86((cid:107)x(i)'
<EOS>
b'E\n\nt (cid:107))'
<EOS>
b'\xc3\x97'
<EOS>
b'(cid:16)'
<EOS>
b'[\xe2\x88\x87if (xt)]j \xc3\x97 g(i)'
<EOS>
b't,j'
<EOS>
b'(cid:17)(cid:105)'
<EOS>
b'h'
<EOS>
b'(cid:88)'
<EOS>
b'di(cid:88)'
<EOS>
b'(cid:34)'
<EOS>
b'\xcf\x86((cid:107)x(i)'
<EOS>
b'E'
<EOS>
b'\xe2\x88\x92'
<EOS>
b'\xce\xb7t'
<EOS>
b'(cid:32)\n\nt (cid:107))'
<EOS>
b'\xc3\x97'
<EOS>
b'[\xe2\x88\x87if ('
<EOS>
b'xt)]j \xc3\x97'
<EOS>
b'1(sign(\xe2\x88\x87if'
<EOS>
b'(xt)]j)'
<EOS>
b'(cid:54)='
<EOS>
b'sign(g(i)\n\nt,j ))'
<EOS>
b'(cid:35)\n\n\xe2\x89\xa4 \xe2\x88\x92\xce\xb7t\n\nh'
<EOS>
b'(cid:88)\n\ndi(cid:88)'
<EOS>
b'i=1'
<EOS>
b'j=1'
<EOS>
b'i=1'
<EOS>
b'j=1'
<EOS>
b'(cid:114)'
<EOS>
b'1 \xe2\x88\x92 \xce\xb22'
<EOS>
b'G2di'
<EOS>
b'(cid:104)(cid:16)\n\nE'
<EOS>
b'\xcf\x86((cid:107)x(i)\n\nt (cid:107))'
<EOS>
b'\xc3\x97'
<EOS>
b'[\xe2\x88\x87if (xt)]j \xc3\x97 g(i)\n\nt,j\n\n(cid:33)\n\nr(i)'
<EOS>
b't,j'
<EOS>
b'(cid:107)r(i)'
<EOS>
b't (cid:107)'
<EOS>
b'(cid:17)(cid:105)\n\nE'
<EOS>
b'(cid:104)'
<EOS>
b'\xce\xb1u|[\xe2\x88\x87if'
<EOS>
b'(xt)]j|1(sign(\xe2\x88\x87if'
<EOS>
b'(xt)]j)'
<EOS>
b'(cid:54)= sign(g(i)'
<EOS>
b'(cid:105)'
<EOS>
b't,j ))\n\nh'
<EOS>
b'(cid:88)\n\ndi(cid:88)'
<EOS>
b'+ \xce\xb7t'
<EOS>
b'i=1'
<EOS>
b'j=1'
<EOS>
b'(cid:114)'
<EOS>
b'1 \xe2\x88\x92 \xce\xb22'
<EOS>
b'G2di\n\nh'
<EOS>
b'(cid:88)\n\ndi(cid:88)'
<EOS>
b'i=1'
<EOS>
b'j=1'
<EOS>
b'\xe2\x88\x92'
<EOS>
b'\xce\xb7t'
<EOS>
b'h'
<EOS>
b'(cid:88)\n\ndi(cid:88)'
<EOS>
b'i=1'
<EOS>
b'j=1'
<EOS>
b'\xe2\x89\xa4 \xe2\x88\x92\xce\xb7t'
<EOS>
b'(cid:104)'
<EOS>
b'\xcf\x86((cid:107)x(i)'
<EOS>
b'E\n\nt (cid:107))'
<EOS>
b'\xc3\x97'
<EOS>
b'(cid:16)'
<EOS>
b'[\xe2\x88\x87if (xt)]j \xc3\x97 g(i)'
<EOS>
b't,j'
<EOS>
b'(cid:17)(cid:105)'
<EOS>
b'\xce\xb1u|[\xe2\x88\x87if'
<EOS>
b'(xt)]j|P(sign(\xe2\x88\x87if'
<EOS>
b'(xt)]j)'
<EOS>
b'(cid:54)='
<EOS>
b'sign(g(i)\n\nt,j ))'
<EOS>
b'(8)'
<EOS>
b'(9)\n\nUsing the bound on the probability that the signs differ, we get:\n\nE[T1] \xe2\x89\xa4'
<EOS>
b'\xe2\x88\x92\xce\xb7t\xce\xb1l'
<EOS>
b'(cid:107)\xe2\x88\x87f ('
<EOS>
b'xt)(cid:107)2 + \xce\xb7t\xce\xb1u'
<EOS>
b'(cid:114)'
<EOS>
b'h(1'
<EOS>
b'\xe2\x88\x92 \xce\xb22)'
<EOS>
b'G2d'
<EOS>
b'h'
<EOS>
b'(cid:88)\n\ndi(cid:88)'
<EOS>
b'i=1'
<EOS>
b'j=1'
<EOS>
b'\xcf\x83i,j\xe2\x88\x9a\nb\n\n.'
<EOS>
b'Substituting the above bound on T1 in equation 7, we have the following bound:'
<EOS>
b'E[f (xt+1)]'
<EOS>
b'\xe2\x89\xa4 f'
<EOS>
b'(xt) \xe2\x88\x92 \xce\xb7t\xce\xb1l'
<EOS>
b'(cid:107)\xe2\x88\x87f ('
<EOS>
b'xt)(cid:107)2 + \xce\xb7t\xce\xb1u'
<EOS>
b'(cid:107)\xcb\x9c\xcf\x83(cid:107)1\xe2\x88\x9a'
<EOS>
b'b'
<EOS>
b'+\n\nt \xce\xb12\n\xce\xb72'
<EOS>
b'u(cid:107)L(cid:107)1'
<EOS>
b'2'
<EOS>
b'(10)'
<EOS>
b'(cid:114)'
<EOS>
b'h(1'
<EOS>
b'\xe2\x88\x92 \xce\xb22)'
<EOS>
b'G2d'
<EOS>
b'13'
<EOS>
b'Published as a conference paper at ICLR 2020'
<EOS>
b'Algorithm 3 N-LAMB'
<EOS>
b'Input:'
<EOS>
b'x1 \xe2\x88\x88'
<EOS>
b'Rd, learning rate {\xce\xb7t}T'
<EOS>
b't=1, parame-'
<EOS>
b'ters 0 < \xce\xb21, \xce\xb22 < 1, scaling function \xcf\x86,'
<EOS>
b'(cid:15) > 0,\nparameters 0 < {\xce\xb2t\nSet m0'
<EOS>
b'= 0'
<EOS>
b', v0 = 0\nfor t'
<EOS>
b'= 1 to T'
<EOS>
b'do'
<EOS>
b't=1'
<EOS>
b'< 1\n\n1}T'
<EOS>
b'Algorithm 4'
<EOS>
b'NN-LAMB'
<EOS>
b'Input:'
<EOS>
b'x1 \xe2\x88\x88'
<EOS>
b'Rd, learning rate {\xce\xb7t}T\nt=1, parameters\n0 < \xce\xb21, \xce\xb22 < 1, scaling function \xcf\x86, (cid:15) > 0, parame-'
<EOS>
b'ters 0'
<EOS>
b'< {\xce\xb2t'
<EOS>
b'Set m0 ='
<EOS>
b'0'
<EOS>
b', v0 = 0\nfor t'
<EOS>
b'= 1 to T'
<EOS>
b'do'
<EOS>
b't=1 < 1'
<EOS>
b'1}T'
<EOS>
b'\xe2\x88\x87(cid:96)(xt, st).'
<EOS>
b'\xe2\x88\x87(cid:96)(xt, st).'
<EOS>
b'(cid:80)'
<EOS>
b'+'
<EOS>
b'(1\xe2\x88\x92\xce\xb2t'
<EOS>
b'1\xe2\x88\x92\xce\xa0t'
<EOS>
b'Draw b samples St from P.'
<EOS>
b'Compute gt = 1'
<EOS>
b'st\xe2\x88\x88St'
<EOS>
b'|St|'
<EOS>
b'mt = \xce\xb21mt\xe2\x88\x921'
<EOS>
b'+ (1 \xe2\x88\x92 \xce\xb21)gt\n\xcb\x86m'
<EOS>
b'= \xce\xb2t+1'
<EOS>
b'1 mt'
<EOS>
b'1)gt'
<EOS>
b'i=1\xce\xb2i'
<EOS>
b'1\xe2\x88\x92\xce\xa0t+1'
<EOS>
b'i=1 \xce\xb2i\n1'
<EOS>
b'1'
<EOS>
b'vt'
<EOS>
b'= \xce\xb22vt\xe2\x88\x921 + (1 \xe2\x88\x92'
<EOS>
b'\xce\xb22)g2\nt'
<EOS>
b'\xcb\x86v ='
<EOS>
b'\xce\xb22vt'
<EOS>
b'1\xe2\x88\x92\xce\xb2t'
<EOS>
b'2'
<EOS>
b'Compute ratio rt ='
<EOS>
b'\xcb\x86m\xe2\x88\x9a\nx(i)'
<EOS>
b't+1'
<EOS>
b'= x(i)'
<EOS>
b't \xe2\x88\x92'
<EOS>
b'\xce\xb7t'
<EOS>
b'\xcb\x86v+(cid:15)'
<EOS>
b'(i)'
<EOS>
b't (cid:107))'
<EOS>
b'(i)'
<EOS>
b't (cid:107)\n\n\xcf\x86((cid:107)x'
<EOS>
b'(i)'
<EOS>
b't +'
<EOS>
b'\xce\xbbx'
<EOS>
b'(cid:107)r'
<EOS>
b'(cid:80)'
<EOS>
b'+'
<EOS>
b'(1\xe2\x88\x92\xce\xb2t'
<EOS>
b'1\xe2\x88\x92\xce\xa0t'
<EOS>
b'Draw b samples St from P.'
<EOS>
b'Compute gt = 1'
<EOS>
b'st\xe2\x88\x88St'
<EOS>
b'|St|'
<EOS>
b'mt = \xce\xb21mt\xe2\x88\x921'
<EOS>
b'+ (1 \xe2\x88\x92 \xce\xb21)gt\n\xcb\x86m'
<EOS>
b'= \xce\xb2t+1'
<EOS>
b'1 mt'
<EOS>
b'1)gt'
<EOS>
b'i=1\xce\xb2i'
<EOS>
b'1\xe2\x88\x92\xce\xa0t+1'
<EOS>
b'i=1 \xce\xb2i\n1'
<EOS>
b'1'
<EOS>
b'vt'
<EOS>
b'= \xce\xb22vt\xe2\x88\x921 + (1 \xe2\x88\x92'
<EOS>
b'\xce\xb22)g2\nt'
<EOS>
b'\xcb\x86v ='
<EOS>
b'\xce\xb2t+1'
<EOS>
b'2)g2'
<EOS>
b'+ (1\xe2\x88\x92\xce\xb2t'
<EOS>
b'vt\nt'
<EOS>
b'1\xe2\x88\x92\xce\xa0t'
<EOS>
b'i=1\xce\xb2i'
<EOS>
b'1\xe2\x88\x92\xce\xa0t+1'
<EOS>
b'2'
<EOS>
b'Compute ratio rt ='
<EOS>
b'\xcb\x86m\xe2\x88\x9a\nx(i)'
<EOS>
b't+1'
<EOS>
b'= x(i)'
<EOS>
b't \xe2\x88\x92'
<EOS>
b'\xce\xb7t'
<EOS>
b'i=1 \xce\xb2i\n2\n\n\xcb\x86v+(cid:15)'
<EOS>
b'(i)'
<EOS>
b't (cid:107))'
<EOS>
b'(i)'
<EOS>
b't (cid:107)\n\n\xcf\x86((cid:107)x'
<EOS>
b'(i)'
<EOS>
b't +'
<EOS>
b'\xce\xbbx'
<EOS>
b'(cid:107)r\n\n2'
<EOS>
b'end for\n\nend for\n\n(r(i)'
<EOS>
b't + \xce\xbbxt)'
<EOS>
b'(r(i)'
<EOS>
b't + \xce\xbbxt)'
<EOS>
b'Summing the above inequality for t'
<EOS>
b'= 1 to T and using telescoping sum'
<EOS>
b', we have the following\ninequality:\n\nE[f'
<EOS>
b'(xT +1)] \xe2\x89\xa4 f (x1) \xe2\x88\x92 \xce\xb7t\xce\xb1l\n\nE[(cid:107)\xe2\x88\x87f (xt)(cid:107)2]'
<EOS>
b'+ \xce\xb7T'
<EOS>
b'\xce\xb1u'
<EOS>
b'(cid:114)'
<EOS>
b'h(1'
<EOS>
b'\xe2\x88\x92 \xce\xb22)'
<EOS>
b'G2d'
<EOS>
b'T\n(cid:88)'
<EOS>
b't=1'
<EOS>
b'(cid:107)\xcb\x9c\xcf\x83(cid:107)1\xe2\x88\x9a'
<EOS>
b'b'
<EOS>
b'+'
<EOS>
b'\xce\xb72\xce\xb12\nuT'
<EOS>
b'2'
<EOS>
b'(cid:107)L(cid:107)1.'
<EOS>
b'Rearranging the terms of the above inequality, and dividing by \xce\xb7T \xce\xb1l, we have:'
<EOS>
b'(cid:114)'
<EOS>
b'h(1'
<EOS>
b'\xe2\x88\x92 \xce\xb22)'
<EOS>
b'G2d'
<EOS>
b'1'
<EOS>
b'T\n\nT\n(cid:88)'
<EOS>
b't=1'
<EOS>
b'E[(cid:107)\xe2\x88\x87f (xt)(cid:107)2] \xe2\x89\xa4'
<EOS>
b'f (x1)'
<EOS>
b'\xe2\x88\x92'
<EOS>
b'E[f'
<EOS>
b'(xT +1)]'
<EOS>
b'T \xce\xb7\xce\xb1l\n\nf'
<EOS>
b'(x1) \xe2\x88\x92 f (x\xe2\x88\x97)'
<EOS>
b'\xe2\x89\xa4'
<EOS>
b'T \xce\xb7\xce\xb1l'
<EOS>
b'+'
<EOS>
b'\xce\xb1u(cid:107)\xcb\x9c\xcf\x83(cid:107)1'
<EOS>
b'\xce\xb1l'
<EOS>
b'b'
<EOS>
b'\xe2\x88\x9a'
<EOS>
b'+'
<EOS>
b'\xe2\x88\x9a'
<EOS>
b'\xce\xb1u(cid:107)\xcb\x9c\xcf\x83(cid:107)1'
<EOS>
b'b'
<EOS>
b'\xce\xb1l'
<EOS>
b'\xce\xb7\xce\xb12'
<EOS>
b'u'
<EOS>
b'2\xce\xb1l'
<EOS>
b'+'
<EOS>
b'+'
<EOS>
b'(cid:107)L(cid:107)1\n\n\xce\xb7'
<EOS>
b'2'
<EOS>
b'(cid:107)L(cid:107)1.'
<EOS>
b'C COMPARISON OF CONVERGENCE RATES OF LARS AND SGD'
<EOS>
b'Inspired by the comparison used by (Bernstein et al., 2018) for comparing SIGN SGD with SGD'
<EOS>
b', we\nde\xef\xac\x81ne the following quantities:'
<EOS>
b'(cid:33)2'
<EOS>
b'(cid:107)\xe2\x88\x87if (xt)(cid:107)\n\n='
<EOS>
b'(cid:32) h'
<EOS>
b'(cid:88)'
<EOS>
b'i=1'
<EOS>
b'\xcf\x88(\xe2\x88\x87f (xt))d(cid:107)\xe2\x88\x87f'
<EOS>
b'(xt)(cid:107)2'
<EOS>
b'\xcf\x88gd(cid:107)\xe2\x88\x87f'
<EOS>
b'(xt)(cid:107)2'
<EOS>
b'\xe2\x89\xa5'
<EOS>
b'h'
<EOS>
b'Then LARS convergence rate can be written in the following manner:'
<EOS>
b'(E[(cid:107)\xe2\x88\x87f (xa)(cid:107))2 \xe2\x89\xa4'
<EOS>
b'O'
<EOS>
b'(cid:18)'
<EOS>
b'(f'
<EOS>
b'(x1) \xe2\x88\x92 f'
<EOS>
b'(x\xe2\x88\x97))L\xe2\x88\x9e\n\n\xcf\x88L\n\xcf\x882'
<EOS>
b'g'
<EOS>
b'+\n\n(cid:107)\xcf\x83(cid:107)2\n\nT'
<EOS>
b'(cid:19)\n\n.'
<EOS>
b'\xcf\x882'
<EOS>
b'\xcf\x83'
<EOS>
b'\xcf\x882'
<EOS>
b'g'
<EOS>
b'If \xcf\x88L (cid:28) \xcf\x882'
<EOS>
b'we gain over SGD.'
<EOS>
b'Otherwise, SGD\xe2\x80\x99s upper bound on convergence rate is better.'
<EOS>
b'g then LARS'
<EOS>
b'(i.e., gradient is more denser than curvature or stochasticity),\n\ng and \xcf\x88\xcf\x83 (cid:28) \xcf\x882'
<EOS>
b'(cid:107)L(cid:107)2\n\n1 \xe2\x89\xa4'
<EOS>
b'(cid:107)\xcf\x83(cid:107)2\n\n1 =\n\n\xcf\x88Ld2(cid:107)L(cid:107)2\n\xe2\x88\x9e'
<EOS>
b'h2\n\n\xcf\x88\xcf\x83d(cid:107)\xcf\x83(cid:107)2\n\n.'
<EOS>
b'h'
<EOS>
b'h'
<EOS>
b'T'
<EOS>
b'14'
<EOS>
b'Published as a conference paper at ICLR 2020'
<EOS>
b'Figure 1:'
<EOS>
b'This \xef\xac\x81gure shows N-LAMB and NN-LAMB can achieve a comparable accuracy compared\nto LAMB optimizer.'
<EOS>
b'Their performances are much better than momentum solver.'
<EOS>
b'The result of\nmomentum optimizer was reported by Goyal et al.'
<EOS>
b'(2017).'
<EOS>
b'For Nadam, we use the learning rate recipe\nof (Goyal et al., 2017):'
<EOS>
b'(1)'
<EOS>
b'5-epoch warmup to stablize the initial stage; and (2) multiply the learning\nrate by 0.1 at 30th, 60th, and 80th epoch.'
<EOS>
b'The target accuracy is around 0.763 (Goyal et al., 2017).'
<EOS>
b'We also tuned the learning rate of Nadam in {1e-4, 2e-4, ..., 9e-4, 1e-3, 2e-3, ..., 9e-3, 1e-2}.'
<EOS>
b'D N-LAMB:'
<EOS>
b'NESTEROV MOMENTUM FOR LAMB'
<EOS>
b'Sutskever'
<EOS>
b'et al.'
<EOS>
b'(2013) report that Nesterov\xe2\x80\x99s accelerated gradient (NAG) proposed by Nesterov (1983)'
<EOS>
b'is conceptually and empirically better than the regular momentum method for convex, non-stochastic\nobjectives.'
<EOS>
b'Dozat (2016) incorporated Nesterov\xe2\x80\x99s momentum into Adam optimizer and proposed\nthe Nadam optimizer.'
<EOS>
b'Speci\xef\xac\x81cally, only the \xef\xac\x81rst moment of Adam was modi\xef\xac\x81ed and the second\nmoment of Adam was unchanged.'
<EOS>
b'The results on several applications (Word2Vec, Image Recognition,\nand LSTM Language Model) showed that Nadam optimizer improves the speed of convergence\nand the quality of the learned models.'
<EOS>
b'We also tried using Nesterov\xe2\x80\x99s momentum to replace the\nregular momentum of LAMB optimizer\xe2\x80\x99s \xef\xac\x81rst moment.'
<EOS>
b'In this way, we got a new algorithm named\nas N-LAMB (Nesterov LAMB).'
<EOS>
b'The complete algorithm is in Algorithm 3.'
<EOS>
b'We can also Nesterov\xe2\x80\x99s'
<EOS>
b'momentum to replace the regular momentum of LAMB optimizer\xe2\x80\x99s second moment.'
<EOS>
b'We refer to this\nalgorithm as NN-LAMB (Nesterov\xe2\x80\x99s momentum for both the \xef\xac\x81rst moment and the second moment).'
<EOS>
b'The details of NN-LAMB were shown in Algorithm 4.'
<EOS>
b'Dozat (2016) suggested the best performance of Nadam was achieved by \xce\xb21 = 0.975,'
<EOS>
b'\xce\xb22 = 0.999, and'
<EOS>
b'(cid:15) = 1e-8.'
<EOS>
b'We used the same settings for N-LAMB and NN-LAMB.'
<EOS>
b'We scaled the batch size to 32K\nfor ImageNet training with ResNet-50.'
<EOS>
b'Our experimental results show that N-LAMB and NN-LAMB\ncan achieve a comparable accuracy compared to LAMB optimizer.'
<EOS>
b'Their performances are much\nbetter than momentum solver (Figure 1).'
<EOS>
b'E LAMB WITH LEARNING RATE CORRECTION'
<EOS>
b'There are two operations at each iteration in original Adam optimizer'
<EOS>
b'(let us call it adam-correction):'
<EOS>
b'mt ='
<EOS>
b'mt/(1 \xe2\x88\x92 \xce\xb2t'
<EOS>
b'1)\n\nvt ='
<EOS>
b'vt/(1'
<EOS>
b'\xe2\x88\x92 \xce\xb2t'
<EOS>
b'2)'
<EOS>
b'It has an impact on the learning rate by \xce\xb7t :='
<EOS>
b'\xce\xb7t\xe2\x88\x97(cid:112)(1 \xe2\x88\x92 \xce\xb2t\n1).'
<EOS>
b'According to our experimental\nresults, adam-correction essentially has the same effect as learning rate warmup (see Figure 2).'
<EOS>
b'The\nwarmup function often was implemented in the modern deep learning system.'
<EOS>
b'Thus, we can remove\nadam-correction from the LAMB optimizer.'
<EOS>
b'We did not observe any drop in the test or validation\naccuracy for BERT and ImageNet training.'
<EOS>
b'2)/(1 \xe2\x88\x92 \xce\xb2t'
<EOS>
b'15'
<EOS>
b'Published as a conference paper at ICLR 2020'
<EOS>
b'Figure 2:'
<EOS>
b'The \xef\xac\x81gure shows that adam-correction has the same effect as learning rate warmup.'
<EOS>
b'We\nremoved adam-correction from the LAMB optimizer.'
<EOS>
b'We did not observe any drop in the test or\nvalidation accuracy for BERT and ImageNet training.'
<EOS>
b'Figure 3:'
<EOS>
b'We tried different norms in LAMB optimizer.'
<EOS>
b'However, we did not observe a signi\xef\xac\x81cant\ndifference in the validation accuracy of ImageNet training with ResNet-50.'
<EOS>
b'We use L2 norm as the\ndefault.'
<EOS>
b'F LAMB WITH DIFFERENT NORMS'
<EOS>
b'We need to compute the matrix/tensor norm for each layer when we do the parameter updating in\nthe LAMB optimizer.'
<EOS>
b'We tried different norms in LAMB optimizer.'
<EOS>
b'However, we did not observe\na signi\xef\xac\x81cant difference in the validation accuracy of ImageNet training with ResNet-50.'
<EOS>
b'In our\nexperiments, the difference in validation accuracy is less than 0.1 percent (Figure 3).'
<EOS>
b'We use L2 norm\nas the default.'
<EOS>
b'G REGULAR BATCH SIZES FOR SMALL DATASETS:'
<EOS>
b'MNIST AND CIFAR-10.'
<EOS>
b'According to DAWNBench, DavidNet (a custom 9-layer Residual ConvNet) is the fastest model\nfor CIFAR-10 dataset (as of April 1st, 2019)5.'
<EOS>
b'The baseline uses the momentum SGD optimizer.\nTable 6 and Figure 4 show the test accuracy of CIFAR-10 training with DavidNet.'
<EOS>
b'The PyTorch\nimplementation (momentum SGD optimizer) on GPUs was reported on Standford DAWNBench\xe2\x80\x99s\nwebsite, which achieves 94.06% in 24 epochs.'
<EOS>
b'The Tensor\xef\xac\x82ow implementation (momentum SGD\noptimizer) on TPU achieves a 93.72% accuracy in 24 epochs6.'
<EOS>
b'We use the implementation of\nTensorFlow on TPUs.'
<EOS>
b'LAMB optimizer is able to achieve 94.08% test accuracy in 24 epochs, which\nis better than other adaptive optimizers and momentum SGD.'
<EOS>
b'Even on the smaller tasks like MNIST\ntraining with LeNet, LAMB is able to achieve a better accuracy than existing solvers (Table 7).'
<EOS>
b'5https://dawn.cs.stanford.edu/'
<EOS>
b'benchmark/CIFAR10/train.html'
<EOS>
b'6https://github.com/fenwickslab/dl_tutorials/blob/master/tutorial3_cifar10_davidnet_\xef\xac\x81x.ipynb'
<EOS>
b'16'
<EOS>
b'Published as a conference paper at ICLR 2020'
<EOS>
b'Figure 4:'
<EOS>
b'LAMB is better than the existing solvers (batch size = 512).'
<EOS>
b'We make sure all the solvers are\ncarefully tuned.'
<EOS>
b'The learning rate tuning space of Adam, AdamW, Adagrad and LAMB is {0.0001,\n0.0002, 0.0004, 0.0006, 0.0008, 0.001, 0.002, 0.004, 0.006, 0.008, 0.01, 0.02, 0.04, 0.06, 0.08, 0.1,\n0.2, 0.4, 0.6, 0.8, 1, 2, 4, 6, 8, 10, 15, 20, 25, 30, 35, 40, 45, 50}.'
<EOS>
b'The momentum optimizer was tuned\nby the baseline implementer.'
<EOS>
b'The weight decay term of AdamW was tuned by {0.0001, 0.001, 0.01,\n0.1, 1.0}.'
<EOS>
b'Table 6:'
<EOS>
b'CIFAR-10 training with DavidNet (batch size = 512).'
<EOS>
b'All of them run 24 epochs and \xef\xac\x81nish'
<EOS>
b'the training under one minute on one cloud TPU.'
<EOS>
b'We make sure all the solvers are carefully tuned.'
<EOS>
b'The learning rate tuning space of Adam, AdamW, Adagrad and LAMB is {0.0001, 0.0002, 0.0004,\n0.0006, 0.0008, 0.001, 0.002, 0.004, 0.006, 0.008, 0.01, 0.02, 0.04, 0.06, 0.08, 0.1, 0.2, 0.4, 0.6, 0.8,\n1, 2, 4, 6, 8, 10, 15, 20, 25, 30, 35, 40, 45, 50}.'
<EOS>
b'The momentum optimizer was tuned by the baseline\nimplementer.'
<EOS>
b'The weight decay term of AdamW was tuned by {0.0001, 0.001, 0.01, 0.1, 1.0}.'
<EOS>
b'Optimizer'
<EOS>
b'ADAGRAD ADAM ADAMW momentum LAMB'
<EOS>
b'Test Accuracy'
<EOS>
b'0.9074'
<EOS>
b'0.9225'
<EOS>
b'0.9271'
<EOS>
b'0.9372'
<EOS>
b'0.9408'
<EOS>
b'H IMPLEMENTATION DETAILS AND ADDITIONAL RESULTS'
<EOS>
b'There are several hyper-parameters in LAMB optimizer.'
<EOS>
b'Although users do not need to tune them,\nwe explain them to help users to have a better understanding.'
<EOS>
b'\xce\xb21 is used for decaying the running\naverage of the gradient.'
<EOS>
b'\xce\xb22 is used for decaying the running average of the square of gradient.'
<EOS>
b'The\ndefault setting for other parameters: weight decay rate \xce\xbb=0.01, \xce\xb21=0.9, \xce\xb22=0.999, (cid:15)=1e-6.'
<EOS>
b'We did not\ntune \xce\xb21 and \xce\xb22.'
<EOS>
b'However, our experiments show that tuning them may get a higher accuracy.'
<EOS>
b'Based on our experience, learning rate is the most important hyper-parameter that affects the learning\nef\xef\xac\x81ciency and \xef\xac\x81nal accuracy.'
<EOS>
b'Bengio (2012) suggests that it is often the single most important\nhyper-parameter and that it always should be tuned.'
<EOS>
b'Thus, to make sure we have a solid baseline, we\ncarefully tune the learning rate of ADAM, ADAMW, ADAGRAD, and momentum SGD'
<EOS>
b'In our experiments, we found that the validation loss is not reliable for large-batch training.'
<EOS>
b'A lower\nvalidation loss does not necessarily lead to a higher validation accuracy (Figure 5).'
<EOS>
b'Thus, we use the\ntest/val accuracy or F1 score on dev set to evaluate the optimizers.'
<EOS>
b'H.0.1 BERT'
<EOS>
b'Table 8 shows some of the tuning information from BERT training with ADAMW optimizer.'
<EOS>
b'ADAMW\nstops scaling at the batch size of 16K.'
<EOS>
b'The target F1 score is 90.5.'
<EOS>
b'LAMB achieves a F1 score of\n91.345.'
<EOS>
b'The table shows the tuning information of ADAMW.'
<EOS>
b'In Table 8, we report the best F1 score\nwe observed from our experiments.'
<EOS>
b'The loss curves of BERT training by LAMB for different batch sizes are shown in Figure 6.'
<EOS>
b'We\nobserve that the loss curves are almost identical to each other, which means our optimizer scales well\nwith the batch size.'
<EOS>
b'The training loss curve of BERT mixed-batch pre-training with LAMB is shown in Figure 7.'
<EOS>
b'This\n\xef\xac\x81gure shows that LAMB can make the training converge smoothly at the batch size of 64K.'
<EOS>
b'Figure 8 shows that we can achieve 76.8% scaling ef\xef\xac\x81ciency by scaling the batch size (49.1 times\nspeedup by 64 times computational resources) and 101.8% scaling ef\xef\xac\x81ciency with mixed-batch ('
<EOS>
b'65.2\ntimes speedup by 64 times computational resources)\n\n17'
<EOS>
b'Published as a conference paper at ICLR 2020'
<EOS>
b'Table 7:'
<EOS>
b'Test Accuracy by MNIST training with LeNet (30 epochs for Batch Size = 1024).'
<EOS>
b'The\ntuning space of learning rate for all the optimizers is {0.0001, 0.001, 0.01, 0.1}.'
<EOS>
b'We use the same\nlearning rate warmup and decay schedule for all of them.'
<EOS>
b'Optimizer'
<EOS>
b'Momentum Addgrad ADAM ADAMW LAMB'
<EOS>
b'Average accuracy'
<EOS>
b'over 5 runs'
<EOS>
b'0.9933\n\n0.9928'
<EOS>
b'0.9936'
<EOS>
b'0.9941'
<EOS>
b'0.9945'
<EOS>
b'Figure 5:'
<EOS>
b'Our experiments show that even the validation loss is not reliable in the large-scale training.'
<EOS>
b'A lower validation loss may lead to a worse accuracy.'
<EOS>
b'Thus, we use the test/val accuracy or F1 score\non dev set to evaluate the optimizers.'
<EOS>
b'H.0.2'
<EOS>
b'IMAGENET'
<EOS>
b'Figures 9 - 14 show the LAMB trust ratio at different iterations for ImageNet training with ResNet-50.'
<EOS>
b'From these \xef\xac\x81gures we can see that these ratios are very different from each other for different layers.'
<EOS>
b'LAMB uses the trust ratio to help the slow learners to train faster.'
<EOS>
b'H.1 BASELINE TUNING DETAILS FOR IMAGENET TRAINING WITH RESNET-50'
<EOS>
b'If you are not interested in the baseline tuning details, please skip this section.'
<EOS>
b'Goyal'
<EOS>
b'et al.'
<EOS>
b'(2017) suggested a proper learning rate warmup and decay scheme may help improve\nthe ImageNet classi\xef\xac\x81cation accuracy.'
<EOS>
b'We included these techniques in Adam/AdamW/AdaGrad\ntuning.'
<EOS>
b'Speci\xef\xac\x81cally, we use the learning rate recipe of Goyal et al.'
<EOS>
b'(2017): (1)'
<EOS>
b'5-epoch warmup'
<EOS>
b'to stablize the initial stage; and (2) multiply the learning rate by 0.1 at 30th, 60th, and 80th\nepoch.'
<EOS>
b'The target accuracy is around 76.3% (Goyal et al., 2017).'
<EOS>
b'There techniques help to im-'
<EOS>
b'prove the accuracy of Adam/AdamW/AdaGrad to around 73%.'
<EOS>
b'However, even with these techniques,\nAdam/AdamW/AdaGrad stil can not achieve the target validation accuracy.'
<EOS>
b'To make sure our baseline is solid, we carefully tuned the hyper-parameters.'
<EOS>
b'Table 9 shows the tuning\ninformation of standard Adagrad.'
<EOS>
b'Table 10 shows the tuning information of adding the learning rate\nscheme of Goyal et al.'
<EOS>
b'(2017) to standard Adagrad.'
<EOS>
b'Table 11 shows the tuning information of standard\nAdam.'
<EOS>
b'Table shows the tuning information of adding the learning rate scheme of Goyal et al.'
<EOS>
b'(2017)\nto standard Adam.'
<EOS>
b'It is tricky to tune the AdamW optimizer since both the L2 regularization and\nweight decay have the effect on the performance.'
<EOS>
b'Thus we have four tuning sets.'
<EOS>
b'The \xef\xac\x81rst tuning set is based on AdamW with default L2 regularization.'
<EOS>
b'We tune the learning rate and\nweight decay.'
<EOS>
b'The tuning information is in Figures 13, 14, 15, and 16.'
<EOS>
b'The second tuning set is based on AdamW with disabled L2 regularization.'
<EOS>
b'We tune the learning rate\nand weight decay.'
<EOS>
b'The tuning information is in Figures 17, 18, 19, and 20.'
<EOS>
b'18'
<EOS>
b'Published as a conference paper at ICLR 2020'
<EOS>
b'Table 8'
<EOS>
b': ADAMW stops scaling at the batch size of 16K.'
<EOS>
b'The target F1 score is 90.5.'
<EOS>
b'LAMB achieves\na F1 score of 91.345.'
<EOS>
b'The table shows the tuning information of ADAMW.'
<EOS>
b'In this table, we report the\nbest F1 score we observed from our experiments.'
<EOS>
b'LR\n\nbatch size warmup steps'
<EOS>
b'last step infomation'
<EOS>
b'F1 score on dev set'
<EOS>
b'Solver'
<EOS>
b'ADAMW'
<EOS>
b'ADAMW'
<EOS>
b'ADAMW'
<EOS>
b'ADAMW'
<EOS>
b'ADAMW'
<EOS>
b'ADAMW'
<EOS>
b'ADAMW'
<EOS>
b'ADAMW'
<EOS>
b'ADAMW'
<EOS>
b'16K'
<EOS>
b'16K'
<EOS>
b'16K'
<EOS>
b'16K'
<EOS>
b'16K'
<EOS>
b'16K'
<EOS>
b'16K'
<EOS>
b'16K'
<EOS>
b'16'
<EOS>
b'K'
<EOS>
b'0.05\xc3\x9731250'
<EOS>
b'0.05\xc3\x9731250'
<EOS>
b'0.05\xc3\x9731250'
<EOS>
b'0.10\xc3\x9731250'
<EOS>
b'0.10\xc3\x9731250'
<EOS>
b'0.10\xc3\x9731250'
<EOS>
b'0.20\xc3\x9731250'
<EOS>
b'0.20\xc3\x9731250'
<EOS>
b'0.20\xc3\x9731250'
<EOS>
b'0.0001'
<EOS>
b'0.0002'
<EOS>
b'0.0003'
<EOS>
b'0.0001\n0.0002'
<EOS>
b'0.0003'
<EOS>
b'0.0001'
<EOS>
b'0.0002'
<EOS>
b'0.0003'
<EOS>
b'loss=8.04471, step=28126'
<EOS>
b'loss=7.89673, step=28126'
<EOS>
b'loss=8.35102, step=28126'
<EOS>
b'loss=2.01419, step=31250'
<EOS>
b'loss=1.04689, step=31250'
<EOS>
b'loss=8.05845, step=20000'
<EOS>
b'loss=1.53706, step=31250'
<EOS>
b'loss=1.15500, step=31250'
<EOS>
b'loss=1.48798, step=31250'
<EOS>
b'diverged'
<EOS>
b'diverged'
<EOS>
b'diverged\n86.034'
<EOS>
b'88.540\ndiverged\n85.231'
<EOS>
b'88.110'
<EOS>
b'85.653'
<EOS>
b'Figure 6:'
<EOS>
b'This \xef\xac\x81gure shows the training loss curve of LAMB optimizer.'
<EOS>
b'We just want to use this \xef\xac\x81gure\nto show that LAMB can make the training converge smoothly.'
<EOS>
b'Even if we scale the batch size to the\nextremely large cases, the loss curves are almost identical to each other.'
<EOS>
b'Then we add the learning rate scheme of Goyal et al.'
<EOS>
b'(2017) to AdamW and refer to it as AdamW+.'
<EOS>
b'The third tuning set is based on AdamW+ with default L2 regularization.'
<EOS>
b'We tune the learning rate\nand weight decay.'
<EOS>
b'The tuning information is Figure 21 and 22.'
<EOS>
b'The fourth tuning set is based on AdamW+ with disabled L2 regularization.'
<EOS>
b'We tune the learning rate\nand weight decay.'
<EOS>
b'The tuning information is in Figures 23, 24, 25.'
<EOS>
b'Based on our comprehensive tuning results, we conclude the existing adaptive solvers do not perform'
<EOS>
b'well on ImageNet training or at least it is hard to tune them.'
<EOS>
b'19'
<EOS>
b'Published as a conference paper at ICLR 2020'
<EOS>
b'Figure 7:'
<EOS>
b'This \xef\xac\x81gure shows the training loss curve of LAMB optimizer.'
<EOS>
b'This \xef\xac\x81gure shows that LAMB\ncan make the training converge smoothly at the extremely large batch size (e.g. 64K).'
<EOS>
b'Figure 8:'
<EOS>
b'We achieve 76.8% scaling ef\xef\xac\x81ciency (49 times speedup by 64 times computational resources)\nand 101.8% scaling ef\xef\xac\x81ciency with a mixed, scaled batch size (65.2 times speedup by 64 times\ncomputational resources).'
<EOS>
b'1024-mixed means the mixed-batch training on 1024 TPUs.'
<EOS>
b'Figure 9:'
<EOS>
b'The LAMB trust ratio.'
<EOS>
b'20'
<EOS>
b'Published as a conference paper at ICLR 2020'
<EOS>
b'Figure 10:'
<EOS>
b'The LAMB trust ratio.'
<EOS>
b'Figure 11:'
<EOS>
b'The LAMB trust ratio.'
<EOS>
b'Figure 12:'
<EOS>
b'The LAMB trust ratio.'
<EOS>
b'Figure 13:'
<EOS>
b'The LAMB trust ratio.'
<EOS>
b'21'
<EOS>
b'Published as a conference paper at ICLR 2020'
<EOS>
b'Figure 14:'
<EOS>
b'The LAMB trust ratio.'
<EOS>
b'Table 9:'
<EOS>
b'The accuracy information of tuning default AdaGrad optimizer for ImageNet training with\nResNet-50 (batch size ='
<EOS>
b'16384, 90 epochs, 7038 iterations).'
<EOS>
b'Learning Rate Top-1 Validation Accuracy\n\n0.0001'
<EOS>
b'0.001'
<EOS>
b'0.002'
<EOS>
b'0.004'
<EOS>
b'0.008\n0.010'
<EOS>
b'0.020'
<EOS>
b'0.040\n0.080'
<EOS>
b'0.100'
<EOS>
b'0.200'
<EOS>
b'0.400'
<EOS>
b'0.800'
<EOS>
b'1.000'
<EOS>
b'2.000\n4.000'
<EOS>
b'6.000'
<EOS>
b'8.000'
<EOS>
b'10.00'
<EOS>
b'12.00\n14.00'
<EOS>
b'16.00'
<EOS>
b'18.00'
<EOS>
b'20.00'
<EOS>
b'30.00\n40.00'
<EOS>
b'50.00'
<EOS>
b'0.0026855469'
<EOS>
b'0.015563965'
<EOS>
b'0.022684732\n0.030924479\n0.04486084\n0.054158527'
<EOS>
b'0.0758667'
<EOS>
b'0.1262614\n0.24037679'
<EOS>
b'0.27357993\n0.458313'
<EOS>
b'0.553833'
<EOS>
b'0.54103595\n0.5489095\n0.47680664'
<EOS>
b'0.5295207'
<EOS>
b'0.36950684\n0.31081137'
<EOS>
b'0.30670166'
<EOS>
b'0.3091024'
<EOS>
b'0.3227946'
<EOS>
b'0.0063680015'
<EOS>
b'0.11287435'
<EOS>
b'0.21602376'
<EOS>
b'0.08315023'
<EOS>
b'0.0132039385'
<EOS>
b'0.0009969076'
<EOS>
b'22'
<EOS>
b'Published as a conference paper at ICLR 2020'
<EOS>
b'Table 10:'
<EOS>
b'The accuracy information of tuning AdaGrad optimizer for ImageNet training with ResNet-\n50'
<EOS>
b'(batch size ='
<EOS>
b'16384, 90 epochs, 7038 iterations).'
<EOS>
b'We use the learning rate recipe of (Goyal et al.,\n2017): (1)'
<EOS>
b'5-epoch warmup to stablize the initial stage; and (2) multiply the learning rate by 0.1 at\n30th, 60th, and 80th epoch.'
<EOS>
b'The target accuracy is around 0.763 (Goyal et al., 2017).'
<EOS>
b'Learning Rate Top-1 Validation Accuracy\n\n0.0001'
<EOS>
b'0.001'
<EOS>
b'0.002'
<EOS>
b'0.004'
<EOS>
b'0.008\n0.010'
<EOS>
b'0.020'
<EOS>
b'0.040\n0.080'
<EOS>
b'0.100'
<EOS>
b'0.200'
<EOS>
b'0.400'
<EOS>
b'0.800'
<EOS>
b'1.000'
<EOS>
b'2.000\n4.000'
<EOS>
b'6.000'
<EOS>
b'8.000'
<EOS>
b'10.00'
<EOS>
b'12.00\n14.00'
<EOS>
b'16.00'
<EOS>
b'18.00'
<EOS>
b'20.00'
<EOS>
b'30.00\n40.00'
<EOS>
b'50.00'
<EOS>
b'0.0011189779'
<EOS>
b'0.00793457'
<EOS>
b'0.012573242\n0.019022623'
<EOS>
b'0.027079264'
<EOS>
b'0.029012045'
<EOS>
b'0.0421346'
<EOS>
b'0.06618246'
<EOS>
b'0.10970052\n0.13429768'
<EOS>
b'0.26550293\n0.41918945'
<EOS>
b'0.5519816'
<EOS>
b'0.58614093'
<EOS>
b'0.67252606'
<EOS>
b'0.70306396'
<EOS>
b'0.709493\n0.7137858'
<EOS>
b'0.71797687\n0.7187703'
<EOS>
b'0.72007245'
<EOS>
b'0.7194214'
<EOS>
b'0.7149251'
<EOS>
b'0.71293133'
<EOS>
b'0.70458984'
<EOS>
b'0.69085693'
<EOS>
b'0.67976886'
<EOS>
b'23'
<EOS>
b'Published as a conference paper at ICLR 2020'
<EOS>
b'Table 11:'
<EOS>
b'The accuracy information of tuning default Adam optimizer for ImageNet training with\nResNet-50 (batch size ='
<EOS>
b'16384, 90 epochs, 7038 iterations).'
<EOS>
b'The target accuracy is around 0.763'
<EOS>
b'(Goyal et al., 2017).'
<EOS>
b'Learning Rate Top-1 Validation Accuracy'
<EOS>
b'Table 12:'
<EOS>
b'The accuracy information of tuning Adam optimizer for ImageNet training with ResNet-50\n(batch size ='
<EOS>
b'16384, 90 epochs, 7038 iterations).'
<EOS>
b'We use the learning rate recipe of (Goyal et al.,\n2017): (1)'
<EOS>
b'5-epoch warmup to stablize the initial stage; and (2) multiply the learning rate by 0.1 at\n30th, 60th, and 80th epoch.'
<EOS>
b'The target accuracy is around 0.763 (Goyal et al., 2017).'
<EOS>
b'Learning Rate Top-1 Validation Accuracy\n\n0.0001'
<EOS>
b'0.0002'
<EOS>
b'0.0004'
<EOS>
b'0.0006\n0.0008'
<EOS>
b'0.001'
<EOS>
b'0.002'
<EOS>
b'0.004\n0.006'
<EOS>
b'0.008'
<EOS>
b'0.010'
<EOS>
b'0.0001'
<EOS>
b'0.0002'
<EOS>
b'0.0004'
<EOS>
b'0.0006\n0.0008'
<EOS>
b'0.001'
<EOS>
b'0.002'
<EOS>
b'0.004\n0.006\n0.008'
<EOS>
b'0.010'
<EOS>
b'0.012'
<EOS>
b'0.014\n0.016\n0.020'
<EOS>
b'0.040\n0.060\n0.080'
<EOS>
b'0.100'
<EOS>
b'0.5521\n0.6089'
<EOS>
b'0.6432\n0.6465\n0.6479'
<EOS>
b'0.6604\n0.6408'
<EOS>
b'0.5687'
<EOS>
b'0.5165'
<EOS>
b'0.4812'
<EOS>
b'0.3673'
<EOS>
b'0.410319'
<EOS>
b'0.55263263'
<EOS>
b'0.6455485'
<EOS>
b'0.6774495'
<EOS>
b'0.6996867\n0.71010333'
<EOS>
b'0.73476154'
<EOS>
b'0.73286945'
<EOS>
b'0.72648114'
<EOS>
b'0.72214764'
<EOS>
b'0.71466064'
<EOS>
b'0.7081502'
<EOS>
b'0.6993001\n0.69108075'
<EOS>
b'0.67997235\n0.58658856\n0.51090497\n0.45174155'
<EOS>
b'0.40297446'
<EOS>
b'24'
<EOS>
b'Published as a conference paper at ICLR 2020'
<EOS>
b'Table 13:'
<EOS>
b'The accuracy information of tuning default AdamW optimizer for ImageNet training with\nResNet-50 (batch size ='
<EOS>
b'16384, 90 epochs, 7038 iterations).'
<EOS>
b'The target accuracy is around 0.763'
<EOS>
b'(Goyal et al., 2017).'
<EOS>
b'learning rate weight decay L2 regularization'
<EOS>
b'Top-1 Validation Accuracy'
<EOS>
b'0.0001'
<EOS>
b'0.0002'
<EOS>
b'0.0004'
<EOS>
b'0.0006\n0.0008'
<EOS>
b'0.001'
<EOS>
b'0.002'
<EOS>
b'0.004\n0.006\n0.008'
<EOS>
b'0.010'
<EOS>
b'0.012'
<EOS>
b'0.014\n0.016\n0.018\n0.020'
<EOS>
b'0.025'
<EOS>
b'0.030'
<EOS>
b'0.040'
<EOS>
b'0.050'
<EOS>
b'0.00001'
<EOS>
b'0.00001'
<EOS>
b'0.00001'
<EOS>
b'0.00001'
<EOS>
b'0.00001'
<EOS>
b'0.00001'
<EOS>
b'0.00001'
<EOS>
b'0.00001'
<EOS>
b'0.00001'
<EOS>
b'0.00001'
<EOS>
b'0.00001'
<EOS>
b'0.00001'
<EOS>
b'0.00001'
<EOS>
b'0.00001'
<EOS>
b'0.00001'
<EOS>
b'0.00001'
<EOS>
b'0.00001'
<EOS>
b'0.00001'
<EOS>
b'0.00001'
<EOS>
b'0.00001\n\ndefault (0.01)\ndefault (0.01)\ndefault (0.01)\ndefault (0.01)\ndefault (0.01)\ndefault (0.01)\ndefault (0.01)\ndefault (0.01)\ndefault (0.01)\ndefault (0.01)\ndefault (0.01)\ndefault (0.01)\ndefault (0.01)\ndefault (0.01)\ndefault (0.01)\ndefault (0.01)\ndefault (0.01)\ndefault (0.01)\ndefault (0.01)\ndefault (0.01)\n\n0.53312176'
<EOS>
b'0.5542806\n0.48769125'
<EOS>
b'0.46317545'
<EOS>
b'0.40903726'
<EOS>
b'0.42401123'
<EOS>
b'0.33870444'
<EOS>
b'0.12339274\n0.122924805'
<EOS>
b'0.08099365\n0.016764322'
<EOS>
b'0.032714844\n0.018147787'
<EOS>
b'0.0066731772'
<EOS>
b'0.010294597'
<EOS>
b'0.008260091\n0.008870442'
<EOS>
b'0.0064493814'
<EOS>
b'0.0018107096'
<EOS>
b'0.003540039'
<EOS>
b'25'
<EOS>
b'Published as a conference paper at ICLR 2020'
<EOS>
b'Table 14:'
<EOS>
b'The accuracy information of tuning default AdamW optimizer for ImageNet training with\nResNet-50 (batch size ='
<EOS>
b'16384, 90 epochs, 7038 iterations).'
<EOS>
b'The target accuracy is around 0.763'
<EOS>
b'(Goyal et al., 2017).'
<EOS>
b'learning rate weight decay L2 regularization'
<EOS>
b'Top-1 Validation Accuracy'
<EOS>
b'0.0001'
<EOS>
b'0.0002'
<EOS>
b'0.0004'
<EOS>
b'0.0006\n0.0008'
<EOS>
b'0.001'
<EOS>
b'0.002'
<EOS>
b'0.004\n0.006\n0.008'
<EOS>
b'0.010'
<EOS>
b'0.012'
<EOS>
b'0.014\n0.016\n0.018\n0.020'
<EOS>
b'0.025'
<EOS>
b'0.030'
<EOS>
b'0.040'
<EOS>
b'0.050'
<EOS>
b'0.0001'
<EOS>
b'0.0001'
<EOS>
b'0.0001'
<EOS>
b'0.0001'
<EOS>
b'0.0001'
<EOS>
b'0.0001'
<EOS>
b'0.0001'
<EOS>
b'0.0001'
<EOS>
b'0.0001'
<EOS>
b'0.0001'
<EOS>
b'0.0001'
<EOS>
b'0.0001'
<EOS>
b'0.0001'
<EOS>
b'0.0001'
<EOS>
b'0.0001'
<EOS>
b'0.0001'
<EOS>
b'0.0001'
<EOS>
b'0.0001'
<EOS>
b'0.0001'
<EOS>
b'0.0001\n\ndefault (0.01)\ndefault (0.01)\ndefault (0.01)\ndefault (0.01)\ndefault (0.01)\ndefault (0.01)\ndefault (0.01)\ndefault (0.01)\ndefault (0.01)\ndefault (0.01)\ndefault (0.01)\ndefault (0.01)\ndefault (0.01)\ndefault (0.01)\ndefault (0.01)\ndefault (0.01)\ndefault (0.01)\ndefault (0.01)\ndefault (0.01)\ndefault (0.01)\n\n0.55489093\n0.56514484'
<EOS>
b'0.4986979'
<EOS>
b'0.47595215'
<EOS>
b'0.44685873\n0.41029868\n0.2808024'
<EOS>
b'0.08111572\n0.068115234'
<EOS>
b'0.057922363\n0.05222575'
<EOS>
b'0.017313639'
<EOS>
b'0.029785156'
<EOS>
b'0.016540527'
<EOS>
b'0.00575765'
<EOS>
b'0.0102335615'
<EOS>
b'0.0060831704'
<EOS>
b'0.0036417644'
<EOS>
b'0.0010782877'
<EOS>
b'0.0037638347\n\n26'
<EOS>
b'Published as a conference paper at ICLR 2020'
<EOS>
b'Table 15:'
<EOS>
b'The accuracy information of tuning default AdamW optimizer for ImageNet training with\nResNet-50 (batch size ='
<EOS>
b'16384, 90 epochs, 7038 iterations).'
<EOS>
b'The target accuracy is around 0.763'
<EOS>
b'(Goyal et al., 2017).'
<EOS>
b'learning rate weight decay L2 regularization'
<EOS>
b'Top-1 Validation Accuracy'
<EOS>
b'0.0001'
<EOS>
b'0.0002'
<EOS>
b'0.0004'
<EOS>
b'0.0006\n0.0008'
<EOS>
b'0.001'
<EOS>
b'0.002'
<EOS>
b'0.004\n0.006\n0.008'
<EOS>
b'0.010'
<EOS>
b'0.012'
<EOS>
b'0.014\n0.016\n0.018\n0.020'
<EOS>
b'0.025'
<EOS>
b'0.030'
<EOS>
b'0.040'
<EOS>
b'0.050'
<EOS>
b'0.001'
<EOS>
b'0.001'
<EOS>
b'0.001'
<EOS>
b'0.001'
<EOS>
b'0.001'
<EOS>
b'0.001'
<EOS>
b'0.001'
<EOS>
b'0.001'
<EOS>
b'0.001'
<EOS>
b'0.001'
<EOS>
b'0.001'
<EOS>
b'0.001'
<EOS>
b'0.001'
<EOS>
b'0.001'
<EOS>
b'0.001'
<EOS>
b'0.001'
<EOS>
b'0.001'
<EOS>
b'0.001'
<EOS>
b'0.001'
<EOS>
b'0.001'
<EOS>
b'default (0.01)\ndefault (0.01)\ndefault (0.01)\ndefault (0.01)\ndefault (0.01)\ndefault (0.01)\ndefault (0.01)\ndefault (0.01)\ndefault (0.01)\ndefault (0.01)\ndefault (0.01)\ndefault (0.01)\ndefault (0.01)\ndefault (0.01)\ndefault (0.01)\ndefault (0.01)\ndefault (0.01)\ndefault (0.01)\ndefault (0.01)\ndefault (0.01)\n\n0.21142578\n0.4289144'
<EOS>
b'0.13537598\n0.33803305\n0.32611084\n0.22194417\n0.1833903'
<EOS>
b'0.08256022\n0.020507812'
<EOS>
b'0.018269857'
<EOS>
b'0.007507324\n0.020080566'
<EOS>
b'0.010762532\n0.0021362305\n0.007954915'
<EOS>
b'0.005859375\n0.009724935'
<EOS>
b'0.0019124349\n0.00390625'
<EOS>
b'0.0009969076\n\n27'
<EOS>
b'Published as a conference paper at ICLR 2020'
<EOS>
b'Table 16:'
<EOS>
b'The accuracy information of tuning default AdamW optimizer for ImageNet training with\nResNet-50 (batch size ='
<EOS>
b'16384, 90 epochs, 7038 iterations).'
<EOS>
b'The target accuracy is around 0.763'
<EOS>
b'(Goyal et al., 2017).'
<EOS>
b'learning rate weight decay L2 regularization'
<EOS>
b'Top-1 Validation Accuracy'
<EOS>
b'0.0001'
<EOS>
b'0.0002'
<EOS>
b'0.0004'
<EOS>
b'0.0006\n0.0008'
<EOS>
b'0.001'
<EOS>
b'0.002'
<EOS>
b'0.004\n0.006\n0.008'
<EOS>
b'0.010'
<EOS>
b'0.012'
<EOS>
b'0.014\n0.016\n0.018\n0.020'
<EOS>
b'0.025'
<EOS>
b'0.030'
<EOS>
b'0.040'
<EOS>
b'0.050\n\n0.01'
<EOS>
b'0.01'
<EOS>
b'0.01\n0.01'
<EOS>
b'0.01\n0.01'
<EOS>
b'0.01\n0.01'
<EOS>
b'0.01\n0.01'
<EOS>
b'0.01\n0.01'
<EOS>
b'0.01\n0.01'
<EOS>
b'0.01\n0.01'
<EOS>
b'0.01\n0.01'
<EOS>
b'0.01'
<EOS>
b'0.01'
<EOS>
b'0.0009765625'
<EOS>
b'0.0009969076\n0.0010172526'
<EOS>
b'0.0009358724'
<EOS>
b'0.0022379558\n0.001566569\n0.009480794'
<EOS>
b'0.0033569336'
<EOS>
b'0.0029907227'
<EOS>
b'0.0018513998\n0.009134929'
<EOS>
b'0.0022176106'
<EOS>
b'0.0040690103'
<EOS>
b'0.0017293295'
<EOS>
b'0.00061035156'
<EOS>
b'0.0022379558\n0.0017089844\n0.0014241537'
<EOS>
b'0.0020345051'
<EOS>
b'0.0012817383\n\ndefault (0.01)\ndefault (0.01)\ndefault (0.01)\ndefault (0.01)\ndefault (0.01)\ndefault (0.01)\ndefault (0.01)\ndefault (0.01)\ndefault (0.01)\ndefault (0.01)\ndefault (0.01)\ndefault (0.01)\ndefault (0.01)\ndefault (0.01)\ndefault (0.01)\ndefault (0.01)\ndefault (0.01)\ndefault (0.01)\ndefault (0.01)\ndefault (0.01)\n\n28'
<EOS>
b'Published as a conference paper at ICLR 2020'
<EOS>
b'Table 17:'
<EOS>
b'The accuracy information of tuning default AdamW optimizer for ImageNet training with\nResNet-50 (batch size ='
<EOS>
b'16384, 90 epochs, 7038 iterations).'
<EOS>
b'The target accuracy is around 0.763'
<EOS>
b'(Goyal et al., 2017).'
<EOS>
b'learning rate weight decay L2 regularization'
<EOS>
b'Top-1 Validation Accuracy'
<EOS>
b'0.0001'
<EOS>
b'0.0002'
<EOS>
b'0.0004'
<EOS>
b'0.0006\n0.0008'
<EOS>
b'0.001'
<EOS>
b'0.002'
<EOS>
b'0.004\n0.006\n0.008'
<EOS>
b'0.010'
<EOS>
b'0.012'
<EOS>
b'0.014\n0.016\n0.018\n0.020'
<EOS>
b'0.025'
<EOS>
b'0.030'
<EOS>
b'0.040'
<EOS>
b'0.050'
<EOS>
b'0.00001'
<EOS>
b'0.00001'
<EOS>
b'0.00001'
<EOS>
b'0.00001'
<EOS>
b'0.00001'
<EOS>
b'0.00001'
<EOS>
b'0.00001'
<EOS>
b'0.00001'
<EOS>
b'0.00001'
<EOS>
b'0.00001'
<EOS>
b'0.00001'
<EOS>
b'0.00001'
<EOS>
b'0.00001'
<EOS>
b'0.00001'
<EOS>
b'0.00001'
<EOS>
b'0.00001'
<EOS>
b'0.00001'
<EOS>
b'0.00001'
<EOS>
b'0.00001'
<EOS>
b'0.00001'
<EOS>
b'0.48917642'
<EOS>
b'0.58152264'
<EOS>
b'0.63460284'
<EOS>
b'0.64849854'
<EOS>
b'0.6598918\n0.6662801\n0.67266846'
<EOS>
b'0.6692708'
<EOS>
b'0.6573079'
<EOS>
b'0.6639404'
<EOS>
b'0.65230304'
<EOS>
b'0.6505534\n0.64990234'
<EOS>
b'0.65323895'
<EOS>
b'0.67026776\n0.66086835'
<EOS>
b'0.65425617'
<EOS>
b'0.6476237'
<EOS>
b'0.55478925'
<EOS>
b'0.61869305'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable\n\n29'
<EOS>
b'Published as a conference paper at ICLR 2020'
<EOS>
b'Table 18:'
<EOS>
b'The accuracy information of tuning default AdamW optimizer for ImageNet training with\nResNet-50 (batch size ='
<EOS>
b'16384, 90 epochs, 7038 iterations).'
<EOS>
b'The target accuracy is around 0.763'
<EOS>
b'(Goyal et al., 2017).'
<EOS>
b'learning rate weight decay L2 regularization'
<EOS>
b'Top-1 Validation Accuracy'
<EOS>
b'0.0001'
<EOS>
b'0.0002'
<EOS>
b'0.0004'
<EOS>
b'0.0006\n0.0008'
<EOS>
b'0.001'
<EOS>
b'0.002'
<EOS>
b'0.004\n0.006\n0.008'
<EOS>
b'0.010'
<EOS>
b'0.012'
<EOS>
b'0.014\n0.016\n0.018\n0.020'
<EOS>
b'0.025'
<EOS>
b'0.030'
<EOS>
b'0.040'
<EOS>
b'0.050'
<EOS>
b'0.0001'
<EOS>
b'0.0001'
<EOS>
b'0.0001'
<EOS>
b'0.0001'
<EOS>
b'0.0001'
<EOS>
b'0.0001'
<EOS>
b'0.0001'
<EOS>
b'0.0001'
<EOS>
b'0.0001'
<EOS>
b'0.0001'
<EOS>
b'0.0001'
<EOS>
b'0.0001'
<EOS>
b'0.0001'
<EOS>
b'0.0001'
<EOS>
b'0.0001'
<EOS>
b'0.0001'
<EOS>
b'0.0001'
<EOS>
b'0.0001'
<EOS>
b'0.0001'
<EOS>
b'0.0001\n\n0.5033366'
<EOS>
b'0.5949707\n0.62561035'
<EOS>
b'0.6545207\n0.66326904'
<EOS>
b'0.6677043'
<EOS>
b'0.67244464\n0.6702881\n0.66033936\n0.66426593\n0.66151935'
<EOS>
b'0.6545817'
<EOS>
b'0.65509033\n0.6529338\n0.65651447'
<EOS>
b'0.65334064'
<EOS>
b'0.655009'
<EOS>
b'0.64552814'
<EOS>
b'0.6425374'
<EOS>
b'0.5988159\n\ndisable'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable\n\n30'
<EOS>
b'Published as a conference paper at ICLR 2020'
<EOS>
b'Table 19:'
<EOS>
b'The accuracy information of tuning default AdamW optimizer for ImageNet training with\nResNet-50 (batch size ='
<EOS>
b'16384, 90 epochs, 7038 iterations).'
<EOS>
b'The target accuracy is around 0.763'
<EOS>
b'(Goyal et al., 2017).'
<EOS>
b'learning rate weight decay L2 regularization'
<EOS>
b'Top-1 Validation Accuracy'
<EOS>
b'0.0001'
<EOS>
b'0.0002'
<EOS>
b'0.0004'
<EOS>
b'0.0006\n0.0008'
<EOS>
b'0.001'
<EOS>
b'0.002'
<EOS>
b'0.004\n0.006\n0.008'
<EOS>
b'0.010'
<EOS>
b'0.012'
<EOS>
b'0.014\n0.016\n0.018\n0.020'
<EOS>
b'0.025'
<EOS>
b'0.030'
<EOS>
b'0.040'
<EOS>
b'0.050'
<EOS>
b'0.001'
<EOS>
b'0.001'
<EOS>
b'0.001'
<EOS>
b'0.001'
<EOS>
b'0.001'
<EOS>
b'0.001'
<EOS>
b'0.001'
<EOS>
b'0.001'
<EOS>
b'0.001'
<EOS>
b'0.001'
<EOS>
b'0.001'
<EOS>
b'0.001'
<EOS>
b'0.001'
<EOS>
b'0.001'
<EOS>
b'0.001'
<EOS>
b'0.001'
<EOS>
b'0.001'
<EOS>
b'0.001'
<EOS>
b'0.001'
<EOS>
b'0.001'
<EOS>
b'0.4611206'
<EOS>
b'0.0076293945'
<EOS>
b'0.29233804'
<EOS>
b'0.57295734'
<EOS>
b'0.5574748'
<EOS>
b'0.5988566'
<EOS>
b'0.586263'
<EOS>
b'0.62076825'
<EOS>
b'0.61503094\n0.4697876\n0.619751'
<EOS>
b'0.54243976'
<EOS>
b'0.5429077'
<EOS>
b'0.55281574\n0.5819295'
<EOS>
b'0.5938924\n0.541097\n0.45890298\n0.56193036'
<EOS>
b'0.5279134'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable\n\n31'
<EOS>
b'Published as a conference paper at ICLR 2020'
<EOS>
b'Table 20:'
<EOS>
b'The accuracy information of tuning default AdamW optimizer for ImageNet training with\nResNet-50 (batch size ='
<EOS>
b'16384, 90 epochs, 7038 iterations).'
<EOS>
b'The target accuracy is around 0.763'
<EOS>
b'(Goyal et al., 2017).'
<EOS>
b'learning rate weight decay L2 regularization'
<EOS>
b'Top-1 Validation Accuracy'
<EOS>
b'0.0001'
<EOS>
b'0.0002'
<EOS>
b'0.0004'
<EOS>
b'0.0006\n0.0008'
<EOS>
b'0.001'
<EOS>
b'0.002'
<EOS>
b'0.004\n0.006\n0.008'
<EOS>
b'0.010'
<EOS>
b'0.012'
<EOS>
b'0.014\n0.016\n0.018\n0.020'
<EOS>
b'0.025'
<EOS>
b'0.030'
<EOS>
b'0.040'
<EOS>
b'0.050\n\n0.01'
<EOS>
b'0.01'
<EOS>
b'0.01\n0.01'
<EOS>
b'0.01\n0.01'
<EOS>
b'0.01\n0.01'
<EOS>
b'0.01\n0.01'
<EOS>
b'0.01\n0.01'
<EOS>
b'0.01\n0.01'
<EOS>
b'0.01\n0.01'
<EOS>
b'0.01\n0.01'
<EOS>
b'0.01'
<EOS>
b'0.01\n\n0.0009969076'
<EOS>
b'0.0008951823\n0.00095621747'
<EOS>
b'0.0012817383'
<EOS>
b'0.016886393'
<EOS>
b'0.038146973'
<EOS>
b'0.0015258789\n0.0014241537'
<EOS>
b'0.081441246'
<EOS>
b'0.028116861\n0.011820476\n0.08138021\n0.010111491\n0.0041910806\n0.0038248699'
<EOS>
b'0.002746582'
<EOS>
b'0.011555989\n0.0065104165'
<EOS>
b'0.016438803'
<EOS>
b'0.007710775'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'32'
<EOS>
b'Published as a conference paper at ICLR 2020'
<EOS>
b'Table 21:'
<EOS>
b'The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet-\n50'
<EOS>
b'(batch size ='
<EOS>
b'16384, 90 epochs, 7038 iterations).'
<EOS>
b'We use the learning rate recipe of (Goyal et al.,\n2017): (1)'
<EOS>
b'5-epoch warmup to stablize the initial stage; and (2) multiply the learning rate by 0.1 at\n30th, 60th, and 80th epoch.'
<EOS>
b'The target accuracy is around 0.763 (Goyal et al., 2017).'
<EOS>
b'learning rate weight decay L2 regularization'
<EOS>
b'Top-1 Validation Accuracy'
<EOS>
b'0.0001'
<EOS>
b'0.0002'
<EOS>
b'0.0004'
<EOS>
b'0.0006\n0.0008'
<EOS>
b'0.001'
<EOS>
b'0.002'
<EOS>
b'0.004\n0.006\n0.008\n0.0001'
<EOS>
b'0.0002'
<EOS>
b'0.0004'
<EOS>
b'0.0006\n0.0008'
<EOS>
b'0.001'
<EOS>
b'0.002'
<EOS>
b'0.004\n0.006'
<EOS>
b'0.008\n\n0.01'
<EOS>
b'0.01\n0.01'
<EOS>
b'0.01\n0.01'
<EOS>
b'0.01\n0.01'
<EOS>
b'0.01'
<EOS>
b'0.01'
<EOS>
b'0.01'
<EOS>
b'0.001'
<EOS>
b'0.001'
<EOS>
b'0.001'
<EOS>
b'0.001'
<EOS>
b'0.001'
<EOS>
b'0.001'
<EOS>
b'0.001'
<EOS>
b'0.001'
<EOS>
b'0.001'
<EOS>
b'0.001'
<EOS>
b'default (0.01)\ndefault (0.01)\ndefault (0.01)\ndefault (0.01)\ndefault (0.01)\ndefault (0.01)\ndefault (0.01)\ndefault (0.01)\ndefault (0.01)\ndefault (0.01)\ndefault (0.01)\ndefault (0.01)\ndefault (0.01)\ndefault (0.01)\ndefault (0.01)\ndefault (0.01)\ndefault (0.01)\ndefault (0.01)\ndefault (0.01)\ndefault (0.01)\n\n0.0009969076'
<EOS>
b'0.0009969076\n0.0009969076'
<EOS>
b'0.0009358724'
<EOS>
b'0.0009969076\n0.0009765625'
<EOS>
b'0.0010172526'
<EOS>
b'0.0010172526'
<EOS>
b'0.0010172526'
<EOS>
b'0.0010172526'
<EOS>
b'0.0010172526'
<EOS>
b'0.0010172526'
<EOS>
b'0.0010172526\n0.0009969076'
<EOS>
b'0.0010172526'
<EOS>
b'0.0010172526'
<EOS>
b'0.0010172526'
<EOS>
b'0.0038452148'
<EOS>
b'0.011881511'
<EOS>
b'0.0061442056'
<EOS>
b'33'
<EOS>
b'Published as a conference paper at ICLR 2020'
<EOS>
b'Table 22:'
<EOS>
b'The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet-\n50'
<EOS>
b'(batch size ='
<EOS>
b'16384, 90 epochs, 7038 iterations).'
<EOS>
b'We use the learning rate recipe of (Goyal et al.,\n2017): (1)'
<EOS>
b'5-epoch warmup to stablize the initial stage; and (2) multiply the learning rate by 0.1 at\n30th, 60th, and 80th epoch.'
<EOS>
b'The target accuracy is around 0.763 (Goyal et al., 2017).'
<EOS>
b'learning rate weight decay L2 regularization'
<EOS>
b'Top-1 Validation Accuracy'
<EOS>
b'0.0001'
<EOS>
b'0.0002'
<EOS>
b'0.0004'
<EOS>
b'0.0006\n0.0008'
<EOS>
b'0.001'
<EOS>
b'0.002'
<EOS>
b'0.004\n0.006\n0.008\n0.0001'
<EOS>
b'0.0002'
<EOS>
b'0.0004'
<EOS>
b'0.0006\n0.0008'
<EOS>
b'0.001'
<EOS>
b'0.002'
<EOS>
b'0.004\n0.006\n0.008'
<EOS>
b'0.0001'
<EOS>
b'0.0001'
<EOS>
b'0.0001'
<EOS>
b'0.0001'
<EOS>
b'0.0001'
<EOS>
b'0.0001'
<EOS>
b'0.0001'
<EOS>
b'0.0001'
<EOS>
b'0.0001'
<EOS>
b'0.0001'
<EOS>
b'0.00001'
<EOS>
b'0.00001'
<EOS>
b'0.00001'
<EOS>
b'0.00001'
<EOS>
b'0.00001'
<EOS>
b'0.00001'
<EOS>
b'0.00001'
<EOS>
b'0.00001'
<EOS>
b'0.00001'
<EOS>
b'0.00001\n\ndefault (0.01)\ndefault (0.01)\ndefault (0.01)\ndefault (0.01)\ndefault (0.01)\ndefault (0.01)\ndefault (0.01)\ndefault (0.01)\ndefault (0.01)\ndefault (0.01)\ndefault (0.01)\ndefault (0.01)\ndefault (0.01)\ndefault (0.01)\ndefault (0.01)\ndefault (0.01)\ndefault (0.01)\ndefault (0.01)\ndefault (0.01)\ndefault (0.01)\n\n0.3665975\n0.5315755'
<EOS>
b'0.6369222'
<EOS>
b'0.6760457'
<EOS>
b'0.69557697'
<EOS>
b'0.7076009'
<EOS>
b'0.73065186'
<EOS>
b'0.72806805'
<EOS>
b'0.72161865'
<EOS>
b'0.71816\n\n0.49804688\n0.6287028'
<EOS>
b'0.6773885'
<EOS>
b'0.67348224\n0.6622111'
<EOS>
b'0.6468709'
<EOS>
b'0.5846761'
<EOS>
b'0.4868978'
<EOS>
b'0.34969077'
<EOS>
b'0.31193033'
<EOS>
b'34'
<EOS>
b'Published as a conference paper at ICLR 2020'
<EOS>
b'Table 23:'
<EOS>
b'The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet-\n50'
<EOS>
b'(batch size ='
<EOS>
b'16384, 90 epochs, 7038 iterations).'
<EOS>
b'We use the learning rate recipe of (Goyal et al.,\n2017): (1)'
<EOS>
b'5-epoch warmup to stablize the initial stage; and (2) multiply the learning rate by 0.1 at\n30th, 60th, and 80th epoch.'
<EOS>
b'The target accuracy is around 0.763 (Goyal et al., 2017).'
<EOS>
b'learning rate weight decay L2 regularization'
<EOS>
b'Top-1 Validation Accuracy'
<EOS>
b'0.0001'
<EOS>
b'0.0002'
<EOS>
b'0.0004'
<EOS>
b'0.0006\n0.0008'
<EOS>
b'0.001'
<EOS>
b'0.002'
<EOS>
b'0.004\n0.006\n0.008\n0.0001'
<EOS>
b'0.0002'
<EOS>
b'0.0004'
<EOS>
b'0.0006\n0.0008'
<EOS>
b'0.001'
<EOS>
b'0.002'
<EOS>
b'0.004\n0.006'
<EOS>
b'0.008\n\n0.01'
<EOS>
b'0.01\n0.01'
<EOS>
b'0.01\n0.01'
<EOS>
b'0.01\n0.01'
<EOS>
b'0.01'
<EOS>
b'0.01'
<EOS>
b'0.01'
<EOS>
b'0.001'
<EOS>
b'0.001'
<EOS>
b'0.001'
<EOS>
b'0.001'
<EOS>
b'0.001'
<EOS>
b'0.001'
<EOS>
b'0.001'
<EOS>
b'0.001'
<EOS>
b'0.001'
<EOS>
b'0.001'
<EOS>
b'0.0010172526'
<EOS>
b'0.0009765625'
<EOS>
b'0.0010172526'
<EOS>
b'0.0009969076'
<EOS>
b'0.0010172526'
<EOS>
b'0.0009765625'
<EOS>
b'0.0009969076\n0.0009969076'
<EOS>
b'0.0009765625'
<EOS>
b'0.0010172526'
<EOS>
b'0.0009765625'
<EOS>
b'0.0010172526'
<EOS>
b'0.0010172526'
<EOS>
b'0.0010172526'
<EOS>
b'0.0010172526'
<EOS>
b'0.0009969076\n0.0010579427'
<EOS>
b'0.0016886393'
<EOS>
b'0.019714355'
<EOS>
b'0.1329956'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable\n\n35'
<EOS>
b'Published as a conference paper at ICLR 2020'
<EOS>
b'Table 24:'
<EOS>
b'The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet-\n50'
<EOS>
b'(batch size ='
<EOS>
b'16384, 90 epochs, 7038 iterations).'
<EOS>
b'We use the learning rate recipe of (Goyal et al.,\n2017): (1)'
<EOS>
b'5-epoch warmup to stablize the initial stage; and (2) multiply the learning rate by 0.1 at\n30th, 60th, and 80th epoch.'
<EOS>
b'The target accuracy is around 0.763 (Goyal et al., 2017).'
<EOS>
b'learning rate weight decay L2 regularization'
<EOS>
b'Top-1 Validation Accuracy'
<EOS>
b'0.28515625'
<EOS>
b'0.44055176'
<EOS>
b'0.56815594'
<EOS>
b'0.6234741'
<EOS>
b'0.6530762'
<EOS>
b'0.6695964'
<EOS>
b'0.70048016'
<EOS>
b'0.71698'
<EOS>
b'0.72021484'
<EOS>
b'0.7223918\n0.72017413'
<EOS>
b'0.72058105\n0.7188924'
<EOS>
b'0.71695966\n0.7154134'
<EOS>
b'0.71358234'
<EOS>
b'0.7145386\n0.7114258'
<EOS>
b'0.7066447'
<EOS>
b'0.70284015'
<EOS>
b'0.0001'
<EOS>
b'0.0002'
<EOS>
b'0.0004'
<EOS>
b'0.0006\n0.0008'
<EOS>
b'0.001'
<EOS>
b'0.002'
<EOS>
b'0.004\n0.006\n0.008'
<EOS>
b'0.010'
<EOS>
b'0.012'
<EOS>
b'0.014\n0.016\n0.018\n0.020'
<EOS>
b'0.025'
<EOS>
b'0.030'
<EOS>
b'0.040'
<EOS>
b'0.050'
<EOS>
b'0.0001'
<EOS>
b'0.0001'
<EOS>
b'0.0001'
<EOS>
b'0.0001'
<EOS>
b'0.0001'
<EOS>
b'0.0001'
<EOS>
b'0.0001'
<EOS>
b'0.0001'
<EOS>
b'0.0001'
<EOS>
b'0.0001'
<EOS>
b'0.0001'
<EOS>
b'0.0001'
<EOS>
b'0.0001'
<EOS>
b'0.0001'
<EOS>
b'0.0001'
<EOS>
b'0.0001'
<EOS>
b'0.0001'
<EOS>
b'0.0001'
<EOS>
b'0.0001'
<EOS>
b'0.0001'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable\n\n36'
<EOS>
b'Published as a conference paper at ICLR 2020'
<EOS>
b'Table 25:'
<EOS>
b'The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet-\n50'
<EOS>
b'(batch size ='
<EOS>
b'16384, 90 epochs, 7038 iterations).'
<EOS>
b'We use the learning rate recipe of (Goyal et al.,\n2017): (1)'
<EOS>
b'5-epoch warmup to stablize the initial stage; and (2) multiply the learning rate by 0.1 at\n30th, 60th, and 80th epoch.'
<EOS>
b'The target accuracy is around 0.763 (Goyal et al., 2017).'
<EOS>
b'learning rate weight decay L2 regularization'
<EOS>
b'Top-1 Validation Accuracy'
<EOS>
b'0.31247965'
<EOS>
b'0.4534912'
<EOS>
b'0.57765704'
<EOS>
b'0.6277669'
<EOS>
b'0.65321857'
<EOS>
b'0.6682129\n0.69938153'
<EOS>
b'0.7095947'
<EOS>
b'0.710612\n0.70857745\n0.7094116'
<EOS>
b'0.70717365\n0.7109375'
<EOS>
b'0.7058309\n0.7052409'
<EOS>
b'0.7064412'
<EOS>
b'0.7035319'
<EOS>
b'0.6994629'
<EOS>
b'0.6972656'
<EOS>
b'0.6971232'
<EOS>
b'0.0001'
<EOS>
b'0.0002'
<EOS>
b'0.0004'
<EOS>
b'0.0006\n0.0008'
<EOS>
b'0.001'
<EOS>
b'0.002'
<EOS>
b'0.004\n0.006\n0.008'
<EOS>
b'0.010'
<EOS>
b'0.012'
<EOS>
b'0.014\n0.016\n0.018\n0.020'
<EOS>
b'0.025'
<EOS>
b'0.030'
<EOS>
b'0.040'
<EOS>
b'0.050'
<EOS>
b'0.00001'
<EOS>
b'0.00001'
<EOS>
b'0.00001'
<EOS>
b'0.00001'
<EOS>
b'0.00001'
<EOS>
b'0.00001'
<EOS>
b'0.00001'
<EOS>
b'0.00001'
<EOS>
b'0.00001'
<EOS>
b'0.00001'
<EOS>
b'0.00001'
<EOS>
b'0.00001'
<EOS>
b'0.00001'
<EOS>
b'0.00001'
<EOS>
b'0.00001'
<EOS>
b'0.00001'
<EOS>
b'0.00001'
<EOS>
b'0.00001'
<EOS>
b'0.00001'
<EOS>
b'0.00001'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'disable'
<EOS>
b'37'
<EOS>
