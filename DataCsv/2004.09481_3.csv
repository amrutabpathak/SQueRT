0 2 0 2    r p A 0 2         ]  R Cs c [      1 v 1 8 4 9 0  4 0 0 2 : v i X r a  Connecting Robust Shuﬄe Privacy and Pan-Privacy  Victor Balcer∗  Albert Cheu†  Matthew Joseph‡  Jieming Mao§  April 21, 2020  Abstract  In the shuﬄe model of diﬀerential privacy, data-holding users send randomized messages to a secure shuﬄer, the shuﬄer permutes the messages, and the resulting collection of messages must be diﬀerentially private with regard to user data.In the pan-private model, an algorithm processes a stream of data while maintaining an internal state that is diﬀerentially private with regard to the stream data.We give evidence connecting these two apparently diﬀerent models.
<EOS>
Our results focus on robustly shuﬄe private protocols whose privacy guarantees are not greatly aﬀected by malicious users.First, we give robustly shuﬄe private protocols and upper bounds for counting distinct elements and uniformity testing.Second, we use pan-private lower bounds to prove robustly shuﬄe private lower bounds for both problems.
<EOS>
Focusing on the dependence on the domain size k, we find that both robust shuﬄe privacy and pan-privacy have additive accuracy Θ(√k) for counting distinct elements and sample complexity ˜Θ(k2/3) for uniformity testing.Both results polynomially separate central privacy and robust shuﬄe privacy.Finally, we show that this connection is useful in both directions: we give a pan-private adaptation of recent work on shuﬄe private histograms and use it to recover further separations between pan-privacy and interactive local privacy.
<EOS>
 1  Introduction  Diﬀerential privacy [17] guarantees that an algorithm’s output is quantifiably insensitive to small changes in its input.This insensitivity ensures that diﬀerentially private algorithms are not greatly aﬀected by any one data point, which in turn provides privacy for data contributors.The basic diﬀerential privacy framework is the foundation for many models, including central [17], local [17, 10, 27], pan- [18], blended [5], and most recently shuﬄe [15] privacy.
<EOS>
 This work focuses on pan-privacy and shuﬄe privacy.A pan-private algorithm receives a stream of raw data and processes it one element at a time.After seeing each element, the algorithm updates its internal state to incorporate information from the new element and then proceeds to the next element in the stream.
<EOS>
At the end of the stream, the algorithm processes its final internal state to extract and output useful information.Privacy constrains the internal state and output to be diﬀerentially private functions of the stream: changing one element of the stream must not greatly aﬀect the joint distribution of any one internal state and the final output. In shuﬄe privacy, there is no algorithm receiving a stream of data.
<EOS>
Instead, the data is dis- tributed, and each user holds a single data point.The users follow a prescribed protocol where each employs a randomizer function to compute messages based on their data point and then sends  ∗Harvard University, vbalcer@g.harvard.edu †Northeastern University, cheu.a@husky.neu.edu ‡University of Pennsylvania, majos@cis.upenn.edu §Google New York, maojm@google.com  1  these messages to a secure shuﬄer1.The shuﬄer randomly permutes the messages and releases the shuﬄed collection of messages publicly.
<EOS>
Finally, an analyzer processes the public shuﬄer output to extract useful information.Here, privacy constrains the shuﬄer’s output: changing one user’s data point must not greatly aﬀect the distribution for the messages released by the shuﬄer.Privacy is over the random coins of users’ randomizers.
<EOS>
Since the analyzer only post-processes these messages, its actions do not aﬀect users’ privacy guarantees (see Fact 2.2 for details on post-processing). One possible complication of the shuﬄe model is that, without further restrictions, the privacy guarantees of shuﬄe private protocols are not necessarily robust to malicious users.For example, it is possible for a shuﬄe private protocol to place all responsibility for “noisy messages” on a single user.
<EOS>
In that case, compromising that single user would destroy the privacy guarantee for all users of the protocol.To avoid this weakness, we focus on protocols that satisfy robust shuﬄe privacy.These protocols still guarantee privacy for honest users in the presence of (a limited number of) malicious users.
<EOS>
 1.1 Our Contributions  We give several results connecting robust shuﬄe privacy and pan-privacy.Details and comparisons appear in Figure 1. 1.
<EOS>
We construct a protocol for counting distinct elements that satisfies approximate robust shuﬄe privacy and obtains additive error O(√k/ε) (Theorem 3.2).We then strengthen and adapt a lower bound from pan-privacy to show that Ω(pk/ε) additive error is necessary when the number of users n ≥ k (Theorem 3.9).In contrast, adding Lap (1/ε) noise to the true distinct count guarantees O(1/ε) error in the central model.
<EOS>
 2.We construct a protocol for uniformity testing that satisfies pure robust shuﬄe privacy with  sample complexity dependence on k of ˜O(cid:0)k2/3(cid:1) (Theorem 4.9).We again adapt a lower bound from pan-privacy to show that this dependence on k is optimal up to a log1/2(k) factor (Theorem 4.13).
<EOS>
We also give a tester that satisfies approximate robust shuﬄe privacy and obtains slightly better dependence on ε (Theorem 4.10).In contrast, the central model requires only O(√k) samples [2]. 3.
<EOS>
We show how to adapt recent work on shuﬄe private histograms [6] for pan-privacy (The- orem 5.2).As a corollary, pan-privacy inherits the same separations from interactive local privacy as the shuﬄe model for “support identification” problems. Taken together, these results oﬀer preliminary evidence connecting pan-privacy and robust shuﬄe privacy.
<EOS>
However, they do not rule out the existence of problems for which pan-privacy and robust shuﬄe privacy must perform diﬀerently.Finding such separations — or a more general connection — may be an interesting avenue for future work. 1.2 Related Work  Dwork, Naor, Pitassi, Rothblum, and Yekhanin [18] introduced pan-privacy and, for a data domain of size k, showed how to pan-privately count distinct elements to additive accuracy O(√k/ε).
<EOS>
Mir, Muthukrishnan, Nikolov, and Wright [28] constructed a low-memory analogue with the same accuracy guarantee and a matching (for n ≥ k) lower bound based on linear program decoding.1 In practice, this may be a cryptographic protocol for multi-party shuﬄing rather than a trusted third-party  shuﬄer. 2  Privacy Type  Histograms  (ℓ∞ error)  Central  Pan  Θ( 1  ε log(cid:0)min(cid:0) 1 δ , k(cid:1)(cid:1)) [12] δ(cid:1)(cid:1) ε2 log(cid:0) 1 O(cid:0) 1  Robust Shuﬄe  O(cid:0) 1 ε2 log(cid:0) 1  δ(cid:1)(cid:1) [6]  α4/3ε2/3 +  α4/3ε4/3 +  α2 +  √k α√ε + k1/3  Θ(cid:16)√k O(cid:16) k2/3 Ω(cid:16) k2/3 O(cid:16)h k2/3 Ω(cid:16) k2/3 O(cid:18)h k2/3  α4/3ε2/3 + √k α2 + √k α2 + √k α2 + √k α2 +  α4/3ε2/3 +  α4/3ε2/3 +  √k  α4/3ε2/3 + 1 √k α2 + √k α√ε + 1 √k  αε(cid:17) [2] αε(cid:17) [4] * αε(cid:17) [4] * αε2iplog(k)(cid:17) * αε(cid:17) * δ(cid:1)(cid:19) αεiqlog(cid:0) k  √k α√ε + 1 √k  α-Uniformity Testing  Distinct Elements  (sample complexity)  (additive error)  ε(cid:1) [17] * Θ(cid:0) 1 O(cid:16)√k ε (cid:17) [18] * ε(cid:19) Ω(cid:18)q k O(cid:16)√k ε (cid:17) Ω(q k  ε )  Figure 1: Overview of our main results given a data domain of size k, ε = O(1) and δ < 1/n < ε.
<EOS>
All bounds hold with constant probability.Results marked by * hold when δ = 0.Uncited results are new in this work.
<EOS>
 Both works employ user-level pan-privacy, where one user may contribute many elements to the stream, and a neighboring stream may replace all of a user’s contributions.We instead imitate Amin, Joseph, and Mao [4] and study record-level pan-privacy.Here, neighboring streams diﬀer in at most one stream element.
<EOS>
For uniformity testing, Amin et al [4] gave tight (in the domain size k) Θ(k2/3) sample complexity bounds for pure pan-privacy.Acharya, Sun, and Zhang [2] showed that Θ(√k) is the optimal dependence under approximate central privacy, and Θ(k) is optimal for the strictly more private model of approximate sequentially interactive local privacy [3, 4] (see Section 2 for a definition of local privacy). Building on the empirical work of Bittau, Erlingsson, Maniatis, Mironov, Raghunathan, Lie, Rudominer, Kode, Tinnes, and Seefeld [11], Cheu, Smith, Ullman, Zeber, and Zhilyaev [15] and Erlingsson, Feldman, Mironov, Raghunathan, Talwar, and Thakurta [20] independently and simul- taneously introduced diﬀerent formal definitions of shuﬄe privacy.
<EOS>
Cheu et al [15] defined a model where a single noninteractive batch of messages is shuﬄed, while Erlingsson et al [20] considered shuﬄed users who may participate in an interactive protocol.We follow most of the shuﬄe privacy literature and use the former variant.We also focus on the multi-message model where each user may send multiple messages to the shuﬄer.
<EOS>
For the problem of shuﬄe private bit summation, a line of papers [15, 7, 9, 23] has obtained O(1/ε) accuracy tight with that of the central model.For the problem of computing histograms, the best known shuﬄe private ℓ∞ guarantee diﬀers by a 1/ε factor from the central private guarantee [21, 6]. We brieﬂy discuss past work touching on robust shuﬄe privacy.
<EOS>
Cheu et al [15] showed that any single-message (ε, δ)-shuﬄe private protocol on n users also grants (ε + ln(n), δ)-local privacy.This is a form of robustness, as it guarantees privacy even when all other users are malicious.However, it only holds for single-message protocols, which are strictly weaker than multi-message protocols.
<EOS>
Along similar lines, Balcer and Cheu [6] showed that any single-message ε-shuﬄe private protocol is also ε-locally private.Balle, Bell, Gasc´on and Nissim [9] discussed the eﬀect of malicious users on the accuracy guarantee of a shuﬄe protocol.In contrast, our notion of robust shuﬄe privacy guarantees privacy even with (a controlled fraction of) malicious users in multi-message protocols.
<EOS>
 3  1.3 Organization  Basic definitions appear in the Preliminaries (Section 2).Further specific preliminaries appear in their corresponding sections.We start with robustly shuﬄe private distinct elements (Section 3), then cover robustly shuﬄe private uniformity testing (Section 4) and pan-private histograms (Sec- tion 5).
<EOS>
We conclude with a brief discussion and some further questions in Section 6. 2 Preliminaries  We typically use [k] = {1, 2,., k}, N = {1, 2,. .}, Zk = {0, 1,., k}, and Z≥0 = N ∪ {0}. 2.1 Diﬀerential Privacy We define a dataset ~x ∈ X n to be an ordered tuple of n rows where each row is drawn from a data universe X and corresponds to the data of one user.
<EOS>
Two datasets ~x, ~x ′ ∈ X n are considered neighbors (denoted as ~x ∼ ~x ′) if they diﬀer in at most one row.Definition 2.1 (Diﬀerential Privacy [17]).An algorithm M : X n → Z satisfies (ε, δ)-diﬀerential privacy if, for every pair of neighboring datasets ~x and ~x′ and every subset T ⊂ Z,  When δ > 0, we say M satisfies approximate diﬀerential privacy.
<EOS>
When δ = 0, M satisfies  pure diﬀerential privacy and we omit the δ parameter. P [M(~x) ∈ T ] ≤ eε · P(cid:2)M(~x ′) ∈ T(cid:3) + δ. Because this definition assumes that the algorithm M has “central” access to compute on the entire raw dataset, we sometimes call this central diﬀerential privacy.
<EOS>
Two common facts about diﬀerentially privacy will be useful; proofs appear in Chapter 2 of the survey of Dwork and Roth [19].First, privacy is preserved under post-processing.Fact 2.2. For (ε, δ)-diﬀerentially private algorithm A : X n → Z and arbitrary random function f : Z → Z′, f ◦ A is (ε, δ)-diﬀerentially private.
<EOS>
 This means that any computation based solely on the output of a diﬀerentially private function  does not aﬀect the privacy guarantee.Second, privacy composes neatly. Fact 2.3. For (ε1, δ1)-diﬀerentially private A1 and (ε2, δ2)-diﬀerentially private A2, A3 defined by A3(D) = (A1(D),A2(D)) is (ε1 + ε2, δ1 + δ2)-diﬀerentially private.
<EOS>
 One useful centrally private algorithm is the binomial mechanism. |f (~x) − f (~x ′)| ≤ 1 for all Lemma 2.4 ([16]).Let f : X n → Z be a 1-sensitive function, i.e. eε−1(cid:17)2 neighboring datasets ~x, ~x ′ ∈ X n. Then for ε > 0, δ ∈ (0, 1), and λ ≥ 20(cid:16) eε+1 δ(cid:1), the ln(cid:0) 2 algorithm that samples η ∼ Bin(λ, 1/2) then outputs f (~x) + η is (ε, δ)-diﬀerentially private. We will also use the geometric mechanism.
<EOS>
Let SG(ε) denote the symmetric geometric distri- eε+1 , and may be viewed as a discrete  bution with parameter ε.For every v ∈ Z, it has mass e−ε|v|· eε−1  analogue of the Laplace distribution with mean 0.Lemma 2.5 ([24]).
<EOS>
Let f : X n → Z be a 1-sensitive function, i.e. |f (~x) − f (~x ′)| ≤ 1 for all neighboring datasets ~x, ~x ′ ∈ X n. Then for ε > 0, the algorithm that samples η ∼ SG(ε) then outputs f (~x) + η is ε-diﬀerentially private. 4  2.2 Pan-privacy  Pan-privacy is defined for a diﬀerent setting.Unlike centrally private algorithms, pan-private algorithms are online: they receive raw data one element at a time in a stream.
<EOS>
At each step in the stream, the algorithm receives a data point, update its internal state based on this data point, and then proceeds to the next element.The only way the algorithm “remembers” past elements is through its internal state.As in the case of datasets, we say that two streams ~x and ~x ′ are neighbors if they diﬀer in at most one element.
<EOS>
Pan-privacy requires the algorithm’s internal state and output to be diﬀerentially private with regard to neighboring streams. Definition 2.6 (Online Algorithm).An online algorithm Q is defined by an internal algorithm QI and an output algorithm QO.
<EOS>
Q processes a stream of elements through repeated application of QI : X ×I → I, which (with randomness) maps a stream element and internal state to an internal state.At the end of the stream, Q publishes a final output based on its final internal state, QO(i).Definition 2.7 (Pan-privacy [18, 4]).
<EOS>
Given an online algorithm Q, let QI(~x) denote its internal state after processing stream ~x, and let ~x≤t be the first t elements of ~x.We say Q is (ε, δ)-pan- private if, for any neighboring streams ~x and ~x ′, any time t, any set of internal states I ⊂ I, and any set of output states O ⊂ O,  PQ [QI(~x≤t) ∈ I,QO(QI (~x)) ∈ O] ≤ eεPQ(cid:2)QI(~x ′  ≤t) ∈ I,QO(QI (~x ′)) ∈ O(cid:3) + δ. When δ = 0, we say Q is ε-pan-private.
<EOS>
 (1)  Taken together, these requirements protect against an adversary that sees any one internal state of Q as well as its final output2.By the output requirement, any pan-private algorithm also satisfies central diﬀerential privacy.The key additional contribution of pan-privacy is the maintenance of the diﬀerentially private internal state.
<EOS>
This strengthens the central privacy guarantee by protecting data contributors against future events.For example, a user may trust the current algorithm operator but wish to protect themselves against the possibility that the operator will be acquired or subpoenaed in the future.Under pan-privacy, post-processing (Fact 2.2) ensures that future views of the pan-private algorithm’s state will be diﬀerentially private with respect to past data.
<EOS>
 Our definition of pan-privacy is the specific variant given by Amin et al [4].This version guarantees record-level (uncertainty about the presence of any single stream element) rather than user-level (uncertainty about the presence of any one data universe element) privacy.We use this variant because, like the shuﬄe model, we assume each data contributor has a single data point.
<EOS>
 2.3 Shuﬄe Privacy  The shuﬄe model of diﬀerential privacy views the dataset as a distributed object where each of n users holds a single row.Each user provides their data point as input to a randomizing function and securely submits the resulting randomized outputs to a shuﬄer.The shuﬄer permutes the users’ outputs and releases the shuﬄed messages.
<EOS>
It is this collection of messages that needs to satisfy diﬀerential privacy: altering one user’s data point must not greatly change the distribution of the shuﬄed messages. In this way, the shuﬄe model strengthens the privacy guarantee that users receive.Here, users need only trust that (1) there is a secure way to shuﬄe the randomized messages3 and (2) suﬃciently  2As shown by Amin et al [4], the precise assumption of one internal state intrusion is necessary.
<EOS>
Pan-privacy  against multiple internal state intrusions collapses to the much stronger notion of local privacy. 3A more detailed description of a shuﬄer and its implementation details appears in the work of Bittau et al [11]. 5  many users follow the protocol.
<EOS>
In contrast, central diﬀerential privacy requires users to trust a third party algorithm operator to securely store and compute on the raw data. Definition 2.8 (Shuﬄe Model [11, 15]).A protocol P in the shuﬄe model consists of three ran- domized algorithms:  • A randomizer R : X → Y∗ mapping data to (possibly variable-length) vectors.
<EOS>
The length of the vector is the number of messages sent.If, on all inputs, the probability of sending a single message is 1, then the protocol is said to be single-message.Otherwise, the protocol is multi-message.
<EOS>
 • A shuﬄer S : Y∗ → Y∗ that concatenates all message vectors and then applies a uniformly  random permutation to the messages. • An analyzer A : Y∗ → Z that computes on a permutation of messages. As S is the same in every protocol, we identify each shuﬄe protocol by P = (R,A) and define its execution on input ~x ∈ X n as  P(~x) = A(S(R(x1),., R(xn))).
<EOS>
 We assume that R and A have access to n and an arbitrary amount of public randomness.With this setup, we use the following definition of shuﬄe diﬀerential privacy. Definition 2.9 (Shuﬄe Diﬀerential Privacy [15]).
<EOS>
A protocol P = (R,A) is (ε, δ)-shuﬄe diﬀeren- tially private if, for all n ∈ N, the algorithm  (S ◦ Rn)(x1,., xn) := S(R(x1),.,R(xn))  is (ε, δ)-diﬀerentially private.The privacy guarantee is over the internal randomness of the users’ randomizers and not the public randomness of the shuﬄe protocol. the goal is to sum both across users,Pn  For brevity, we typically call these protocols “shuﬄe private”.
<EOS>
We now sketch an example of a shuﬄe private protocol.Consider the setting where each user i’s data point is two bits (xi,1, xi,2) and i=1 xi,2.A simple randomizer independently ﬂips each bit with probability p and outputs the resulting two randomized bits.
<EOS>
When there are n users, the shuﬄer takes in 2n messages and outputs one of the (2n)! permutations of the messages uniformly at random.The analyzer can then recover unbiased estimates by rescaling according to the randomization probability p and the number of users n. A generalization of Lemma 2.4 guarantees  i=1 xi,1 and Pn  by Ghazi, Golowich, Kumar, Pagh, and Velingker [21] implies that setting p ≈ log(1/δ) (ε, δ)-shuﬄe privacy, so the analyzer may recover estimates with error ≈ Note, however, that Definition 2.9 assumes all users follow the protocol.It does not account for malicious users that aim to make the protocol less private.
<EOS>
A simple attack is for such users to drop out: for γ ≤ 1, let S ◦ Rγn denote the case where only γn out of n users execute R. Because the behavior of the randomizer may depend on n, S ◦ Rn may satisfy a particular level of diﬀerential privacy but S ◦ Rγn may not4.Ideally, the privacy guarantee should not suﬀer too much from a small number of malicious users.This motivates a robust variant of shuﬄe privacy.
<EOS>
 √log(1/δ)  ε2n  ε  4Note that, with respect to diﬀerential privacy, dropping out is “the worst” malicious users can do.This is because adding messages from malicious users to those from honest users is a post-processing of S ◦ Rγn.If S ◦ Rγn is already diﬀerentially private for the outputs of the γn users alone, then diﬀerential privacy’s resilience to post-processing (Fact 2.2) ensures that adding other messages does not aﬀect this guarantee.
<EOS>
Hence, it is without loss of generality to focus on drop-out attacks. 6  Definition 2.10 (Robust Shuﬄe Diﬀerential Privacy).Fix γ ∈ (0, 1].
<EOS>
A protocol P = (R,A) is (ε, δ, γ)-robustly shuﬄe diﬀerentially private if, for all n ∈ N and γ′ ≥ γ, the algorithm S ◦ Rγ′n is (ε, δ)-diﬀerentially private.In other words, P guarantees (ε, δ)-shuﬄe privacy whenever at least a γ fraction of users follow the protocol. As with generic shuﬄe diﬀerential privacy, we often shorthand this as “robust shuﬄe privacy”.
<EOS>
Note that we define robustness with regard to privacy rather than accuracy.A robustly shuﬄe private protocol promises its users that their privacy will not suﬀer much from a limited fraction of malicious users.It does not make any guarantees about the accuracy of the analysis.
<EOS>
We state our accuracy guarantees under the assumption that all users follow the protocol.In general, we assume γ ∈ {1/n, 2/n,., 1} to avoid ceilings and ﬂoors.We emphasize that robust shuﬄe privacy is not implied by the generic shuﬄe privacy of Defi- nition 2.9. This is easy to see if we relax the model to allow users to execute diﬀerent randomizers.
<EOS>
In this case, all responsibility to add noise may rest on one user.For example, if each user has a single-bit datum, all users can report their data without noise while one designated user reports multiple randomized bits.If that designated user drops out then the remaining output clearly fails to be diﬀerentially private (in particular, an attacker who knows n− 1 of the users’ data learns the nth user’s data as well).
<EOS>
At the same time, many existing shuﬄe protocols are robustly shuﬄe private.This stems from a common protocol structure: each user contributes some noisy messages (e.g. Bernoulli bits) so that the union of n of these sets (e.g. a binomial distribution) suﬃces for a target level of diﬀerential privacy.A fraction of malicious users worsens the privacy guarantee, but the eﬀect is limited due to the noise contributions of the remaining honest users.
<EOS>
For an example of a formal argument, see work by Ghazi, Pagh, and Velingker [22].We also give robust shuﬄe private adaptations of past work on binary summation (Theorem 4.2 and Theorem 4.10) and histograms (Claim A.15). 3 Distinct Elements  We begin with the basic problem of counting distinct elements.
<EOS>
Without loss of generality, define the data universe to be X = [k].For all ~x ∈ [k]n, let D(~x) denote the number of distinct elements in ~x, i.e. D(~x) := |{j ∈ [k] | ∃i where xi = j}|.Definition 3.1 (Distinct Elements Problem).
<EOS>
An algorithm M solves the (α, β)-distinct elements problem on input length n if for all ~x ∈ [k]n, PM [|M(~x) − D(~x)| ≤ α] ≥ 1 − β. 3.1 Upper Bound for Robust Shuﬄe Privacy  function: D(~x) =Pk  to relatively good accuracy, then we can appropriately de-bias their sum to estimate D(~x). The main idea of our protocol is to reduce the distinct elements problem to computing the OR i=1((x1 = i) ∨.∨ (xn = i)).
<EOS>
If we can estimate each of these k OR functions The main problem is now to compute OR under robust shuﬄe privacy.We start with a basic (and suboptimal) centrally private solution to OR: output a sample from Ber (1/2) if OR(~x) = 1, and output a sample from Ber (p = 1/2eε) otherwise.By concentration across the k instances of OR, this protocol achieves accuracy roughly O(√k).
<EOS>
 The key property of the above solution is that we can simulate it in the shuﬄe model.The first step is to equate a sample from Ber (p) with the sum (mod 2) of n samples from some other distribution Ber (p′) (Lemma 3.4).The second step is to implement this modular arithmetic in the shuﬄe model.
<EOS>
We use a robustly shuﬄe private protocol due to Balle et al [8], itself based upon  7  work by Ishai, Kushilevitz, Ostrovsky, and Sahai [25].This enables us to distribute the samples from Ber (p′) to the users.Put together, these results yield a protocol PDE that exactly simulates the basic centrally private solution above and obtains the guarantees of Theorem 3.2. Pseudocode for PDE = (RDE,ADE) appears in Algorithms 1 and 2.
<EOS>
Algorithm 1: Randomizer RDE Input: user data x ∈ [k]; number of users n, privacy parameters ε > 0 and δ ∈ (0, 1) Output: message vector ~y ∈ ([k] × {0, 1})∗  2  1 Initialize output messages ~y ← ∅ 2 Set p′ ← 1−(1−e−ε)1/n δ (cid:1) 3 Set σ ← log(cid:0) eε+1 4 Set m ← 7 + ⌈2σ + 2 log(n − 1)⌉ 5 For domain element j ∈ [k] Draw u(j) ∼ Ber (1/2)  If x = j :  6  7  Else  8  9  10  11  12  Draw u(j) ∼ Ber (p′) Sample ℓ ∈ {0, 1}m uniformly from {v ∈ {0, 1}m |Pm For t ∈ [m] Append ~y ← ~y ◦ (j, ℓt)  t=1 vt mod 2 = u(j)}  13 Return ~y  privacy parameters ε > 0 and δ ∈ (0, 1)  Algorithm 2: Analyzer ADE Input: message vector ~y ∈ ([k] × {0, 1})∗; number of users n ∈ N, Output: z ∈ R δ (cid:1) 1 Set σ ← log(cid:0) eε+1 2 For domain element j ∈ [k] Initialize ~y(j) ← ∅ For (j, ℓt) ∈ ~y Cj ←P|~y(j)|  Append ℓt to ~y(j)  t=1 ~y(j)  i mod 2  4  5  6  3  j=1 Cj  7 C ←Pk 8 Return z ← 2Ceε−k eε−1 Theorem 3.2. Given ε > 0, γ ∈ (0, 1], and β, δ ∈ (0, 1), the protocol PDE = (RDE,ADE) γ , γ(cid:17)-robustly shuﬄe private, where ε(γ) = 2 εγ I. is(cid:16)2ε(γ), 4δ ε + ln(cid:16) 1 γ(cid:17);  II. solves the (α, β)-distinct elements problem for  γ for ε ≤ ln(2) and otherwise ε(γ) =  α =  eε  eε − 1 ·p2k ln(2/β) = O(cid:16)max(cid:0)1, 1  ε(cid:1) ·pk log(1/β)(cid:17) ;  8  III. requires each user to communicate at most O(cid:16)k log(cid:16) n(eǫ+1) (cid:17)(cid:17) messages of length O(log(k)).Proof.Privacy (I): We will show that (S ◦ Rγn DE) is diﬀerentially private.
<EOS>
Consider neighboring datasets ~x ∼ ~x ′ ∈ [k]γn where xi = a 6= b = x′i.The output distributions of RDE(a) and RDE(b) diﬀer only in the messages labeled by a and b, ~y(a) and ~y(b).It follows that to prove privacy we need only analyze the distributions of ~y(a), ~y(b).
<EOS>
 δ  1 ,., u(a)  To do so, let ~u(a) = (u(a)  n ) be the binary vector where each u(a)  i ∼ Ber (p′) otherwise, where we defined p′ = 1−(1−e−ε)1/n  i ∼ Ber (1/2) when xi = a and u(a) in the pseudocode for RDE.We define ~u(a) ′ similarly for ~x ′.Let RDE,∗ denote the randomizer that takes a bit u(j) as input and simply executes line 10 of RDE and outputs the result.
<EOS>
It will suﬃce to prove that, for any T ⊆ {0, 1}∗,  2  DE,∗  Ph(S ◦ Rγn Ph(S ◦ Rγn  DE,∗  )(~u(a)) ∈ Ti ≤ eε(γ) · Ph(S ◦ Rγn )(~u(a) ′) ∈ Ti ≤ eε(γ) · Ph(S ◦ Rγn  DE,∗  DE,∗  )(~u(a) ′) ∈ Ti + )(~u(a)) ∈ Ti +  2δ γ 2δ γ  This is because the inequalities above guarantee (cid:16)ε(γ), 2δ  γ(cid:17)-privacy for the view of ~y(a).Identical  arguments hold for ~u(b) and ~u(b) ′, so our privacy guarantee follows from composition. These results rely on the following lemma.
<EOS>
 Lemma 3.3 (Lemma 1.2 [8]).Let M,M′ be algorithms such that, for every ~u, kM(~u)−M′(~u)kTV ≤ ∆.If M is ε-diﬀerentially private then M′ is (ε, (eε + 1)∆)-diﬀerentially private.
<EOS>
 Our protocol is built on top of the modular arithmetic protocol of Balle, Bell, Gasc´on, and Nissim [8], which itself adapts a protocol from Kushilevitz, Ostrovsky, and Sahai [25], so we obtain this distance condition.5 To be precise, define M(~u) to be the algorithm that takes input ~y and )((P ut mod 2, 0,., 0)).The proof of Lemma 4.1 from Balle et al [8] guarantees outputs (S◦Rγn DE,∗ that (S ◦ Rγn )(~u) is within total variation distance 2−σ of M(~u).DE,∗ Our new goal is to prove that M is ε′-diﬀerentially private where  1 − (1 − e−ε)γ(cid:19)Once we do so, we can use Lemma 3.3 to conclude that (S ◦ Rγn DE,∗ where δ′ := (eε′ + 1) · 2−σ = eε′  ε′ = ln(cid:18)  ) is (ε′, δ′)-diﬀerentially private, eε+1 · δ.
<EOS>
Hence, we need to show that the following holds for both  +1  1  z = 1 and z = 0:  ≤  e−ε′  PhP u(a) PhP u(a)  i mod 2 = zi ′ mod 2 = zi ≤ eε′ 2(cid:1). t mod 2 is distributed as Ber(cid:0) 1 If a ∈ ~x ′ then Pγn xi = a, so Pγn distributed as Ber(cid:0) 1 2(cid:1) as well, so the ratio of probabilities is exactly 1.Otherwise,Pγn is a sum of i.i.d. draws from Ber (p′).We argue that this is distributed as Ber(cid:16) 1 1−(1−e−ε)γ(cid:17) using the following lemma, proven in Appendix A.1: ln(cid:16)  t=1 u(a)  1  i  i=1 u(a)  i  (2)  ′ mod 2 is ′ mod 2  i=1 u(a)  i  2eε′(cid:17) for ε′ =  5Although their setting assumes that all n users execute the protocol, their bound holds as long as there are at  least two honest users.
<EOS>
 9  Lemma 3.4. Let n ∈ N, γ ∈ (0, 1], and p ∈ [0, 1/2].Define p′ = 1−(1−2p)1/n X1,., Xγn ∼ Ber(p′), X =Pγn  2 i=1 Xi mod 2 is identically distributed with  Ber(cid:18) 1 − (1 − 2p)γ  2  (cid:19) Using Lemma 3.4 with our p′ gives p = 1  2eε , so  γn  Xi=1  u(a) i  ′ mod 2 = Ber(cid:18)1 − (1 − e−ε)γ  2  (cid:19) = Ber(cid:18) 1 2eε′(cid:19) Then given i.i.d.  P[Ber( 1 PhBer(cid:16) 1  2 )=0] 2eε′ (cid:17)=0i  = 1  P[Ber( 1 PhBer(cid:16) 1  2 )=1] 2eε′ (cid:17)=1i  = eε′  2−e−ε′Because and we return to Equation 2 to find ε′ > 0, we have eε′ + e−ε′ > 2 and therefore 2 − e−ε′ < eε′.Thus we have proven the inequality in Equation 2.
<EOS>
 and  All that is left is to prove ε′ ≤ ε(γ) and δ′ ≤ 2δ/γ.We split into cases based on ε.Case 1: ε > ln(2).
<EOS>
Then, as defined in the theorem statement, ε(γ) = ε + ln(1/γ).We use the following variant of Bernoulli’s inequality: for x ≥ −1 and r ∈ [0, 1], (1 + x)r ≤ 1 + xr.Using r = γ and x = −e−ε, we have (1 − e−ε)γ ≤ 1 − γe−ε and therefore  ε′ = ln(cid:18)  1  1 − (1 − e−ε)γ(cid:19) ≤ ln(cid:18)  1  1 − (1 − γe−ε)(cid:19) = ε + ln(cid:18) 1 γ(cid:19) We substitute this into δ′ to get  δ′ = δ ·  + 1  eε′ eε + 1 ≤ δ ·  eε γ + 1 eε + 1 ≤  δ γ  since γ ∈ (0, 1]. the following:  1  Case 2: ε ≤ ln(2), in which case we wish to show ε′ ≤ 2εγ eεγ − (eε − 1)γ(cid:19) = ln(cid:18)1 + ε′ = ln(cid:18) Given that ε ≤ ln(2), we have eε − 1 ≤ eε (eε − 1)γ  1 − (1 − e−ε)γ(cid:19) = ln(cid:18)  eεγ  2In turn, (eε − 1)γ ≤ eεγ  eεγ − (eε − 1)γ ≤  2γ  (eε − 1)γ eεγ − eεγ 2γ − 1 ·(cid:18) eε − 1 eε (cid:19)γ 2γ 2γ − 1 · εγ  2γ  =  ≤  γThe inequality 1 + x ≤ ex implies eεγ − (eε − 1)γ(cid:19) ≤ (eε − 1)γ  eεγ − (eε − 1)γ  (eε − 1)γ  2γThus  because eε−1  eε = 1 − e−ε ≤ ε.
<EOS>
We use the following lemma, also proven in Appendix A.1:  Lemma 3.5. For γ > 0,  2γ  2γ−1 ≤ 2 γ 10  Thus ε′ ≤ 2εγ  γWe now prove δ′ ≤ 2δ/γ using  2 + (eε−1)γ  eε′ + 1 eε + 1  =  eεγ−(eε−1)γ eε + 1 1 2 · εγ γ  (eε − 1)γ  eεγ − (eε − 1)γ  < 1 +  ≤ 1 + ≤ 2/γ. (ε > 0)  (ε, γ < 1)  i=1 u(j)  j=1 Cj has expectation E [C] = D(~x)  2 + k−D(~x)  2eε  Accuracy (II): If all n users follow the protocol, thenPn when there is some xi = j. Otherwise, Lemma 3.4 implies the distribution is Ber(cid:0) 1 C =Pk eε−1 i = D(~x).A Hoeﬀding bound implies that Eh 2Ceε−k P(cid:20)|PDE(~x) − D(~x)| >  2(cid:1) i mod 2 is distributed as Ber(cid:0) 1 2eε(cid:1).
<EOS>
ThusIn turn, the output of PDE has expectation  eε − 1r2k ln  β(cid:21) ≤ β  eε  2  where the probability is over users’ randomizers. Communication (III): For each domain element, there are O(log(n)+σ) messages, each of which is one labeled bit.Since each label is ∈ [k], each user sends O(k[log(n) + σ]) messages of length O(log(k)).
<EOS>
Substituting in σ = log(cid:0) eε+1 We focus on the setting where n ≥ k, as this is the setting for our lower bound in the next section.For completeness, we also give an O(n2/3) guarantee for the small-n setting.At a high level, this modified protocol simply hashes the initial domain [k] to a smaller domain of size O(n4/3) and then runs the protocol given above for this new domain.
<EOS>
Details appear in Appendix A.2.  δ (cid:1) yields the claim. 3.2 Lower Bound for Robust Shuﬄe Privacy  We now show that this dependence on k is tight for the setting where n = Ω(k).To do so, we give a way to transform a robustly shuﬄe private protocol into a pan-private one (Algorithm 3) and then invoke a lower bound for pan-private distinct elements [28].
<EOS>
 Our transformation is simple: the pan-private algorithm uses the shuﬄe protocol to maintain a set of shuﬄe protocol messages as its internal state.More concretely, the pan-private algorithm initializes its internal state using n/3 draws from the protocol randomizer R(1), processes the stream ~x by adding R(x1),.,R(xn/3) to its collection of messages, adds another n/3 draws from R(1) to its internal state after the stream, and finally applies the protocol analyzer A to this final internal state to produce output.Pan-privacy follows from the original protocol’s robust shuﬄe privacy combined with our incorporation of “dummy” messages into the state.
<EOS>
By the original protocol’s accuracy guarantee, these dummy messages – all generated from a single element– increase final error by at most 1. We remark that this construction assumes n is a multiple of 3, but this constraint can be removed  by using ⌈n/3⌉ and ⌊n/3⌋ where appropriate.We avoid this technicality for sake of clarity.
<EOS>
Lemma 3.6. Suppose there exists a protocol P = (R,A) that is (ε, δ, 1/3)-robustly shuﬄe private and solves the (α, β)-distinct elements problem on input length n. Then QP is an (ε, δ)-pan-private algorithm that solves the (α + 1, β)-distinct elements problem on input length n/3. 11  Algorithm 3: QP , an online algorithm for distinct elements Input: Data stream ~x ∈ [k]n/3; a shuﬄe protocol P = (R,A) for distinct elements Output: An integer in [k]  1 Create vector ~x(1) ← (1,., 1) ∈ Nn/3 2 Initialize internal state I0 ← (S ◦ Rn/3)(~x(1)) 3 For i ∈ [n/3]  4  Set Ii ← Ii−1 ∪ R(xi)  5 Set final state ~y ← In/3 ∪ (S ◦ Rn/3)(~x(1)) 6 Return A(~y)  Proof.Privacy: The main idea of the proof is that, by the robust shuﬄe privacy of P, the first draw from (S ◦ Rn/3)(~x(1)) ensures privacy of the internal state view, and the second draw ensures privacy for the output view.
<EOS>
For clarity, we write this out explicitly below.For any t ∈ [n/3], let PI(t),~x be the distribution for the internal state of QP after seeing the first t elements of ~x and let PO,~x be the distribution for the output of QP after seeing ~x.Choose arbitrary internal state i, output o, and intrusion time t. It suﬃces to show that for any neighboring ~x ∼ ~x ′,  PI(t),~x(i) · PO,~x|I(t)=i(o) ≤ eεPI(t),~x ′(i) · PO,~x ′|I(t)=i(o) + δ.
<EOS>
 (3)  In the first case, ~x≤t = ~x ′  ≤t.Then PI(t),~x(i) = PI(t),~x ′(i), so it suﬃces to show PO,~x|I(t)=i(o) ≤ eεPO,~x ′|I(t)=i(o) + δ. The final output is a post-processing of the final internal state.
<EOS>
Therefore, since diﬀerential privacy is closed under post-processing it is enough to show that {R(xt+1),.,R(xn/3),R(1),.,R(1)} is diﬀerentially private with regard to xt+1,., xn/3.This follows from the inclusion of the n/3 draws from R(1) and the fact that P is (ε, δ, 1/3)-robustly shuﬄe private. ≤t.Then since ~x ∼ ~x ′, ~x>t = ~x ′>t.
<EOS>
Thus PI(t),~x(i) · PO,~x|I(t)=i(o) = PI(t),~x(i) · PO,~x ′|I(t)=i(o)  In the second case, ~x≤t 6= ~x ′  ≤ (eεPI(t),~x ′(i) + δ) · PO,~x ′|I(t)=i(o) ≤ eεPI(t),~x ′(i) · PO,~x ′|I(t)=i(o) + δ  where the first inequality uses the same argument from the first case, here applied to the pool of messages {R(1),.,R(1),R(x1),.,R(xt)}.Accuracy: Consider the vector ~w = (1,., 1, x1,., xn/3, 1,., 1) ∈ [k]n.By the accuracy guarantee of the original shuﬄe protocol P, we have P [|P( ~w) − D( ~w)| > α] < β.
<EOS>
By the con- struction of QP , QP (~x) is identically distributed to P( ~w).Combining the triangle inequality and |D(~x) − D( ~w)| ≤ 1, we conclude QP solves the (α + 1, β)-distinct elements problem We now recall the pan-private lower bound for distinct elements.Mir et al [28] stated their result for pure user-level pan-privacy.
<EOS>
However, the same argument works for record-level privacy.The proof also concludes by recovering, for ω(1) elements, whether or not those elements appeared in the stream.This immediately yields the following approximate record-level result:  Lemma 3.7 (Implicit in Corollary 3 [28]).
<EOS>
If (ε, δ)-pan-private Q solves the (α, β)-distinct elements  problem on input length n for α = o(√k), β < 0.1, and n ≥ k, then ε = ω(1) or δ = ω(cid:0) 1 n(cid:1). 12  In fact, we can strengthen Lemma 3.7 to incorporate ε.Throughout, we ignore ceilings and  ﬂoors for neatness.
<EOS>
 ε for domain [k].Then  n(cid:1), and β < 0.1. Suppose there exists (ε, δ)-pan-private Q  Lemma 3.8. Let ε ≤ 1, δ = O(cid:0) ε that solves the (α, β)-distinct elements problem on input length n ≥ k α = Ω(cid:18)q k ε(cid:19). ε(cid:1)-pan-private Q′ that solves the (αε, β)-distinct elements Proof.We first show that Q yields a(cid:0)1, δ problem on input length n′ ≥ kε for domain [kε].
<EOS>
At a high level, Q′ transforms distinct elements on [kε] into distinct elements over a larger domain [k] and uses Q. Concretely, Q′ initializes Q and then, for each received element j2 ∈ [kε], creates 1 ε copies (j2, 1),.(j2, 1/ε) and passes them to Q. The cost is that, by composition across ε copies passed to Q, each element from [kε] is now only guaranteed (cid:0)1, δ ε(cid:1)-pan-privacy in the the 1 state maintained by Q. The stream passed to Q has length n′ ε ≥ k, so with probability at least 1 − β, Q outputs an α-accurate estimate of the number of distinct elements for a domain of size [k]; by our transformation, Q′ can multiply this by ε to get an αε-accurate estimate of the number of distinct elements in its stream from [kε].We now apply Lemma 3.7 to Q′.To check that the required conditions hold, δ = O(cid:0) ε n(cid:1) implies n(cid:1), the input length n′ ≥ kε, and β < 1/10.
<EOS>
Thus αε = Ω(√kε), and we rearrange into ε = O(cid:0) 1 α = Ω(cid:18)q k ε(cid:19). δ  We now combine Lemma 3.6 and Lemma 3.8 to get the following robust shuﬄe private lower  bound. Theorem 3.9. Let ε ≤ 1, δ = O(cid:0) ε the (α, β)-distinct elements problem on input length n ≥ 3k  n(cid:1), β < 0.1. If P is (ε, δ, 1/3)-robustly shuﬄe private and solves  ε , then α = Ω(cid:18)q k ε(cid:19).
<EOS>
 In contrast, the trivial central private solution of computing the number of distinct elements  and adding Lap(cid:0) 1  ε(cid:1) noise is ε-central private and achieves error O(cid:0) 1  ε(cid:1) for any ε and n.  4 Uniformity Testing  We now move to the second main problem of this paper, uniformity testing.A uniformity tester uses i.i.d. sample access to an unknown distribution over a domain [k] to distinguish the cases where the distribution is uniform or far from uniform. Definition 4.1 (Uniformity Testing).
<EOS>
An algorithm M solves α-uniformity testing with sample complexity m when:  • If ~x ∼ Um, then P [M(~x) = “uniform”] ≥ 2/3, and • If ~x ∼ Dm where kD − UkTV > α, then P [M(~x) = “not uniform”] ≥ 2/3  where the probabilities are taken over the randomness of M and ~x. Note that achieving an overall 2/3 success probability is essentially equivalent to achieving Ω(1) separation between the probabilities of outputting “uniform” given uniform and non-uniform  13  this reason, we generally focus on achieving any such constant separation. ∆2(cid:1) repetitions.
<EOS>
For samples.This is because any such ∆ separation can be amplified using O(cid:0) 1 Our algorithms will also often rely on Poissonization and use not m samples but n ∼ Pois(m).Doing so ensures that sampled counts of diﬀerent elements are independent over the randomness of drawing n, which will be useful in their analysis.
<EOS>
Fortunately, Pois(m) concentrates around m [14].We can therefore guarantee O(m) samples at the cost of a constant decrease in success probability.Because we generally focus on constant separations, we typically elide the distinction between “sample complexity m” and “sample complexity distributed as Pois(m)”.
<EOS>
 Throughout, in the shuﬄe model we assume that users receive i.i.d. samples from D, one sample  per user.In the pan-private model, we assume that the stream consists of i.i.d. samples from D.  4.1 Upper Bound for Robust Shuﬄe Privacy  In this section, we give a robustly shuﬄe private uniformity tester.At a high level, our protocol imitates the pan-private uniformity tester of Amin et al [4] (which itself imitates a centrally private uniformity tester suggested by Cai et al [13]).
<EOS>
This tester maintains k sample counts, one for each element, and compares a χ2-style statistic of the counts to a threshold to determine its decision.To ensure privacy, the algorithm adds Laplace noise to each count before computing the statistic.Our protocol is similar, but the shuﬄe model introduces a complication: we cannot privately count the frequency of a universe element, but instead rely on private communications from users.
<EOS>
More concretely, let cj(~x) denote the true count of j ∈ [k] in ~x.Users will execute a private binary sum protocol (Section 4.1.1) to count each j. The analyzer obtains a vector of estimates (˜c1(~x),., ˜ck(~x)) and uses them to compute the test statistic used by Amin et al [4] (Section 4.1.2).We then apply the binning trick from Amin et al [4] – roughly, maintaining coarser counts for random groups of elements rather than every element separately – to obtain our final uniformity tester with sample complexity ˜O(k2/3) (Section 4.1.3).
<EOS>
 4.1.1 Private Binary Sums with symmetric noise  The basic privacy building block for our robust shuﬄe private uniformity tester is a pure robust shuﬄe private protocol for binary sums.Relative to existing work on pure shuﬄe private binary sums [23], the main contribution of our protocol is that its noise is distributed symmetrically around the true sum.We rely on this symmetry in our sample complexity analysis.
<EOS>
 Pseudocode for the protocol PSYM = (RSYM,ASYM) appears in Algorithms 4 and 5.At a high level, each user ﬂips a biased coin to determine their message distribution.With high probability 1− p, the user sends a deterministic function of their message, without noise.
<EOS>
With low probability p, they instead add symmetric geometric noise SG(ε) to the input.The messages sent to the shuﬄer take values in {±1} and add up to the noised input (e.g. it encodes −2 as (−1,−1) and +3 as (+1, +1, +1)).The analyzer computes the sum of all the messages, which is precisely the sum of the data plus ≈ pn samples from SG(ε).
<EOS>
This mimics addition of Laplace noise in a discrete and distributed way.Note that the message vector (+1) is Ω(1/p) more likely coming from a user with data point 1 than with data point 0.The randomizer is therefore Ω(ln(1/p)) private on its own.
<EOS>
However, once we shuﬄe messages from all n users, we may argue that (+1) could be attributed to any user as long as p ≈ 1 ε2nThe same logic applies to the vector (−1).For all other outputs ~y, the probability that the randomizer reports ~y on input 0 is within an e2ε multiplicative factor of the probability on input 1. 14  Algorithm 4: RSYM, a randomizer for private binary sums Input: User datum x ∈ {0, 1}; privacy parameter ε > 0 and number of users n ∈ N Output: A vector ∈ {±1}∗  1 Set ˆx ← 2x − 1 2 Set η ← 0 3 Set c ← 2 · 4 Sample noise ∼ Ber (min(1, c/n)) 5 If noise = 1 : 6  eε+1 (eε−1)2  Sample η ∼ SG(ε)  |  |ˆx|copies  7 Reassign ˆx ← ˆx + η 8 Return (sgn(ˆx),., sgn(ˆx) ) {z  } Algorithm 5: ASYM, an analyzer for private binary sums Input: A vector ~y ∈ {±1}∗ Output: An integer ∈ Z 2 ((P|~y|i=1 yi) + n) 1 ˆz ← 1 2 Return ˆz  γ , 0, γ(cid:17)- Theorem 4.2. For any ε > 0 and γ ∈ (0, 1], the protocol PSYM = (RSYM,ASYM) is (cid:16) 3ε robustly shuﬄe private.
<EOS>
The output of PSYM(~x) is drawn from a symmetric distribution with mean ε4(cid:1).P xi and, for ε = O(1), variance at most O(cid:0) 1  n > 1, RSYM is simply the geometric mechanism.Because ˆx ∈ {−1, +1}, Proof.
<EOS>
Privacy: the output is 2-sensitive and thus 2ε-locally private (Lemma 2.5). (2ε, 0, γ)-robust shuﬄe privacy follows for all γ.The rest of the proof analyzes the case c  If c  n < 1. First, for any pair of neighboring databases, we prove privacy restricting attention only to the 2 users have data 0 or they have data 1; our arguments assume the  γn honest users.
<EOS>
Either n′ = γn former case but symmetric arguments hold in the latter case.We choose the neighbors  ~w = (1, 0,··· , 0 | {z }  n′ copies  , x2+n′,., xγn) and ~w ′ = (0, 0,··· , 0 | {z }  n′ copies  , x2+n′,., xγn). We will also restrict attention to output vectors ~y ∈ {−1}∗{+1}∗.
<EOS>
These choices of inputs and outputs are without loss of generality: permuting input order does not change the shuﬄer’s output distribution and, for any given input, two vectors with the same count of +1 and −1 are equally likely to be output by the shuﬄer. Because  (S ◦ Rγn  and  (S ◦ Rγn  ,RSYM(x2+n′),.,RSYM(xγn))  SYM )(x2+n′,., xγn))  n′ copies  SYM)( ~w) = S(RSYM(1),R(0),··· ,R(0) } | = S((S ◦ R1+n′ ), (S ◦ R−1+n′ SYM )(1, 0,··· , 0 } ), (S ◦ R−1+n′ SYM )(0, 0,··· , 0 }  SYM)( ~w ′) = S((S ◦ R1+n′  {z {z {z  |  |  ~x ′  ~x  15  SYM )(x2+n′,., xγn))  by post-processing (Fact 2.2) it suﬃces to argue  SYM )(~x ′) = ~yi ≤ Ph(S ◦ R1+n′  e−3ε/γ · Ph(S ◦ R1+n′ We will prove the upper bound of (4).The lower bound follows from the same arguments, switching −1 for +1.
<EOS>
First,  SYM )(~x) = ~yi ≤ e3ε/γ · Ph(S ◦ R1+n′  SYM )(~x ′) = ~yi  (4)  We expand the first term as  SYM )(~x) = ~y, R(x1) = 1i  Ph(S ◦ R1+n′ = Ph(S ◦ R1+n′ Ph(S ◦ R1+n′ = Ph(S ◦ R1+n′ = Ph(S ◦ R1+n′ ≤ Ph(S ◦ R1+n′ = e2ε · Ph(S ◦ R1+n′  SYM )(~x) = ~yi SYM )(~x) = ~y, R(x1) 6= 1i + Ph(S ◦ R1+n′ SYM )(~x) = ~y,R(x1) 6= 1i SYM )(~x) = ~y | RSYM(x1) 6= 1i · P [RSYM(x1) 6= 1] SYM )(~x ′) = ~y | RSYM(x′1) 6= 1i · P [RSYM(x1) 6= 1] SYM )(~x ′) = ~y | RSYM(x′1) 6= 1i · e2ε · P(cid:2)RSYM(x′1) 6= 1(cid:3) SYM )(~x ′) = ~y,RSYM(x′1) 6= 1i  (5)  (6)  where the second equality uses the fact that ~x and ~x ′ only diﬀer at the first index, and the inequality uses the privacy guarantee of the symmetric geometric distribution (Lemma 2.5).We next expand the second term in (5) as  Ph(S ◦ R1+n′ Ph(S ◦ R1+n′ Ph(S ◦ R1+n′ Ph(S ◦ R1+n′ Ph(S ◦ R1+n′  SYM )(~x) = ~y,RSYM(x1) = 1i SYM )(~x) = ~y,RSYM(x1) = 1i SYM )(~x ′) = ~y,RSYM(x′1) 6= 1i · Ph(S ◦ R1+n′ SYM )(~x) = ~y | RSYM(x1) = 1i SYM )(~x ′) = ~y, RSYM(x′1) ∈ {−1}∗i · Ph(S ◦ R1+n′  =  ≤  SYM )(~x ′) = ~y,RSYM(x′1) 6= 1i  SYM )(~x ′) = ~y,RSYM(x′1) 6= 1i  (7)  since we have discarded the probability mass where +1 ∈ RSYM(x′1).For neatness, let F denote the fraction above,  F =  Ph(S ◦ R1+n′ Ph(S ◦ R1+n′  SYM )(~x) = ~y | RSYM(x1) = 1i SYM )(~x ′) = ~y, RSYM(x′1) ∈ {−1}∗i Substituting (6) and (7) into (5) yields  Ph(S ◦ R1+n′  SYM )(~x) = ~yi ≤ (e2ε + F ) · Ph(S ◦ R1+n′ ≤ (e2ε + F ) · Ph(S ◦ R1+n′  SYM )(~x ′) = ~y, R(x′1) 6= 1i SYM )(~x ′) = ~yi  16  We will show that F ≤ e2ε(eε−1)  γ Given that upper bound on F ,  e2ε + F ≤ e2ε +  γ  e2ε(eε − 1) γ (cid:19) = e3ε(cid:18)e−ε + 1 − e−ε = e3ε(cid:18)1 + (e−ε − 1) + 1 − e−ε eε (cid:19) = e3ε(cid:18)1 + eε − 1 ≤ exp(cid:18)3ε + eε (cid:19)eε − 1  1 − γ · γ 1 − γ γ  γ  ·  (cid:19)  have  If ε < 1, then eε − 1 < 3ε and eε−1 eε < 3ε.
<EOS>
If instead ε ≥ 1, then eε−1 3ε(1 − γ) 1 − γ γ  e2ε + F ≤ exp(cid:18)3ε +  · which gives our overall privacy bound. eε (cid:19) < exp(cid:18) 3εγ eε − 1  +  γ  γ  (cid:19) = exp(cid:18) 3ε γ (cid:19)  eε ≤ 1 < 3ε.Thus we always  It only remains to show F ≤ e2ε(eε−1)There are three cases depending on ~y.
<EOS>
Case 1: ~y has no 1s, i.e. ~y = −1b for some b. Then the numerator of F is 0.Case 2: ~y has a single 1, i.e. ~y = −1b1.Then, by our choice of ~x, ~x ′,  γ  Ph(S ◦ Rn′  SYM)(x2,., xn′+1) = −1bi  P [RSYM(x′1) = −1j] · P(cid:2)(S ◦ Rn′ P(cid:2)RSYM(x2) = −1j(cid:3) · Ph(S ◦ Rn′−1 P [RSYM(x′1) = −1j] · P(cid:2)(S ◦ Rn′ Pb P [RSYM(0) = −1j] · n′ · P [RSYM(0) = 1] }  P(cid:2)RSYM(0) = −1j(cid:3) · Ph(S ◦ Rn′−1  SYM)(x′2,., x′n′+1) = −1b−j1(cid:3) SYM )(x3,., xn′+1) = −1b−ji SYM)(x2,., xn′+1) = −1b−j1(cid:3) SYM )(0n′−1) = −1b−ji ·Ph(S ◦ Rn′−1  {z  |  j=0  F ∗  SYM )(0n′−1) = −1b−ji  F =  j=0  j=0  j=0  j=0  Pb = Pb Pb Pb  =  =  1 F ∗  We now express F ∗ in terms of ε:  F ∗ = n′ · P [RSYM(0) = 1]  c n · Pη∼SG(ε) [η = 2]  = n′ · cγ 2 · Pη∼SG(ε) [η = 2] = cγ 2 ·  e−2ε(eε − 1) γ  eε + 1  =  =  e2ε(eε − 1) (eε−1)2Thus F = e2ε(eε−1) eε+1  γ  17  since we chose c = 2 ·  (n′ = γn/2)  Case 3: ~y has multiple 1s, i.e. ~y = −1b1d for some b ≥ 0, d > 1.
<EOS>
Then  F =  P [R(0) = −1j11j2] · P [R(~x>1) = −1b−j11d−j2]  SYM)(x2,., xn′+1) = −1b1d−1i Ph(S ◦ Rn′ SYM )(x3,., xn′+1) = −1b−j11d−1−j2i P(cid:2)RSYM(x2) = −1j11j2(cid:3) · Ph(S ◦ Rn′−1 P [RSYM(0) = −1j11j2] · P(cid:2)(S ◦ Rn′ SYM)(x2,., xn′+1) = −1b−j11d−j2(cid:3) SYM )(0n′−1) = −1b−j11d−1−j2i Pb,d−1 P [R(0) = −1j11j2] · Ph∃i ≥ 2 RSYM(xi) = 1, (S ◦ Rn′−1 SYM )(0n′−1) = −1b−j11d−1−j2i Pb,d−1 ·Ph(S ◦ Rn′−1 P [R(0) = −1j11j2] · n′ · P [RSYM(0) = 1] }  P(cid:2)RSYM(0) = −1j11j2(cid:3) · Ph(S ◦ Rn′−1 P(cid:2)RSYM(0) = −1j11j2(cid:3) · Ph(S ◦ Rn′−1  SYM )(~x>1,−i) = −1b−j11d−1−j2i SYM )(0n′−1) = −1b−j11d−1−j2i  (j1=0,j2=0)  (j1=0,j2=0)  {z  |  F ∗  (j1=0,j2=0)  (j1=0,j2=0)  (j1=0,j2=0)  Pb,d = Pb,d−1 Pb,d Pb,d Pb,d  ≤  =  =  1 F ∗  (j1=0,j2=0)  (j1=0,j2=0)  where the inequality shrinks the denominator’s sum by focusing only on the cases where some user in {2,., n′ + 1} outputs a single 1.Substituting in our analysis of F ∗ from Case 2 completes the result.Accuracy: Let D ⊂ [n] be the random variable for the “data” group of users who send xi, and let N = [n] − D be the “noisy data” group of users.
<EOS>
Then, denoting ηi ∼i.id.SG(ε), PSYM(~x) outputs  1  2    |~y|Xi=1    + n yi  =  =  1  2 "Xi∈D Xi=1  xi +  n  (2xi − 1) +Xi∈N 2Xi∈N  ηi. 1  (2xi − 1 + ηi)# + n!
<EOS>
 where |N| ∼ Bin(n, min(c/n, 1)).Because SG(ε) is symmetric with mean 0, P(~x) is symmetric ε4(cid:1) in Lemma A.5 in around Pn  i=1 xi.For ε = O(1), we upper bound its variance by  (eε−1)4 = O(cid:0) 1  Appendix A.3.  8e2ε  4.1.2 Preliminary Uniformity Tester  We now put the binary sum protocol PSYM = (RSYM,ASYM) from the previous section to use in a preliminary robust shuﬄe private uniformity tester P UT = (RUT,AUT) .
<EOS>
This tester first uses the binary sum protocol to compile robust shuﬄe private estimates of the It then uses these counts to compute a statistic Z′ that sample counts for each element in [k]. is, roughly, large when the underlying distribution is suﬃciently non-uniform and small otherwise.The resulting uniformity obtains sample complexity scaling with k3/4 ln1/2(k).In the next section, we use this initial tester as a black box to obtain a tester that improves k3/4 to k2/3.
<EOS>
Theorem 4.3. For any ε = O(1), γ ∈ (0, 1], and α ∈ (0, 1), the protocol P UT = (RUT,AUT) is γ , 0, γ(cid:17)-robustly shuﬄe private and solves α-uniformity testing with sample complexity (cid:16) 6ε  m = O  k3/4 ln1/2(k)  αε2  +  k2/3 ln1/3(k)  α4/3ε4/3  +  k1/2  α2 !18  Algorithm 6: RUT, a randomizer for private uniformity testing Input: User data point x ∈ [k]; number of users n ∈ N Output: Message vector ~y ∈ {−k,., k}∗  1 Initialize empty vector ~y 2 For j ∈ [k]  ~z ← RSYM(1 [x = j]) For message z ∈ ~z  3  4  5  Append ~y ← ~y ◦ (j · z)  6 Return ~y  Algorithm 7: AUT, an analyzer for private uniformity testing Input: Message vector ~y ∈ {−k,., k}∗; testing distance α ∈ (0, 1), Poisson parameter m, Output: Decision in {“uniform”, “not uniform”}  number of samples n, threshold τ  1 For j ∈ [k]  Initialize empty vector ~z For y ∈ ~y  If |y| = j :  Append ~z ← ~z ◦ y  j  2  3  4  5  6  Compute overall count for j, cj(~y) ← ASYM(~z)  7 Compute statistic Z′ ← k 8 Return “not uniform” if Z′ > τ , otherwise “uniform”  j=1(cid:2)(cj(~y) − m  k )2 − cj(~y)(cid:3)  mPk  19  Proof.Privacy: Let ~x ∼ ~x ′ be neighboring datasets of size n. Without loss of generality, suppose x1 = j 6= x′1 = j′.
<EOS>
The distributions of the messages containing j′′ 6= j, j′ are identical for ~x and ~x ′, so it suﬃces to restrict our analysis to messages containing either j or j′.To prove our guarantee, by composition it is enough to show that the pool of messages containing j and the pool of messages γ , 0, γ(cid:17)-robustly shuﬄe private.Since RUT only touches user data through containing j′ are each(cid:16) 3ε PSYM, this follows from the privacy guarantee for PSYM (Theorem 4.2).
<EOS>
Sample Complexity: For each j ∈ [k], let Ej denote the (signed) error of cj(~y) with respect to cj(~x), Ej = cj(~y) − cj(~x).We rewrite Z′ in terms of Ej:  m  m  m  k  k  k  =  =  k m  k m  k m  Z′ =  − (cj(~x) + Ej)(cid:21)  − cj(~y)(cid:21) k(cid:17)2 − cj(~x)(cid:21) }  Xj=1(cid:20)(cid:16)cj(~y) − k(cid:17)2 Xj=1(cid:20)(cid:16)cj(~x) + Ej − Xj=1(cid:20)(cid:16)cj(~x) − k(cid:17)2 {z To show that this gives a valid uniformity tester, we will show that (Case 1) when D = U, Z′ falls below threshold τ with probability ≥ 9/10, and (Case 2) when kD − UkTV > α, Z′ follows below τ with probability ≤ 23/40.We begin with bounds on A and C that hold regardless of the identity of D. All unproven claims appear in Appendix A.4. We first show that A − C concentrates around E [A].
<EOS>
Claim 4.4. There exists a constant h1 such that  Ej ·(cid:16)cj(~x) −  k(cid:17) }  Xj=1 {z  Xj=1 {z  (By definition)  Xj=1  {z  2k m  k m  k m  E2 j  +  }  |  |  k  C  k  A  }  |  m  −  |  Ej  +  k  Z  B  P"|(A − C) − E [A]| >  k3/2 m ·  h1  ε4# < 1/20. We now move to specific cases.Case 1: D = U, and we want to upper bound Z′.
<EOS>
We start by upper bounding Z + B.  Claim 4.5. Let m = Ω(cid:16)√k P"Z + B >  α2(cid:17).If D = U, then there exists a constant h2 such that ε2# < 1/20. +  k3/2 ln(k)  m1/2 ! · kpln(k)  3α2m 250  h2  m  +  (8)  (9)  Let h3 = max(h1, h2) as defined in the two prior claims.
<EOS>
By a union bound, with probability  ≥ 9/10,  Z′ = Z + B + (A − C)  11α2m 1000  +  k3/2 ln(k)  m  +  ≤  =  ≤  11α2m 1000 11α2m 1000  + E [A] +  + E [A] +  m1/2 ! · kpln(k) h2 · ε2 + h2 ε2 +  ·  m1/2  kpln(k) kpln(k)  m1/2  h2 ε2 + E [A] + m (cid:18) ε2 ln(k)h2  ε4  k3/2  h1 ε4  k3/2 m · ε4(cid:19) h1  +  k3/2 ln(k)  m  h4 ε4  ·  20  where the last step uses ε2 ln(k)h2 + h1 ≤ h3(ε2 ln(k) + 1) ≤ h4 ln(k) for some constant h4 since ε = O(1).We set τ to be this upper bound for Z′.Thus with probability ≥ 9/10, when D = U we compute Z′ ≤ τ , and the protocol correctly returns “uniform.” Case 2: kD − UkTV > α.
<EOS>
The goal now is to lower bound Z′, starting with Z. Claim 4.6. Let m = Ω(cid:16)√k  α2(cid:17) and kD − UkTV > α.Then 25 (cid:21) <  P(cid:20)Z ≤  α2m  1 40  Next, we lower bound B. On any input ~x, each error term Ej is drawn from a symmetric  distribution with mean zero (Theorem 4.2).We will use the following technical claim:  Claim 4.7. Let E1,., Ek be independent random variables where each Ej is symmetrically dis- j=1 Ej · dj  tributed over the integers with mean zero.
<EOS>
For any d1,., dk ∈ R, the random variablePk This implies P [B ≥ 0] ≥ 1/2.By a union bound, the following holds except with probability  is symmetrically distributed with mean zero. 1/20 + 1/40 + 1/2 = 23/40:  Z′ = Z + B + (A − C) + E [A] −  36α2m  500  k3/2 m ·  h1 ε4  ≥ kpln(k)  m1/2  so Z′ > τ whenever  k3/2  61α2m 1000 −  h2 · ε2 − Dropping constants, we require α2m = Ω(cid:16) k ln1/2(k) ε2√m + k3/2 ln(k) m = Ω k3/4pln(k)  m (cid:18) h1 + ln(k)h4 ε4m (cid:17), i.e. α4/3ε4/3 ! k2/3 ln1/3(k)  αε2  ε4  +  (cid:19) 4.1.3 Final Uniformity Tester  We now use a technique from Amin et al [4] (itself a generalization of a similar technique from Acharya et al [3]) to reduce the sample complexity dependence on k from k3/4 to k2/3.
<EOS>
The idea is to reduce the size of the data universe [k] by grouping random elements and then performing the test on the smaller universe [ˆk].The randomized grouping also reduces testing distance — partitions may group together elements with non-uniform mass to produce a group with near-uniform overall mass, thus hiding some of the original distance — but the reduction in universe size outweighs this side eﬀect. We first introduce some notation.
<EOS>
Given a partition G of [k] into G1,., Gˆk , let DG denote the distribution over [ˆk] such that the probability of sampling ˆj from DG is the probability that  j ∈ Gˆj for j ∼ D. Formally, PhDG = ˆji =Pj∈Gˆj  P [D = j]. 21  Lemma 4.8 (Lemma 5 [4]).Let D be a distribution over [k] such that kD − UkTV = α.
<EOS>
If G is a uniformly random of [k] into ˆk groups, then with probability ≥ 1/954 over G,  kDG − UkTV ≥ α · pˆk 477√10k  Applying this trick to reduce [k] to [ˆk] and then running our initial protocol P UT = (RUT,AUT) gives our final uniformity tester.The given asymptotic  on [ˆk] with distance parameter ˆα = α bound requires diﬀerent values of ˆk depending on parameter settings; these appear in the proof. √ˆk 477√10k  Theorem 4.9. For any ε = O(1), γ ∈ (0, 1], and α ∈ (0, 1), there exists a protocol that is (cid:16) 6ε γ , 0, γ(cid:17)-robustly shuﬄe private and solves α-uniformity testing with sample complexity  m = O   k2/3  α4/3ε4/3 +  k1/2 α2 +  k1/2  αε2! ln1/2(k)!
<EOS>
 Proof.The protocol assigns ˆk according to the following rule:  2  k k2/3ε8/3  α4/3  α4/3 < 2 α4/3 > k  if k2/3ε8/3 if k2/3ε8/3 otherwise  ˆk ←   The users and analyzer determine the partition G using public randomness.Users execute RUT as if the data were drawn from DG and the analyzer executes AUT.
<EOS>
Privacy is immediate from Theorem 4.3, so it remains to argue that the protocol is accurate for large enough m.  Privacy: Immediate from the privacy guarantee of Theorem 4.3.  √ˆk 477√10k We thus bound m using the sample  Sample Complexity: Recall that we set ˆα = α  complexity guarantee of Theorem 4.3:  +  αε2  ˆαε2  m = O  ˆk3/4 ln1/2(ˆk) = O  k1/2ˆk1/4 ln1/2(ˆk) = O     |  k1/2ˆk1/4  αε2 T1  {z  }  |  +  ˆk2/3 ln1/3(ˆk)  ˆk1/2  +  ˆα4/3ε4/3  +  k2/3 ln1/3(ˆk)  α4/3ε4/3  k2/3  α4/3ε4/3  +  T2  {z  }  k  α2ˆk1/2    | {z }  T3  k  +  ˆα2 ! α2ˆk1/2! ln1/2(k)   We split into cases based on ˆk.They yield, respectively, the k1/2 O(T1 + T2 + T3). αε2 , k1/2  α2 , and  (Value of ˆα)  (ˆk ≤ k)  k2/3  α4/3ε4/3 terms for  22  Case 1: ˆk = 2.
<EOS>
Then k2/3ε8/3  α4/3 < 2, so k1/2 = O(cid:0) α T1 + T2 + T3 = O  k1/2 αε2 + = O  k1/2 αε2 + = O  k1/2 αε2! ε2/3(cid:17).Thus  ε2(cid:1) and k1/6 = O(cid:16) α1/3 α2 ! k1/2k1/6 α4/3ε4/3  k1/2k1/2  +  k1/2 αε2 +  k1/2  αε2!
<EOS>
 Case 2: ˆk = k. Then k < k2/3ε8/3 which dominates both T1 and T2 since  α4/3  , so k3/4 < k1/2ε2  α  and k1/6 < ε4/3  α2/3We also get T3 = k1/2 α2 ,  and  T1 =  k3/4 αε2 <  k1/2ε2 α2ε2 = T3  T2 =  k2/3  α4/3ε4/3  = k1/2 ·  k1/6  α4/3ε4/3  <  k1/2 α2 Case 3: ˆk = k2/3ε8/3  α4/3 Here, T2 is equal to both T1 and T3:  T1 =  T3 =  k1/2(k2/3ε8/3α−4/3)1/4  αε2 k  α2(k2/3ε8/3α−4/3)1/2  = T2  = T2. Correctness: We apply Theorem 4.3 (with a constant number of repetitions) to get • If ~x ∼ Dn • If ~x ∼ Dn  G where DG = U, then P [PUT(~x) = “uniform”] ≥ 9539/9540, and G where kDG − UkTV > ˆα, then P [PUT(~x) = “not uniform”] ≥ 9539/9540  If D = U, then DG = U and so the probability of “uniform” is ≥ 9539/9540.If kD − UkTV > α then by Lemma 4.8, with probability ≥ 10/9540, kDG − UkTV > ˆα.
<EOS>
By Theorem 4.3, with probability ≥ 5/9540, the tester returns “non-uniform”.This constant separation gives the overall testing guarantee. Since our lower bound (Theorem 4.13 in the next section) only holds for pure robust shuﬄe privacy, Theorem 4.9 is our main uniformity testing result.
<EOS>
However, for completeness we also give an approximate robustly shuﬄe private result that achieves better dependence on ε.The main diﬀerence is that the approximately private tester adds binomial noise rather than symmetric geometric noise.Details and proofs appear in Appendix A.5.  Theorem 4.10.
<EOS>
Let γ ∈ (0, 1], ε > 0, and α, δ ∈ (0, 1).Then there exists a protocol that is (2ε, 4δ, γ)-robustly shuﬄe private and, for ε = O(1), solves α-uniformity testing with sample com- plexity  m = O   k2/3  α4/3ε2/3  k1/2  α2 ! · ln1/2(cid:18) k  δ(cid:19)! +  k1/2 αε  +  23  4.2 Lower Bound for Robust Shuﬄe Privacy  To obtain a robustly shuﬄe private lower bound, we first show how to transform a robustly shuﬄe private uniformity tester into a pan-private uniformity tester.The main idea of this transformation is the same as that for distinct elements.
<EOS>
We initialize the pan-private algorithm’s state using dummy data, handle new stream elements as shuﬄe protocol users contributing to a growing pool of (repeatedly) shuﬄed messages, and add more dummy data to the internal state at the end of the stream.Here, the dummy data consists of samples from a uniform distribution.This has the eﬀect of diluting the true samples and worsens the testing accuracy, but to a controlled extent.
<EOS>
Pseudocode for this procedure appears in Algorithm 8. Algorithm 8: QP , an ε-pan-private tester built from a 1/3-robust ε-shuﬄe private tester Input: Data stream ~x ∈ [k]n/3; shuﬄe-private uniformity tester P = (R,A) Output: Decision in {“uniform”, “not uniform”}  1 Draw uniform samples ~xU ∼ Un/3 2 Initialize internal state I0 ← (S ◦ Rn/3)(~xU) 3 Draw n′ ∼ Bin(n, 2/9) 4 Set n′ ← min(n′, n/3) 5 For i ∈ [n/3] If i ≤ n′ :  6  Set Ii ← Ii−1 ∪ R(xi)  Set Ii ← Ii−1 ∪ R(U)  7  8  9  Else  10 Draw (new) uniform samples ~xU ∼ Un/3 11 Set final state ~y ← In/3 ∪ (S ◦ Rn/3)(~xU) 12 Return A(~y)  Lemma 4.11.Let P = (R,A) be an (ε, 0, 1/3)-robustly shuﬄe private α-uniformity tester with sample complexity n. If n is larger than some absolute constant, then QP described in Algorithm 8 is an ε-pan-private algorithm that solves 9α  2 uniformity testing with sample complexity n/3.
<EOS>
 Proof.Privacy: The privacy argument is nearly identical to that used to prove Lemma 3.6. The only diﬀerence is that the dummy data now consists of uniform samples.However, since this dummy data is still independent of the true data, we can apply the same argument to translate robust shuﬄe privacy into pan-privacy.
<EOS>
 Accuracy: Assuming that P errs with probability at most 1/3, we will prove that the algorithm QP errs with probability at most 1/3 on data drawn from U. Then we bound the error probability by 1/2 when data is drawn from a distribution that is 9α 2 -far from U. This constant separation implies a valid tester. Let P(~x) denote the output distribution of the original shuﬄe private uniformity tester, and let QP (~x) denote the output distribution for the pan-private uniformity tester given in Algorithm 8 on stream ~x.If ~x consists of uniform samples, then QP (Un/3) = A((S ◦ R)(Un)) = P(Un) and so  P~x,QP [QP outputs “not uniform”] = P~x,P [P outputs “not uniform”] ≤ 1/3.
<EOS>
 Having upper bounded the probability that QP errs on uniform samples, we now control the probability that it errs on non-uniform samples.Suppose ~x consists of samples from D where  24  9 · D + 7  2Define the mixture distribution D2/9 := 2  that n′ is not distributed as Bin(n, 2/9) is less than 1/6.In turn, the distance between  kD − UkTV > 9α 9 · U. Then kD2/9 − UkTV > α 2/9) = “uniform”i ≤ 1/3.
<EOS>
We will now show that P(Dn and so PhP(Dn 2/9) and QP (Dn/3) are statistically close enough that QP has a bounded error probability on non-uniform samples as well.In n samples from D2/9, the number of samples drawn from D is distributed as Bin(n, 2/9).By a binomial Chernoﬀ bound, for n >pln(12), P [Bin(n, 2/9) > n/3] < 1 6Thus the probability { }| z ,R(U),.,R(U)) R(D),.,R(D) 2/9) = A( | } z { }| QP (Dn/3) = A( ,R(U),.,R(U)) R(D),.,R(D) } | is less than 1/6 as well.
<EOS>
Tracing back, we have shown that given samples from suﬃciently non- uniform D, P [QP outputs “uniform”] ≤ 1/3 + 1/6 = 1/2. n′ copies  {z  Bin(n,2/9) copies  {z  n terms  and  P(Dn  n terms  Next, we recall the pan-private lower bound for uniformity testing. Lemma 4.12 (Theorem 3 from Amin et al [4]).
<EOS>
For ε = O(1) and α < 1/2, any ε-pan-private α-uniformity tester has sample complexity  Ω(cid:18) k2/3  α4/3ε2/3 +  √k α2 +  √k α√ε +  1  αε(cid:19) Together, Lemmas 4.11 and 4.12 imply our lower bound for (pure) robustly shuﬄe private  uniformity testing. Theorem 4.13.For ε = O(1) and α < 1/9, any (ε, 0, 1/3)-robustly shuﬄe private protocol α- uniformity tester has sample complexity  Ω  k2/3  α4/3ε2/3  √k α2 +  √k α√ε  +  +  1  αε!
<EOS>
This implies that Theorem 4.9 is tight in k up to a ln1/2(k) factor, although there is a 1  ε2/3 gap  in the first term and a 1  ε3/2 gap in the third term. 5 Pan-private Histograms  We now depart from our previous results by transforming a shuﬄe private protocol into a pan- private algorithm.Specifically, Balcer and Cheu [6] gave a shuﬄe private protocol for estimating histograms with error independent of the domain size.
<EOS>
Their protocol relies on adding noise from a binomial distribution for privacy; in Appendix A.6, we show that this strategy is also robust.We then show that a pan-private analogue using binomial noise achieves the same error. The main building block in our histogram algorithm is a counting algorithm QZSUM (Algorithm 9).
<EOS>
To count a sum of bits, QZSUM adds binomial noise to its counter before beginning the stream, updates the counter deterministically for each bit in the stream, adds more binomial noise at the end of the stream, and releases the resulting noisy count.Because binomial noise is bounded, we can always bound the final error. 25  Algorithm 9: An online algorithm QZSUM for binary sums Input: Data stream ~x ∈ {0, 1}n; parameter λ ∈ N Output: z ∈ N 1 I0 ∼ Bin(λ, 1/2) 2 For i ∈ [n] Ii ← Ii−1 + xi 4 η ∼ Bin(λ, 1/2) 5 ˜c ← In + η 6 Return ˜c  3  Theorem 5.1. Given ε > 0, δ ∈ (0, 1), and λ ≥ 20 ·(cid:16) eε+1 eε−1(cid:17)2 δ(cid:1)(cid:1). ε2 log(cid:0) 1 and computes binary sums with error ≤ 2λ = O(cid:0) 1  δ(cid:1), QZSUM is (ε, δ)-pan-private ln(cid:0) 2  Proof.
<EOS>
Privacy: The basic idea of the proof is that, by Lemma 2.4, the first draw of binomial noise ensures privacy the internal state view, and the second draw of binomial noise ensures privacy for the output view.Substituting in Lemma 2.4, the privacy analysis is almost identical to that for Lemma 3.6.  i=1 xi + 2λ. i=1 xi to Pn  i=1 xi + Bin(2λ, 1/2), which has support ranging  Accuracy: For any stream ~x, we have ˜c =Pn from Pn With QZSUM in hand, our histogram algorithm is simple: we run QZSUM on each value in the data domain.
<EOS>
Crucially, changing one stream element only changes the true value of at most two bins in the histogram, so by composition the resulting algorithm’s privacy guarantee is within a factor of two of QZSUM.Since each count is 2λ-accurate, we get the same ℓ∞ accuracy.This improves over a naive solution that adds Laplace noise to each bin, which incurs a logarithmic dependence on the domain size.
<EOS>
 described above is (2ε, 2δ)-pan-private and computes a histogram with ℓ∞ error at most 2λ =  eε−1(cid:17)2 Theorem 5.2. Given ε > 0, δ ∈ (0, 1), and λ ≥ 20 ·(cid:16) eε+1 ε2 log(cid:0) 1 O(cid:0) 1 δ(cid:1)(cid:1). an (ε, δ)-pan-private solution to pointer-chasing on length-ℓ vectors using O(cid:0) 1  Sequentially interactive (ε, δ)-local privacy, however, requires Ω(ℓ) samples [26]. Pan-privacy thus inherits the same separations from (sequentially interactive) local privacy as those outlined by Balcer and Cheu [6] for shuﬄe privacy.For example, using histogram gives  ln(cid:0) 2 δ(cid:1), the histogram algorithm  δ(cid:1)(cid:1) samples.
<EOS>
 ε2 log(cid:0) 1  6 Conclusion and Further Questions  Our results suggest a relationship between robust shuﬄe privacy and pan-privacy.In addition to the straightforward problems of closing gaps in our upper and lower bounds, we conclude with more general questions:  1.When can we convert robust shuﬄe private protocols to pan-private algorithms?
<EOS>
Both of our robust shuﬄe private lower bounds rely on this kind of conversion.In particularly, they use the fact that the underlying problem is resilient to “fake” data.For example, in the course of converting a robust shuﬄe private distinct elements protocol to a pan-private one, it was important that adding many draws from R(1) — i.e., adding many copies of 1 to the data  26  — only changed the true answer by at most 1.
<EOS>
Similarly, when converting the robust shuﬄe private uniformity tester to a pan-private one, adding fake uniform samples only diluted the original testing distance, so the resulting pan-private tester was still useful.It is not clear which problems do or do not have this property, or whether this property is necessary in general. 2.
<EOS>
When can we convert pan-private algorithms to robust shuﬄe private protocols?Our robust shuﬄe private distinct elements counter is structurally similar to its pan-private counter- part [18]: both essentially break distinct elements into a sum of noisy ORs and then de-bias the result.The only diﬀerence is in the distributed noise generation of our shuﬄe protocol.
<EOS>
Similarly, our robust shuﬄe private uniformity tester diﬀers from the pan-private version [4] only in the kind of noise added.Is there a generic structural condition that allows for this kind of transformation? 3.
<EOS>
Can we separate robust shuﬄe privacy and pan-privacy?Proving that robust shuﬄe privacy must obtain worse performance for some problem requires a lower bound that holds for robust shuﬄe privacy but not pan-privacy.Unfortunately, almost all multi-message robust shuﬄe privacy lower bounds are either (1) imported directly from central privacy or (2) imported from pan-privacy.
<EOS>
In either case, such lower bounds also apply to pan-privacy and thus do not yield a separation.One exception is the summation lower bound of Ghazi et al [23], but it holds only if each user is limited to O(poly log(n)) communication.In the other direction, the only pan-private lower bounds that do not also hold for central privacy are for distinct elements and uniformity testing, but we have shown that they do not give a (polynomial in domain size) separation here.
<EOS>
