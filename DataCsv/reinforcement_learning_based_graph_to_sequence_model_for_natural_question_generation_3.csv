Under review as a conference paper at ICLR 2020  REINFORCEMENT LEARNING BASED GRAPH-TO-SEQUENCE MODEL FOR NATURAL QUESTION GENERATION  Anonymous authors Paper under double-blind review  ABSTRACT  Natural question generation (QG) aims to generate questions from a passage and an answer.Previous works on QG either (i) ignore the rich structure informa- tion hidden in text, (ii) solely rely on cross-entropy loss that leads to issues like exposure bias and inconsistency between train/test measurement, or (iii) fail to fully exploit the answer information.To address these limitations, in this paper, we propose a reinforcement learning (RL) based graph-to-sequence (Graph2Seq) model for QG.
<EOS>
Our model consists of a Graph2Seq generator with a novel Bidi- rectional Gated Graph Neural Network based encoder to embed the passage, and a hybrid evaluator with a mixed objective function that combines both the cross- entropy and RL loss to ensure the generation of syntactically and semantically valid text.We also introduce an effective Deep Alignment Network for incorpo- rating the answer information into the passage at both the word and contextual level.Our model is end-to-end trainable and achieves new state-of-the-art scores, outperforming existing methods by a significant margin on the standard SQuAD benchmark for QG.
<EOS>
 1  INTRODUCTION  Natural question generation (QG) has many useful applications such as improving the question an- swering task (Chen et al, 2017; 2019a) by providing more training data (Tang et al, 2017; Yuan et al, 2017), generating practice exercises and assessments for educational purposes (Heilman & Smith, 2010; Danon & Last, 2017), and helping dialog systems to kick-start and continue a conver- sation with human users .While many existing works focus on QG from images (Fan et al, 2018; Li et al, 2018) or knowledge bases (Serban et al, 2016; Elsahar et al, 2018), in this work, we focus on QG from text. Conventional methods (Mostow & Chen, 2009; Heilman & Smith, 2010; Heilman, 2011) for QG rely on heuristic rules or hand-crafted templates, leading to the issues of low generalizability and scal- ability.
<EOS>
Recent attempts have been focused on exploiting Neural Network (NN) based approaches that do not require manually-designed rules and are end-to-end trainable.Encouraged by the huge success of neural machine translation, these approaches formulate the QG task as a sequence-to- sequence (Seq2Seq) learning problem.Specifically, attention-based Seq2Seq models (Bahdanau et al, 2014; Luong et al, 2015) and their enhanced versions with copy (Vinyals et al, 2015; Gu et al, 2016) and coverage  mechanisms have been widely applied and show promis- ing results on this task (Du et al, 2017; Zhou et al, 2017; Song et al, 2018a; Kumar et al, 2018a).
<EOS>
However, these methods typically ignore the hidden structural information associated with a word sequence such as the syntactic parsing tree.Failing to utilize the rich text structure information beyond the simple word sequence may limit the effectiveness of these models for QG. It has been observed that in general, cross-entropy based sequence training has several limitations like exposure bias and inconsistency between train/test measurement (Ranzato et al, 2015; Wu et al, 2016).
<EOS>
As a result, they do not always produce the best results on discrete evaluation metrics on sequence generation tasks such as text summerization  or question gener- ation .To cope with these issues, some recent QG approaches (Song et al, 2017; Kumar et al, 2018b) directly optimize evaluation metrics using Reinforcement Learning  1  Under review as a conference paper at ICLR 2020  (RL) (Williams, 1992).However, existing approaches usually only employ evaluation metrics like BLEU and ROUGE-L as rewards for RL optimization.
<EOS>
More importantly, they fail to exploit other important metrics such as syntactic and semantic constraints for guiding high-quality text genera- tion. Early works on neural QG did not take into account the answer information when generating a question.Recent works have started to explore various means of utilizing the answer information.
<EOS>
When question generation is guided by the semantics of an answer, the resulting questions become more relevant and readable.Conceptually, there are three different ways to incorporate the answer information by simply marking the answer location in the passage (Zhou et al, 2017; Zhao et al, 2018; Liu et al, 2019), or using complex passage-answer matching strategies , or separating answers from passages when applying a Seq2Seq model (Kim et al, 2018; Sun et al, 2018).However, they neglect potential semantic relations between passage words and answer words, and thus fail to explicitly model the global interactions among them in the embedding space.
<EOS>
 To address these aforementioned issues, in this paper, we present a novel reinforcement learning based generator-evaluator architecture that aims to: i) make full use of rich hidden structure in- formation beyond the simple word sequence; ii) generate syntactically and semantically valid text while maintaining the consistency of train/test measurement; iii) model explicitly the global interac- tions of semantic relationships between passage and answer at both word-level and contextual-level.In particular, to achieve the first goal, we first explore two different means to either construct a syntax-based static graph or a semantics-aware dynamic graph from the text sequence, as well as its rich hidden structure information.Then, we design a graph-to-sequence (Graph2Seq) model based generator that encodes the graph representation of a text passage and decodes a question sequence using a Recurrent Neural Network (RNN).
<EOS>
Our Graph2Seq model is based on a novel bidirectional gated graph neural network, which extends the original gated graph neural network  by considering both incoming and outgoing edges, and fusing them during the graph embedding learning.To achieve the second goal, we design a hybrid evaluator which is trained by optimizing a mixed objective function that combines both cross-entropy and RL loss.We use not only discrete evaluation metrics like BLEU, but also semantic metrics like word mover’s distance  to encourage both syntactically and semantically valid text generation.
<EOS>
To achieve the third goal, we propose a novel Deep Alignment Network (DAN) for effectively incorporating answer information into the passage at multiple granularity levels. Our main contributions are as follows:  • We propose a novel RL-based Graph2Seq model for natural question generation.To the  best of our knowledge, we are the first to introduce the Graph2Seq architecture for QG.
<EOS>
 • We explore both static and dynamic ways of constructing graph from text and are the first  to systematically investigate their performance impacts on a GNN encoder. • The proposed model is end-to-end trainable, achieves new state-of-the-art scores, and out- performs existing methods by a significant margin on the standard SQuAD benchmark for QG.Our human evaluation study also corroborates that the questions generated by our model are more natural (semantically and syntactically) compared to other baselines.
<EOS>
 2 AN RL-BASED GENERATOR-EVALUATOR ARCHITECTURE  In this section, we define the question generation task, and then present our RL-based Graph2Seq model for question generation.We first motivate the design, and then present the details of each component as shown in Fig 1. 2.1 PROBLEM FORMULATION  The goal of question generation is to generate natural language questions based on a given form of data, such as knowledge base triples or tables , sentences (Du et al, 2017; Song et al, 2018a), or images , where the generated questions need to be answerable from the input data.
<EOS>
In this paper, we focus on QG from a given text passage, along with a target answer.We assume that a text passage is a collection of word tokens X p “ txp answer is also a collection of word tokens X a “ txa  N u, and a target Lu. The task of natural question  2, ..., xp  2, ..., xa  1, xp  1, xa  2  Under review as a conference paper at ICLR 2020  Figure 1: Overall architecture of the proposed model.Best viewed in color.
<EOS>
 generation is to generate the best natural language question consisting of a sequence of word tokens ˆY “ ty1, y2, ..., yT u which maximizes the conditional likelihood ˆY “ arg maxY P pY |X p, X aq.Here N , L, and T are the lenghts of the passage, answer and question, respectively.We focus on the problem setting where we have a set of passage (and answers) and target questions pairs, to learn the mapping; existing QG approaches (Du et al, 2017; Song et al, 2018a; Zhao et al, 2018; Kim et al, 2018) make a similar assumption.
<EOS>
 2.2 DEEP ALIGNMENT NETWORK  Answer information is crucial for generating relevant and high quality questions from a passage.Un- like previous methods that neglect potential semantic relations between passage and answer words, we explicitly model the global interactions among them in the embedding space.To this end, we propose a novel Deep Alignment Network (DAN) component for effectively incorporating answer information into the passage with multiple granularity levels.
<EOS>
Specifically, we perform attention- based soft-alignment at the word level, as well as at the contextualized hidden state level, so that multiple levels of alignments can help learn hierarchical representations. Figure 2: The attention-based soft-alignment mechanism. Let Xp P RF ˆN and rXp P R rFpˆN denote two embeddings associated with passage text.
<EOS>
Similarly, let Xa P RF ˆL and rXa P R rFaˆL denoted two embeddings associated with answer text.Concep- tually, as shown in Fig 2, the soft-alignment mechanism consists of three steps: i) compute the attention score βi,j for each pair of passage word xp j : ii) multiply the atten- tion matrix β with the answer embeddings rXa to obtain the aligned answer embeddings Hp for the passage; iii) concatenate the resulting aligned answer embeddings Hp with the passage embeddings rXp to get the final passage embeddings rHp P Rp Formally, we define our soft-alignment function as following:  i and answer word xa  rFaqˆN rFp`  rHp “ AlignpXp, Xa, rXp, rXaq “ CATp  (1) where the matrix rHp is the final passage embedding, the function CAT is a simple concatenation operation, and β is a N ˆ L attention score matrix, computed by  rXp; Hpq “ CATp  rXp; rXaβT q  ¯ ´ ReLUpWXpqT ReLUpWXaq  β 9 exp  (2)  3  Under review as a conference paper at ICLR 2020  where W P RdˆF is a trainable weight matrix, with d being the hidden state size and ReLU is the rectified linear unit .After introducing the general soft-alignment mechanism, we next introduce how we do soft-alignment at both word-level and contextualized hidden state level.
<EOS>
 2.2.1 WORD-LEVEL ALIGNMENT  In the word-level alignment stage, we first perform a soft-alignment between the passage and the answer based only on their pretrained GloVe embeddings and compute the final passage embed- dings by rHp “ AlignpGp, Ga, rGp; Bp; Lps, Gaq, where Gp, Bp, and Lp are the corresponding GloVe embedding , BERT embedding , and linguistic feature (i.e., case, NER and POS) embedding of the passage text, respectively.Then a bidirectional LSTM  is applied to the final passage embeddings rHp “ t to obtain contextualized passage embeddings sHp P R sF ˆNOn the other hand, for the answer text Xa, we simply concatenate its GloVe embedding Ga and its BERT embedding Ba to obtain its word embedding matrix Ha P Rd1ˆL.Another BiLSTM is then applied to the concatenated answer embedding sequence to obtain the contextualized answer embeddings sHa P R sF ˆL.
<EOS>
 rhp i uN i“1  2.2.2 HIDDEN-LEVEL ALIGNMENT  In the hidden-level alignment stage, we perform another soft-alignment based on the contextual- ized passage and answer embeddings.Similarly, we compute the aligned answer embedding, and concatenate it with the contextualized passage embedding to obtain the final passage embedding ma- trix AlignprGp; Bp; sHps, rGa; Ba; sHas, sHp, sHaq.Finally, we apply another BiLSTM to the above concatenated embedding to get a sF ˆ N passage embedding matrix X.  2.3 BIDIECTIONAL GRAPH-TO-SEQUENCE GENERATOR  While RNNs are good at capturing local dependencies among consecutive words in text, GNNs have been shown to better utilize the rich hidden text structure information such as syntactic parsing (Xu et al, 2018b) or semantic parsing (Song et al, 2018b), and can model the global interactions (relations) among sequence words to further improve the representations.
<EOS>
Therefore, unlike most of the existing methods that rely on RNNs to encode the input passage, we first construct a passage graph G from text where each passage word is treated as a graph node, and then employ a novel Graph2Seq model to encode the passage graph (and answer), and to decode the natural language question. 2.3.1 PASSAGE GRAPH CONSTRUCTION  Existing GNNs assume a graph structured input and directly consume it for computing the corre- sponding node embeddings.However, we need to construct a graph from the text.
<EOS>
Although there are early attempts on constructing a graph from a sentence (Xu et al, 2018b), there is no clear an- swer as to the best way of representing text as a graph.We explore both static and dynamic graph construction approaches, and systematically investigate the performance differences between these two methods in the experimental section. Syntax-based static graph construction: We construct a directed and unweighted passage graph based on dependency parsing.
<EOS>
For each sentence in a passage, we first get its dependency parse tree.We then connect neighboring dependency parse trees by connecting those nodes that are at a sentence boundary and next to each other in text. Semantics-aware dynamic graph construction: We dynamically build a directed and weighted graph to model semantic relationships among passage words.
<EOS>
We make the process of building such a graph depend on not only the passage, but also on the answer.The graph construction procedure consists of three steps: i) we compute a dense adjacency matrix A for the passage graph by applying self-attention to the word-level passage embeddings rHp, ii) a kNN-style graph sparsification strat- egy (Chen et al, 2019b) is adopted to obtain a sparse adjacency matrix ¯A, where we only keep the  4  Under review as a conference paper at ICLR 2020  K nearest neighbors (including itself) as well as the associated attention scores (i.e., the remaining attentions scores are masked off) for each node; and iii) inspired by BiLSTM over LSTM, we also compute two normalized adjacency matrices A% and A$ according to their incoming and outgo- ing directions, by applying softmax operation on the resulting sparse adjacency matrix ¯A and its transpose, respectively. A “ ReLUpU rHpqT ReLUpU rHpq,  ¯A “ kNNpAq, A%, A$ “ softmaxpt ¯A, ¯AT uq  (3)  rFaq trainable weight matrix.
<EOS>
Note that the supervision signal is able to where U is a d ˆ p back-propagate through the kNN-style graph sparsification operation since the K nearest attention scores are kept. rFp `  2.3.2 BIDIRECTIONAL GATED GRAPH NEURAL NETWORKS  To effectively learn the graph embeddings from the constructed text graph, we propose a novel Bidi- rectional Gated Graph Neural Network (BiGGNN) which extends Gated Graph Sequence Neural Networks  by learning node embedding from both incoming and outgoing edges in an interleaved fashion when processing the directed passage graph.Similar idea has also been exploited in (Xu et al, 2018a), which extended another popular variant of GNNs - GraphSAGE .
<EOS>
However, one of key difference between our BiGGNN and their bidi- rectional GraphSAGE is that we fuse the intermediate node embeddings from both incoming and outgoing edges in every iteration during the training, whereas their model simply trains the node embeddings of each direction independently and concatenates them in the final step. In BiGGNN, node embeddings are initialized to the passage embeddings X returned by DAN.The same set of network parameters are shared at every hop of computation.
<EOS>
At each computation hop, for every node in the graph, we apply an aggregation function which takes as input a set of incoming (or outgoing) neighboring node vectors and outputs a backward (or forward) aggregation vector.For the syntax-based static graph, we use a mean aggregator for simplicity although other operators such as max or attention (Veliˇckovi´c et al, 2017) could also be employed,  hk N hk N  %pvq  $pvq  “ MEANpthk´1 “ MEANpthk´1  v  u  u Y thk´1 u Y thk´1  , @u P N , @u P N  u  v  %pvquq  $pvquq  For the semantics-aware dynamic graph we compute a weighted average for aggregation where the weights come from the normalized adjacency matrices A% and A$, defined as,  hk N  %pvq  “  v,uhk´1 a%  u  , hk N  “  $pvq  v,uhk´1 a$  u  ÿ  @uPN  %pvq  ÿ  @uPN  $pvq  While (Xu et al, 2018a) learn separate node embeddings for both directions independently, we choose to fuse the information aggregated in the two directions at each hop, which we find works better in general. hk  N “ Fusephk N  , hk N  q  %pvq  $pvq  We design the fusion function as a gated sum of two information sources,  Fusepa, bq “ z d a ` p1 ´ zq d b,  z “ σpWzra; b; a d b; a ´ bs ` bzq  where d is the component-wise multiplication, σ is a sigmoid function, and z is a gating vector.
<EOS>
 Finally, a Gated Recurrent Unit (GRU)  is used to update the node embeddings by incorporating the aggregation information. (4)  (5)  (6)  (7)  (8)  After n hops of GNN computation, where n is a hyperparameter, we obtain the final state embedding hn v for node v. To compute the graph-level embedding, we first apply a linear projection to the node embeddings, and then apply max-pooling over all node embeddings to get a d-dim vector hG. hk v “ GRUphk´1  v  , hk  Nq  5  Under review as a conference paper at ICLR 2020  2.3.3 RNN DECODER  On the decoder side, we adopt the same model architecture as other state-of-the-art Seq2Seq mdoels where an attention-based (Bahdanau et al, 2014; Luong et al, 2015) LSTM decoder with copy (Vinyals et al, 2015; Gu et al, 2016) and coverage mechanisms  is empolyed.
<EOS>
The decoder takes the graph-level embedding hG followed by two separate fully-connected layers as initial hidden states (i.e., c0 and s0) and the node embeddings thn v , @v P Gu as the attention memory, and generates the output sequence one word at a time.The particular decoder used in this work closely follows .We refer the readers to Appendix A for more details.
<EOS>
 2.4 HYBRID EVALUATOR  It has been observed that optimizing such cross-entropy based training objectives for sequence learn- ing does not always produce the best results on discrete evaluation metrics (Ranzato et al, 2015; Wu et al, 2016; Paulus et al, 2017).Major limitations of this strategy include exposure bias and evaluation discrepancy between training and testing.To tackle these issues, some recent QG ap- proaches (Song et al, 2017; Kumar et al, 2018b) directly optimize evaluation metrics using REIN- FORCE.
<EOS>
We further use a mixed objective function with both syntactic and semantic constraints for guiding text generation.In particular, we present a hybrid evaluator with a mixed objective function that combines both cross-entropy loss and RL loss in order to ensure the generation of syntactically and semantically valid text. For the RL part, we employ the self-critical sequence training (SCST) algorithm  to directly optimize the evaluation metrics.
<EOS>
SCST is an efficient REINFORCE algorithm that utilizes the output of its own test-time inference algorithm to normalize the rewards it experiences.In SCST, at each training iteration, the model generates two output sequences: the sampled output Y s, produced by multinomial sampling, that is, each word ys t is sampled according to the likelihood P pyt|X, yătq predicted by the generator, and the baseline output ˆY , obtained by greedy search, that is, by maximizing the output probability distribution at each decoding step.We define rpY q as the reward of an output sequence Y , computed by comparing it to corresponding ground-truth sequence Y ˚ with some reward metrics.
<EOS>
The loss function is defined as: log P pys  Lrl “ prp ˆY q ´ rpY sqq  (9)  ÿ  t |X, ys  ătq  t  As we can see, if the sampled output has a higher reward than the baseline one, we maximize its likelihood, and vice versa. One of the key factors for RL is to pick the proper reward function.To take syntactic and semantic constraints into account, we consider the following metrics as our reward functions:  Evaluation metric as reward function: We use one of our evaluation metrics, BLEU-4, as our reward function feval, which lets us directly optimize the model towards the evaluation metrics.
<EOS>
 Semantic metric as reward function: One drawback of some evaluation metrics like BLEU is that they do not measure meaning, but only reward systems for n-grams that have exact matches in the reference system.To make our reward function more effective and robust, we additionally use word movers distance (WMD) as a semantic reward function fsem.WMD is the state-of-the-art approach to measure the dissimilarity between two sentences based on word embeddings .
<EOS>
Following Gong et al (2019), we take the negative of the WMD distance between a generated sequence and the ground-truth sequence and divide it by the sequence length as its semantic score. We define the final reward function as rpY q “ fevalpY, Y ˚q ` αfsempY, Y ˚q where α is a scalar. 2.5 TRAINING AND TESTING  We train our model in two stages.
<EOS>
In the first state, we train the model using regular cross-entropy loss, defined as,  ÿ  Llm “  ´ log P py˚  t |X, y˚  ătq ` λ covlosst  (10)  where y˚ coverage loss defined as  t is the word at the t-th position of the ground-truth output sequence and covlosst is the i being the i-th element of the attention vector over  iq, with at  i minpat  i, ct  ř  t  6  Under review as a conference paper at ICLR 2020  the input sequence at time step t. Scheduled teacher forcing  is adopted to alleviate the exposure bias problem.In the second stage, we fine-tune the model by optimizing a mixed objective function combining both cross-entropy loss and RL loss, defined as,  where γ is a scaling factor controling the trade-off between cross-entropy loss and RL loss.During the testing phase, we use beam search to generate final predictions.
<EOS>
 L “ γLrl ` p1 ´ γqLlm  (11)  3 EXPERIMENTS  We evaluate our proposed model against state-of-the-art methods on the SQuAD dataset .Our full models have two variants G2Ssta+BERT+RL and G2Sdyn+BERT+RL which adopts static graph construction or dynamic graph construction, respectively.For model settings and sensitivity analysis, please refer to Appendix B and C. 1  3.1 BASELINE METHODS  We compare against the following baselines in our experiments: i) SeqCopyNet , ii) NQG++ , iii) MPQG+R , iv) AFPQA , v) s2sa-at-mp-gsa , vi) ASs2s , and vii) CGC-QG .
<EOS>
Detailed descriptions of the baselines are provided in Appendix D. Experiments on baselines followed by * are conducted using released source codes.Results of other baselines are taken from the corresponding papers, with unreported metrics marked as –. 3.2 DATA AND METRICS  SQuAD contains more than 100K questions posed by crowd workers on 536 Wikipedia arti- cles.
<EOS>
Since the test set of the original SQuAD is not publicly available, the accessible parts («90%) are used as the entire dataset in our experiments.For fair comparison with previ- ous methods, we evaluated our model on both data split-1 (Song et al, 2018a)2 that contains 75,500/17,934/11,805 (train/development/test) examples and data split-2  3 that contains 86,635/8,965/8,964 examples. Following previous works, we use BLEU-4 , METEOR , ROUGE-L (Lin, 2004) and Q-BLEU1  as our evaluation metrics.
<EOS>
Initially, BLEU-4 and METEOR were designed for evaluating machine translation systems and ROUGE-L was designed for evaluating text summarization systems.Recently, Q-BLEU1 was de- signed for better evaluating question generation systems, which was shown to correlate significantly better with human judgments compared to existing metrics. Besides automatic evaluation metrics, we also conduct a human evaluation study on split-2.
<EOS>
We ask human evaluators to rate generated questions from a set of anonymized competing systems based on whether they are syntactically correct, semantically correct and relevant to the passage.The rating scale is from 1 to 5, on each of the three categories.Evaluation scores from all evaluators were collected and averaged as final scores.
<EOS>
Further details on human evaluation can be found in Appendix E.  3.3 EXPERIMENTAL RESULTS AND HUMAN EVALUATION  Table 1 shows the automatic evaluation results comparing our proposed models against other state- of-the-art baseline methods.First of all, we can see that both of our full models G2Ssta+BERT+RL and G2Sdyn+BERT+RL achieve the new state-of-the-art scores on both data splits and consistently outperform previous methods by a significant margin.This highlights that our RL-based Graph2Seq model, together with the deep alignment network, successfully addresses the three issues we high- lighted in Sec.
<EOS>
1.Between these two variants, G2Ssta+BERT+RL outperforms G2Sdyn+BERT+RL  1The implementation of our model will be made publicly available after the review period.2https://www.cs.rochester.edu/˜lsong10/downloads/nqg_data.tgz 3https://res.qyzhou.me/redistribute.zip  7  Under review as a conference paper at ICLR 2020  Table 1: Automatic evaluation results on the SQuAD test set.
<EOS>
 BLEU-4 METEOR ROUGE-L Q-BLEU1 BLEU-4 METEOR ROUGE-L Q-BLEU1  Methods  Split-1  – –  14.39  SeqCopyNet NQG++ MPQG+R* AFPQA s2sa-at-mp-gsa ASs2s CGC-QG G2Sdyn+BERT+RL 17.55 17.94 G2Ssta+BERT+RL  15.32 16.20  –  –  – –  –  –  19.29 19.92  21.42 21.76  – –  –  –  43.91 43.96  45.59 46.02  – –  – – – –  55.40 55.60  Split-2  – –  –  –  44.00  –  –  –  19.67  44.24  21.24 21.53 21.70  44.53 45.91 45.98  13.02 13.29 14.71 15.64 15.82 16.17 17.55 18.06 18.30  – –  – – – –  55.00 55.20  18.99  42.46  52.00  18.93  42.60  50.30  Table 2: Human evaluation results (˘ standard deviation) on the SQuAD split-2 test set.The rating scale is from 1 to 5 (higher scores indicate better results). Methods MPQG+R* G2Ssta+BERT+RL Ground-truth  Syntactically correct 4.34 (0.15) 4.41 (0.09) 4.74 (0.14)  Semantically correct Relevant 4.01 (0.23) 4.31 (0.12) 4.74 (0.19)  3.21 (0.31) 3.79 (0.45) 4.25 (0.38)  on all the metrics.
<EOS>
Also, unlike the baseline methods, our model does not rely on any hand-crafted rules or ad-hoc strategies, and is fully end-to-end trainable. As shown in Table 2, we conducted a human evaluation study to assess the quality of the questions generated by our model, the baseline method MPQG+R, and the ground-truth data in terms of syn- tax, semantics and relevance metrics.We can see that our best performing model achieves good results even compared to the ground-truth, and outperforms the strong baseline method MPQG+R.
<EOS>
Our error analysis shows that main syntactic error occurs in repeated/unknown words in generated questions.Further, the slightly lower quality on semantics also impacts the relevance. 3.4 ABLATION STUDY  Table 3: Ablation study on the SQuAD split-2 test set.
<EOS>
 Methods G2Sdyn+BERT+RL G2Ssta+BERT+RL G2Ssta+BERT-fixed+RL G2Sdyn+BERT G2Ssta+BERT G2Ssta+BERT-fixed G2Sdyn+RL G2Ssta+RL  BLEU-4 18.06 18.30 18.20 17.56 18.02 17.86 17.18 17.49  Methods G2Sdyn G2Ssta G2Sdyn w/o DAN G2Ssta w/o DAN G2Ssta w/o BiGGNN, w/ Seq2Seq G2Ssta w/o BiGGNN, w/ GCN G2Ssta w/ GGNN-forward G2Ssta w/ GGNN-backward  BLEU-4 16.81 16.96 12.58 12.62 16.14 14.47 16.53 16.75  As shown in Table 3, we perform an ablation study to systematically assess the impact of differ- ent model components (e.g., BERT, RL, DAN, and BiGGNN) for two proposed full model variants (static vs dynamic) on the SQuAD split-2 test set.It confirms our finding that syntax-based static graph construction (G2Ssta+BERT+RL) performs better than semantics-aware dynamic graph con- struction (G2Sdyn+BERT+RL) in almost every setting.However, it may be too early to conclude which one is the method of choice for QG.
<EOS>
On the one hand, an advantage of static graph construc- tion is that useful domain knowledge can be hard-coded into the graph, which can greatly benefit the downstream task.However, it might suffer if there is a lack of prior knowledge for a specific domain knowledge.On the other hand, dynamic graph construction does not need any prior knowledge about the hidden structure of text, and only relies on the attention matrix to capture these structured information, which provides an easy way to achieve a decent performance.
<EOS>
One interesting direction is to explore effective ways of combining both static and dynamic graphs. 8  Under review as a conference paper at ICLR 2020  By turning off the Deep Alignment Network (DAN), the BLEU-4 score of G2Ssta (similarly for G2Sdyn) dramatically drops from 16.96% to 12.62%, which indicates the importance of answer information for QG and shows the effectiveness of DAN.This can also be verified by comparing the performance between the DAN-enhanced Seq2Seq model (16.14 BLEU-4 score) and other carefully designed answer-aware Seq2Seq baselines such as NQG++ (13.29 BLEU-4 score), MPQG+R (14.71 BLEU-4 score) and AFPQA (15.82 BLEU-4 score).
<EOS>
Further experiments demonstrate that both word-level (G2Ssta w/ DAN-word only) and hidden-level (G2Ssta w/ DAN-hidden only) answer alignments in DAN are helpful. We can see the advantages of Graph2Seq learning over Seq2Seq learning on this task by comparing the performance between G2Ssta and Seq2Seq.Compared to Seq2Seq based QG methods that com- pletely ignore hidden structure information in the passage, our Graph2Seq based method is aware of more hidden structure information such as semantic similarity between any pair of words that are not directly connected or syntactic relationships between two words captured in a dependency parsing tree.
<EOS>
In our experiments, we also observe that doing both forward and backward message passing in the GNN encoder is beneficial.Surprisingly, using GCN  as the graph en- coder (and converting the input graph to an undirected graph) does not provide good performance.In addition, fine-tuning the model using REINFORCE can further improve the model performance in all settings (i.e., w/ and w/o BERT), which shows the benefits of directly optimizing the evalu- ation metrics.
<EOS>
Besides, we find that the pretrained BERT embedding has a considerable impact on the performance and fine-tuning BERT embedding even further improves the performance, which demonstrates the power of large-scale pretrained language models. 3.5 CASE STUDY  Table 4: Generated questions on SQuAD split-2 test set.Target answers are underlined.
<EOS>
 Passage: for the successful execution of a project , effective planning is essentialGold: what is essential for the successful execution of a project ?G2Ssta w/o BiGGNN (Seq2Seq): what type of planning is essential for the project ?G2Ssta w/o DAN.: what type of planning is essential for the successful execution of a project ?
<EOS>
G2Ssta: what is essential for the successful execution of a project ?G2Ssta+BERT: what is essential for the successful execution of a project ?G2Ssta+BERT+RL: what is essential for the successful execution of a project ?
<EOS>
G2Sdyn+BERT+RL: what is essential for the successful execution of a project ?Passage: the church operates three hundred sixty schools and institutions overseasGold: how many schools and institutions does the church operate overseas ?G2Ssta w/o BiGGNN (Seq2Seq): how many schools does the church have ?
<EOS>
G2Ssta w/o DAN.: how many schools does the church have ?G2Ssta: how many schools and institutions does the church have ?G2Ssta+BERT: how many schools and institutions does the church have ?
<EOS>
G2Ssta+BERT+RL: how many schools and institutions does the church operate ?G2Sdyn+BERT+RL: how many schools does the church operate ? In Table 4, we further show a few examples that illustrate the quality of generated text given a pas- sage under different ablated systems.
<EOS>
As we can see, incorporating answer information helps the model identify the answer type of the question to be generated, and thus makes the generated ques- tions more relevant and specific.Also, we find our Graph2Seq model can generate more complete and valid questions compared to the Seq2Seq baseline.We think it is because a Graph2Seq model is able to exploit the rich text structure information better than a Seq2Seq model.
<EOS>
Lastly, it shows that fine-tuning the model using REINFORCE can improve the quality of the generated questions. 4 RELATED WORK  4.1 NATURAL QUESTION GENERATION  Early works (Mostow & Chen, 2009; Heilman & Smith, 2010; Heilman, 2011) for QG focused on rule-based approaches that rely on heuristic rules or hand-crafted templates, with low generaliz- ability and scalability.Recent attempts have focused on NN-based approaches that do not require  9  Under review as a conference paper at ICLR 2020  manually-designed rules and are end-to-end trainable.
<EOS>
Existing NN-based approaches (Du et al, 2017; Yao et al; Zhou et al, 2018) rely on the Seq2Seq model with attention, copy or coverage mechanisms.In addition, various ways (Zhou et al, 2017; Song et al, 2017; Zhao et al, 2018; Sun et al, 2018; Kim et al, 2018; Liu et al, 2019) have been proposed to utilize the target answer so as to guide the generation of the question.To address the limitations of cross-entropy based sequence learning, some approaches (Song et al, 2017; Kumar et al, 2018b) aim at directly optimizing eval- uation metrics using REINFORCE.
<EOS>
 However, the existing approaches for QG suffer from several limitations; they (i) ignore the rich structure information hidden in text, (ii) solely rely on cross-entropy loss that leads to issues like exposure bias and inconsistency between train/test measurement, and (iii) fail to fully exploit the answer information.To address these limitations, we propose a reinforcement learning (RL) based graph-to-sequence (Graph2Seq) model for QG as well as deep alignment networks to effectively cope with the QG task.To the best of our knowledge, we are the first to introduce the Graph2Seq architecture to solve the question generation task.
<EOS>
 4.2 GRAPH NEURAL NETWORKS  Over the past few years, graph neural networks (GNNs) (Kipf & Welling, 2016; Gilmer et al, 2017; Hamilton et al, 2017; Li et al, 2015) have attracted increasing attention.Due to more recent ad- vances in graph representation learning, a number of works have extended the widely used Seq2Seq architectures (Sutskever et al, 2014; Cho et al, 2014) to Graph2Seq architectures for machine trans- lation, semantic parsing, and AMR(SQL)-to-text tasks (Bastings et al, 2017; Beck et al, 2018; Xu et al, 2018a;b;c; Song et al, 2018b).While the high-quality graph structure is crucial for the per- formance of GNN-based approaches, most existing works use syntax-based static graph structures when applied to textual data.
<EOS>
Very recently, researchers have started exploring methods to automat- ically construct a graph of visual objects  or words (Liu et al, 2018; Chen et al, 2019b) when applying GNNs to non-graph structured data. To the best of our knowledge, we are the first to investigate systematically the performance difference between syntactic-aware static graph construction and semantics-aware dynamic graph construction in the context of question generation. 5 CONCLUSION  We proposed a novel RL based Graph2Seq model for QG, where the answer information is utilized by an effective Deep Alignment Network and a novel bidirectional GNN is proposed to process the directed passage graph.
<EOS>
Our two-stage training strategy benefits from both cross-entropy based and REINFORCE based sequence training.We also explore both static and dynamic graph construction from text, and systematically investigate and analyze the performance difference between the two.On the benchmark SQuAD dataset, our proposed model outperforms previous state-of-the-art meth- ods by a significant margin and achieve new best results.
<EOS>
