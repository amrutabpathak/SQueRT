b'Published as a conference paper at ICLR 2020\n\nON THE WEAKNESSES OF REINFORCEMENT LEARN-ING FOR NEURAL MACHINE TRANSLATIONLeshem Choshen1, Lior Fox2, Zohar Aizenbud1, Omri Abend1,3'
<EOS>
b'1 School of Computer Science and Engineering, 2The Edmond and Lily Safra Center for Brain Sciences3 Department of Cognitive Sciences'
<EOS>
b'The Hebrew University of Jerusalemfirst.last@mail.huji.ac.il, oabend@cs.huji.ac.ilABSTRACT\n\nReinforcement learning (RL) is frequently used to increase performance in text\ngeneration tasks, including machine translation (MT), notably through the use\nof Minimum Risk Training (MRT) and Generative Adversarial Networks (GAN).'
<EOS>
b'However, little is known about what and how these methods learn in the context of\nMT.We prove that one of the most common RL methods for MT does not optimize\nthe expected reward, as well as show that other methods take an infeasibly long\ntime to converge.In fact, our results suggest that RL practices in MT are likely to\nimprove performance only where the pre-trained parameters are already close to\nyielding the correct translation.'
<EOS>
b'Our \xef\xac\x81ndings further suggest that observed gains\nmay be due to effects unrelated to the training signal, concretely, changes in the\nshape of the distribution curve.1INTRODUCTION'
<EOS>
b'Reinforcement learning (RL) is an appealing path for advancement in Machine Translation (MT), as\nit allows training systems to optimize non-differentiable score functions, common in MT evaluation,\nas well as tackling the \xe2\x80\x9cexposure bias\xe2\x80\x9d (Ranzato et al., 2015) in standard training, namely that the\nmodel is not exposed during training to incorrectly generated tokens, and is thus unlikely to recover\nfrom generating such tokens at test time.These motivations have led to much interest in RL for text\ngeneration in general and MT in particular (see \xc2\xa72).Various policy gradient methods have been used,\nnotably REINFORCE (Williams, 1992) and variants thereof (e.g., Ranzato et al., 2015; Edunov et al.,\n2018) and Minimum Risk Training (MRT; e.g., Och, 2003; Shen et al., 2016).'
<EOS>
b'Another popular use\nof RL is for training GANs(Yang et al., 2018; Tevet et al., 2018).Nevertheless, despite increasing\ninterest and strong results, little is known about what accounts for these performance gains, and the\ntraining dynamics involved.'
<EOS>
b'We present the following contributions.First, our theoretical analysis shows that commonly used\napproximation methods are theoretically ill-founded, and may converge to parameter values that do\nnot minimize the risk, nor are local minima thereof (\xc2\xa72.2).Second, using both naturalistic experiments and carefully constructed simulations, we show that\nperformance gains observed in the literature likely stem not from making target tokens the most\nprobable, but from unrelated effects, such as increasing the peakiness of the output distribution (i.e.,\nthe probability mass of the most probable tokens).'
<EOS>
b'We do so by comparing a setting where the\nreward is informative, vs. one where it is constant.In \xc2\xa74 we discuss this peakiness effect (PKE).Third, we show that promoting the target token to be the mode is likely to take a prohibitively long\ntime.'
<EOS>
b'The only case we \xef\xac\x81nd, where improvements are likely, is where the target token is among\nthe \xef\xac\x81rst 2-3 most probable tokens according to the pretrained model.These \xef\xac\x81ndings suggest that\nREINFORCE (\xc2\xa75) and CMRT (\xc2\xa76) are likely to improve over the pre-trained model only under the\nbest possible conditions, i.e., where the pre-trained model is \xe2\x80\x9cnearly\xe2\x80\x9d correct.We conclude by discussing other RL practices in MT which should be avoided for practical and\ntheoretical reasons, and brie\xef\xac\x82y discuss alternative RL approaches that will allow RL to tackle a\nlarger class of errors in pre-trained models (\xc2\xa77).'
<EOS>
b'1Published as a conference paper at ICLR 20202 RL IN MACHINE TRANSLATION'
<EOS>
b'An MT system generates tokens y =(y1, ..., yn) from a vocabulary V one token at a time.The\nprobability of generating yi given preceding tokens y<i is given by P\xce\xb8(\xc2\xb7|x, y<i), where x is the\nsource sentence and \xce\xb8 are the model parameters.'
<EOS>
b'For each generated token yi, we denote with\nr(yi; y<i, x, y(ref ))the score, or reward, for generating yigiven y<i, x, and the reference sentence\ny(ref ).'
<EOS>
b'For brevity, we omit parameters where they are \xef\xac\x81xed within context.For simplicity, we\nassume r does not depend on followingtokens y'
<EOS>
b'>i.We also assume there is exactly one valid target token, as de facto, training is done against a single\nreference (Schulz et al., 2018).'
<EOS>
b'In practice, either a token-level reward is approximated using Monte-\nCarlo methods (e.g., Yang et al., 2018), ora sentence-level (sparse) reward is given at the end of the\nepisode (sentence).'
<EOS>
b'The latter is equivalent to a uniform token-level reward.r is often the negative log-likelihood, or a standard MT metric, e.g., BLEU (Papineni et al., 2002).RL\xe2\x80\x99s goal is to maximize the expected episode reward (denoted with R); i.e., to \xef\xac\x81nd'
<EOS>
b'\xce\xb8\xe2\x88\x97 =argmax'
<EOS>
b'R(\xce\xb8)=arg'
<EOS>
b'maxEy\xe2\x88\xbcP\xce\xb8[r(y)]'
<EOS>
b'\xce\xb8\xce\xb82.1 REINFORCE'
<EOS>
b'For a given source sentence, and past predictions y<i, REINFORCE (Williams, 1992) samples k\ntokens(k is a hyperparameter)'
<EOS>
b'S =(cid:0)y(1), ..., y(k)(cid:1) from P\xce\xb8 and updates \xce\xb8 according to this rule:(1)\n\n(2)\n\n\xe2\x88\x86\xce\xb8 \xe2\x88\x9d\n\nr(yi)\xe2\x88\x87 log(P\xce\xb8(yi))'
<EOS>
b'1kk'
<EOS>
b'(cid:88)i=1The right-hand side of equation 2 is an unbiased estimator of the gradient of the objective function,\ni.e., E [\xe2\x88\x86\xce\xb8] \xe2\x88\x9d'
<EOS>
b'\xe2\x88\x87\xce\xb8R (\xce\xb8).Therefore, REINFORCE is performing a form of stochastic gradient ascent\non R, and has similar formal guarantees.From here follows that if R is constant with respect to \xce\xb8,\nthen the expected \xe2\x88\x86\xce\xb8 prescribed by REINFORCE is zero.'
<EOS>
b'We note that r may be shifted by a constant\nterm (called a \xe2\x80\x9cbaseline\xe2\x80\x9d), without affecting the optimal value for \xce\xb8.REINFORCE is used in MT, text generation, and image-to-text tasks (Liu et al., 2016; Wu et al., 2018;\nRennie et al., 2017; Shetty et al., 2017;Hendricks et al., 2016) \xe2\x80\x93 in isolation, or as a part of training'
<EOS>
b'(Ranzato et al., 2015).Lately, an especially prominent use for REINFORCE is adversarial training\nwith discrete data, where another network predicts the reward (GAN).For some recent work on RL\nfor NMT, see (Zhang et al., 2016; Li et al., 2017; Wu et al., 2017; Yu et al., 2017;'
<EOS>
b'Yang et al., 2018).2.2 MINIMUM RISK TRAININGThe term Minimum Risk Training (MRT) is used ambiguously in MT to refer either to the appli-\ncation of REINFORCE to minimizing the risk (equivalently, to maximizing the expected reward, the\nnegative loss), or more commonly to a somewhat different estimation method, which we term'
<EOS>
b'Con-\ntrastive MRT (CMRT) andturn now to analyzing.CMRT was proposed by Och (2003)'
<EOS>
b', adapted to\nNMT by Shen et al.(2016), and often used since (Ayana et al., 2016; Neubig, 2016; Shen et al.,\n2017; Edunov et al., 2018; Makarov & Clematide, 2018; Neubig et al., 2018).The method works as follows: at each iteration, sample k tokens S ='
<EOS>
b'{y1, . . ., yk} from P\xce\xb8, and\nupdate \xce\xb8 according to the gradient of\n\nwhere\n\n(cid:101)R(\xce\xb8, S) =Q\xce\xb8,S(yi)r(yi)'
<EOS>
b'=Ey\xe2\x88\xbcQ(cid:2)r(y)(cid:3)'
<EOS>
b'k(cid:88)i=1'
<EOS>
b'Q\xce\xb8,S(yi) =\n\nP (yi)\xce\xb1yj \xe2\x88\x88S P (yj)\xce\xb1\n\n(cid:80)Commonly (but not universally), deduplication is performed, so (cid:101)R sums over a set of unique values'
<EOS>
b'(Sennrich et al., 2017).This changes little in our empirical results and theoretical analysis.Despite the resemblance in de\xef\xac\x81nitions of R (equation 1) and (cid:101)R (indeed, (cid:101)R is sometimes presented as\nan approximation of R), they differ in two important aspects.'
<EOS>
b'First, Q\xe2\x80\x99s support is S, so increasing\n\n2Published as a conference paper at ICLR 2020Q(yi) for some yi necessarily comes at the expense of Q(y) for some y \xe2\x88\x88'
<EOS>
b'S.In contrast, increas-\ning P (yi), as in REINFORCE, may come at the expense of P (y) for any y \xe2\x88\x88 V .Second, \xce\xb1 is a\nsmoothness parameter'
<EOS>
b': the closer \xce\xb1 is to 0, the closer Q is to be uniform.We show in Appendix A.1 that despite its name, CMRT does not optimize R, nor does it optimize\nE[ (cid:101)R].That is, it may well converge to values that are not local maxima of R, making it theoretically\nill'
<EOS>
b'-founded.1However, given CMRT popularity, the strong results it yielded and the absence of\ntheory for explaining it, we discuss it here.Given a sample S, the gradient of (cid:101)R is given by\n\n\xe2\x88\x87 (cid:101)R = \xce\xb1\n\nQ(yi) \xc2\xb7 r(yi) \xc2\xb7'
<EOS>
b'\xe2\x88\x87 log P (yi)\xe2\x88\x92 EQ[r]\xe2\x88\x87 log Z(S)(3)'
<EOS>
b'(cid:17)k(cid:88)'
<EOS>
b'(cid:16)i=1where Z(S) ='
<EOS>
b'(cid:80)i P (yi)\xce\xb1.See Appendix A.2.'
<EOS>
b'Comparing Equations 2 and 3, the differences between REINFORCE and CMRT are re\xef\xac\x82ected again.First, \xe2\x88\x87 (cid:101)R has an additional term, proportional to \xe2\x88\x87 log Z(S), which yields the contrastive effect.This contrast may improve the rate of convergence since it counters the decrease of probability mass\nfor non-sampled tokens.'
<EOS>
b'Second, given S, the relative weighting of the gradients\xe2\x88\x87log P (yi) is proportional to r(yi)Q(yi), or'
<EOS>
b'equivalently to r(yi)P (yi)\xce\xb1.CMRT with deduplication sums over distinct values in S (equation 3),while REINFORCE sums over all values.'
<EOS>
b'This means that the relative weight of the unique value yi\nis r(yi)|{yi\xe2\x88\x88S}|\nin REINFORCE.For \xce\xb1 = 1 the expected value of these relative weights is the same,\nand so for \xce\xb1 < 1 (as is commonly used), more weight is given to improbable tokens, which could\nalso have a positive effect on the convergence rate.2'
<EOS>
b'However, if \xce\xb1 is too close to 0, \xe2\x88\x87 (cid:101)R vanishes,\nas it is not affected by \xce\xb8.This tradeoff explains the importance of tuning \xce\xb1 reported in the literature.In \xc2\xa76 we present simulations with CMRT, showing very similar trends as presented by REINFORCE.'
<EOS>
b'k3 MOTIVATING DISCUSSIONImplementing a stochastic gradient ascent, REINFORCE is guaranteed to converge to a stationary\npoint of R under broad conditions.'
<EOS>
b'However, not much is known about its convergence rate under\nthe prevailing conditions in NMT.We begin with a qualitative, motivating analysis of these questions.As work on language generation\nempirically showed, RNNs quickly learn to output very peaky distributions (Press et al., 2017).'
<EOS>
b'This\ntendency is advantageous for generating \xef\xac\x82uent sentences with high probability, but may also entail\nslower convergence rates when using RL to \xef\xac\x81ne-tune the model, because RL methods used in text\ngeneration sample from the (pretrained) policy distribution, which means they mostly sample what\nthe pretrained model deems to be likely.Since the pretrained model (or policy) is peaky, exploration\nof other potentially more rewarding tokens will be limited, hampering convergence.Intuitively, REINFORCE increases the probabilities of successful (positively rewarding)'
<EOS>
b'observa-tions, weighing updates by how rewarding they were.When sampling a handful of tokens in each\ncontext (source sentence x and generated pre\xef\xac\x81x y<i), and where the number of epochs is not large,\nit is unlikely that more than a few unique tokens will be sampled from P\xce\xb8(\xc2\xb7|x, y<i).'
<EOS>
b'(In practice, k\nis typically between 1 and 20, and the number of epochs between 1 and 100.)It is thus unlikely that\nanything but the initially most probable candidates will be observed.Consequently, REINFORCE\ninitially raises their probabilities, even if more rewarding tokens can be found down the list.'
<EOS>
b'We thus hypothesize the peakiness of the distribution, i.e., the probability mass allocated to the most\nprobable tokens,will increase, at least in the \xef\xac\x81rst phase.We call this the peakiness-effect (PKE),\nand show it occurs both in simulations (\xc2\xa74.1) and in full-scale NMT experiments (\xc2\xa74.2).'
<EOS>
b'With more iterations, the most-rewarding tokens will be eventually sampled, and gradually gain\nprobability mass.This discussion suggests that training will be extremely sample-inef\xef\xac\x81cient.We\nassess the rate of convergence empirically in \xc2\xa75, \xef\xac\x81nding this to be indeed the case.'
<EOS>
b'1Sakaguchiet al.(2017) discuss the relation between CMRT and REINFORCE, claiming that CMRT is a\n\nvariant .'
<EOS>
b'Appendix A.1 shows that CMRT does not in fact optimize the same objective.2Not performing deduplication (e.g. in THUMT (Zhang et al., 2017)) results in assigning higher relativeweight to high-probability tokens, which may have an adverse effect on convergence rate.'
<EOS>
b'3Published as a conference paper at ICLR 2020Figure 1:'
<EOS>
b'A histogram of the up-\ndate size (x-axis) to the total pre-\ndicted probability of the 10 most\nprobable tokens (left) or the most\nprobable token (right) in the Con-\nstant Reward setting.An update\nis overwhelmingly more probable\nto increase this probability than to\ndecrease it.(a)'
<EOS>
b'Top 10(b) Mode\n\n4THE PEAKINESS EFFECT'
<EOS>
b'We turn to demonstrate that the initially most probable tokens will initially gain probability mass,\neven if they are not the most rewarding, yielding a PKE.Caccia et al.(2018) recently observed in the context of language modeling using GANs that per-'
<EOS>
b'formance gains similar to those GAN yield can be achieved by decreasing the temperature for the\nprediction softmax (i.e., making it peakier).However, they proposed no causes for this effect.Our\n\xef\xac\x81ndings propose an underlying mechanism leading to this trend.'
<EOS>
b'We return to this point in \xc2\xa77.Fur-thermore, given their \xef\xac\x81ndings, it is reasonable to assume that our results are relevant for RL use in\nother generation tasks, whose output space too is discrete, high-dimensional and concentrated.'
<EOS>
b'4.1 CONTROLLED SIMULATIONSWe experiment with a 1-layer softmax model, that predicts a single tokeni \xe2\x88\x88 V with probability'
<EOS>
b'e\xce\xb8ij e\xce\xb8j .\xce\xb8 ='
<EOS>
b'{\xce\xb8j}j\xe2\x88\x88V are the model\xe2\x80\x99s parameters.This model simulates the top of any MT decoder(cid:80)\nthat ends with a softmax layer, as essentially all NMT decoders do.'
<EOS>
b'To make experiments realistic,\nwe use similar parameters as those reported in the in\xef\xac\x82uential Transformer NMT system (Vaswani\net al., 2017).Speci\xef\xac\x81cally, the size of V (distinct BPE tokens) is 30,715, and the initial values for \xce\xb8\nwere sampled from 1,000 sets of logits taken from decoding the standard newstest2013 development\nset, using a pretrained Transformer model.The model was pretrained on WMT2015 training data'
<EOS>
b'(Bojar et al., 2015).Hyperparameters are reported in Appendix A.3.We de\xef\xac\x81ne one of the tokens in\nV to be the target token and denote it with ybest.'
<EOS>
b'We assign deterministic token reward, this makes\nlearning easier than when relying on approximations and our predictions optimistic.We experiment\nwith two reward functions:\n\n1.Simulated Reward: r(y)'
<EOS>
b'=2 for y = ybest, r(y)='
<EOS>
b'1if y is one of the 10 initially highest\nscoring tokens, and r(y)= 0'
<EOS>
b'otherwise.This simulates a condition where the pretrained\nmodel is of decent but sub-optimal quality.r here is at the scale of popular rewards used in\nMT, such as GAN-based rewards or BLEU (which are between 0 and 1).'
<EOS>
b'2.Constant Reward: r is constantly equal to 1, for all tokens.This setting is aimed to\n\ncon\xef\xac\x81rm that PKE is not a result of the signal carried by the reward.'
<EOS>
b'Experiments with the \xef\xac\x81rst setting were run 100 times, each time for 50K steps, updating \xce\xb8 after\neach step.With the second setting, it is suf\xef\xac\x81cient to take a single step at a time, as the expected\nupdate after each step is zero, and so any PKE seen in a single step is only accentuated in the next.It is, therefore, more telling to run more repetitions rather than more steps per initialization.'
<EOS>
b'We,\ntherefore, sample 10,000 pretrained distributions, and perform a single REINFORCE step.As RL training lasts about 30 epochs before stopping, samples about 100K tokens per epoch, and\nas the network already predicts ybest in about two thirds of the contexts,3we estimate the number\nof steps used in practice to be in the order of magnitude of 1M. For visual clarity, we present'
<EOS>
b'\xef\xac\x81gures for 50K-100K steps.However, full experiments (with 1M steps) exhibit similar trends:where REINFORCE was not close to converging after 50K steps, the same was true after 1M steps.'
<EOS>
b'We evaluate the peakiness of a distribution in terms of the probability of the most probable token\n(the mode), the total probability of the ten most probable tokens, and the entropy of the distribution\n(lower entropy indicates more peakiness).3Based on our NMT experiments, which we assume to be representative of the error rate of other systems.4'
<EOS>
b'Published as a conference paper at ICLR 2020(a)(b)'
<EOS>
b'(c)\n\nFigure 2:Token probabilities through REINFORCE training, in the controlled simulations in the Simulated\nReward setting.The left/center/right \xef\xac\x81gures correspond to simulations where the target token (ybest)'
<EOS>
b'wasinitially the second/third/fourth most probable token.The green line corresponds to the target token, yellow\nlines to medium-reward tokens and red lines to no-reward tokens.'
<EOS>
b'Results.The distributions become peakier in terms of all three measures: on average, the mode\xe2\x80\x99s\nprobability and the 10 most probable tokens increases, and the entropy decreases.Figure 1a presents'
<EOS>
b'the histogram of the update size, the difference in the probability of the 10 most probable tokens inthe Constant Reward setting, after a single step.Figure 1b depicts similar statistics for the mode.'
<EOS>
b'The average entropy in the pretrained model is 2.9 is reduced to 2.85 after one REINFORCE step.Simulated Reward setting shows similar trends.For example, entropy decreases from 3 to about\n0.001'
<EOS>
b'in 100K steps.This extreme decrease suggests it is effectively a deterministic policy.PKE is\nachieved in a few hundred steps, usually before other effects become prominent (see Figure 2), and\nis stronger than for Constant Reward.'
<EOS>
b'4.2NMT EXPERIMENTSWe turn to analyzing a real-world application of REINFORCE to\nNMT.'
<EOS>
b'Important differences between this and the previous simula-tions are: (1)it is rare in NMT for REINFORCE to sample from the\nsame conditional distribution more than a handful of times, given\nthe number of source sentences x and sentence pre\xef\xac\x81xes y'
<EOS>
b'<i (con-\ntexts); and (2) in NMT P\xce\xb8(\xc2\xb7|x,y<i) shares parameters between con-\ntexts, which means that updating P\xce\xb8 for one context may in\xef\xac\x82uence'
<EOS>
b'P\xce\xb8 for another.We follow the same pretraining as in \xc2\xa74.1.We then follow Yang'
<EOS>
b'et al.(2018) in de\xef\xac\x81ning the reward function based on the expected\nBLEU score.Expected BLEU is computed by sampling suf\xef\xac\x81xes for\nthe sentence, and averaging the BLEU score of the sampled sen-\ntences against the reference.'
<EOS>
b'We use early stopping with a patience of 10 epochs, where each\nepoch consists of 5,000 sentences sampled from the WMT2015(Bojar et al., 2015)German-English training data.'
<EOS>
b'We use k = 1.We retuned the learning-rate, and positive baseline settings against\nthe development set.Other hyper-parameters were an exact repli-\ncation of the experiments reported in (Yang et al., 2018).'
<EOS>
b'Figure 3:The cumulative distri-\nbution of the probability of the\nmost likely token in the NMT ex-\nperiments.The green distribu-\ntion corresponds to the pretrained\nmodel, and the blue corresponds\nto the reinforced model.'
<EOS>
b'The y-axis is the proportion of condi-\ntional probabilities with a mode\nof value \xe2\x89\xa4x (the x-axis).'
<EOS>
b'Note\nthat a lower cumulative percent-age means a more peaked output\ndistribution.A lower cumulative\npercentage means a more peaked\noutput distribution.'
<EOS>
b'Results.Results indicate an increase in the peakiness of the conditional distributions.Our results\nare based on a sample of 1,000 contexts from the pretrained model, and another (independent)\nsample from the reinforced model.'
<EOS>
b'The modes of the conditional distributions tend to increase.Figure 3 presents the distribution of the\nmodes\xe2\x80\x99 probability in the reinforced conditional distributions compared with the pretrained model,\nshowing a shift of probability mass towards higher probabilities for the mode, following RL.Another\nindication of the increased peakiness is the decrease in the average entropy of P\xce\xb8, which was reduced\nfrom 3.45 in the pretrained model to an average of 2.82 following RL.'
<EOS>
b'This more modest reduction\nin entropy (compared to \xc2\xa74.1) might also suggest that the procedure did not converge to the optimal\n\n5Published as a conference paper at ICLR 2020\n\nvalue for \xce\xb8, as then we would have expected the entropy to substantially drop if not to 0(over\xef\xac\x81t),\nthen to the average entropy of valid next tokens (given the source and a pre\xef\xac\x81x of the sentence).'
<EOS>
b'5 PERFORMANCE FOLLOWING REINFORCEWe now turn to assessing under what conditions it is likely that REINFORCE will lead to an improve-\nment in the performance of an NMT system.As in the previous section, we use both controlled\nsimulations and NMT experiments.'
<EOS>
b'5.1 CONTROLLED SIMULATIONSWe use the same model and experimental setup described in Section 4.1, this time only exploringthe Simulated Reward setting, as a Constant Reward is not expected to converge to any meaningful\n\xce\xb8.'
<EOS>
b'Results are averaged over 100 conditional distributions sampled from the pretrained model.Caution should be exercised when determining the learning rate\n(LR).Common LRs used in the NMT literature are of the scale of\n10\xe2\x88\x924.'
<EOS>
b'However, in our simulations, no LR smaller than 0.1 yieldedany improvement inR. We thus set the LR to be 0.1.'
<EOS>
b'We note\nthat in our simulations, a higher learning rate means faster conver-gence as our reward is noise-free: it is always highest for the best\noption.'
<EOS>
b'In practice, increasing the learning rate may deteriorate re-sults, as it may cause the system to over\xef\xac\x81t to the sampled instances.Indeed, when increasing the learning rate in our NMT experiments\n(see below) by an order of magnitude, early stopping caused the RL\nprocedure to stop without any parameter updates.'
<EOS>
b'Figure 2 shows the change in P\xce\xb8 over the \xef\xac\x81rst 50K REINFORCE\nsteps (probabilities are averaged over 100 repetitions), for a case\nwhere ybest was initially the second, third and fourth most probable.Although these are the easiest settings, and despite the high learning\nrate, it fails to make ybest the mode of the distribution within 100K\nsteps, unless ybest was initially the second most probable.In cases where ybest is initially of a lower\nrank than four, it is hard to see any increase in its probability, even after 1M steps.'
<EOS>
b'Figure 4:Cumulative percentage\nof contexts where the pretrained\nmodel ranks ybest in rank x or\nbelow and where it does not rank\nybest \xef\xac\x81rst (x = 0).In about half\nthe cases it is ranked fourth or be-'
<EOS>
b'low.5.2NMT EXPERIMENTS'
<EOS>
b'We trained an NMT system, using the same procedure as in Section 4.2, and report BLEU scores\nover the news2014 test set.After training with an expected BLEU reward, we indeed see a minor\nimprovement which is consistent between trials and pretrained models.While the pretrain BLEU\nscore is 30.31, the reinforced one is 30.73.'
<EOS>
b'Analyzing what words were in\xef\xac\x82uenced by the RL procedure, we begin by computing the cumulative\nprobability of the target token ybest to be ranked lower than a given rank according to the pretrained\nmodel.Results (Figure 4) show that in about half of the cases, ybest is not among the top three\nchoices of the pretrained model, and we thus expect it not to gain substantial probability following\nREINFORCE, according to our simulations.We next turn to compare the ranks the reinforced model assigns to the target tokens, and their\nranks according to the pretrained model.'
<EOS>
b'Figure 6 presents the difference in the probability that\nybest is ranked at a given rank following RL and the probability it is ranked there initially.Results\nindicate that indeed more target tokens are ranked \xef\xac\x81rst, and less second, but little consistent shift of\nprobability mass occurs otherwise across the ten \xef\xac\x81rst ranks.It is possible that RL has managed to\npush ybest in some cases between very low ranks (<1,000) to medium-low ranks (between 10 and\n1,000).'
<EOS>
b'However, token probabilities in these ranks are so low that it is unlikely to affect the system\noutputs in any way.This \xef\xac\x81ts well with the results of our simulations that predicted that only the\ninitially top-ranked tokens are likely to change.In an attempt to explain the improved BLEU score following RL with PKE, we repeat the NMT'
<EOS>
b'ex-periment this time using a constant reward of 1.Our results present a nearly identical improvement\nin BLEU, achieving 30.72, and a similar pattern in the change of the target tokens\xe2\x80\x99 ranks (see'
<EOS>
b'Ap-6Published as a conference paper at ICLR 2020'
<EOS>
b'Figure 5:The probability of dif-\nferent tokens following CMRT, in\nthe controlled simulations in the\nSimulated Reward setting.The\nleft/right \xef\xac\x81gures correspond to\nsimulations where the target'
<EOS>
b'to-ken (ybest) was initially the sec-ond/third most probable token.'
<EOS>
b'The green line corresponds tothe target token, yellow lines to\nmedium-reward tokens and red\nlines to tokens with r(y)= 0.'
<EOS>
b'pendix 8).Therefore, there is room to suspect that even in cases where RL yields an improvement\nin BLEU, it may partially result from reward-independent factors, such as PKE.4\n\n6 EXPERIMENTS WITH CONTRASTIVE MRTIn \xc2\xa72.2 we showed that CMRT does not, in fact, maximize R, and so\ndoes not enjoy the same theoretical guarantees as REINFORCE and\nsimilar policy gradient methods.'
<EOS>
b'However, being the RL procedure\nof choice in much recent work we repeat the simulations described\nin \xc2\xa74 and \xc2\xa75, assessing CMRT\xe2\x80\x99s performance in these conditions.We experiment with \xce\xb1 = 0.005 and k = 20, common settings inthe literature, and average over 100 trials.'
<EOS>
b'Figure 5 shows how the distributionP\xce\xb8 changes over the course\nof 50K update steps to \xce\xb8, where ybest is taken to be the second\nand third initially most probable token (Simulated Reward setting).Results are similar in trends to those obtained with REINFORCE:\nMRT succeeds in pushing ybest to be the highest ranked token if it\nwas initially second, but struggles where it was initially ranked third\nor below.'
<EOS>
b'We only observe a small PKE in MRT.This is probably\ndue to the contrastive effect, which means that tokens that were not\nsampled do not lose probability mass.All graphs we present here allow sampling the same token more\nthan once in each batch'
<EOS>
b'(i.e., S is a sample with replacements).Simulations with deduplication show similar results.7 DISCUSSION\n\nFigure 6:'
<EOS>
b'Difference between\nthe ranks of ybest in the rein-\nforced and the pretrained model.Each columnx corresponds to'
<EOS>
b'the difference in the probabil-\nity that ybest is ranked in rankx in the reinforced model, andthe same probability in the pre-\ntrained model.'
<EOS>
b'In this paper, we showed that the type of distributions used in NMT entail that promoting the target\ntoken to be the mode is likely to take a prohibitively long times for existing RL practices, except\nunder the best conditions (where the pretrained model is \xe2\x80\x9cnearly\xe2\x80\x9d correct).This leads us to concludethat observed improvements from using RL for NMT are likely due either to \xef\xac\x81ne-tuning the most\nprobable tokens in the pretrained model (an effect which may be more easily achieved using rerank-'
<EOS>
b'ing methods, and uses but little of the power of RL methods), or to effects unrelated to the signal\ncarried by the reward, such as PKE.Another contribution of this paper is in showing that CMRT\ndoes not optimize the expected reward and is thus theoretically unmotivated.A number of reasons lead us to believe that in our NMT experiments, improvements are not due to\nthe reward function, but to artefacts such as PKE.'
<EOS>
b'First, reducing a constant baseline from r, so as to\nmake the expected reward zero, disallows learning.This is surprising, as REINFORCE, generally and\nin our simulations, converges faster where the reward is centered around zero, and so the fact that\nthis procedure here disallows learning hints that other factors are in play.As PKE can be observed\neven where the reward is constant (if the expected reward is positive; see \xc2\xa74.1), this suggests PKE\n\n4We tried several other reward functions as well, all of which got BLEU scores of 30.73\xe2\x80\x9330.84.'
<EOS>
b'This\n\nimprovement is very stable across metrics, trials and pretrained models.7Published as a conference paper at ICLR 2020\n\nmay play a role here.'
<EOS>
b'Second, we observe more peakiness in the reinforced model and in such\ncases, we expect improvements in BLEU (Caccia et al., 2018).Third, we achieve similar results\nwith a constant reward in our NMT experiments (\xc2\xa75.2).Fourth, our controlled simulations show'
<EOS>
b'that asymptotic convergence is not reached in any but the easiest conditions (\xc2\xa75.1).Our analysis further suggests that gradient clipping, sometimes used in NMT (Zhang et al., 2016;\nWieting et al., 2019), is expected to hinder convergence further.It should be avoided when using\nREINFORCE as it violates REINFORCE\xe2\x80\x99s assumptions.'
<EOS>
b'The per-token sampling as done in our experiments is more exploratory than beam search(Wu et al.,\n2018), reducing PKE.Furthermore, the latter does not sample from the behavior policy, but does not\nproperly account for being off-policy in the parameter updates.'
<EOS>
b'Adding the reference to the sample S, which some implementations allow (Sennrich et al., 2017)\nmay help reduce the problems of never sampling the target tokens.However, as Edunov et al.(2018) point out, this practice may lower results, as it may destabilize training by leading the model\nto improve over outputs it cannot generalize over, as they are very different from anything the model'
<EOS>
b'assigns a high probability to, at the cost of other outputs.8 CONCLUSIONThe standard MT scenario poses several uncommon challenges for RL.'
<EOS>
b'First, the action space in\nMT problems is a high-dimensional discrete space (generally in the size of the vocabulary of the\ntarget language or the product thereof for sentences).This contrasts with the more common sce-\nnario studied by contemporary RL methods, which focuses mostly on much smaller discrete action\nspaces (e.g., video games (Mnih et al., 2015; 2016)), or continuous action spaces of relatively low\ndimensions (e.g., simulation of robotic control tasks (Lillicrap et al., 2015)).Second, reward for MT\nis naturally very sparse \xe2\x80\x93 almost all possible sentences are \xe2\x80\x9cwrong\xe2\x80\x9d (hence, not rewarding) in a given\ncontext.'
<EOS>
b'Finally, it is common in MT to use RL for tuning a pretrained model.Using a pretrained\nmodel ameliorates the last problem.But then, these pretrained models are in general quite peaky,\nand because training is done on-policy \xe2\x80\x93 that is, actions are being sampled from the same model\nbeing optimized \xe2\x80\x93 exploration is inherently limited.'
<EOS>
b'Here we argued that, taken together, these challenges result in signi\xef\xac\x81cant weaknesses for current RL\npractices for NMT, that may ultimately prevent them from being truly useful.At least some of these\nchallenges have been widely studied in the RL literature, with numerous techniques developed to\naddress them, but were not yet adopted in NLP.We turn to discuss some of them.'
<EOS>
b'Off-policy methods, in which observations are sampled from a different policy than the one being\ncurrently optimized, are prominent in RL (Watkins & Dayan, 1992; Sutton & Barto, 1998), and\nwere also studied in the context of policy gradient methods(Degris et al., 2012; Silver et al., 2014).In principle, such methods allow learning from a more \xe2\x80\x9cexploratory\xe2\x80\x9d policy.'
<EOS>
b'Moreover, a key mo-\ntivation for using \xce\xb1 in CMRT is smoothing; off-policy sampling allows smoothing while keeping\nconvergence guarantees.In its basic form, exploration in REINFORCE relies on stochasticity in the action-selection (in MT,this is due to sampling).'
<EOS>
b'More sophisticated exploration methods have been extensively studied,\nfor example using measures for the exploratory usefulness of states or actions(Fox et al., 2018), orrelying on parameter-space noise rather than action-space noise (Plappert et al., 2017).'
<EOS>
b'For MT, an additional challenge is that even effective exploration (sampling diverse sets of obser-\nvations), may not be enough, since the state-action space is too large to be effectively covered, with\nalmost all sentences being not rewarding.Recently, diversity-based and multi-goal methods for RL\nwere proposed to tackle similar challenges (Andrychowicz et al., 2017; Ghosh et al., 2018; Eysen-\nbach et al., 2019).We believe the adoption of such methods is a promising path forward for the\napplication of RL in NLP.'
<EOS>
b'8Published as a conference paper at ICLR 20209 ACKNOWLEDGMENTS'
<EOS>
b'This work was supported by the Israel Science Foundation(grant no. 929/17) and by the HUJI\nCyber Security Research Center in conjunction with the Israel National Cyber Bureau in the Prime\nMinister\xe2\x80\x99s Of\xef\xac\x81ce.REFERENCES'
<EOS>
b'Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob\nMcGrew, Josh Tobin, OpenAI Pieter Abbeel, and Wojciech Zaremba.Hindsight experiencere-'
<EOS>
b'play.In Advances in Neural Information Processing Systems, pp.5048\xe2\x80\x935058, 2017.'
<EOS>
b'Shiqi Shen Ayana, Zhiyuan Liu, and Maosong Sun.Neural headline generation with minimum risk\n\ntraining.arXiv preprint arXiv:1604.01904, 2016.'
<EOS>
b'Ondrej Bojar, Rajen Chatterjee, Christian Federmann, Barry Haddow, Matthias Huck, Chris\nHokamp, Philipp Koehn, Varvara Logacheva, Christof Monz, Matteo Negri, Matt Post, Carolina\nScarton, Lucia Specia, and Marco Turchi.Findings of the 2015 workshop on statistical machine\ntranslation.In WMT@EMNLP, 2015.'
<EOS>
b'Massimo Caccia, Lucas Caccia, William Fedus, Hugo Larochelle, Joelle Pineau, and Laurent Char-lin.Language gans falling short.'
<EOS>
b'arXiv preprint arXiv:1811.02549, 2018.URL https://arxiv.org/pdf/1811.02549.pdf.'
<EOS>
b'Thomas Degris, Martha White, and Richard S Sutton.Off-policy actor-critic.arXiv preprint'
<EOS>
b'arXiv:1205.4839, 2012.Sergey Edunov, Myle Ott, Michael Auli, David Grangier, and Marc\xe2\x80\x99Aurelio Ranzato.Classical'
<EOS>
b'structured prediction losses for sequence to sequence learning.In Proceedings of the 2018 Con-\nference of the North American Chapter of the Association for Computational Linguistics: Human\nLanguage Technologies, Volume 1(Long Papers), pp. 355\xe2\x80\x93364.'
<EOS>
b'Association for Computational\nLinguistics, 2018.doi: 10.18653/v1/N18-1033.URL'
<EOS>
b'http://aclweb.org/anthology/N18-1033.Benjamin Eysenbach, Abhishek Gupta, Julian Ibarz, and Sergey Levine.'
<EOS>
b'Diversity is all you need:\nLearning skills without a reward function.In International Conference on Learning Representa-\ntions, 2019.URL https://openreview.net/forum?id=SJx63jRqFm.'
<EOS>
b'Lior Fox, Leshem Choshen, and Yonatan Loewenstein.Dora the explorer:Directed outreaching\n\nreinforcement action-selection.'
<EOS>
b'ICLR, abs/1804.04012, 2018.Dibya Ghosh, Avi Singh, Aravind Rajeswaran, Vikash Kumar, and Sergey Levine.Divide-and-\nconquer reinforcement learning.'
<EOS>
b'In International Conference on Learning Representations, 2018.URLhttps://openreview.net/forum?id=rJwelMbR-.'
<EOS>
b'Lisa Anne Hendricks, Zeynep Akata, Marcus Rohrbach, Jeff Donahue, Bernt Schiele, and TrevorDarrell.Generating visual explanations.'
<EOS>
b'In ECCV, 2016.Diederik P. Kingma and Jimmy Ba.Adam:'
<EOS>
b'A method for stochastic optimization.CoRR,\n\nabs/1412.6980, 2015.Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola'
<EOS>
b'Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar,\nAlexandra Constantin, and Evan Herbst.Moses: Open source toolkit for statistical machineIn Proceedings of the 45th Annual Meeting of the Association for Computational\ntranslation.'
<EOS>
b'Linguistics Companion Volume Proceedings of the Demo and Poster Sessions, pp.177\xe2\x80\x93180, 2007.Jiwei Li, Will Monroe, Tianlin Shi, S\xc3\xa9bastien Jean, Alan Ritter, and Dan Jurafsky.'
<EOS>
b'Adversarial learn-ing for neural dialogue generation.In Proceedings of the 2017 Conference on Empirical Methods\nin Natural Language Processing, pp. 2157\xe2\x80\x932169, Copenhagen, Denmark, September 2017.'
<EOS>
b'As-sociation for Computational Linguistics.URL https://www.aclweb.org/anthology/'
<EOS>
b'D17-1230.9Published as a conference paper at ICLR 2020'
<EOS>
b'Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,\nDavid Silver, and Daan Wierstra.Continuous control with deep reinforcement learning.arXiv\npreprint'
<EOS>
b'arXiv:1509.02971, 2015.Siqi Liu, Zhenhai Zhu, Ning Ye, Sergio Guadarrama, and Kevin Murphy.Optimization of image'
<EOS>
b'description metrics using policy gradient methods.CoRR, abs/1612.00370, 2, 2016.Peter Makarov and Simon Clematide.'
<EOS>
b'Neural transition-based string transduction for limited-resource setting in morphology.In COLING, 2018.'
<EOS>
b'Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Belle-mare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al.Human-level\ncontrol through deep reinforcement learning.'
<EOS>
b'Nature, 518(7540):529\xe2\x80\x93533, 2015.Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim\nHarley, David Silver, and Koray Kavukcuoglu.Asynchronous methods for deep reinforcement\nlearning.'
<EOS>
b'In International conference on machine learning, pp. 1928\xe2\x80\x931937, 2016.Graham Neubig.Lexicons and minimum risk training for neural machine translation: Naist-cmu at\n\nwat2016.'
<EOS>
b'In WAT@COLING, 2016.Graham Neubig, Matthias Sperber, Xinyi Wang, Matthieu Felix, Austin Matthews, Sarguna Pad-\nmanabhan, Ye Qi, Devendra Singh Sachan, Philip Arthur, Pierre Godard, John Hewitt, Rachid\nRiad, and Liming Wang.Xnmt:'
<EOS>
b'The extensible neural machine translation toolkit.In AMTA,\n2018.Franz Josef Och.'
<EOS>
b'Minimum error rate training in statistical machine translation.In Proceedings of\nthe 41st Annual Meeting on Association for Computational Linguistics-Volume 1, pp.160\xe2\x80\x93167.'
<EOS>
b'Association for Computational Linguistics, 2003.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.Bleu: a method for automatic\nevaluation of machine translation.'
<EOS>
b'In Proceedings of the 40th annual meeting on association for\ncomputational linguistics, pp. 311\xe2\x80\x93318.Association for Computational Linguistics, 2002.URL\nhttps://www.aclweb.org/anthology/P02-1040.pdf.'
<EOS>
b'Matthias Plappert, Rein Houthooft, Prafulla Dhariwal, Szymon Sidor, Richard Y Chen, Xi Chen,Tamim Asfour, Pieter Abbeel, and Marcin Andrychowicz.Parameter space noise for exploration.'
<EOS>
b'arXiv preprint arXiv:1706.01905, 2017.O. Press, A. Bar, B. Bogin, J. Berant, and L. Wolf.Language generation with recurrent generative'
<EOS>
b'In Fist Workshop on Learning to Generate Naturaladversarial networks without pre-training.Language@ICML, 2017.'
<EOS>
b'Marc\xe2\x80\x99Aurelio Ranzato, Sumit Chopra, Michael Auli, and Wojciech Zaremba.Sequence level train-ing with recurrent neural networks.'
<EOS>
b'arXiv preprint arXiv:1511.06732, 2015.Steven J Rennie, Etienne Marcheret, Youssef Mroueh, Jerret Ross, and Vaibhava Goel.Self-critical\nsequence training for image captioning.'
<EOS>
b'In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pp. 7008\xe2\x80\x937024, 2017.Keisuke Sakaguchi, Matt Post, and Benjamin Van Durme.Grammatical error correction with neural\n\nreinforcement learning.'
<EOS>
b'arXiv preprint arXiv:1707.00299, 2017.Philip Schulz, Wilker Aziz, and Trevor Cohn.A stochastic decoder for neural machine translation.'
<EOS>
b'In ACL, 2018.Rico Sennrich, Barry Haddow, and Alexandra Birch.Neural machine translation of rare words with\nsubword units.'
<EOS>
b'In Proceedings of the 54th Annual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers), volume 1, pp. 1715\xe2\x80\x931725, 2016.URL http://www.'
<EOS>
b'aclweb.org/anthology/P16-1162.Rico Sennrich, Orhan Firat, Kyunghyun Cho, Alexandra Birch, Barry Haddow, Julian Hitschler,Marcin Junczys-Dowmunt, Samuel L\xc3\xa4ubli, Antonio Valerio Miceli Barone, Jozef Mokry, and\nMaria Nadejde.'
<EOS>
b'Nematus: a toolkit for neural machine translation.In EACL, 2017.10'
<EOS>
b'Published as a conference paper at ICLR 2020Shiqi Shen, Yong Cheng, Zhongjun He,Wei He, Hua Wu, Maosong Sun, and Yang Liu.'
<EOS>
b'Minimum\nrisk training for neural machine translation.In Proceedings of the 54th Annual Meeting of the\nAssociation for Computational Linguistics(Volume 1: Long Papers),'
<EOS>
b'pp.1683\xe2\x80\x931692.Association\nfor Computational Linguistics, 2016.'
<EOS>
b'doi: 10.18653/v1/P16-1159.URL http://aclweb.org/anthology/P16-1159.'
<EOS>
b'Shiqi Shen, Yang Liu, and Maosong Sun.Optimizing non-decomposable evaluation metrics for\n\nneural machine translation.Journal of Computer Science and Technology, 32:796\xe2\x80\x93804, 2017.'
<EOS>
b'Rakshith Shetty, Marcus Rohrbach, Lisa Anne Hendricks, Mario Fritz, and Bernt Schiele.Speakingthe same language: Matching machine to human captions by adversarial training.'
<EOS>
b'In 2017 IEEEInternational Conference on Computer Vision (ICCV), pp.4155\xe2\x80\x934164. IEEE, 2017.'
<EOS>
b'David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller.Deterministic policy gradient algorithms.In ICML, 2014.'
<EOS>
b'Richard S Sutton and Andrew G Barto.Reinforcement learning:An introduction.'
<EOS>
b'MIT press, 1998.G. Tevet, G. Habib, V. Shwartz, and J. Berant.Evaluating text GANs as language models.'
<EOS>
b'arXivpreprint arXiv:1810.12686, 2018.Tijmen Tieleman and Geoffrey Hinton.'
<EOS>
b'Lecture 6.5-rmsprop:Divide the gradient by a running\naverage of its recent magnitude.COURSERA:'
<EOS>
b'Neural networks for machine learning, 4(2):26\xe2\x80\x93\n31, 2012.Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n\xc5\x81ukasz Kaiser, and Illia Polosukhin.Attention is all you need.'
<EOS>
b'In Advances in Neural Informa-\ntion Processing Systems, pp. 5998\xe2\x80\x936008, 2017.URL https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf.'
<EOS>
b'Christopher JCH Watkins and Peter Dayan.Q-learning.Machine learning, 8(3-4):279\xe2\x80\x93292, 1992.'
<EOS>
b'John Wieting, Taylor Berg-Kirkpatrick, Kevin Gimpel, and Graham Neubig.Beyond BLEU: Train-In The 57th Annual Meeting of the\ning neural machine translation with semantic similarity.'
<EOS>
b'Association for Computational Linguistics (ACL), Florence, Italy, July 2019.URL https://arxiv.org'
<EOS>
b'/abs/1909.06694.Ronald J Williams.Simple statistical gradient-following algorithms for connectionist reinforcement\n\nlearning.'
<EOS>
b'Machine learning, 8(3-4):229\xe2\x80\x93256, 1992.Lijun Wu, Yingce Xia, Li Zhao, Fei Tian, Tao Qin, Jianhuang Lai, and Tie-Yan Liu.Adversarial\n\nneural machine translation.'
<EOS>
b'arXiv preprint arXiv:1704.06933, 2017.Lijun Wu, Fei Tian, Tao Qin, Jianhuang Lai, and Tie-Yan Liu.A study of reinforcement learning\n\nfor neural machine translation.'
<EOS>
b'In EMNLP, 2018.Zhen Yang, Wei Chen, Feng Wang, and Bo Xu.Improving neural machine translation with condi-'
<EOS>
b'tional sequence generative adversarial nets.In Proceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computational Linguistics: Human Language Technolo-\ngies, Volume 1 (Long Papers), pp.1346\xe2\x80\x931355.'
<EOS>
b'Association for Computational Linguistics, 2018.doi: 10.18653/v1/N18-1122.URL http://aclweb.org/anthology/N18-1122.'
<EOS>
b'Lantao Yu, Weinan Zhang, Jun Wang, and Yong Yu.Seqgan:Sequence generative adversarial nets\n\nwith policy gradient.'
<EOS>
b'In AAAI, pp. 2852\xe2\x80\x932858, 2017.Jiac heng Zhang, Yanzhuo Ding, Shiqi Shen, Yong Cheng, Maosong Sun, Huanbo Luan, and\nYang Liu.Thumt:'
<EOS>
b'An open source toolkit for neural machine translation.arXiv preprintarXiv:1706.06415, 2017.'
<EOS>
b'Yizhe Zhang, Zhe Gan, and Lawrence Carin.Generating text via adversarial training.In NIPS\n\nworkshop on Adversarial Training, volume 21, 2016.'
<EOS>
b'11Published as a conference paper at ICLR 2020A APPENDIX'
<EOS>
b'A.1 CONTRASTIVE MRT DOES NOT MAXIMIZE THE EXPECTED REWARDWe hereby detail a simple example where following the Contrastive MRT method (see \xc2\xa72.2) does\nnot converge to the parameter value that maximizes R.Let \xce\xb8 be a real number in [0, 0.5], and let P\xce\xb8 be a family of distributions over three values a, b, c'
<EOS>
b'suchthat:\n\nLet r(a) =1, r(b) = 0, r(c)'
<EOS>
b'= 0.5.The expected reward as a function of \xce\xb8 is:P\xce\xb8 (x) =\n\n\xef\xa3\xb1'
<EOS>
b'\xef\xa3\xb2\xef\xa3\xb3x ='
<EOS>
b'a\n\xce\xb82\xce\xb82x'
<EOS>
b'= b\n1 \xe2\x88\x92 \xce\xb8 \xe2\x88\x92 2\xce\xb82 x= cR(\xce\xb8)'
<EOS>
b'=\xce\xb8 + 0.5(1\xe2\x88\x92 \xce\xb8'
<EOS>
b'\xe2\x88\x92 2\xce\xb82)R(\xce\xb8) is uniquely maximized by \xce\xb8\xe2\x88\x97 = 0.25.Table 1 details the possible samples of size k = 2, their probabilities, the corresponding (cid:101)R and\nits gradient.'
<EOS>
b'Standard numerical methods show that E[\xe2\x88\x87 (cid:101)R] over possible samplesS is positive for\n\xce\xb8 \xe2\x88\x88 (0, \xce\xb3) and negative for \xce\xb8 \xe2\x88\x88 (\xce\xb3, 0.5], where \xce\xb3 \xe2\x89\x88 0.295.'
<EOS>
b'This means that for any initialization of\n\xce\xb8 \xe2\x88\x88 (0, 0.5], Contrastive MRT will converge to \xce\xb3 if the learning rate is suf\xef\xac\x81ciently small.For \xce\xb8 = 0,\n(cid:101)R \xe2\x89\xa1 0.5, and there will be no gradient updates, so the method will converge to \xce\xb8 = 0.Neither of'
<EOS>
b'these values maximizes R(\xce\xb8).We note that by using some g (\xce\xb8)the \xce\xb3 could be arbitrarily far from \xce\xb8\xe2\x88\x97. g'
<EOS>
b'could also map to\n(\xe2\x88\x92inf, inf ) more often used in neural networks parameters.We further note that resorting to maximizing E[ (cid:101)R]instead, does not maximize R(\xce\xb8) either.'
<EOS>
b'Indeed,\nplotting E[ (cid:101)R]as a function of \xce\xb8 for this example, yields a maximum at \xce\xb8 \xe2\x89\x88 0.32.Table 1:'
<EOS>
b'The gradients of (cid:101)R for each possible sample S.The batch size is k = 2.Rows correspond to different'
<EOS>
b'sampled outcomes.\xe2\x88\x87(cid:101)R is the gradient of (cid:101)R given the corresponding value for S.'
<EOS>
b'S\n\n{a, b}\n\n{a, c}\n\n{b, c}a, a\nb, b\nc, c'
<EOS>
b'P (S)4\xce\xb83\xce\xb82'
<EOS>
b'4\xce\xb84(1-\xce\xb8-2\xce\xb82)22\xce\xb8(1-\xce\xb8-2\xce\xb82)'
<EOS>
b'4\xce\xb82(1-\xce\xb8-2\xce\xb82)0.5+\n\n\xce\xb8'
<EOS>
b'2\xe2\x88\x924\xce\xb821\xe2\x88\x92\xce\xb8\xe2\x88\x922\xce\xb822\xe2\x88\x922\xce\xb8'
<EOS>
b'(cid:101)R11+'
<EOS>
b'2\xce\xb8\n\n100.5\n\n\xe2\x88\x87 (cid:101)R\n\xe2\x88\x922'
<EOS>
b'(1+2\xce\xb8)22x2+1\n\n2(1\xe2\x88\x922\xce\xb82)2\xce\xb82\xe2\x88\x922\xce\xb8\n('
<EOS>
b'1\xe2\x88\x92\xce\xb8)2\n\n000'
<EOS>
b'A.2 DERIVING THE GRADIENT OF (cid:101)RGiven S, recall the de\xef\xac\x81nition of (cid:101)R:\n\nTaking the deriviative w.r.t. \xce\xb8:'
<EOS>
b'(cid:101)R(\xce\xb8, S) =Q\xce\xb8,S(yi)r(yi)\n\nk(cid:88)'
<EOS>
b'i=112Published as a conference paper at ICLR 2020'
<EOS>
b'k(cid:88)i=1'
<EOS>
b'r(yi)\n\n\xe2\x88\x87P(y) \xc2\xb7\xce\xb1P (y)\xce\xb1\xe2\x88\x921 \xc2\xb7 Z(S) \xe2\x88\x92 \xe2\x88\x87Z(S) \xc2\xb7'
<EOS>
b'P (y)\xce\xb1\n\n=\n\nZ(S)2k(cid:88)'
<EOS>
b'i=1k(cid:88)'
<EOS>
b'i=1k(cid:88)'
<EOS>
b'(cid:16)\xce\xb1i=1'
<EOS>
b'r(yi)(cid:16)\xce\xb1\xe2\x88\x87P'
<EOS>
b'(yi)P (yi)Q(yi)'
<EOS>
b'\xe2\x88\x92\n\n\xe2\x88\x87Z(S)Z(S)(cid:17)'
<EOS>
b'Q(yi)\n\n=r(yi)Q(yi)\xce\xb1\xe2\x88\x87 log P (yi'
<EOS>
b') \xe2\x88\x92 \xe2\x88\x87 log Z(S)=(cid:16)'
<EOS>
b'(cid:17)r(yi)Q(yi)\xe2\x88\x87 log P (yi)\xe2\x88\x92 EQ[r]\xe2\x88\x87 log Z(S)'
<EOS>
b'(cid:17)(a)(b)'
<EOS>
b'(c)\n\nFigure 7:The probability of different tokens following REINFORCE, in the controlled simulations in the Con-\nstant Reward setting.The left/center/right \xef\xac\x81gures correspond to simulations where the target token (ybest)'
<EOS>
b'wasinitially the second/third/fourth most probable token.The green line corresponds to the target token, yellow\nlines to medium-reward tokens and red lines to tokens with r(y)'
<EOS>
b'= 0.A.3 NMT IMPLEMENTATION DETAILSTrue casing and tokenization were used (Koehn et al., 2007), including escaping html symbols and\n"-" that represents a compound was changed into a separate token of =.'
<EOS>
b'Some preprocessing used\nbefore us converted the latter to ##AT##-##AT## but standard tokenizers in use process that into 11\ndifferent tokens, which over-represents the signi\xef\xac\x81cance of that character when BLEU is calculated.BPE (Sennrich et al., 2016) extracted 30,715 tokens.For the MT experiments we used 6 layers in\nthe encoder and the decoder.'
<EOS>
b'The size of the embeddings was 512.Gradient clipping was used with\nsize of 5 for pre-training (see Discussion on why not to use it in training).We did not use attention\ndropout, but 0.1 residual dropout rate was used.'
<EOS>
b'In pretraining and training sentences of more than\n50 tokens were discarded.Pretraining and training were considered \xef\xac\x81nished when BLEU did not\nincrease in the development set for 10 consecutive evaluations, and evaluation was done every 1,000\nand 5,000 for batches of size 100 and 256 for pretraining and training respectively.Learning rate\nused for rmsprop (Tieleman & Hinton, 2012) was 0.01 in pretraining and for adam (Kingma & Ba,\n2015) with decay was 0.005 for training.'
<EOS>
b'4,000 learning rate warm up steps were used.Pretraining\ntook about 7 days with 4 GPUs, afterwards, training took roughly the same time.Monte Carlo used\n20 sentence rolls per word.'
<EOS>
b'A.4 DETAILED RESULTS FOR CONSTANT REWARD SETTINGWe present graphs for the constant reward setting in Figures 8 and 7.Trends are similar to the ones\nobtained for the Simulated Reward setting.'
<EOS>
b'13Published as a conference paper at ICLR 2020Figure 8:'
<EOS>
b'Difference between the ranks of ybest in the reinforced with constant reward and the pretrained model.Each column x corresponds to the difference in the probability that ybest is ranked in rank x in the reinforced\nmodel, and the same probability in the pretrained model.14'
<EOS>
