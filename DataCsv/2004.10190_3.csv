Efficient Adaptation for End-to-End Vision-Based  Robotic Manipulation  Ryan Julian†‡, Benjamin Swanson†, Gaurav S. Sukhatme‡, Sergey Levine†§, Chelsea Finn†¶ and Karol Hausman†  †Google Research, Robotics at Google Team  ‡Department of Computer Science, University of Southern California  §Department of Electrical Engineering and Computer Sciences, University of California, Berkeley  ¶Department of Computer Science, Stanford University  0 2 0 2    r p A 1 2         ]  G Ls c [      1 v 0 9 1 0 1  4 0 0 2 : v i X r a  Fig 1: Original robot configuration used for pre-training (left), and adaptation challenges (highlighted in pink) studied in this work (right) with associated performance improvements (top) obtained using our fine-tuning method. Abstract—One of the great promises of robot learning sys- tems is that they will be able to learn from their mistakes and continuously adapt to ever-changing environments.Despite this potential, most of the robot learning systems today are deployed as a fixed policy and they are not being adapted after their deployment.
<EOS>
Can we efficiently adapt previously learned behaviors to new environments, objects and percepts in the real world?In this paper, we present a method and empirical evidence towards a robot learning framework that facilitates continuous adaption.In particular, we demonstrate how to adapt vision-based robotic manipulation policies to new variations by fine-tuning via off-policy reinforcement learning, including changes in background, object shape and appearance, lighting conditions, and robot morphology.
<EOS>
Further, this adaptation uses less than 0.2% of the data necessary to learn the task from scratch.We find that the simple approach of fine-tuning pre- trained policies leads to substantial performance gains over the course of fine-tuning, and that pre-training via RL is essential: training from scratch or adapting from supervised ImageNet features are both unsuccessful with such small amounts of data.We also find that these positive results hold in a limited continual learning setting, in which we repeatedly fine-tune a single lineage of policies using data from a succession of new tasks.
<EOS>
Our empirical conclusions are consistently supported by experiments on simulated manipulation tasks, and by 52 unique fine-tuning experiments on a real robotic grasping system pre- trained on 580,000 grasps.For video results and an overview of the methods and experiments in this study, see the project website at https://ryanjulian.me/continual-fine-tuning. I. INTRODUCTION  The ability to constantly learn, adapt, and evolve is arguably one of the most important properties of an intelligent agent prepared to exist in the real world.
<EOS>
Similarly, our robots should be able to continuously learn and adapt throughout their lifetime to the ever-changing environments that they are deployed in.This is a widely recognized requirement.In fact, there is an entire academic sub-field of lifelong learning [65] that is interested in the problem of agents that never stop learning.
<EOS>
Despite the wide interest in this ability, most of the intelligent agents deployed today are not tested for their adaptation capabilities.Even though techniques such as reinforcement learning theoretically provide the ability to perpetually learn from trial and error, this is not how they are typically evaluated.Instead, the predominant method of acquiring a new task with reinforcement learning is to initialize a policy from scratch, collect entirely new data in a stationary environment, and evaluate a static policy that was trained with this data.
<EOS>
 This static paradigm does not evaluate the robot’s capability to adapt.It also traps robotic reinforcement learning in the worst-case regime for sample efficiency: the cost to acquire a new task is dominated by sample efficiency of the learning algorithm and the complexity of the task, as reﬂected in cost of  32% → 63%49% → 66%50% → 90%75% → 93%43% → 98%https://colordesigner.io/convert/hextohsv (HSV)86%acquiring diverse task data starting from na¨ıve (e.g. random) exploration. Most machine learning models successfully deployed in the real world, such as those used for computer vision and natural language processing (NLP) do not live in this regime.
<EOS>
For instance, the predominant method of acquiring a new computer vision task is to start learning the new task with a pre-trained model for a related task, acquired from a pre-collected data set, and fine-tune that model to achieve the new task [11, 24, 10].This changes the sample efficiency regime of the learning process from one which is dominated by task complexity to one that is dominated by task novelty, i.e. the difference between the new task and the task on which the model was pre-trained.While a number of works have studied how to use pre-trained ImageNet [9] features for robotics [68, 25, 34], there are remarkably few works that study how to adapt motor skills themselves.
<EOS>
Our work attempts to bridge this gap. We adapt an image-based grasping policy to changes in background, object shape and appearance, lighting conditions, and robot morphology and kinematics, while using less than 0.2% of the data necessary to learn the same task from scratch (see Fig 1).Our results, supported by simulation and extensive real-world experiments, indicate that a pre-adaptation policy acquired for a task using reinforcement learning can be used to acquire policies for nearby tasks using very little new data and a simple update procedure.
<EOS>
Furthermore, we find that this approach of adapting pre-trained policies with off-policy reinforcement learning (RL) leads to substantial improvements over the course of fine-tuning, and that pre-training via RL is essential: it significantly outperforms conventional pre-training techniques using supervised learning on task-agnostic datasets.We believe this simple adaptation scheme provides a promis- ing solution for creating a lifelong learning robotic agent, and show this potential using a simple continual learning experiment. The main contributions of this work are (1) a careful real-world study of the problem of end-to-end skill adap- tation for a continuously-learning robot, and (2) evidence that a very simple fine-tuning method can achieve that adaptation.
<EOS>
 Instead of focusing on the robot’s performance in the environment in which it was trained, we purposefully modify the robot and its environment, characteristic of the persistent change of the real world, and investigate its ability to adapt.Likewise, rather than proposing a new adaptation algorithm, with new complexity and caveats, we show how to successfully adapt robotic policies to substantial changes, using only the most basic components of existing off-policy reinforcement learning algorithms.To our knowledge, this work is the first to demonstrate that simple fine-tuning of off-policy reinforcement learning can successfully adapt to substan- tial task, robot, and environment variations which were not present in the original training distribution (i.e. off- distribution).
<EOS>
 II.RELATED WORK  Reinforcement  learning is a long-standing approach for enabling robots to autonomously acquire skills [32] such as locomotion [33, 64], pushing objects [37, 12], ball-in-cup manipulation [31], peg insertion [16, 36, 59, 35, 71], throwing objects [15, 72], and grasping [49, 29].We particularly focus on the problem of deep reinforcement learning from raw pixel observations [36], as it allows us to place little restrictions on state representation.
<EOS>
A number of works have also considered this problem setting [13, 15, 12, 71, 1, 40].However, a key challenge with deep RL methods is that they typically learn each skill from scratch, disregarding previously-learned skills.If we hope for robots to generalize to a broad range of real world environments, this approach is not practical.
<EOS>
 We instead consider how we might  transfer knowledge for efficient learning in new conditions [63, 46, 61], a widely-studied problem particularly outside of the robotics domain [11, 24, 10, 7, 51].Prior works in robotics have considered how we might transfer information from models trained with supervised learning on ImageNet [9] by fine- tuning [36, 13, 17, 49] or other means [60, 20].Our exper- iments show that transfer from pre-trained conditions is sig- nificantly more successful than transfer from ImageNet.
<EOS>
Other works have leveraged experience in simulation [56, 66, 57, 62, 45, 55, 48, 22, 19] or representations learned with auxiliary losses [53, 39, 58] for effective transfer.While successful, these approaches either require significant engineering effort to construct an appropriate simulation or significant supervision.Most relevantly, recent work in model-based RL has used predictive models for fast transfer to new experimental set- ups [4, 18], i.e. by fine-tuning predictive models [8], via online search of a pre-learned representation of the space models, policies, or high-level skills [5, 6, 30, 38], or by learning physics simulation parameters from real data [52, 28].
<EOS>
We show how fine-tuning is successful with a model-free RL approach, and show how a state-of-the-art grasping system can be adapted to new conditions. Other works have aimed to share and transfer knowledge across tasks and conditions by simultaneously learning across multiple goals and tasks [54].For example, prior works in model-based RL [12, 67, 43] and in goal-conditioned RL [1, 44, 47, 50, 70] have shared data and representations across multiple goals and objects.
<EOS>
Along a similar vein, prior work in robotic meta-learning has aimed to learn representations that can be quickly adapted to new dynamics [41, 2, 42] and objects [14, 27, 69, 3].We consider adaptation to a broad class of changes including dynamics, object classes, and visual observations, including conditions that shift substantially from the training conditions, and do not require the full set of conditions to be represented during the initial training phase. III.
<EOS>
THE ROBUSTNESS OF LEARNED POLICIES: A CASE  STUDY  To study the problem of adaptation, we utilize a grasping policy pre-trained with RL, which we evaluate in five different conditions that were not encountered during pre-training.In  Challenge Task Checkerboard Backing Harsh Lighting Extend Gripper 1 cm Offset Gripper 10 cm Transparent Bottles  Type Background Lighting conditions Gripper shape Robot morphology Unseen objects  Base Policy 50% 31% 76% 47% 49%  ∆ -36% -55% -10% -39% -37%  TABLE I: Summary of modifications to the robot and environ- ment, and their effect on the performance of the base policy.Changing the background lighting, morphology, and objects leads to substantial degradation in performance compared to the original training conditions.
<EOS>
 this section, we will describe the pre-training process and test the robustness of the pre-trained policy to various robot and environment modifications.We choose these modifications to reﬂect changes we believe a learning robot would experience, and should be expected to a adapt to, when deployed “on the job” in the real world.In Section IV, we will describe a simple fine-tuning based adaptation process, and evaluate it using these modifications.
<EOS>
 A. Pre-training process  We pre-train the grasping policy, which we refer to as the “base policy,” using the QT-Opt algorithm in two stages, as described in [29].First, we train a Q-function network ofﬂine using data from 580,000 real grasp attempts over a corpus of 1,000 visually and physically diverse objects.Second, we continue training this network online1 over the course of 28,000 real grasp attempts on the same corpus of objects.
<EOS>
That is, we use a real robot to collect trials using the current network, update the network using these new trials, deploy the updated network to the real robot, and repeat.This procedure yields a final base policy that achieves 96% accuracy on a set of previously-unseen test objects.We use a challenging subset of six of these test objects for most experiments in this work.
<EOS>
On this set, our base model achieves a success rate of 86% on the baseline grasping task. B. Robustness of the pre-trained policy  We begin by choosing set of significant modifications to the robot and environment, which we believe are characteristic of a real-world continual learning scenario.We then evaluate the performance of the base policy on increasingly-severe versions of these modifications.
<EOS>
This process allows us to assess the limits of robustness of policies trained using the pre-training method.Once we find a modification that is sufficiently-severe to compromise the base policy’s performance in each category, we use it to define a “Challenge Task” for our study of adaptation methods. Next, we describe these challenges and the corresponding  performance of the base policy.
<EOS>
 1Following the example set by [29], we refer this procedure as “online” rather than “on-policy,” because the policy is still updated by the off-policy reinforcement learning algorithm  Fig 2: Views of from the robot camera for each of our six Challenge Tasks and the base grasping task. Background: We introduce a black-white 1 inch checker- board pattern that we glue to the bottom of the robot’s workspace (see Fig 1, fourth from left).We observe that conventional variations in the workspace surface, such as uniform changes in color or specularity, have no effect on the base policy’s performance.
<EOS>
Introducing an checkerboard pat- tern often fools the robot into grasping at checkerboard edges rather than objects.This adversarial modification compromises the base policy’s performance to 50% (-36% compared to the base task). Lighting conditions: We introduce a high-intensity halo- gen light source parallel with the workspace (see Fig 1, second from left), creating a bright spot in the robot’s cam- era view, and intense light-dark contrasts along the plane of the workspace.
<EOS>
The base policy was trained in standard indoor lighting conditions, with no exposure to natural light or significant variation.We observe that mild perturbations in lighting conditions (i.e. those which can be created by standard-intensity household lights) have no effect on the base policy’s performance.Using the very bright halogen light source has a severe impact, and degrades the base policy’s performance to 31% (-55% compared to the baseline).
<EOS>
 Gripper shape: We extend the parallel gripper attached to the robot by 1 cm and significantly narrow its width and compliance in the process (see Fig 1, fifth from left).This changes the robot’s kinematics (lengthening the gripper in the distal direction), while also lowering the relative pose of the robot with respect to the workspace surface by 1 cm.This modification compromises the base policy’s performance to 76% (-10% compared to the baseline).
<EOS>
 Robot morphology: We translate the gripper laterally by 10 cm (see Fig 1, far-right).Note that during training this policy experienced absolutely no variation in robot morphol- ogy.We observe that translating the gripper laterally by up to 5 cm has no impact on performance.
<EOS>
By translating the gripper laterally by 10 cm (approximately a full gripper or arm link width), we degrade the base policy’s performance to 47% (- 39% compared to the baseline). Unseen objects: We introduce completely-transparent plas-  Harsh LightingTransparent BottlesCheckerboard BackingExtend Gripper 1cmOﬀset Gripper 10cmBase Graspingperforms the task, e.g. through the introduction of significantly brighter lighting, or a peculiar and unexpected type of object.The robot must adapt to this change quickly in order to recover a proficient policy.
<EOS>
Handling these changes reﬂects what we expect to be a common requirement of reinforcement learning policies deployed in the real world: since an RL policy can learn from all of the experience that it has collected, there is no need to separate learning into clearly distinct training and deployment phases.Instead, it is likely desirable to allow the policy to simply continue learning “on the job” so as to adapt to these changes. A. A very simple fine-tuning method  We define a very simple fine-tuning procedure for off-policy  RL, as follows (Fig.
<EOS>
3). First, we (1) pre-train a general grasping policy, as describe in Section III-A and [29].To fine-tune a policy onto a new target task, we (2) use the pre-trained policy to collect an exploration dataset of attempts on the target task; then (3) initialize the same off-policy reinforcement learning algorithm which was used for pre-training (QT-Opt, in our case) with the parameters of the pre-trained policy, and both the target task and base task datasets2 as the data sources (e.g. replay buffers); we then (4) update the policy with this training algorithm, using a reduced learning rate, and sampling training examples with equal probability from the base and target task datasets, for some number of update steps.
<EOS>
Finally, we (5) evaluate the fine-tuned policy on the target task. Our method is ofﬂine, i.e. it uses a single dataset of target task attempts, and requires no robot interaction after initial dataset collection to compute a fine-tuned policy, which may then be deployed onto a robot. B. Evaluating ofﬂine fine-tuning for real-world grasping  We now turn our attention to how to evaluating this simple method’s effectiveness as an adaptation procedure for end-to- end robot learning, and perhaps continual learning.
<EOS>
Our goal is to determine whether the method is sample efficient, whether it works over a broad range of possible variations, and to determine whether it performs better than simpler ways of acquiring the target tasks. With this goal in mind, we conduct a large panel of ablation experiments experiments on a real 7 DoF Kuka arm.These experiments evaluate the performance of our method across the diverse range of previously-defined Challenge Tasks and a continuum of target task dataset sizes, and compare this performance to two comparison methods.
<EOS>
 The experiments are very challenging.The Transparent Bottles task in particular presents a major challenge to most grasping systems: the transparent bottles generally confuse depth-based sensors and, especially in cluttered bins, require the robot to singulate individual items and position the gripper in the right orientation for grasping.Although our base policy uses only RGB images, it is still not able to grasp the glass  2We assume this dataset was saved during training of the base policy  Fig 3: Schematic of the simple method we test in Section IV, using the conceptual framework we discuss in Appendix A. We pre-train a policy using the old data from the pre-training task, which is then adapted using the new data from the fine- tuning task.
<EOS>
 tic beverage bottles (see Fig 1, third from left) that were not present in the training set.Based on our experiments, the system is robust to a broad variety of previously-unseen objects, as long as they have significant opaque components.For example, even though there are no drinking bottles in the training set, we find the system is able to pick up labeled drink bottles with 98% success rate.
<EOS>
Success rates for other novel, opaque objects are similarly consistent with the baseline performance on the test set.However, we find that introducing completely-transparent drink bottles causes the base policy to often grasp where two bottles are adjacent, i.e. as though it cannot differentiate which parts of the scene are inside vs outside a bottle.By introducing completely-transparent plastic beverage bottles, we are able to compromise the base policy’s performance to 49% (-37% compared to the baseline).
<EOS>
 See Table I for a summary of the modification experiments,  and their effect on base policy performance. IV.LARGE-SCALE EXPERIMENTAL EVALUATION  We define then evaluate a simple technique for ofﬂine fine-  tuning.
<EOS>
 Our experiments model an “on the job” adaptation scenario, where a robot is initially trained to perform a general task (in our case, grasping diverse objects), and then the conditions of the task change in a drastic and substantial way as the robot  AdaptedQ-functionQT-OptQT-Opt50%50%2.Explore ℒBaseQ-functionTargetQ-functionℒ1.Pre-Train4.
<EOS>
Adapt5.EvaluateTarget Data<800Base Data≈608,00043%Success98% Success3.Initialize86%SuccessOurs (exploration grasps)  Challenge Task  Original Policy  Checkerboard Backing Harsh Lighting Extend Gripper 1 cm Offset Gripper 10 cm Transparent Bottles Baseline Grasping Task  50% 32% 75% 43% 49% 86%  25 67% 23% 93% 73% 46% 98%  50 48% 16% 67% 50% 43% 81%  100 71% 52% 80% 60% 65% 84%  200 47% 44% 51% 56% 65% 78%  400 89% 58% 90% 91% 58% 93%  800 90% 63% 69% 98% 66% 89%  Best (∆) 90% (+40) 63% (+31) 93% (+18) 98% (+55) 66% (+17) 98% (+12)  Comparisons  Scratch  ImageNet  0% 4% 0% 37% 27% 0%  0% 2% 14% 47% 20% 12%  TABLE II: Summary of grasping success rates (N ≥ 50) for the experiments by challenge task, fine-tuning method, and number of exploration grasps.
<EOS>
The experiments “Scratch” and “ResNet 50 + ImageNet” both use 800 exploration grasps and the same update process as the other experiments.“Scratch” starts the grasping network with randomly-initialized parameters.“ResNet 50 + ImageNet” refers to training a grasping network with an equivalent architecture to the other experiments, but with its convolutional layers replaced with a ResNet 50 architecture and pre-loaded with ImageNet features; the non-CNN parts of the network (MLPs for the action inputs and the Q-value output) are randomly-initialized.
<EOS>
 bottles reliably, because they differ so much from the objects it observed during training.However, after fine-tuning with only 1 hour (100 grasp attempts) of experience, we observe that the transparent bottles can be picked up with a success rate of 66%, 20% better than the base policy.Figure 2 shows how the robot’s view changes for each challenge task.
<EOS>
Note the extreme glare and robot reﬂections visible in images from the Harsh Lighting challenge. For videos of our experimental results, see the project  website.3  a) Collect datasets: First, we collect a dataset of 800 grasp attempts for each of our 5 challenge tasks (see Table I) plus the base grasping task.We then partitioned each dataset into 6 tiers of difficulty by number of exploration grasps (25, 50, 100, 200, 400, and 800 grasp attempts), yielding 36 individual datasets.
<EOS>
 b) Train fine-tuned policies: We train a fine-tuned policy for each of these 36 datasets using the procedure described above.We execute the fine-tuning algorithm for 500,000 gradient steps (see Sec.VI for more information on how we chose this number) and use a learning rate of 10−4, which is 25% of learning rate used for pre-training.
<EOS>
This yields 36 fine-tuned policies, each trained with a different combination of target task and target dataset size.This set of 36 policies includes 6 policies fine-tuned on data from the base grasping task, for validation. c) Train comparisons: To provide points of comparison, we train two additional policies for each challenge task and the base grasping task, yielding 12 additional policies.
<EOS>
 The first comparison (“Scratch”) is a policy trained using the aforementioned fine-tuning procedure and an 800-grasp data set, but using a randomly-initialized Q-function rather than the Q-function obtained from pre-training.The purpose of this comparison is to help us assess the contribution of the pre-trained parameters to the fine-tuning process’ performance.The second comparison (“ImageNet”) is also trained using an identical fine-tuning procedure and the 800-grasp dataset, but uses a modified Q-function architecture in which we replace the convolutional trunk of the network with that of  3For video results, see https://ryanjulian.me/continual-fine-tuning  the popular ResNet50 architecture [21], initialized with the weights obtained by training the network to classify images from the ImageNet dataset [9].
<EOS>
Refer to to Fig 7 for a diagram of the unmodified architecture.We initialize the remaining fully-connected layers with random parameters, and concatenate the action input features at the end of the CNN (rather than the adding them in middle of the CNN, as in the original architecture).Note that in this comparison, the fine- tuning process still updates all parameters, including those of the ResNet50 sub-network.
<EOS>
The purpose of this comparison is to provide a comparison to a strong alternative to end-to-end RL for obtaining pre-training parameters. d) Evaluate performance: Finally, we evaluate all 48 policies on their target task by deploying them to the robot and executing 50 or more grasp attempts to calculate the policy’s final performance.To reduce the variance of our evaluation statistics, we shufﬂe the contents of the bin between each trial by executing a randomly-generated sequence of sweeping movements with the end-effector.
<EOS>
 The full experiment required more than 15,000 grasp at- tempts and 14 days of real robot time, and was conducted over approximately one month. We present a full summary of our results in Table II.Across the board, we observe substantial benefits arising from fine- tuning, suggesting that the robot can indeed adapt to drastically new condition with a modest amount of data: our most data- intensive experiment uses just 0.2% of the data used train the base grasping policy to similar performance.
<EOS>
Our method consistently outperforms both the “ImageNet” and “Scratch” comparison methods.We provide more detailed analysis of this experiment in the next section. The experiments are very challenging.
<EOS>
For example, the “Transparent Bottles” task presents a major challenge to most grasping systems: the transparent bottles generally confuse depth-based sensors and, especially in cluttered bins, require the robot to singulate individual items and position the gripper in the right orientation for grasping.Although our base policy uses only RGB images, is still not able to grasp the transparent bottles reliably, because they differ so much from the objects it observed during training.However, after fine- tuning with only 1 hour (100 grasp attempts) of experience,  it   Fig 4: Flow chart of the continual learning experiment, in which we fine-tune on a sequence of conditions.
<EOS>
Every transition to a new scenario happens after 800 grasps. we observe that the transparent bottles can be picked up with a success rate of 66%, 20% better than the base policy.Similarly, the “Checkerboard Backing” challenge task asks the robot to differentiate edges associated with real objects from edges on an adversarial checkerboard pattern.
<EOS>
It never needed this capability to succeed during pre-training, where the background is always featureless and grey, and all edges can be assumed to be associated with a graspable object.After 1 hour (100 grasp attempts) of experience, using our method the robot can grasp objects on the checkerboard background with a 71% success rate, 21% better than the base policy, and this success rate reaches 90% after 8 hours of experience (800 grasp attempts). V. EVALUATING OFFLINE FINE-TUNING FOR CONTINUAL  LEARNING  Now that we have defined and evaluated a simple method for ofﬂine fine-tuning, we evaluate its suitability for use in continual learning, which could allow us to achieve the goal of an robot which adapts to ever-changing environments and tasks.
<EOS>
To do so, we define a simple continual learning challenge as follows (Fig.4). As in the fine-tuning experiments, we begin with a base policy pre-trained for general object grasping.
<EOS>
Likewise, we also use our fine-tuning method to adapt the base policy to a target task, in this case “Harsh Lighting.” Not content to stop there, we use this adapted policy—not the base policy— as the initialization for another iteration of our fine-tuning algorithm, this time targeting “Transparent Bottles.” We repeat this process until we have run out of new tasks, ending at the task “Offset Gripper 10cm,” at which point we evaluate the policy on the last task. We perform this experiment using 800 exploration-grasp  Challenge Task  Continual Learning  ∆  Base  Single  -  +8% −4% −5% −7%  63% 74% 86% 88% 91%  +32% +25% +36% +12% +44%  Harsh Lighting Transparent Bottles Checkerboard Backing Extend Gripper 1 cm Offset Gripper 10 cm TABLE III: Summary of grasping success rates (N ≥ 50) for the continual learning experiment by challenge task, and comparison to single-step fine-tuning.“Base” refers to the baseline grasping policy before fine-tuning, and “Single” refers to the best performance from the single-step fine-tuning exper- iment in Table II.
<EOS>
Note that because it is the first step of the continual learning experiment, the policy for “Harsh Lighting” is identical to that of the 800-grasp variant of the single-step experiment. datasets for each Challenge Task from our ablation study of online fine-tuning with real robots.We summarize the results in Table III.
<EOS>
Note that because it is the first step of the continual learning experiment, the policy for “Harsh Lighting” is identical to that of the 800-grasp variant of the single-step experiment. Recall that our goal for this experiment is to determine whether continual fine-tuning incurs a significant performance penalty compared to the single-step variant, because we are interested in using this method as a building block for con- tinual learning algorithms.We find that continual fine-tuning does not impose a drastic performance penalty compared to single-step fine-tuning.
<EOS>
The continual fine-tuning policies for the “Checkerboard Backing,” “Extend Gripper 1 cm,” and “Offset Gripper 10 cm,” challenges succeeded in grasping  86%50%Harsh LightingGrasping(Pre-Train)Adapt86%QT-OptℒHarsh LightQ-functionInitializeQT-OptGraspingQ-functionℒQT-OptℒBottlesQ-functionQT-OptℒExtendQ-functionQT-OptℒOﬀsetQ-functionQT-OptℒCheckerQ-functionInitializeInitializeInitializeInitialize50%50%50%50%50%50%50%50%50%AdaptAdaptAdaptAdaptPre-TrainTransparent BottlesCheckerboard BackingExtend GripperOﬀset GripperBottles800Checker800Harsh Light800Extend800Oﬀset800Grasping≈608,00032%49%50%75%43%63%74%88%91% Fig 5: Sample efficiency of our fine-tuning method on selected real-robot challenge tasks. between 4% and 7% less often than their single-step fine- tuning counterparts, whereas the policy for the challenging “Transparent Bottles” case actually succeeded 8% more often.These small deltas are within the margin-of-error of our eval- uation procedure, so we conclude that the effect of continual fine-tuning on the performance compared to single-step fine- tuning is very small.
<EOS>
This experiment demonstrates that our method can perform continual adaptation, and may serve as the basis for a continual end-to-end robot learning method. VI.EMPIRICAL ANALYSIS  In this section, we aim to further investigate the efficiency, performance, and characteristics of our large-scale real-world adaptation experiments.
<EOS>
A. Performance and sample efficiency of our method  Figure 5 shows the success rates for our method from Table II against the amount of data used to achieve that success rate for selected tasks.The data indicates that our simple ofﬂine fine-tuning method can adapt policies to many new tasks with performance at or even above the state-of-the- art base policy, using modest amounts of data.For instance, “Extend Gripper 1cm” and “Offset Gripper 10cm” both needed only 25 exploration grasps to achieve substantial gains in performance (+18% and +30%, respectively).
<EOS>
All policies attain substantial performance gains over the base policy by the time they are exposed to 800 exploration grasps, which is less than 0.2% of the data necessary to train an equivalently- performing policy on the base task. this relationship is not  While the general trend is that more exploration data leads linear.All to higher performance, methods experience a substantial improvement in performance after 100 or fewer exploration grasps.
<EOS>
However, we observe that these performance improvements in the very low-data regime (e.g. ≤ 200 grasp attempts) are also unstable.B. The downside of ofﬂine fine-tuning: deciding when to stop Our results indicate that ofﬂine fine-tuning can train robotic policies to substantial performance improvements with modest  Fig 6: Evaluation performance of a single ofﬂine fine-tuning experiment at different numbers of gradient steps (optimization epochs).The blue curve is real robot performance on the target task (Offset Gripper 10cm) when trained using 400 exploration grasps.
<EOS>
The green dotted line is the performance of training the same policy from scratch (random initialization) using 800 exploration grasps, and the yellow dotted line the performance of the base policy.The red dotted line portrays the number of gradient steps we choose to use for our large-scale fine-tuning study. amounts of data, and that ofﬂine methods are not limited by the need to preserve an always-sufficient exploration policy as with online methods.
<EOS>
However, we identify one significant drawback to the method compared to online fine-tuning. A pure ofﬂine fine-tuning method has no built-in evaluation step which would inform us when the robot’s performance on the target task has stopped improving, and therefore when we should stop fine-tuning with a fixed set of target task data.This is a subset of the off-policy evaluation problem [26].
<EOS>
Knowing when the policy stops improving is important, because fine- tuning exists in a low-data regime, and repeatedly updating a neural network model with small amounts of data leads to overfitting onto that data.Not only does this degrade the performance on the target task, but also the ability of the network to adapt to new tasks later (i.e. for continual learning).We can see this phenomenon in Figure 6 showing a real robot’s performance on the “Offset Gripper 10cm” target task at different numbers of steps into an ofﬂine fine-tuning process that uses 400 exploration grasps.
<EOS>
Performance quickly rises until around 500,000 gradient steps.Past this point, it precipitously drops and never recovers, dropping below even the initial performance of the base policy from which it was trained, as the initialization is being overwritten by overfitting to the target samples.The point at which overfitting begins is a function of the initialized model, target dataset, learning algorithm, and many other factors, and is not necessarily stable or easily predictable.
<EOS>
 For the purposes of our large-scale fine-tuning study, we use this experiment and several others to determine that 500,000  050100200400800Exploration Grasps5060708090100Success Rate (%)Sample EfficiencyCheckerboard BackingOffset Gripper 10cmTransparent Bottles1063×1055×1057×1053×106Gradient Steps020406080100Success Rate (%, N10)Forgetting in Off-Policy Fine-Tuning - Offset Gripper 10cmFine-tuned policy, 400 exploration graspsOur experiments (500k)Scratch policy, 800 exploration graspsBase policy Fig 7: Analysis of parameter changes induced by different fine-tuning target tasks.This plot portrays the cosine distance between the parameters of the pre-trained and fine-tuned networks for our 5 fine-tuning target tasks.The bar heights are normalized by the magnitude of parameter changes induced in the Q-function network by fine-tuning the baseline grasping task.
<EOS>
 gradient steps was an acceptable choice for the real-world experiments, but the variance in the results in Table II and Figure 5 shows that this choice was not necessarily optimal for all of our tasks and datasets.We believe one practical a solution to this problem of a continual learning robot is to use a mix of ofﬂine fine-tuning and online evaluation.The point, at which performance stops improving represents when the training process has exhausted the fine-tuning dataset of new information, and the robot must return to exploring online to continue improving.
<EOS>
C. Comparing initializing with RL to initializing with super- vised learning  In order to answer the question whether RL is better suited for creating a continually-learning robotic agent than supervised learning, we compare our results to an ImageNet- pretrained baseline.The ImageNet baseline uses a similar grasping network where its convolutional layers are replaced with ResNet 50 architecture and pre-loaded with ImageNet features.Since the part of the network that process robot’s state and action inputs cannot be initialized using supervised learning, we initialize them randomly.
<EOS>
As shown in Table II, the best performing ImageNet-based agent achieves the suc- cess rate of 47% on “Offset Gripper 10cm,” which corresponds to 4% improvement over the base policy performance.This result seems to confirm our hypothesis that our RL-based pre- training is crucial for good subsequent fine-tuning.Note that we first attempted to fine-tune these ImageNet-based policies while holding the ImageNet feature layers constant, but this procedure failed to achieve any non-zero success rate.
<EOS>
This suggests that, unlike adapting computer vision networks to new visual tasks, adapting end-to-end robot learning to new sensorimotor tasks may require changing the features used to  represent the problem, and not just the post-processing of said features. Figure 7 highlights some of the changes that happen during the RL-based fine-tuning in greater detail.It demonstrates the normalized distance in parameter space of a fine-tuned policy for each of our challenge tasks from its base policy.
<EOS>
While it is unsurprising that primarily-visual challenges such as “Checkerboard Backing” and “Harsh Lighting” induce large changes in the parameters of the convolutional parts of the network, we observe that even ‘Offset Gripper 10cm,” a purely-morphological change to the robot, induces substantial changes to the network’s image-processing parameters (e.g. layers conv2-conv7).We attribute this to the successful agent’s need for hand-eye coordination to complete the task: offsetting the gripper not only changes robot morphology, it changes the location of the robot in its own visual field drastically.In order to perform effective visual servoing with a new morphology, both the image and action-processing parts of the network must be updated.
<EOS>
 VII.CONCLUSION AND FUTURE WORK  For robots to be able to operate in unconstrained envi- ronments, they must be able to continuously adapt to new situations.We empirically studied this challenge by evaluating a state-of-the-art vision-based robotic grasping system, and testing its robustness to a range of new conditions such as varying backgrounds, lighting conditions, the shape and appearance of objects, and robot morphologies.
<EOS>
We found that these new conditions degraded performance of the trained grasping system substantially.Motivated by this initial study, we explored how to adapt vision-based robotic manipulation policies by fine-tuning with off-policy reinforcement learning. conv1conv2conv3conv4conv6conv7conv8conv9conv10conv11conv12conv13conv14conv15conv16fc0fc1logitpixelsCheckerboard BackingHarsh LightingOﬀset Gripper 10cmExtend Gripper 1cmTransparent Bottlesconv5fcgrasp1fcgrasp2actionexp. returnOur large-scale study shows that combining off-policy RL with a very simple fine-tuning procedure is an effective adaptation method, and this method is capable of achieving remarkable improvements in robot performance on new tasks with very little new data.
<EOS>
Furthermore, our continual learning experiment shows that using this simple method in a continual setting imposes very little performance penalty compared to the single-step setting.This suggests that the combination of off-policy RL and fine-tuning can serve as a building block for future continual learning methods. Our results comparing supervised-learning-based initializa- tion to those acquired with our RL-fine-tuning approach high- light a familiar truism about robotics: that robotic agents must do more than perceive the world, they must also act in it.
<EOS>
The ability to learn the combination of these two capabilities is what makes RL well-suited for creating continually-learning robots. While our work demonstrated promising results on a real- world robotic grasping system under a wide range of scenarios, both perceptual and physical, further work is needed to un- derstand how such adaptation performs on a broader range of robotic manipulation tasks.In the future, we would also like to focus on using off-policy metrics such as [26] for the purposes of early stopping, which would allow us to continuously mon- itor progress of the online fine-tuning process without costly real-robot evaluations.
<EOS>
We would also like to further assess our method’s suitability for continual adaptation, by assessing its performance on longer continual learning sequences, and measuring the how continual fine-tuning updates for new tasks affects the performance of previously-seen tasks. ACKNOWLEDGMENTS  The authors thank Noah Brown and Ivonne Fajardo for their superb and unyielding support with real robot experiments.We also thank Alex Irpan and Eric Jang for their help with robot learning software, Yevgen Chebotar for his advice on early revisions of this work and always-insightful discussions, Dmitry Kalashnikov and Jake Varley for their help with QT- Opt, and K.R. Zentner for her help with editing and artwork for this paper.
<EOS>
