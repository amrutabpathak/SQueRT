Published as a conference paper at ICLR 2020  SQIL: IMITATION LEARNING VIA REINFORCEMENT LEARNING WITH SPARSE REWARDS  Siddharth Reddy, Anca D. Dragan, Sergey Levine Department of Electrical Engineering and Computer Science University of California, Berkeley sgr,anca,svlevine {  @berkeley.edu }  ABSTRACT  Learning to imitate expert behavior from demonstrations can be challenging, es- pecially in environments with high-dimensional, continuous observations and un- known dynamics.
<EOS>
Supervised learning methods based on behavioral cloning (BC) suffer from distribution shift: because the agent greedily imitates demonstrated actions, it can drift away from demonstrated states due to error accumulation.
<EOS>
Re- cent methods based on reinforcement learning (RL), such as inverse RL and gen- erative adversarial imitation learning (GAIL), overcome this issue by training an RL agent to match the demonstrations over a long horizon.
<EOS>
Since the true reward function for the task is unknown, these methods learn a reward function from the demonstrations, often using complex and brittle approximation techniques that in- volve adversarial training.
<EOS>
We propose a simple alternative that still uses RL, but does not require learning a reward function.
<EOS>
The key idea is to provide the agent with an incentive to match the demonstrations over a long horizon, by encourag- ing it to return to demonstrated states upon encountering new, out-of-distribution states.
<EOS>
We accomplish this by giving the agent a constant reward of r = +1 for matching the demonstrated action in a demonstrated state, and a constant reward of r = 0 for all other behavior.
<EOS>
Our method, which we call soft Q imitation learn- ing (SQIL), can be implemented with a handful of minor modifications to any standard Q-learning or off-policy actor-critic algorithm.
<EOS>
Theoretically, we show that SQIL can be interpreted as a regularized variant of BC that uses a sparsity prior to encourage long-horizon imitation.
<EOS>
Empirically, we show that SQIL out- performs BC and achieves competitive results compared to GAIL, on a variety of image-based and low-dimensional tasks in Box2D, Atari, and MuJoCo.
<EOS>
This pa- per is a proof of concept that illustrates how a simple imitation method based on RL with constant rewards can be as effective as more complex methods that use learned rewards.
<EOS>
 1  INTRODUCTION  Many sequential decision-making problems can be tackled by imitation learning: an expert demon- strates near-optimal behavior to an agent, and the agent attempts to replicate that behavior in novel situations .
<EOS>
This paper considers the problem of training an agent to imitate an expert, given expert action demonstrations and the ability to interact with the environment.
<EOS>
The agent does not observe a reward signal or query the expert, and does not know the state transition dynamics.
<EOS>
 Standard approaches based on behavioral cloning (BC) use supervised learning to greedily imitate demonstrated actions, without reasoning about the consequences of actions (Pomerleau, 1991).
<EOS>
As a result, compounding errors cause the agent to drift away from the demonstrated states .
<EOS>
The problem with BC is that, when the agent drifts and encounters out-of-distribution states, the agent does not know how to return to the demonstrated states.
<EOS>
Recent methods based on in- verse reinforcement learning (IRL) overcome this issue by training an RL agent not only to imitate demonstrated actions, but also to visit demonstrated states (Ng et al, 2000; Wulfmeier et al, 2015; Finn et al, 2016b; Fu et al, 2017).
<EOS>
This is also the core idea behind generative adversarial imi- tation learning (GAIL) , which implements IRL using generative adversarial  1  Published as a conference paper at ICLR 2020  networks (Goodfellow et al, 2014; Finn et al, 2016a).
<EOS>
Since the true reward function for the task is unknown, these methods construct a reward signal from the demonstrations through adversarial training, making them difficult to implement and use in practice .
<EOS>
 The main idea in this paper is that the effectiveness of adversarial imitation methods can be achieved by a much simpler approach that does not require adversarial training, or indeed learning a reward function at all.
<EOS>
Intuitively, adversarial methods encourage long-horizon imitation by providing the agent with (1) an incentive to imitate the demonstrated actions in demonstrated states, and (2) an incentive to take actions that lead it back to demonstrated states when it encounters new, out-of- distribution states.
<EOS>
One of the reasons why adversarial methods outperform greedy methods, such as BC, is that greedy methods only do (1), while adversarial methods do both (1) and (2).
<EOS>
Our approach is intended to do both (1) and (2) without adversarial training, by using constant rewards instead of learned rewards.
<EOS>
The key idea is that, instead of using a learned reward function to provide a reward signal to the agent, we can simply give the agent a constant reward of r = +1 for matching the demonstrated action in a demonstrated state, and a constant reward of r = 0 for all other behavior.
<EOS>
 We motivate this approach theoretically, by showing that it implements a regularized variant of BC that learns long-horizon imitation by (a) imposing a sparsity prior on the reward function implied by the imitation policy, and (b) incorporating information about the state transition dynamics into the imitation policy.
<EOS>
Intuitively, our method accomplishes (a) by training the agent using an ex- tremely sparse reward function – +1 for demonstrations, 0 everywhere else – and accomplishes (b) by training the agent with RL instead of supervised learning.
<EOS>
 We instantiate our approach with soft Q-learning  by initializing the agent’s experience replay buffer with expert demonstrations, setting the rewards to a constant r = +1 in the demonstration experiences, and setting rewards to a constant r = 0 in all of the new experiences the agent collects while interacting with the environment.
<EOS>
Since soft Q-learning is an off-policy algorithm, the agent does not necessarily have to visit the demonstrated states in order to experience positive rewards.
<EOS>
Instead, the agent replays the demonstrations that were initially added to its buffer.
<EOS>
Thus, our method can be applied in environments with stochastic dynamics and continuous states, where the demonstrated states are not necessarily reachable by the agent.
<EOS>
We call this method soft Q imitation learning (SQIL).
<EOS>
 The main contribution of this paper is SQIL: a simple and general imitation learning algorithm that is effective in MDPs with high-dimensional, continuous observations and unknown dynamics.
<EOS>
We run experiments in four image-based environments – Car Racing, Pong, Breakout, and Space Invaders – and three low-dimensional environments – Humanoid, HalfCheetah, and Lunar Lander – to compare SQIL to two prior methods: BC and GAIL.
<EOS>
We find that SQIL outperforms BC and achieves com- petitive results compared to GAIL.
<EOS>
Our experiments illustrate two key benefits of SQIL: (1) that it can overcome the state distribution shift problem of BC without adversarial training or learning a re- ward function, which makes it easier to use, e.g., with images, and (2) that it is simple to implement using existing Q-learning or off-policy actor-critic algorithms.
<EOS>
 2 SOFT Q IMITATION LEARNING  SQIL performs soft Q-learning  with three small, but important, modifications: (1) it initially fills the agent’s experience replay buffer with demonstrations, where the rewards are set to a constant r = +1; (2) as the agent interacts with the world and accumulates new experiences, it adds them to the replay buffer, and sets the rewards for these new experiences to a constant r = 0; and (3) it balances the number of demonstration experiences and new experiences (50% each) in each sample from the replay buffer.1 These three modifications are motivated theoretically in Section 3, via an equivalence to a regularized variant of BC.
<EOS>
Intuitively, these modifications create a simple reward structure that gives the agent an incentive to imitate the expert in demonstrated states, and to take actions that lead it back to demonstrated states when it strays from the demonstrations.
<EOS>
 1 SQIL resembles the Deep Q-learning from Demonstrations (DQfD)  and Normalized Actor-Critic (NAC) algorithms , in that it initially fills the agent’s experience replay buffer with demonstrations.
<EOS>
The key difference is that DQfD and NAC are RL algorithms that assume access to a reward signal, while SQIL is an imitation learning algorithm that does not require an extrinsic reward signal from the environment.
<EOS>
Instead, SQIL automatically constructs a reward signal from the demonstrations.
<EOS>
 2  Published as a conference paper at ICLR 2020  Algorithm 1 Soft Q Imitation Learning (SQIL) 1: Require λsamp ∈ R≥0, Ddemo 2: Initialize Dsamp ← ∅ 3: while Qθ not converged do 4: 5: 6: 7: end while  θ ← θ − η∇θ(δ2(Ddemo, 1) + λsampδ2(Dsamp, 0)) {See Equation 1} Sample transition (s, a, s(cid:48)) with imitation policy π(a|s) ∝ exp (Qθ(s, a)) Dsamp ← Dsamp ∪ {(s, a, s(cid:48))}  Crucially, since soft Q-learning is an off-policy algorithm, the agent does not necessarily have to visit the demonstrated states in order to experience positive rewards.
<EOS>
Instead, the agent replays the demonstrations that were initially added to its buffer.
<EOS>
Thus, SQIL can be used in stochastic environments with high-dimensional, continuous states, where the demonstrated states may never actually be encountered by the agent.
<EOS>
 SQIL is summarized in Algorithm 1, where Qθ is the soft Q function, is the squared soft Bellman error,  D  demo are demonstrations, δ2  δ2(  , r) (cid:44) 1 |D|  D  (cid:32)  (cid:88)  (s,a,s(cid:48))∈D  (cid:32)  −  (cid:32)  (cid:88)  a(cid:48)∈A  Qθ(s, a)  r + γ log  exp (Qθ(s(cid:48), a(cid:48)))  ,  (1)  (cid:33)(cid:33)(cid:33)2  ∈ {  0, 1 }  is a constant reward.2 The experiments in Section 4 use a convolutional neural and r network or multi-layer perceptron to model Qθ, where θ are the weights of the neural network.
<EOS>
Section A.3 in the appendix contains additional implementation details, including values for the hyperparameter λsamp; note that the simple default value of λsamp = 1 works well across a variety of environments.
<EOS>
 As the imitation policy in line 5 of Algorithm 1 learns to behave more like the expert, a growing number of expert-like transitions get added to the buffer samp with an assigned reward of zero.
<EOS>
This causes the effective reward for mimicking the expert to decay over time.
<EOS>
Balancing the number of demonstration experiences and new experiences (50% each) sampled for the gradient step in line 4 ensures that this effective reward remains at least 1/(1 + λsamp), instead of decaying to zero.
<EOS>
In practice, we find that this reward decay does not degrade performance if SQIL is halted once the squared soft Bellman error loss converges to a minimum (e.g., see Figure 8 in the appendix).
<EOS>
Note that prior methods also require similar techniques: both GAIL and adversarial IRL (AIRL)  balance the number of positive and negative examples in the training set of the discriminator, and AIRL tends to require early stopping to avoid overfitting.
<EOS>
 D  3  INTERPRETING SQIL AS REGULARIZED BEHAVIORAL CLONING  To understand why SQIL works, we sketch a surprising theoretical result: SQIL is equivalent to a variant of behavioral cloning (BC) that uses regularization to overcome state distribution shift.
<EOS>
 BC is a simple approach that seeks to imitate the expert’s actions using supervised learning – in particular, greedily maximizing the conditional likelihood of the demonstrated actions given the demonstrated states, without reasoning about the consequences of actions.
<EOS>
Thus, when the agent makes small mistakes and enters states that are slightly different from those in the demonstrations, the distribution mismatch between the states in the demonstrations and those actually encountered by the agent leads to compounding errors .
<EOS>
We show that SQIL is equivalent to augmenting BC with a regularization term that incorporates information about the state transition dynamics into the imitation policy, and thus enables long-horizon imitation.
<EOS>
 3.1 PRELIMINARIES  Maximum entropy model of expert behavior.
<EOS>
SQIL is built on soft Q-learning, which assumes that expert behavior follows the maximum entropy model (Ziebart et al, 2010; Levine, 2018).
<EOS>
In  2Equation 1 assumes discrete actions, but SQIL can also be used with continuous actions, as shown in  Section 4.3.  3  Published as a conference paper at ICLR 2020  (2)  (3)  (4)  (5)  (6)  an infinite-horizon Markov Decision Process (MDP) with a continuous state space action space policy π forms a Boltzmann distribution over actions,  and discrete ,3 the expert is assumed to follow a policy π that maximizes reward R(s, a).
<EOS>
The  A  S  s) (cid:44)  π(a |  (cid:80)  exp (Q(s, a))  a(cid:48)∈A exp (Q(s, a(cid:48)))  ,  where Q is the soft Q function.
<EOS>
The soft Q values are a function of the rewards and dynamics, given by the soft Bellman equation,  Q(s, a) (cid:44) R(s, a) + γEs(cid:48)  log  exp (Q(s(cid:48), a(cid:48)))  (cid:34)  (cid:32)  (cid:33)(cid:35)  (cid:88)  a(cid:48)∈A  In our imitation setting, the rewards and dynamics are unknown.
<EOS>
The expert generates a fixed set of demo, by rolling out their policy π in the environment and generating state transi- demonstrations tions (s, a, s(cid:48)) demo.
<EOS>
 D ∈ D  Behavioral cloning (BC).
<EOS>
Training an imitation policy with standard BC corresponds to fitting a parametric model πθ that minimizes the negative log-likelihood loss,  In our setting, instead of explicitly modeling the policy πθ, we can represent the policy π in terms of a soft Q function Qθ via Equation 2:  (cid:96)BC(θ) (cid:44) (cid:88)  (s,a)∈Ddemo  log πθ(a  s). |  −  s) (cid:44)  π(a |  (cid:80)  exp (Qθ(s, a))  a(cid:48)∈A exp (Qθ(s, a(cid:48)))  Using this representation of the policy, we can train Qθ via the maximum-likelihood objective in Equation 4:  (cid:32)  (cid:96)BC(θ) (cid:44) (cid:88)  (s,a)∈Ddemo  Qθ(s, a)  log  −  −  (cid:33)(cid:33)  exp (Qθ(s, a(cid:48)))  (cid:32)  (cid:88)  a(cid:48)∈A  However, optimizing the BC loss in Equation 6 does not in general yield a valid soft Q function Qθ – i.e., a soft Q function that satisfies the soft Bellman equation (Equation 3) with respect to the dynamics and some reward function.
<EOS>
The problem is that the BC loss does not incorporate any information about the dynamics into the learning objective, so Qθ learns to greedily assign high values to demonstrated actions, without considering the state transitions that occur as a consequence of actions.
<EOS>
As a result, Qθ may output arbitrary values in states that are off-distribution from the demonstrations  demo.
<EOS>
 In Section 3.2, we describe a regularized BC algorithm that adds constraints to ensure that Qθ is a valid soft Q function with respect to some implicitly-represented reward function, and further regularizes the implicit rewards with a sparsity prior.
<EOS>
In Section 3.3, we show that this approach recovers an algorithm similar to SQIL.
<EOS>
 D  3.2 REGULARIZED BEHAVIORAL CLONING  Under the maximum entropy model described in Section 3.1, expert behavior is driven by a reward function, a soft Q function that computes expected future returns, and a policy that takes actions with high soft Q values.
<EOS>
In the previous section, we used these assumptions to represent the imitation policy in terms of a model of the soft Q function Qθ (Equation 5).
<EOS>
In this section, we represent the reward function implicitly in terms of Qθ, as shown in Equation 7.
<EOS>
This allows us to derive SQIL as a variant of BC that imposes a sparsity prior on the implicitly-represented rewards.
<EOS>
 Sparsity regularization.
<EOS>
The issue with BC is that, when the agent encounters states that are out- demo, Qθ may output arbitrary values.
<EOS>
One solution from prior work of-distribution with respect to  D  3Assuming a discrete action space simplifies our analysis.
<EOS>
SQIL can be applied to continuous control tasks  using existing sampling methods (Haarnoja et al, 2017; 2018), as illustrated in Section 4.3.  4  Published as a conference paper at ICLR 2020   is to regularize Qθ with a sparsity prior on the implied rewards – in particular, a penalty on the magnitude of the rewards (cid:80) implied by Qθ via the soft Bellman equation (Equation 3), where  Rq(s, a) |  s∈S,a∈A |  Rq(s, a) (cid:44) Qθ(s, a)  γEs(cid:48)  log  exp (Qθ(s(cid:48), a(cid:48)))  (7)  (cid:34)  (cid:32)  (cid:88)  a(cid:48)∈A  −  (cid:33)(cid:35)  Note that the reward function Rq is not explicitly modeled in this method.
<EOS>
Instead, we directly minimize the magnitude of the right-hand side of Equation 7, which is equivalent to minimizing Rq(s, a) | | The purpose of the penalty on is two-fold: (1) it imposes a sparsity prior motivated by prior work , and (2) it incorporates information about the state transition dynamics into the imitation learning objective, since Rq(s, a) is a function of an expectation over next state s(cid:48). (2) is critical for learning long-horizon behavior that imitates the demonstrations, instead of greedy maximization of the action likelihoods in standard BC.
<EOS>
For details, see Piot et al (2014).
<EOS>
 Rq(s, a)  |  |  Approximations for continuous states.
<EOS>
Unlike the discrete environments tested in Piot et al (2014), we assume the continuous state space cannot be enumerated.
<EOS>
Hence, we approximate the penalty (cid:80) by estimating it from samples: transitions (s, a, s(cid:48)) observed in the demonstrations samp periodically sampled during training using the latest imitation policy.
<EOS>
This approximation, which follows the standard approach to con- straint sampling , ensures that the penalty covers the state distribution actually encountered by the agent, instead of only the demonstrations.
<EOS>
 demo, as well as additional rollouts  Rq(s, a) |  s∈S,a∈A |  D  D  S  To make the penalty continuously differentiable, we introduce an additional approximation: instead , we penalize the squared value (Rq(s, a))2.
<EOS>
Note that of penalizing the absolute value since the reward function Rq is not explicitly modeled, but instead defined via Qθ in Equation 7, the squared penalty (Rq(s, a))2 is equivalent to the squared soft Bellman error δ2( samp, 0) from Equation 1.
<EOS>
 Rq(s, a) | |  ∪ D  demo  D  Regularized BC algorithm.
<EOS>
Formally, we define the regularized BC loss function adapted from Piot et al (2014) as  (cid:96)RBC(θ) (cid:44) (cid:96)BC(θ) + λδ2(  demo  D  ∪ D  samp, 0),  (8)  ∈  R≥0 is a constant hyperparameter, and δ2 denotes the squared soft Bellman error de- where λ fined in Equation 1.
<EOS>
The BC loss encourages Qθ to output high values for demonstrated actions at demonstrated states, and the penalty term propagates those high values to nearby states.
<EOS>
In other words, Qθ outputs high values for actions that lead to states from which the demonstrated states are reachable.
<EOS>
Hence, when the agent finds itself far from the demonstrated states, it takes actions that lead it back to the demonstrated states.
<EOS>
 The RBC algorithm follows the same procedure as Algorithm 1, except that in line 4, RBC takes a gradient step on the RBC loss from Equation 8 instead of the SQIL loss.
<EOS>
 3.3 CONNECTION BETWEEN SQIL AND REGULARIZED BEHAVIORAL CLONING  The gradient of the RBC loss in Equation 8 is proportional to the gradient of the SQIL loss in line 4 of Algorithm 1, plus an additional term that penalizes the soft value of the initial state s0 (full derivation in Section A.1 of the appendix):  θ(cid:96)RBC(θ)  ∇  θ ∝ ∇  (cid:0)δ2(  D  demo, 1) + λsampδ2(  samp, 0) + V (s0)(cid:1) (9)  D  In other words, SQIL solves a similar optimization problem to RBC.
<EOS>
The reward function in SQIL also has a clear connection to the sparsity prior in RBC: SQIL imposes the sparsity prior from RBC, by training the agent with an extremely sparse reward function – r = +1 at the demonstrations, and r = 0 everywhere else.
<EOS>
Thus, SQIL can be motivated as a practical way to implement the ideas for regularizing BC proposed in Piot et al (2014).
<EOS>
 The main benefit of using SQIL instead of RBC is that SQIL is trivial to implement, since it only requires a few small changes to any standard Q-learning implementation (see Section 2).
<EOS>
Extending SQIL to MDPs with a continuous action space is also easy, since we can simply replace Q-learning  5  Published as a conference paper at ICLR 2020  with an off-policy actor-critic method  (see Section 4.3).
<EOS>
Given the difficulty of implementing deep RL algorithms correctly , this ﬂexibility makes SQIL more practical to use, since it can be built on top of existing implementations of deep RL algorithms.
<EOS>
Furthermore, the ablation study in Section 4.4 suggests that SQIL actually performs better than RBC.
<EOS>
 4 EXPERIMENTAL EVALUATION  Our experiments aim to compare SQIL to existing imitation learning methods on a variety of tasks with high-dimensional, continuous observations, such as images, and unknown dynamics.
<EOS>
We benchmark SQIL against BC and GAIL4 on four image-based games – Car Racing, Pong, Break- out, and Space Invaders – and three state-based tasks – Humanoid, HalfCheetah, and Lunar Lander (Brockman et al, 2016; Bellemare et al, 2013; Todorov et al, 2012).
<EOS>
We also investigate which components of SQIL contribute most to its performance via an ablation study on the Lunar Lander game.
<EOS>
Section A.3 in the appendix contains additional experimental details.
<EOS>
 4.1 TESTING GENERALIZATION IN IMAGE-BASED CAR RACING  train 0 S  than that of the expert demonstrations  The goal of this experiment is to study not only how well each method can mimic the expert demon- strations, but also how well they can acquire policies that generalize to new states that are not seen in the demonstrations.
<EOS>
To do so, we train the imitation agents in an environment with a different initial state distribution , allowing us to system- atically control the mismatch between the distribution of states in the demonstrations and the states actually encountered by the agent.
<EOS>
We run experiments on the Car Racing game from OpenAI Gym. , the car is rotated 90 degrees so that it begins perpendicular to the track, instead of To create parallel to the track as inThis intervention presents a significant generalization challenge to the imitation learner, since the expert demonstrations do not contain any examples of states where the car is perpendicular to the road, or even significantly off the road axis.
<EOS>
The agent must learn to make a tight turn to get back on the road, then stabilize its orientation so that it is parallel to the road, and only then proceed forward to mimic the expert demonstrations.
<EOS>
 demo 0 S  demo 0 S  train 0 S  0  0  )  ) No Shift (S demo  Domain Shift (S train  −21 ± 56 −45 ± 18 −97 ± 3 375 ± 19 480 ± 11  Random BC GAIL-DQL SQIL (Ours) Expert  The results in Figure 1 show that SQIL and BC perform equally well when there is no variation in the ini- tial state.
<EOS>
The task is easy enough that even BC achieves a high reward.
<EOS>
Note that, in the unperturbed condi- tion (right column), BC substantially outperforms GAIL, despite the well- known shortcomings of BC.
<EOS>
This in- dicates that the adversarial optimiza- tion in GAIL can substantially hinder learning, even in settings where standard BC is sufficient.
<EOS>
SQIL performs much better than BC when starting from , showing that SQIL is capable of generalizing to a new initial state distribution, while BC is not.
<EOS>
SQIL learns to make a tight turn that takes the car through the grass and back onto the road, then stabilizes the car’s orientation so that it is parallel to the track, and then proceeds forward like the expert does in the demonstrations.
<EOS>
BC tends to drive straight ahead into the grass instead of turning back onto the road.
<EOS>
 Figure 1: Average reward on 100 episodes after training.
<EOS>
Standard error on three random seeds.
<EOS>
 −68 ± 4 698 ± 10 −66 ± 8 704 ± 6 704 ± 79  train 0 S  4 For all the image-based tasks, we implement a version of GAIL that uses deep Q-learning (GAIL-DQL) instead of TRPO as in the original GAIL paper , since Q-learning performs better than TRPO in these environments, and because this allows for a head-to-head comparison of SQIL and GAIL: both algorithms use the same underlying RL algorithm, but provide the agent with different rewards – SQIL provides constant rewards, while GAIL provides learned rewards.
<EOS>
We use the standard GAIL-TRPO method as a baseline for all the low-dimensional tasks, since TRPO performs better than Q-learning in these environments.
<EOS>
The original GAIL method implicitly encodes prior knowledge – namely, that terminating an episode is either always desirable or always undesirable.
<EOS>
As pointed out in Kostrikov et al (2019), this makes comparisons to alternative methods unfair.
<EOS>
We implement the unbiased version of GAIL proposed by Kostrikov et al (2019), and use this in all of the experiments.
<EOS>
Comparisons to the biased version with implicit termination knowledge are included in Section A.2 in the appendix.
<EOS>
 6  Published as a conference paper at ICLR 2020  Figure 2: Image-based Atari.
<EOS>
Smoothed with a rolling window of 100 episodes.
<EOS>
Standard error on three random seeds.
<EOS>
X-axis represents amount of interaction with the environment (not expert demonstrations).
<EOS>
 SQIL outperforms GAIL in both conditions.
<EOS>
Since SQIL and GAIL both use deep Q-learning for RL in this experiment, the gap between them may be attributed to the difference in the reward functions they use to train the agent.
<EOS>
SQIL benefits from providing a constant reward that does not require fitting a discriminator, while GAIL struggles to train a discriminator to provide learned rewards directly from images.
<EOS>
 4.2  IMAGE-BASED EXPERIMENTS ON ATARI  The results in Figure 2 show that SQIL outperforms BC on Pong, Breakout, and Space Invaders – additional evidence that BC suffers from compounding errors, while SQIL does not.
<EOS>
SQIL also outperforms GAIL on all three games, illustrating the difficulty of using GAIL to train an image- based discriminator, as in Section 4.1.  4.3  INSTANTIATING SQIL FOR CONTINUOUS CONTROL IN LOW-DIMENSIONAL MUJOCO  The experiments in the previous sections evaluate SQIL on MDPs with a discrete action space.
<EOS>
This section illustrates how SQIL can be adapted to continuous actions.
<EOS>
We instantiate SQIL using soft actor-critic (SAC) – an off-policy RL algorithm that can solve con- tinuous control tasks .
<EOS>
In particular, SAC is modified in the following ways: (1) the agent’s experience re- play buffer is initially filled with expert demonstrations, where re- wards are set to r = +1, (2) when taking gradient steps to fit the agent’s soft Q function, a balanced number of demonstration ex- periences and new experiences (50% each) are sampled from the replay buffer, and (3) the agent observes rewards of r = 0 during its interactions with the environment, instead of an extrinsic reward signal that specifies the desired task.
<EOS>
This instantiation of SQIL is compared to GAIL on the Humanoid (17 DoF) and HalfCheetah (6 DoF) tasks from MuJoCo.
<EOS>
 The results show that SQIL outperforms BC and performs compa- rably to GAIL on both tasks, demonstrating that SQIL can be suc- cessfully deployed on problems with continuous actions, and that SQIL can perform well even with a small number of demonstra- tions.
<EOS>
This experiment also illustrates how SQIL can be run on top of SAC or any other off-policy value-based RL algorithm.
<EOS>
 Figure 3: SQIL: best per- formance on 10 consecutive training episodes.
<EOS>
BC, GAIL: results from Dhariwal et al (2017).
<EOS>
 4.4 ABLATION STUDY ON LOW-DIMENSIONAL LUNAR LANDER  We hypothesize that SQIL works well because it combines information about the expert’s policy from demonstrations with information about the environment dynamics from rollouts of the imi- tation policy periodically sampled during training.
<EOS>
We also expect RBC to perform comparably to SQIL, since their objectives are similar.
<EOS>
To test these hypotheses, we conduct an ablation study using the Lunar Lander game from OpenAI Gym.
<EOS>
As in Section 4.1, we control the mismatch between the  7  02000400060008000NumberofOn-PolicyRollouts0100200300400RewardBreakoutSQIL(Ours)GAIL-DQLBC(P’91)Expert0500100015002000NumberofOn-PolicyRollouts−20−1001020RewardPong01000200030004000NumberofOn-PolicyRollouts200400600RewardSpaceInvaders02040NumberofDemonstrationRollouts05001000RewardHalfCheetah-v1ExpertGAIL-TRPO(HE’16)BC(P’91)SQIL(Ours)02040NumberofDemonstrationRollouts0200400600800RewardHumanoid-v1Published as a conference paper at ICLR 2020  distribution of states in the demonstrations and the states encountered by the agent by manipulating the initial state distribution.
<EOS>
To create , the agent is placed in a starting position never visited in the demonstrations.
<EOS>
 train 0 S  In the first variant of SQIL, λsamp is set to zero, to prevent SQIL from using additional samples drawn from the environment (see line 4 of Algorithm 1).
<EOS>
This comparison tests if SQIL really needs to interact with the environment, or if it can rely solely on the demonstrations.
<EOS>
In the second condition, γ is set to zero to prevent SQIL from accessing information about state transitions (see Equation 1 and line 4 of Algorithm 1).
<EOS>
This comparison tests if SQIL is actually extracting information about the dynamics from the samples, or if it can perform just as well with a na¨ıve regularizer (setting γ to zero effectively imposes a penalty on the L2-norm of the soft Q values instead of the squared soft Bellman error).
<EOS>
In the third condition, a uniform random policy is used to sample additional rollouts, instead of the imitation policy πθ (see line 6 of Algorithm 1).
<EOS>
This comparison tests how important it is that the samples cover the states encountered by the agent during training.
<EOS>
In the fourth condition, we use RBC to optimize the loss in Equation 8. instead of using SQIL to optimize the loss in line 4 of Algorithm 1.
<EOS>
This comparison tests the effect of the additional V (s0) term in RBC vs.
<EOS>
SQIL (see Equation 9).
<EOS>
 )  Domain Shift (S train  ) No Shift (S demo  0  0  0.10 ± 0.30 Random 0.07 ± 0.03 BC GAIL-TRPO 0.67 ± 0.04 SQIL (Ours)  The results in Figure 4 show that all methods perform well when there is no variation in the initial state.
<EOS>
When the initial state is varied, SQIL per- forms significantly better than BC, GAIL, and the ablated variants of SQIL.
<EOS>
This confirms our hypothesis that SQIL needs to sample from the environment using the imitation pol- icy, and relies on information about the dynamics encoded in the samples.
<EOS>
Surprisingly, SQIL outperforms RBC by a large margin, suggesting that the penalty on the soft value of the initial state V (s0), which is present in RBC but not in SQIL (see Equation 9), degrades performance.
<EOS>
 0.04 ± 0.02 0.93 ± 0.03 0.93 ± 0.03 0.88 ± 0.03 0.87 ± 0.02 0.84 ± 0.02 0.82 ± 0.02 0.89 ± 0.01 0.89 ± 0.31  Figure 4: Best success rate on 100 consecutive episodes dur- ing training.
<EOS>
Standard error on five random seeds.
<EOS>
Perfor- mance bolded if at least within one standard error of expert.
<EOS>
 0.89 ± 0.02 0.12 ± 0.02 0.41 ± 0.02 0.47 ± 0.02 0.66 ± 0.02 0.93 ± 0.03  γ = 0 π = Unif RBC Expert  n λsamp = 0  o i t a l b A  5 DISCUSSION AND RELATED WORK  Related work.
<EOS>
Concurrently with SQIL, two other imitation learning algorithms that use constant rewards instead of a learned reward function were developed (Sasaki et al, 2019; Wang et al, 2019).
<EOS>
We see our paper as contributing additional evidence to support this core idea, rather than proposing a competing method.
<EOS>
First, SQIL is derived from sparsity-regularized BC, while the prior meth- ods are derived from an alternative formulation of the IRL objective  and from support estimation methods , showing that different theoretical approaches inde- pendently lead to using RL with constant rewards as an alternative to adversarial training – a sign that this idea may be a promising direction for future work.
<EOS>
Second, SQIL is shown to outperform BC and GAIL in domains that were not evaluated in Sasaki et al (2019) or Wang et al (2019) – in particular, tasks with image observations and significant shift in the state distribution between the demonstrations and the training environment.
<EOS>
 Summary.
<EOS>
We contribute the SQIL algorithm: a general method for learning to imitate an expert given action demonstrations and access to the environment.
<EOS>
Simulation experiments on tasks with high-dimensional, continuous observations and unknown dynamics show that our method outper- forms BC and achieves competitive results compared to GAIL, while being simple to implement on top of existing off-policy RL algorithms.
<EOS>
 Limitations and future work.
<EOS>
We have not yet proven that SQIL matches the expert’s state occu- pancy measure in the limit of infinite demonstrations.
<EOS>
One direction for future work would be to rigorously show whether or not SQIL has this property.
<EOS>
Another direction would be to extend SQIL to recover not just the expert’s policy, but also their reward function; e.g., by using a parameterized reward function to model rewards in the soft Bellman error terms, instead of using constant rewards.
<EOS>
This could provide a simpler alternative to existing adversarial IRL algorithms.
<EOS>
