b'Published as a conference paper at ICLR 2020\n\nSELF:'
<EOS>
b'LEARNING TO FILTER NOISY LABELS WITH'
<EOS>
b'SELF-ENSEMBLING'
<EOS>
b'Duc'
<EOS>
b'Tam Nguyen \xe2\x88\x97, Chaithanya Kumar Mummadi \xe2\x88\x97\xe2\x80\xa0, Thi Phuong Nhung Ngo \xe2\x80\xa0,'
<EOS>
b'Thi Hoai Phuong Nguyen \xe2\x80\xa1,'
<EOS>
b'Laura Beggel \xe2\x80\xa0, Thomas Brox \xe2\x80\xa0'
<EOS>
b'ABSTRACT'
<EOS>
b'Deep neural networks (DNNs) have been shown to over-\xef\xac\x81t a dataset when be-'
<EOS>
b'ing trained with noisy labels for a long enough time.'
<EOS>
b'To overcome this problem,\nwe present a simple and effective method self-ensemble label \xef\xac\x81ltering (SELF) to'
<EOS>
b'progressively \xef\xac\x81lter out the wrong labels during training.'
<EOS>
b'Our method improves\nthe task performance by gradually allowing supervision only from the potentially\nnon-noisy (clean) labels and stops learning on the \xef\xac\x81ltered noisy labels.'
<EOS>
b'For the\n\xef\xac\x81ltering, we form running averages of predictions over the entire training dataset\nusing the network output at different training epochs.'
<EOS>
b'We show that these en-'
<EOS>
b'semble estimates yield more accurate identi\xef\xac\x81cation of inconsistent predictions\nthroughout training than the single estimates of the network at the most recent\ntraining epoch.'
<EOS>
b'While \xef\xac\x81ltered samples are removed entirely from the supervised\ntraining loss, we dynamically leverage them via semi-supervised learning in the\nunsupervised loss.'
<EOS>
b'We demonstrate the positive effect of such an approach on var-\nious image classi\xef\xac\x81cation tasks under both symmetric and asymmetric label noise\nand at different noise ratios.'
<EOS>
b'It substantially outperforms all previous works on\nnoise-aware learning across different datasets and can be applied to a broad set of\nnetwork architectures.'
<EOS>
b'1'
<EOS>
b'INTRODUCTION'
<EOS>
b'The acquisition of large quantities of a high-quality human annotation is a frequent bottleneck in\napplying DNNs.'
<EOS>
b'There are two cheap but imperfect alternatives to collect annotation at large scale:\ncrowdsourcing from non-experts and web annotations, particularly for image data where the tags and\nonline query keywords are treated as valid labels.'
<EOS>
b'Both these alternatives typically introduce noisy\n(wrong) labels.'
<EOS>
b'While Rolnick et al.'
<EOS>
b'(2017) empirically demonstrated that DNNs can be surprisingly\nrobust to label noise under certain conditions, Zhang et al.'
<EOS>
b'(2017) has shown that DNNs have the\ncapacity to memorize the data and will do so eventually when being confronted with too many noisy\nlabels.'
<EOS>
b'Consequently, training DNNs with traditional learning procedures on noisy data strongly\ndeteriorates their ability to generalize \xe2\x80\x93 a severe problem.'
<EOS>
b'Hence, limiting the in\xef\xac\x82uence of label\nnoise is of great practical importance.'
<EOS>
b'A common approach to mitigate the negative in\xef\xac\x82uence of noisy labels is to eliminate them from the\ntraining data and train deep learning models just with the clean labels (Fr\xc2\xb4enay & Verleysen, 2013).'
<EOS>
b'Employing semi-supervised learning can even counteract the noisy labels (Laine & Aila, 2016; Luo\net al., 2018).'
<EOS>
b'However, the decision which labels are noisy and which are not is decisive for learning\nrobust models.'
<EOS>
b'Otherwise, un\xef\xac\x81ltered noisy labels still in\xef\xac\x82uence the (supervised) loss and affect the\ntask performance as in these previous works.'
<EOS>
b'They use the entire label set to compute the loss and\nseverely lack a mechanism to identify and \xef\xac\x81lter out the erroneous labels from the labels set.'
<EOS>
b'In this paper, we propose a self-ensemble label \xef\xac\x81ltering (SELF) framework that identi\xef\xac\x81es'
<EOS>
b'potentially\nnoisy labels during training and keeps the network from receiving supervision from the \xef\xac\x81ltered noisy\nlabels.'
<EOS>
b'This allows DNNs to gradually focus on learning from undoubtedly correct samples even\nwith an extreme level of noise in the labels (e.g., 80% noise ratio) and leads to improved performance\nas the supervision become less noisy.'
<EOS>
b'The key contribution of our work is progressive \xef\xac\x81ltering, i.e.,\n\n\xe2\x88\x97Computer Vision Group, University of Freiburg, Germany\n\xe2\x80\xa0Bosch Center for AI, Bosch GmbH, Germany'
<EOS>
b'\xe2\x80\xa1Karlsruhe Institute of Technology, Germany'
<EOS>
b'1'
<EOS>
b'Published as a conference paper at ICLR 2020'
<EOS>
b'(a) Evaluation on CIFAR-10'
<EOS>
b'(b) Evaluation on CIFAR-100'
<EOS>
b'Figure 1: Comparing the performance of SELF with previous works for learning under different\n(symmetric)'
<EOS>
b'label noise ratios on the (a)'
<EOS>
b'CIFAR-10 &'
<EOS>
b'(b) CIFAR-100 datasets.'
<EOS>
b'SELF retains higher\nrobust classi\xef\xac\x81cation accuracy at all noise levels.'
<EOS>
b'leverage the knowledge provided in the network\xe2\x80\x99s output over different training iterations to form a\nconsensus of predictions (self-ensemble predictions) to progressively identify and \xef\xac\x81lter out the noisy\nlabels from the labeled data.'
<EOS>
b'When learning under label noise, the network receives noisy updates and hence \xef\xac\x82uctuates strongly.'
<EOS>
b'Such conduct of training would impede to learn stable neural representations and further mislead the\nconsensus of the predictions.'
<EOS>
b'Therefore, it is essential to incorporate a model with stable training\nbehavior to obtain better estimates from the consensus.'
<EOS>
b'Concretely, we employ the semi-supervised\ntechnique as a backbone to our framework to stabilize the learning process of the model.'
<EOS>
b'Correctly,\nwe maintain the running average model, such as proposed by Tarvainen & Valpola (2017),'
<EOS>
b'a.k.a.'
<EOS>
b'the Mean-Teacher model.'
<EOS>
b'This model ensemble learning provides a more stable supervisory signal\nthan the noisy model snapshots and provides a stable ground for progressive \xef\xac\x81ltering to \xef\xac\x81lter out\npotential noisy labels.'
<EOS>
b'Note that this is different from just a mere combination of semi-supervised\ntechniques with a noisy label \xef\xac\x81ltering method.'
<EOS>
b'We call our approach self-ensemble label \xef\xac\x81ltering (SELF) - that establishes model ensemble learning\nas a backbone to form a solid consensus of the self-ensemble predictions to \xef\xac\x81lter out the noisy labels\nprogressively.'
<EOS>
b'Our framework allows to compute supervised loss on cleaner subsets rather than the\nentire noisy labeled data as in previous works.'
<EOS>
b'It further leverages the entire dataset, including the\n\xef\xac\x81ltered out erroneous samples in the unsupervised loss.'
<EOS>
b'To best of our knowledge, we are the \xef\xac\x81rst\nto identify and propose self-ensemble as a principled technique against learning under noisy labels.'
<EOS>
b'Our motivation stems from the observation that DNNs start to learn from easy samples in initial\nphases and gradually adapt to hard ones during training.'
<EOS>
b'When trained on wrongly labeled data,\nDNNs learn from clean labels at ease and receive inconsistent error signals from the noisy labels\nbefore over-\xef\xac\x81tting to the dataset.'
<EOS>
b'The network\xe2\x80\x99s prediction is likely to be consistent on clean samples\nand inconsistent or oscillates strongly on wrongly labeled samples over different training iterations.'
<EOS>
b'Based on this observation, we record the outputs of a single network made on different training\nepochs and treat them as an ensemble of predictions obtained from different individual networks.'
<EOS>
b'We\ncall these ensembles that are evolved from a single network self-ensemble predictions.'
<EOS>
b'Subsequently,\nwe identify the correctly labeled samples via the agreement between the provided label set and our\nrunning average of self-ensemble predictions.'
<EOS>
b'The samples of ensemble predictions that agree with\nthe provided labels are likely to be consistent and treated as clean samples.'
<EOS>
b'In summary, our SELF framework stabilizes the training process and improves the generalization\nability of DNNs.'
<EOS>
b'We evaluate the proposed technique on image classi\xef\xac\x81cation tasks using CI-'
<EOS>
b'FAR10, CIFAR100 & ImageNet.'
<EOS>
b'We demonstrate that SELF consistently outperforms the existing\napproaches on asymmetric and symmetric noise at all noise levels, as shown in Fig.'
<EOS>
b'1.'
<EOS>
b'Besides,\nSELF remains robust towards the choice of the network architecture.'
<EOS>
b'Our work is transferable to'
<EOS>
b'other tasks without the need to modify the architecture or the primary learning objective.'
<EOS>
b'2'
<EOS>
b'Published as a conference paper at ICLR 2020'
<EOS>
b'Figure 2: Overview of the self-ensemble label \xef\xac\x81ltering (SELF) framework.'
<EOS>
b'The model starts in\niteration 0 with training from the noisy label set.'
<EOS>
b'During training, the model maintains a self-'
<EOS>
b'ensemble, a running average of itself (Tarvainen & Valpola, 2017) to provide a stable learning signal.'
<EOS>
b'Also, the model collects a self-ensemble prediction (moving-average) for the subsequent \xef\xac\x81ltering.'
<EOS>
b'Once the best model is found, these predictions identify and \xef\xac\x81lter out noisy labels using the original\nlabel set L0.'
<EOS>
b'The model performs this progressive \xef\xac\x81ltering until there is no more better model.'
<EOS>
b'For\ndetails see Algorithm 1.'
<EOS>
b'2 SELF-ENSEMBLE LABEL FILTERING'
<EOS>
b'2.1 OVERVIEW'
<EOS>
b'Fig. 2 shows an overview of our proposed approach.'
<EOS>
b'In the beginning, we assume that the labels\nof the training set are noisy.'
<EOS>
b'The model attempts to identify correct labels progressively using self-\nforming ensembles of models and predictions.'
<EOS>
b'Since wrong labels cause strong \xef\xac\x82uctuations in the\nmodel\xe2\x80\x99s predictions, using ensembles is a natural way to counteract noisy labels.'
<EOS>
b'Concretely, in each iteration, the model learns from a detected set of potentially correct labels and\nmaintains a running average of model snapshots (realized by the Mean Teacher model Tarvainen &\nValpola (2017)).'
<EOS>
b'This ensemble model is evaluated on the entire dataset and provides an additional\nlearning signal for training the single models.'
<EOS>
b'Additionally, our framework maintains the running-\naverage of the model\xe2\x80\x99s predictions for the \xef\xac\x81ltering process.'
<EOS>
b'The model is trained until we \xef\xac\x81nd the\nbest model'
<EOS>
b'w.r.t.'
<EOS>
b'the performance on the validation set (e.g., by early-stopping).'
<EOS>
b'The set of correct\nlabels is detected based on the strategy de\xef\xac\x81ned in Sec.'
<EOS>
b'2.2.'
<EOS>
b'In the next iteration, we again use all data\nand the new \xef\xac\x81ltered label set as input for the model training.'
<EOS>
b'The iterative training procedure stops\nwhen no better model can be found.'
<EOS>
b'In the following, we give more details about the combination\nof this training and \xef\xac\x81ltering procedure.'
<EOS>
b'2.2 PROGRESSIVE'
<EOS>
b'LABEL FILTERING'
<EOS>
b'Progressive detection of correctly labeled samples Our framework'
<EOS>
b'Self-Ensemble Label Filter-'
<EOS>
b'ing (Algorithm 1) focuses on the detection of certainly correct labels from the provided label set L0.'
<EOS>
b'In each iteration i, the model is trained using the label set of potentially correct labels Li.'
<EOS>
b'At the end\nof each iteration, the model determines the next correct label set Li+1 using the \xef\xac\x81ltering strategy\ndescribed in 2.2'
<EOS>
b'The model stops learning when no improvement was achieved after training on the\nre\xef\xac\x81ned label set Li+1.'
<EOS>
b'In other words, in each iteration, the model attempts to learn from the easy, in some sense, obviously\ncorrect labels.'
<EOS>
b'However, learning from easy samples also affects similar but harder samples from the\nsame classes.'
<EOS>
b'Therefore, by learning from these easy samples, the network can gradually distinguish\nbetween hard and wrongly-labeled samples.'
<EOS>
b'3'
<EOS>
b'Published as a conference paper at ICLR 2020'
<EOS>
b'Algorithm 1 SELF:'
<EOS>
b'Self-Ensemble Label Filtering pseudocode'
<EOS>
b'Require:'
<EOS>
b'Dtrain'
<EOS>
b'= noisy labeled training set'
<EOS>
b'Require:'
<EOS>
b'Dval = noisy'
<EOS>
b'labeled validation set'
<EOS>
b'Require: (x, y) ='
<EOS>
b'training stimuli and label'
<EOS>
b'Require: \xce\xb1 = ensembling momentum, 0 \xe2\x89\xa4 \xce\xb1 \xe2\x89\xa4 1'
<EOS>
b'while acc(Mi, Dval) \xe2\x89\xa5'
<EOS>
b'acc(Mbest, Dval) do\n\ni \xe2\x86\x90 0'
<EOS>
b'Mi'
<EOS>
b'\xe2\x86\x90 train(Dtrain, Dval)'
<EOS>
b'Mbest \xe2\x86\x90'
<EOS>
b'Mi'
<EOS>
b'zi'
<EOS>
b'\xe2\x86\x90 0'
<EOS>
b'Mbest \xe2\x86\x90'
<EOS>
b'Mi'
<EOS>
b'Df ilter'
<EOS>
b'\xe2\x86\x90'
<EOS>
b'Dtrain'
<EOS>
b'i'
<EOS>
b'\xe2\x86\x90'
<EOS>
b'i + 1'
<EOS>
b'for (x, y) in Df ilter'
<EOS>
b'do\n\n\xcb\x86zi \xe2\x86\x90 Mbest(x)'
<EOS>
b'zi'
<EOS>
b'\xe2\x86\x90'
<EOS>
b'\xce\xb1zi\xe2\x88\x921 + (1'
<EOS>
b'\xe2\x88\x92 \xce\xb1)\xcb\x86zi'
<EOS>
b'if y'
<EOS>
b'(cid:54)= argmax(zi)'
<EOS>
b'then\n\ny \xe2\x86\x90 \xe2\x88\x85 in Df ilter'
<EOS>
b'end'
<EOS>
b'if\nend for'
<EOS>
b'Mi \xe2\x86\x90 train(Df ilter, Dval)\n\nend while\nreturn Mbest\n\n(cid:46) counter to track iterations'
<EOS>
b'(cid:46) initial Mean-Teacher ensemble model training\n(cid:46) set initial model as best model'
<EOS>
b'(cid:46) initialize ensemble predictions of all samples\n(ignored sample index for simplicity)'
<EOS>
b'(cid:46) iterate until no best model is found on Dval\n(cid:46) save the best model'
<EOS>
b'(cid:46) set \xef\xac\x81ltered'
<EOS>
b'dataset as initial label set\n\n(cid:46)'
<EOS>
b'evaluate model output \xcb\x86zi\n(cid:46) accumulate ensemble predictions zi'
<EOS>
b'(cid:46) verify agreement of ensemble predictions & label'
<EOS>
b'(cid:46) identify it as noisy label & remove from label set\n\n(cid:46)'
<EOS>
b'train'
<EOS>
b'Mean-Teacher model on \xef\xac\x81ltered label set'
<EOS>
b'Our framework does not focus on repairing all noisy labels.'
<EOS>
b'Although the detection of wrong labels is\nsometimes easy, \xef\xac\x81nding their correct hidden label might be extremely challenging in case of having\nmany classes.'
<EOS>
b'If the noise is suf\xef\xac\x81ciently random, the set of correct labels will be representative to\nachieve high model performance.'
<EOS>
b'Further, in our framework, the label \xef\xac\x81ltering is performed on the\noriginal label set L0 from iteration 0.'
<EOS>
b'Clean labels erroneously removed in an earlier iteration (e.g.'
<EOS>
b',\nlabels of hard to classify samples) can be reconsidered for model training again in later iterations.'
<EOS>
b'Filtering strategy'
<EOS>
b'The model can determine the set of potentially correct labels'
<EOS>
b'Li based on agree-\nment between the label y and its maximal likelihood prediction \xcb\x86y|x with Li ='
<EOS>
b'{(y, x) | \xcb\x86yx ='
<EOS>
b'y; \xe2\x88\x80(y, x) \xe2\x88\x88 L0}.'
<EOS>
b'L0 is the label set provided in the beginning, (y, x) are the samples and their\nrespective noisy labels in the iteration i.'
<EOS>
b'In other words, the labels are only used for supervised\ntraining if in the current epoch, the model predicts the respective label to be the correct class with\nthe highest likelihood.'
<EOS>
b'In practice, our framework does not use \xcb\x86y(x) of model snapshots for \xef\xac\x81ltering\nbut a moving-average of the ensemble models and predictions to improve the \xef\xac\x81ltering decision.'
<EOS>
b'2.3 SELF-ENSEMBLE LEARNING'
<EOS>
b'The model\xe2\x80\x99s predictions for noisy samples tend to \xef\xac\x82uctuate.'
<EOS>
b'For example, take a cat wrongly labeled\nas a tiger.'
<EOS>
b'Other cat samples would encourage the model to predict the given cat image as a cat.'
<EOS>
b'Contrary, the wrong label tiger regularly pulls the model back to predict the cat as a tiger.'
<EOS>
b'Hence,\nusing the model'
<EOS>
b'\xe2\x80\x99s predictions gathered in one single training epoch for \xef\xac\x81ltering'
<EOS>
b'is sub-optimal.'
<EOS>
b'Therefore, in our framework SELF, our model relies on ensembles of models and predictions.'
<EOS>
b'Model ensemble with Mean Teacher A natural way to form a model ensemble is by using an\nexponential running average of model snapshots (Fig. 3a).'
<EOS>
b'This idea was proposed in Tarvainen\n& Valpola (2017) for semi-supervised learning and is known as the Mean Teacher model.'
<EOS>
b'In our\nframework, both the mean teacher model and the normal model are evaluated on all data to preserve\nthe consistency between both models.'
<EOS>
b'The consistency loss between student and teacher output\ndistribution can be realized with Mean-Square-Error loss or Kullback-Leibler-divergence.'
<EOS>
b'More\ndetails for training with the model ensemble can be found in Appendix A.1'
<EOS>
b'Prediction ensemble'
<EOS>
b'Additionally, we propose to collect the sample predictions over multiple\ntraining epochs:'
<EOS>
b'zj = \xce\xb1zj\xe2\x88\x921 +'
<EOS>
b'(1 \xe2\x88\x92 \xce\xb1)\xcb\x86zj , whereby zj depicts the moving-average prediction\nof sample k at epoch j,'
<EOS>
b'\xce\xb1 is a momentum, \xcb\x86zj is the model prediction for sample k in epoch j.'
<EOS>
b'This scheme is displayed in Fig.'
<EOS>
b'3b.'
<EOS>
b'For each sample, we store the moving-average predictions,\naccumulated over the past iterations.'
<EOS>
b'Besides having a more stable basis for the \xef\xac\x81ltering step, our\nproposed procedure also leads to negligible memory and computation overhead.'
<EOS>
b'4'
<EOS>
b'Published as a conference paper at ICLR 2020'
<EOS>
b'(a) Model ensemble'
<EOS>
b'(Mean teacher)'
<EOS>
b'(b) Predictions ensemble'
<EOS>
b'Figure 3:'
<EOS>
b'Maintaining the (a) model and (b) predictions ensembles is very effective against noisy\nmodel updates.'
<EOS>
b'These ensembles are self-forming during the training process as a moving-average\nof (a) model snapshots or (b) class predictions from previous training steps.'
<EOS>
b'Further, due to continuous training of the best model from the previous model, computation time can\nbe signi\xef\xac\x81cantly reduced, compared to re-training the model from scratch.'
<EOS>
b'On the new \xef\xac\x81ltered dataset,\nthe model must only slowly adapt to the new noise ratio contained in the training set.'
<EOS>
b'Depending on\nthe computation budget, a maximal number of iterations for \xef\xac\x81ltering can be set to save time.'
<EOS>
b'3 RELATED WORKS'
<EOS>
b'Reed et al.'
<EOS>
b'(2014); Azadi et al.'
<EOS>
b'(2015) performed early works on learning robustly under label\nnoise for deep neural networks.'
<EOS>
b'Recently, Rolnick et al.'
<EOS>
b'(2017) have shown for classi\xef\xac\x81cation that\ndeep neural networks come with natural robustness to label noise following a particular random\ndistribution.'
<EOS>
b'No modi\xef\xac\x81cation of the network or the training procedure is required to achieve this\nrobustness.'
<EOS>
b'Following this insight, our framework SELF relies on this natural robustness to kickstart'
<EOS>
b'the self-ensemble \xef\xac\x81ltering process to extend the robust behavior to more challenging scenarios.'
<EOS>
b'Laine & Aila (2016); Luo et al.'
<EOS>
b'(2018) proposed to apply semi-supervised techniques on the data to\ncounteract noise.'
<EOS>
b'These and other semi-supervised learning techniques learn from a static, initial set\nof noisy labels and have no mechanisms to repair labels.'
<EOS>
b'Therefore, the supervised losses in their\nlearning objective are typically high until the model strongly over\xef\xac\x81ts to the label noise.'
<EOS>
b'Compared\nto these works, our framework performs a variant of self-supervised label corrections.'
<EOS>
b'The network\nlearns from a dynamic, variable set of labels, which is determined by the network itself.'
<EOS>
b'Progressive\n\xef\xac\x81ltering allows the network to (1) focus on a label set with a signi\xef\xac\x81cantly lower noise ratio and'
<EOS>
b'(2)\nrepair wrong decisions made by itself in an earlier iteration.'
<EOS>
b'Other works assign weights to potentially wrong labels to reduce the learning signal (Jiang et al.,\n2017; Ren et al., 2018; Jenni & Favaro, 2018).'
<EOS>
b'These approaches tend to assign less extreme weights\nor hyperparameters that are hard to set.'
<EOS>
b'Since the typical classi\xef\xac\x81cation loss is highly non-linear,'
<EOS>
b'a\nlower weight might still lead to learning from wrong labels.'
<EOS>
b'Compared to these works, the samples\nin SELF only receive extreme weights: either they are zero or one.'
<EOS>
b'Further, SELF focuses only on\nself-detecting the correct samples, instead of repairing the wrong labels.'
<EOS>
b'Typically, the set of correct\nsamples are much easier to detect and are suf\xef\xac\x81ciently representative to achieve high performance.'
<EOS>
b'Han'
<EOS>
b'et al.'
<EOS>
b'(2018b)'
<EOS>
b'; Jiang et al.'
<EOS>
b'(2017) employ two collaborating and simultaneously learning net-'
<EOS>
b'works to determine which samples to learn from and which not.'
<EOS>
b'However, the second network is\nfree in its predictions and hence hard to tune.'
<EOS>
b'Compared to these works, we use ensemble learning as\na principled approach to counteract model \xef\xac\x82uctuations.'
<EOS>
b'In SELF, the second network is extremely\nrestricted and is only composed of running averages of the \xef\xac\x81rst network.'
<EOS>
b'To realize the second\nnetwork, we use the mean-teacher model (Tarvainen & Valpola, 2017) as a backbone.'
<EOS>
b'Compared\nto their work, our self-ensemble label \xef\xac\x81ltering gradually detects the correct labels and learns from\nthem, so the label set is variable.'
<EOS>
b'Further, we do use not only model ensembles but also an ensemble\nof predictions to detect correct labels.'
<EOS>
b'Other works modify the primary loss function of the classi\xef\xac\x81cation tasks.'
<EOS>
b'Patrini et al.'
<EOS>
b'(2017) es-'
<EOS>
b'timates the noise transition matrix to correct the loss, Han et al.'
<EOS>
b'(2018a)'
<EOS>
b'uses human-in-the-loop,\nZhang & Sabuncu (2018);'
<EOS>
b'Thulasidasan et al.'
<EOS>
b'(2019) propose other forms of cross-entropy losses.'
<EOS>
b'The loss modi\xef\xac\x81cation impedes the transfer of these ideas to other tasks than classi\xef\xac\x81cation.'
<EOS>
b'Compared\nto these works, our framework SELF does not modify the primary loss.'
<EOS>
b'However, many tasks rely on\nthe presence of clean labels such as anomaly detection'
<EOS>
b'(Nguyen et al., 2019a) or self-supervised and'
<EOS>
b'unsupervised learning (Nguyen et al., 2019b).'
<EOS>
b'The progressive \xef\xac\x81ltering procedure and self-ensemble'
<EOS>
b'learning proposed are also applicable in these tasks to counteract noise effectively.'
<EOS>
b'5'
<EOS>
b'Published as a conference paper at ICLR 2020'
<EOS>
b'Table 1: Comparison of classi\xef\xac\x81cation accuracy when learning under uniform label noise on CIFAR-\n10 and CIFAR-100.'
<EOS>
b'Following previous works, we compare two evaluation scenarios: with a noisy\nvalidation set (top) and with 1000 clean validation samples (bottom).'
<EOS>
b'The best model is marked in\nbold.'
<EOS>
b'Having a small clean validation set improves the model but is not necessary.'
<EOS>
b'NOISE RATIO'
<EOS>
b'CIFAR-10'
<EOS>
b'CIFAR-100'
<EOS>
b'40% 60% 80% 40% 60% 80 %'
<EOS>
b'USING NOISY VALIDATION SET'
<EOS>
b'69.66\n70.64\n78.15'
<EOS>
b'86.06'
<EOS>
b'89.00'
<EOS>
b'89.00'
<EOS>
b'87.13\n87.62\n83.25\n81.85'
<EOS>
b'83.36\n85.34'
<EOS>
b'83.27'
<EOS>
b'93.70'
<EOS>
b'90.93'
<EOS>
b'78.00'
<EOS>
b'86.55\n86.92'
<EOS>
b'95.10\n\n-'
<EOS>
b'-\n-\n-\n-\n-\n82.54\n82.70\n74.96\n74.04'
<EOS>
b'72.84'
<EOS>
b'80.07\n74.39'
<EOS>
b'93.15'
<EOS>
b'87.58\n-'
<EOS>
b'-\n-\n93.77\n\n-\n-\n-\n-\n20.00\n49.00'
<EOS>
b'64.07'
<EOS>
b'67.92'
<EOS>
b'54.64'
<EOS>
b'29.22'
<EOS>
b'-\n53.81'
<EOS>
b'40.09'
<EOS>
b'69.91'
<EOS>
b'70.80\n-\n-'
<EOS>
b'-'
<EOS>
b'79.93'
<EOS>
b'51.34'
<EOS>
b'49.10\n-\n58.01'
<EOS>
b'61.00'
<EOS>
b'68.00'
<EOS>
b'61.77'
<EOS>
b'62.64'
<EOS>
b'31.05'
<EOS>
b'55.95\n52.01'
<EOS>
b'53.69'
<EOS>
b'52.88'
<EOS>
b'71.98'
<EOS>
b'68.20'
<EOS>
b'59.00'
<EOS>
b'58.34\n61.31'
<EOS>
b'74.76\n\n-'
<EOS>
b'-\n-\n-\n-\n-\n53.16'
<EOS>
b'54.04'
<EOS>
b'19.12'
<EOS>
b'47.98'
<EOS>
b'42.27\n41.47'
<EOS>
b'42.64'
<EOS>
b'66.21\n\n59.44'
<EOS>
b'-\n-\n-\n68.35\n\n-'
<EOS>
b'-\n-\n-\n13.00\n35.00'
<EOS>
b'29.16'
<EOS>
b'29.60'
<EOS>
b'08.90'
<EOS>
b'23.22'
<EOS>
b'15.00\n18.46'
<EOS>
b'42.09'
<EOS>
b'34.06'
<EOS>
b'-\n-\n-\n46.43'
<EOS>
b'REED-HARD (REED ET AL., 2014)'
<EOS>
b'S-MODEL (GOLDBERGER & BEN-REUVEN, 2016)'
<EOS>
b'OPEN'
<EOS>
b'-SET WANG ET AL.'
<EOS>
b'(2018)\nRAND.'
<EOS>
b'WEIGHTS (REN ET AL., 2018)'
<EOS>
b'BI-LEVEL-MODEL (JENNI & FAVARO, 2018)'
<EOS>
b'MENTORNET (JIANG ET AL., 2017)'
<EOS>
b'Lq'
<EOS>
b'(ZHANG & SABUNCU, 2018)'
<EOS>
b'TRUNC'
<EOS>
b'Lq'
<EOS>
b'(ZHANG & SABUNCU, 2018)'
<EOS>
b'FORWARD'
<EOS>
b'\xcb\x86T'
<EOS>
b'(PATRINI ET AL., 2017)'
<EOS>
b'CO-TEACHING (HAN ET AL., 2018B)'
<EOS>
b'D2L (MA ET AL., 2018)'
<EOS>
b'SL (WANG ET AL., 2019)\nJOINTOPT (TANAKA ET AL., 2018)\nSELF (OURS)\n\nDAC (THULASIDASAN ET AL., 2019)'
<EOS>
b'MENTORNET (JIANG ET AL., 2017)\nRAND.'
<EOS>
b'WEIGHTS (REN ET AL., 2018)'
<EOS>
b'REN ET AL (REN ET AL., 2018)'
<EOS>
b'SELF*'
<EOS>
b'(OURS)'
<EOS>
b'4 EVALUATION'
<EOS>
b'4.1 EXPERIMENTS DESCRIPTIONS'
<EOS>
b'4.1.1 STRUCTURE OF THE ANALYSIS'
<EOS>
b'USING CLEAN VALIDATION SET (1000 IMAGES)'
<EOS>
b'We evaluate our approach on CIFAR-10, CIFAR-100, an ImageNet-ILSVRC on different noise'
<EOS>
b'sce-\nnarios.'
<EOS>
b'For CIFAR-10, CIFAR-100, and ImageNet'
<EOS>
b', we consider the typical situation with symmetric\nand asymmetric label noise.'
<EOS>
b'In the case of the symmetric noise, a label is randomly \xef\xac\x82ipped to another\nclass with probability'
<EOS>
b'p.'
<EOS>
b'Following previous works, we also consider label \xef\xac\x82ips of semantically sim-'
<EOS>
b'ilar classes on CIFAR-10, and pair-wise label \xef\xac\x82ips on CIFAR-100.'
<EOS>
b'Finally, we perform studies on\nthe choice of the network architecture and the ablation of the components in our framework.'
<EOS>
b'Tab.'
<EOS>
b'6'
<EOS>
b'(Appendix) shows the in-deep analysis of semi-supervised learning strategies combined with recent\nworks.'
<EOS>
b'Overall, the proposed framework SELF outperforms all these combinations.'
<EOS>
b'4.1.2 COMPARISONS TO PREVIOUS WORKS'
<EOS>
b'We compare our work to previous methods from Reed-Hard'
<EOS>
b'(Reed et al., 2014), S-model (Gold-\nberger & Ben-Reuven, 2016), Wang et al.'
<EOS>
b'(2018)'
<EOS>
b', Rand.'
<EOS>
b'weights (Ren et al., 2018), Bi-level-model'
<EOS>
b'(Jenni & Favaro, 2018), D2L (Ma et al., 2018), SL (Wang et al., 2019), Lq (Zhang & Sabuncu,\n2018), Trunc Lq'
<EOS>
b'(Zhang & Sabuncu, 2018),'
<EOS>
b'Forward \xcb\x86T'
<EOS>
b'(Patrini et al., 2017), DAC (Thulasidasan\net al., 2019),'
<EOS>
b'Random reweighting (Ren et al., 2018), and Learning to reweight (Ren et al., 2018).'
<EOS>
b'For co-teaching (Han et al., 2018b), MentorNet (Jiang et al., 2017), JointOpt (Tanaka et al., 2018),\nthe source codes are available and hence used for evaluation.'
<EOS>
b'(Ren et al., 2018) and DAC (Thulasidasan et al., 2019) considered the setting of having a small clean\nvalidation set of 1000 and 5000 images respectively.'
<EOS>
b'For comparison purposes, we also experiment\nwith a small clean set of 1000 images additionally.'
<EOS>
b'Further, we abandon oracle experiments or\nmethods using additional information to keep the evaluation comparable.'
<EOS>
b'For instance, Forward T\n(Patrini et al., 2017) uses the true underlying confusion matrix to correct the loss.'
<EOS>
b'This information\nis neither known in typical scenarios nor used by other methods.'
<EOS>
b'6'
<EOS>
b'Published as a conference paper at ICLR 2020'
<EOS>
b'Table 2: Asymmetric noise on CIFAR-10, CIFAR-100.'
<EOS>
b'All methods use Resnet34.'
<EOS>
b'CIFAR-10: \xef\xac\x82ip\nTRUCK \xe2\x86\x92 AUTOMOBILE, BIRD \xe2\x86\x92 AIRPLANE, DEER'
<EOS>
b'\xe2\x86\x92 HORSE, CAT\xe2\x86\x94DOG with prob.'
<EOS>
b'p.'
<EOS>
b'CIFAR-100:'
<EOS>
b'\xef\xac\x82ip class'
<EOS>
b'i to ('
<EOS>
b'i + 1)%100 with prob.'
<EOS>
b'p. SELF retains high performances across all\nnoise ratios and outperforms all previous works.'
<EOS>
b'CIFAR-10'
<EOS>
b'NOISE RATIO'
<EOS>
b'10%\n\n20%\n\n30%\n\n40%\n\nCCE'
<EOS>
b'MAE'
<EOS>
b'FORWARD'
<EOS>
b'\xcb\x86T'
<EOS>
b'Lq'
<EOS>
b'TRUNC'
<EOS>
b'Lq'
<EOS>
b'SL\nJOINTOPT'
<EOS>
b'SELF (OURS)\n\n90.69'
<EOS>
b'82.61'
<EOS>
b'90.52'
<EOS>
b'90.91'
<EOS>
b'90.43'
<EOS>
b'88.24'
<EOS>
b'90.12'
<EOS>
b'93.75'
<EOS>
b'88.59'
<EOS>
b'52.93\n89.09\n89.33\n89.45\n85.36'
<EOS>
b'89.45\n92.76'
<EOS>
b'86.14\n50.36\n86.79\n85.45'
<EOS>
b'87.10\n80.64'
<EOS>
b'87.18'
<EOS>
b'92.42'
<EOS>
b'80.11'
<EOS>
b'45.52'
<EOS>
b'83.55\n76.74'
<EOS>
b'82.28'
<EOS>
b'-'
<EOS>
b'87.97'
<EOS>
b'89.07\n\n10%'
<EOS>
b'66.54'
<EOS>
b'13.38'
<EOS>
b'45.96\n68.36'
<EOS>
b'68.86'
<EOS>
b'65.58'
<EOS>
b'69.61'
<EOS>
b'72.45'
<EOS>
b'CIFAR-100'
<EOS>
b'20%\n\n30%\n\n59.20'
<EOS>
b'11.50\n42.46'
<EOS>
b'66.59'
<EOS>
b'66.59'
<EOS>
b'65.14'
<EOS>
b'68.94'
<EOS>
b'70.53'
<EOS>
b'51.40'
<EOS>
b'08.91'
<EOS>
b'38.13'
<EOS>
b'61.45'
<EOS>
b'61.87'
<EOS>
b'63.10\n63.99\n65.09'
<EOS>
b'40%\n\n42.74'
<EOS>
b'08.20'
<EOS>
b'34.44\n47.22'
<EOS>
b'47.66\n\n-\n\n53.71'
<EOS>
b'53.83'
<EOS>
b'Whenever possible, we adopt the reported performance from the corresponding publications.'
<EOS>
b'The\ntesting scenarios are kept as similar as possible to enable a fair comparison.'
<EOS>
b'All tested scenarios use\na noisy validation set with the same noise distribution as the training set unless stated otherwise.'
<EOS>
b'All\nmodel performances are reported on the clean test set.'
<EOS>
b'Table 3: Effect of the choice of network architecture on classi\xef\xac\x81cation accuracy on CIFAR-10 &\n-100 with uniform label noise.'
<EOS>
b'SELF is compatible with all tested architectures.'
<EOS>
b'Here * represents\nbaseline accuracy of the architectures that are trained on fully supervised setting at 0% label noise.'
<EOS>
b'CIFAR-10\n\nCIFAR-100'
<EOS>
b'CIFAR-10'
<EOS>
b'CIFAR-100'
<EOS>
b'RESNET101'
<EOS>
b'93.89'
<EOS>
b'*'
<EOS>
b'81.14'
<EOS>
b'*'
<EOS>
b'RESNET34'
<EOS>
b'93.5'
<EOS>
b'*'
<EOS>
b'76.76'
<EOS>
b'*'
<EOS>
b'NOISE\n\n40% 80% 40% 80%\n\nNOISE\n\n40% 80% 40% 80%\n\nMENTORNET'
<EOS>
b'CO-T.\nSELF'
<EOS>
b'89.00'
<EOS>
b'62.58'
<EOS>
b'92.77'
<EOS>
b'49.00\n21.79'
<EOS>
b'64.52'
<EOS>
b'68.00\n39.58'
<EOS>
b'69.00'
<EOS>
b'35.00\n16.79'
<EOS>
b'39.73'
<EOS>
b'Lq'
<EOS>
b'TRUNC'
<EOS>
b'Lq'
<EOS>
b'FORWARD'
<EOS>
b'\xcb\x86T'
<EOS>
b'SELF'
<EOS>
b'87.13\n87.62\n83.25'
<EOS>
b'91.13'
<EOS>
b'64.07'
<EOS>
b'67.92\n54.64'
<EOS>
b'63.59'
<EOS>
b'61.77'
<EOS>
b'62.64\n31.05'
<EOS>
b'66.71'
<EOS>
b'29.16\n29.60\n8.90'
<EOS>
b'35.56'
<EOS>
b'WRN 28'
<EOS>
b'-10'
<EOS>
b'96.21'
<EOS>
b'*'
<EOS>
b'81.02'
<EOS>
b'*'
<EOS>
b'NOISE\n\n40% 80% 40% 80%\n\nMENTORNET'
<EOS>
b'REWEIGHT'
<EOS>
b'SELF\n\n88.7'
<EOS>
b'86.02'
<EOS>
b'93.34'
<EOS>
b'46.30'
<EOS>
b'-'
<EOS>
b'67.41'
<EOS>
b'67.50\n58.01'
<EOS>
b'72.48'
<EOS>
b'30.10'
<EOS>
b'-\n42.06'
<EOS>
b'RESNET26'
<EOS>
b'96.37'
<EOS>
b'*'
<EOS>
b'81.20'
<EOS>
b'*'
<EOS>
b'NOISE'
<EOS>
b'CO-T.\nSELF'
<EOS>
b'40% 80% 40% 80%\n\n81.85'
<EOS>
b'93.70'
<EOS>
b'29.22'
<EOS>
b'69.91'
<EOS>
b'55.95'
<EOS>
b'71.98'
<EOS>
b'23.22\n42.09\n\n4.1.3 NETWORKS CONFIGURATION AND TRAINING'
<EOS>
b'For the basic training of self-ensemble model, we use the Mean Teacher model (Tarvainen &\nValpola, 2017) available on GitHub 1 .'
<EOS>
b'The students and teacher networks are residual networks'
<EOS>
b'(He\net al., 2016) with 26 layers with Shake-Shake-regularization (Gastaldi, 2017).'
<EOS>
b'We use the Py-'
<EOS>
b'Torch'
<EOS>
b'(Paszke et al., 2017) implementation of the network and keep the training settings close\nto (Tarvainen & Valpola, 2017).'
<EOS>
b'The network is trained with Stochastic Gradient Descent.'
<EOS>
b'In each\n\xef\xac\x81ltering iteration, the model is trained for a maximum of 300 epochs, with patience of 50 epochs.'
<EOS>
b'For more training details, see the appendix.'
<EOS>
b'4.2 EXPERIMENTS RESULTS\n\n4.2.1'
<EOS>
b'SYMMETRIC LABEL NOISE\n\nCIFAR-10 and 100 Results for typical uniform noise scenarios with noise ratios on CIFAR-10\nand CIFAR-100 are shown in Tab.'
<EOS>
b'1.'
<EOS>
b'More results are visualized in Fig.'
<EOS>
b'1a (CIFAR-10) and Fig.'
<EOS>
b'1b\n(CIFAR-100).'
<EOS>
b'Our approach SELF performs robustly in the case of lower noise ratios with up to 60%\nand outperforms previous works.'
<EOS>
b'Although a strong performance loss occurs at 80% label noise,\n\n1https://github.com/CuriousAI/mean-teacher'
<EOS>
b'7'
<EOS>
b'Published as a conference paper at ICLR 2020'
<EOS>
b'Table 4:'
<EOS>
b'Classi\xef\xac\x81cation accuracy on clean'
<EOS>
b'ImageNet validation dataset.'
<EOS>
b'The mod-\nels are trained at 40% label noise and the\nbest model is picked based on the evalu-\nation on noisy validation data.'
<EOS>
b'Mentornet\nshows the best previously reported results.'
<EOS>
b'Mentornet* is based on Resnet-101.'
<EOS>
b'We\nchose the smaller Resnext50 model to re-'
<EOS>
b'duce the run-time.'
<EOS>
b'Accurracy'
<EOS>
b'Mentornet'
<EOS>
b'*'
<EOS>
b'ResNext'
<EOS>
b'Mean-T.\nSELF (Ours)'
<EOS>
b'Resnext18 Resnext50'
<EOS>
b'P@1 P@5'
<EOS>
b'P@1 P@5'
<EOS>
b'-'
<EOS>
b'65.10 85.90\n-\n50.6'
<EOS>
b'75.99 56.25 80.90'
<EOS>
b'58.04 81.82 62.96 85.72'
<EOS>
b'66.92 86.65 71.31 89.92\n\nTable 5:'
<EOS>
b'Ablation study on CIFAR-10 and CIFAR-\n100.'
<EOS>
b'The Resnet baseline was trained on the full\nnoisy label set.'
<EOS>
b'Adding progressive \xef\xac\x81ltering im-'
<EOS>
b'proves over this baseline.'
<EOS>
b'The Mean Teacher main-'
<EOS>
b'tains an ensemble of model snapshots, which helps\ncounteract noise.'
<EOS>
b'Having progressive \xef\xac\x81ltering and\nmodel ensembles (-MVA-pred.) makes the model\nmore robust but still fails at 80% noise.'
<EOS>
b'The full\nSELF framework additionally uses the prediction\nensemble for detection of correct labels.'
<EOS>
b'NOISE RATIO'
<EOS>
b'RESNET26'
<EOS>
b'FILTERING'
<EOS>
b'MEAN-T.\n- MVA-PRED.\nSELF (OURS)\n\nCIFAR-10\n\nCIFAR-100'
<EOS>
b'40% 80% 40% 80%\n\n83.20\n87.35'
<EOS>
b'93.70'
<EOS>
b'93.77'
<EOS>
b'93.70\n\n41.37'
<EOS>
b'49.58'
<EOS>
b'52.50'
<EOS>
b'57.40'
<EOS>
b'69.91'
<EOS>
b'53.18'
<EOS>
b'61.40'
<EOS>
b'65.85'
<EOS>
b'71.69'
<EOS>
b'71.98'
<EOS>
b'19.92'
<EOS>
b'23.42\n26.31'
<EOS>
b'38,61\n42.09\n\nSELF still outperforms most of the previous approaches.'
<EOS>
b'The experiment SELF*'
<EOS>
b'using a 1000 clean\nvalidation images shows that the performance loss mostly originates from the progressive \xef\xac\x81ltering\nrelying too strongly on the extremely noisy validation set.'
<EOS>
b'ImageNet-ILSVRC Tab.'
<EOS>
b'4 shows the precision@1 and @5 of various models, given 40% label\nnoise in the training set.'
<EOS>
b'Our networks are based on ResNext18 and Resnext50.'
<EOS>
b'Note that MentorNet\n(Jiang et al., 2017) uses Resnet101 (P@1: 78.25)'
<EOS>
b'(Goyal et al., 2017), which has higher performance\ncompared to Resnext50'
<EOS>
b'(P@1: 77.8)'
<EOS>
b'(Xie et al., 2017) on the standard ImageNet validation set.'
<EOS>
b'Despite the weaker model, SELF (ResNext50) surpasses the best previously reported results by\nmore than 5% absolute improvement.'
<EOS>
b'Even the signi\xef\xac\x81cantly weaker model ResNext18 outperforms\nMentorNet, which is based on a more powerful ResNet101 network.'
<EOS>
b'4.2.2'
<EOS>
b'ASYMMETRIC LABEL NOISE'
<EOS>
b'Tab.'
<EOS>
b'2 shows more challenging noise scenarios when the noise is not class-symmetric and uniform.'
<EOS>
b'Concretely, labels are \xef\xac\x82ipped among semantically similar classes such as CAT and DOG on CIFAR-\n10.'
<EOS>
b'On CIFAR-100, each label is \xef\xac\x82ipped to the next class with a probability'
<EOS>
b'p.'
<EOS>
b'In these scenarios, our\nframework SELF also retains high performance and only shows a small performance drop at 40%\nnoise.'
<EOS>
b'The high label noise resistance of our framework indicates that the proposed self-ensemble\n\xef\xac\x81ltering process helps the network identify correct samples, even under extreme noise ratios.'
<EOS>
b'4.2.3 EFFECTS OF DIFFERENT ARCHITECTURES'
<EOS>
b'Previous works utilize a various set of different architectures, which hinders a fair comparison.\nTab.'
<EOS>
b'3 shows the performance of our framework SELF compared to previous approaches.'
<EOS>
b'SELF'
<EOS>
b'outperforms other works in all scenarios except for CIFAR-10 with 80% noise.'
<EOS>
b'Typical robust'
<EOS>
b'learning approaches lead to signi\xef\xac\x81cant accuracy losses at 40% noise, while SELF still retains high\nperformance.'
<EOS>
b'Further, note that SELF allows the network\xe2\x80\x99s performance to remain consistent across\nthe different underlying architectures.'
<EOS>
b'4.2.4 ABLATION STUDY\nTab.'
<EOS>
b'5 shows the importance of each component in our framework.'
<EOS>
b'See Fig.'
<EOS>
b'4a, Fig.'
<EOS>
b'4b for experi-'
<EOS>
b'ments on more noise ratios.'
<EOS>
b'As expected, the Resnet-baseline rapidly breaks down with increasing\nnoise ratios.'
<EOS>
b'Adding self-supervised \xef\xac\x81ltering increases the performance slightly in lower noise ratios.'
<EOS>
b'However, the model has to rely on extremely noisy snapshots.'
<EOS>
b'Contrary, using a model ensemble\nalone such as in Mean-Teacher can counteract noise on the noisy dataset CIFAR-10.'
<EOS>
b'On the more\nchallenging CIFAR-100, however, the performance decreases strongly.'
<EOS>
b'With self-supervised \xef\xac\x81lter-'
<EOS>
b'ing and model ensembles, SELF (without MVA-pred) is more robust and only impairs performance\nat 80% noise.'
<EOS>
b'The last performance boost is given by using moving-average predictions so that the\nnetwork can reliably detect correctly labeled samples gradually.'
<EOS>
b'Fig. 4 shows the ablation experiments on more noise ratios.'
<EOS>
b'The analyses shows that each component\nin SELF is essential for the model to learn robustly.'
<EOS>
b'8'
<EOS>
b'Published as a conference paper at ICLR 2020'
<EOS>
b'(a) Ablation exps.'
<EOS>
b'on CIFAR-10\n\n(b) Ablation exps.'
<EOS>
b'on CIFAR-100'
<EOS>
b'Figure 4:'
<EOS>
b'Ablation study on the importance of the components in our framework SELF, evaluated\non (a) Cifar-10 and (b) Cifar-100 with uniform noise.'
<EOS>
b'Please refer Tab.'
<EOS>
b'5 for details of components.'
<EOS>
b'Table 6:'
<EOS>
b'Analysis of semi-supervised learning (SSL) strategies: entropy learning, mean-teacher'
<EOS>
b'combined with recent works.'
<EOS>
b'Our progressive \xef\xac\x81ltering strategy is shown to be effective and per-'
<EOS>
b'forms well regardless of the choice of the semi-supervised learning backbone.'
<EOS>
b'Overall, the proposed\nmethod SELF outperforms all these combinations.'
<EOS>
b'Best model in each SSL-category is marked\nin bold.'
<EOS>
b'Running mean-teacher+ co-teaching using the same con\xef\xac\x81guration is not possible due to\nmemory constraints.'
<EOS>
b'NOISE RATIO'
<EOS>
b'CIFAR-10'
<EOS>
b'CIFAR-100'
<EOS>
b'40%\n\n60%\n\n80%'
<EOS>
b'40%\n\n60%\n\n80%'
<EOS>
b'BASELINE MODELS'
<EOS>
b'RESNET26'
<EOS>
b'(GASTALDI, 2017)'
<EOS>
b'CO-TEACHING (HAN ET AL., 2018B)'
<EOS>
b'JOINTOPT (TANAKA ET AL., 2018)'
<EOS>
b'PROGRESSIVE FILTERING (OURS)'
<EOS>
b'ENTROPY'
<EOS>
b'ENTROPY + CO-TEACHING'
<EOS>
b'ENTROPY + JOINT-OPT'
<EOS>
b'ENTROPY+FILTERING (OURS)\n\n83.20'
<EOS>
b'81.85'
<EOS>
b'83.27\n87.35'
<EOS>
b'79.13\n84.94'
<EOS>
b'84.44'
<EOS>
b'90.04'
<EOS>
b'72.35'
<EOS>
b'74.04\n74.39'
<EOS>
b'75.47\n\n85.98'
<EOS>
b'74.28'
<EOS>
b'75.86'
<EOS>
b'83.88'
<EOS>
b'41.37'
<EOS>
b'29.22\n40.09'
<EOS>
b'49.58'
<EOS>
b'46.93'
<EOS>
b'35.16\n39.16'
<EOS>
b'52.46\n\n53.18'
<EOS>
b'55.95\n52.88'
<EOS>
b'61.40'
<EOS>
b'54.65'
<EOS>
b'55.68'
<EOS>
b'56.73'
<EOS>
b'59.97'
<EOS>
b'44.31\n47.98'
<EOS>
b'42.64'
<EOS>
b'50.60'
<EOS>
b'41.34'
<EOS>
b'43.52'
<EOS>
b'43.27'
<EOS>
b'46.45'
<EOS>
b'19.92'
<EOS>
b'23.22'
<EOS>
b'18.46'
<EOS>
b'23.42'
<EOS>
b'21.29'
<EOS>
b'20.5\n17.24\n23.53\n\nSEMI-SUPERVISED LEARNING WITH ENTROPY LEARNING\n\nSEMI-SUPERVISED LEARNING WITH MEAN-TEACHER'
<EOS>
b'MEAN'
<EOS>
b'TEACHER\nMEAN-TEACHER +'
<EOS>
b'JOINTOPT'
<EOS>
b'MEAN-TEACHER +'
<EOS>
b'FILTERING - SELF (OURS)'
<EOS>
b'93.70'
<EOS>
b'91.40'
<EOS>
b'93.70'
<EOS>
b'90.40'
<EOS>
b'83.62'
<EOS>
b'92.85'
<EOS>
b'52.5'
<EOS>
b'45.12'
<EOS>
b'69.91'
<EOS>
b'65.85'
<EOS>
b'60.09\n71.98'
<EOS>
b'54.48'
<EOS>
b'45.92'
<EOS>
b'66.21\n\n26.31\n23.54\n42.58'
<EOS>
b'4.2.5 SEMI-SUPERVISED LEARNING FOR PROGRESSIVE FILTERING'
<EOS>
b'Tab.'
<EOS>
b'6 shows different semi-supervised learning strategies: entropy learning, mean-teacher com-\nbined with recent works.'
<EOS>
b'Note that Co-Teaching+Mean-Teacher cannot be implemented and run in\nthe same con\xef\xac\x81guration as other experiments, due to memory constraints.'
<EOS>
b'The analysis indicates the semi-supervised losses mostly stabilize the baselines, compared to the\nmodel without semi-supervised learning.'
<EOS>
b'However, Co-teaching and JointOpt sometimes perform\nworse than the purely semi-supervised model.'
<EOS>
b'This result indicates that their proposed frameworks\nare not always compatible with semi-supervised losses.'
<EOS>
b'The progressive \xef\xac\x81ltering technique is seamlessly compatible with different semi-supervised losses.'
<EOS>
b'The \xef\xac\x81ltering outperforms its counterparts when combined with Entropy Learning or Mean-teacher\nmodel.'
<EOS>
b'Overall, SELF outperforms all considered combinations.'
<EOS>
b'9'
<EOS>
b'Published as a conference paper at ICLR 2020'
<EOS>
b'5 CONCLUSION'
<EOS>
b'We propose a simple and easy to implement a framework to train robust deep learning models under\nincorrect or noisy labels.'
<EOS>
b'We \xef\xac\x81lter out the training samples that are hard to learn (possibly noisy\nlabeled samples) by leveraging ensemble of predictions of the single network\xe2\x80\x99s output over different\ntraining epochs.'
<EOS>
b'Subsequently, we allow clean supervision from the non-hard samples and further\nleverage additional unsupervised loss from the entire dataset.'
<EOS>
b'We show that our framework results in\nDNN models with superior generalization performance on CIFAR-10, CIFAR-100 & ImageNet and\noutperforms all previous works under symmetric (uniform) and asymmetric noises.'
<EOS>
b'Furthermore,\nour models remain robust despite the increasing noise ratio and change in network architectures.'
<EOS>
b'REFERENCES'
<EOS>
b'Samaneh Azadi, Jiashi Feng, Stefanie Jegelka, and Trevor Darrell.'
<EOS>
b'Auxiliary image regularization\n\nfor deep cnns with noisy labels.'
<EOS>
b'arXiv preprint arXiv:1511.07069, 2015.'
<EOS>
b'Beno\xcb\x86\xc4\xb1t Fr\xc2\xb4enay and Michel Verleysen.'
<EOS>
b'Classi\xef\xac\x81cation in the presence of label noise: a survey.'
<EOS>
b'IEEE'
<EOS>
b'transactions on neural networks and learning systems, 25(5):845\xe2\x80\x93869, 2013.'
<EOS>
b'Xavier Gastaldi.'
<EOS>
b'Shake-shake regularization.'
<EOS>
b'arXiv preprint arXiv:1705.07485, 2017.'
<EOS>
b'Jacob Goldberger and Ehud Ben-Reuven.'
<EOS>
b'Training deep neural-networks using a noise adaptation\n\nlayer.'
<EOS>
b'2016.'
<EOS>
b'Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,\n\nAaron Courville, and Yoshua Bengio.'
<EOS>
b'Generative Adversarial Nets.'
<EOS>
b'pp. 9.'
<EOS>
b'Priya Goyal, Piotr Doll\xc2\xb4ar, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola,'
<EOS>
b'An-'
<EOS>
b'drew Tulloch, Yangqing Jia, and Kaiming He.'
<EOS>
b'Accurate, large minibatch sgd:'
<EOS>
b'Training imagenet\nin 1 hour.'
<EOS>
b'arXiv preprint arXiv:1706.02677, 2017.'
<EOS>
b'Bo Han, Jiangchao Yao, Gang Niu, Mingyuan Zhou, Ivor Tsang, Ya Zhang, and Masashi Sugiyama.\nMasking:'
<EOS>
b'A new perspective of noisy supervision.'
<EOS>
b'In Advances in Neural Information Processing\nSystems, pp.'
<EOS>
b'5836\xe2\x80\x935846, 2018a.'
<EOS>
b'Bo Han, Quanming Yao, Xingrui Yu, Gang Niu, Miao Xu, Weihua Hu, Ivor Tsang, and Masashi\nSugiyama.'
<EOS>
b'Co-teaching:'
<EOS>
b'Robust training of deep neural networks with extremely noisy labels.'
<EOS>
b'In\nNeurIPS, pp.'
<EOS>
b'8535\xe2\x80\x938545, 2018b.'
<EOS>
b'Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.'
<EOS>
b'Deep residual learning for image recog-\nnition.'
<EOS>
b'In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.\n770\xe2\x80\x93778, 2016.'
<EOS>
b'Simon Jenni and Paolo Favaro.'
<EOS>
b'Deep bilevel learning.'
<EOS>
b'In ECCV, 2018.'
<EOS>
b'Lu Jiang, Zhengyuan Zhou, Thomas Leung, Li-Jia Li, and Li Fei-Fei.'
<EOS>
b'MentorNet:'
<EOS>
b'Learning Data-'
<EOS>
b'Driven Curriculum for Very Deep Neural Networks on Corrupted Labels.'
<EOS>
b'arXiv:1712.05055'
<EOS>
b'[cs],\nDecember 2017.'
<EOS>
b'URL http://arxiv.org/abs/1712.05055.'
<EOS>
b'arXiv: 1712.05055.'
<EOS>
b'Samuli Laine and Timo Aila.'
<EOS>
b'Temporal ensembling for semi-supervised learning.'
<EOS>
b'arXiv preprint'
<EOS>
b'arXiv:1610.02242, 2016.'
<EOS>
b'Ilya Loshchilov and Frank Hutter.'
<EOS>
b'Sgdr:'
<EOS>
b'Stochastic gradient descent with warm restarts.'
<EOS>
b'arXiv'
<EOS>
b'preprint arXiv:1608.03983, 2016.'
<EOS>
b'Yucen Luo, Jun Zhu, Mengxi Li, Yong Ren, and Bo Zhang.'
<EOS>
b'Smooth neighbors on teacher graphs\nfor semi-supervised learning.'
<EOS>
b'In Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition, pp. 8896\xe2\x80\x938905, 2018.'
<EOS>
b'Xingjun Ma, Yisen Wang, Michael E Houle, Shuo Zhou, Sarah M Erfani, Shu-Tao Xia, Sudan-\nthi Wijewickrema, and James Bailey.'
<EOS>
b'Dimensionality-driven learning with noisy labels.'
<EOS>
b'arXiv'
<EOS>
b'preprint'
<EOS>
b'arXiv:1806.02612, 2018.'
<EOS>
b'10'
<EOS>
b'Published as a conference paper at ICLR 2020'
<EOS>
b'Duc Tam Nguyen, Zhongyu Lou, Michael Klar, and Thomas Brox.'
<EOS>
b'Anomaly detection with\nmultiple-hypotheses predictions.'
<EOS>
b'In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Pro-\nceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings\nof Machine Learning Research, pp.'
<EOS>
b'4800\xe2\x80\x934809, Long Beach, California, USA, 09\xe2\x80\x9315 Jun 2019a.\nPMLR.'
<EOS>
b'URL http://proceedings.mlr.press/v97/nguyen19b.html.'
<EOS>
b'Tam Nguyen, Maximilian Dax, Chaithanya Kumar Mummadi, Nhung Ngo, Thi Hoai Phuong\nNguyen, Zhongyu Lou, and Thomas Brox.'
<EOS>
b'Deepusps:'
<EOS>
b'Deep robust unsupervised saliency predic-\ntion via self-supervision.'
<EOS>
b'In Advances in Neural Information Processing Systems, pp.'
<EOS>
b'204\xe2\x80\x93214,\n2019b.'
<EOS>
b'Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito,'
<EOS>
b'Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer.'
<EOS>
b'Automatic differentiation in\npytorch.'
<EOS>
b'2017.'
<EOS>
b'Giorgio Patrini, Alessandro Rozza, Aditya Krishna Menon, Richard Nock, and Lizhen Qu.'
<EOS>
b'Making\ndeep neural networks robust to label noise: A loss correction approach.'
<EOS>
b'In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recognition, pp. 1944\xe2\x80\x931952, 2017.'
<EOS>
b'Scott Reed, Honglak Lee, Dragomir Anguelov, Christian Szegedy, Dumitru Erhan, and Andrew'
<EOS>
b'Rabinovich.'
<EOS>
b'Training deep neural networks on noisy labels with bootstrapping.'
<EOS>
b'arXiv preprint'
<EOS>
b'arXiv:1412.6596, 2014.'
<EOS>
b'Mengye Ren, Wenyuan Zeng, Bin Yang, and Raquel Urtasun.'
<EOS>
b'Learning to Reweight Examples for\nRobust Deep Learning.'
<EOS>
b'arXiv:1803.09050'
<EOS>
b'[cs, stat], March 2018.'
<EOS>
b'URL http://arxiv.org/'
<EOS>
b'abs/1803.09050.'
<EOS>
b'arXiv: 1803.09050.'
<EOS>
b'David Rolnick, Andreas Veit, Serge Belongie, and Nir Shavit.'
<EOS>
b'Deep learning is robust to massive\n\nlabel noise.'
<EOS>
b'arXiv preprint arXiv:1705.10694, 2017.'
<EOS>
b'Ilya Sutskever, James Martens, George E Dahl, and Geoffrey E Hinton.'
<EOS>
b'On the importance of\n\ninitialization and momentum in deep learning.'
<EOS>
b'ICML (3), 28(1139-1147):5, 2013.'
<EOS>
b'Daiki Tanaka, Daiki Ikami, Toshihiko Yamasaki, and Kiyoharu Aizawa.'
<EOS>
b'Joint optimization'
<EOS>
b'frame-'
<EOS>
b'work for learning with noisy labels.'
<EOS>
b'In Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, pp.'
<EOS>
b'5552\xe2\x80\x935560, 2018.'
<EOS>
b'Antti Tarvainen and Harri Valpola.'
<EOS>
b'Mean teachers are better role models:'
<EOS>
b'Weight-averaged consis-'
<EOS>
b'tency targets improve semi-supervised deep learning results.'
<EOS>
b'In Advances in neural information\nprocessing systems, pp. 1195\xe2\x80\x931204, 2017.'
<EOS>
b'Sunil Thulasidasan, Tanmoy Bhattacharya, Jeff Bilmes, Gopinath Chennupati, and Jamal'
<EOS>
b'Mohd-'
<EOS>
b'Yusof.'
<EOS>
b'Combating label noise in deep learning using abstention.'
<EOS>
b'arXiv preprint arXiv:1905.10964,\n2019.'
<EOS>
b'Yisen Wang, Weiyang Liu, Xingjun Ma, James Bailey, Hongyuan Zha, Le Song, and Shu-Tao Xia.'
<EOS>
b'Iterative learning with open-set noisy labels.'
<EOS>
b'arXiv preprint arXiv:1804.00092, 2018.'
<EOS>
b'Yisen Wang, Xingjun Ma, Zaiyi Chen, Yuan Luo, Jinfeng Yi, and James Bailey.'
<EOS>
b'Symmetric cross en-'
<EOS>
b'tropy for robust learning with noisy labels.'
<EOS>
b'In Proceedings of the IEEE International Conference\non Computer Vision, pp. 322\xe2\x80\x93330, 2019.'
<EOS>
b'Saining Xie, Ross Girshick, Piotr Doll\xc2\xb4ar, Zhuowen Tu, and Kaiming He.'
<EOS>
b'Aggregated residual trans-\nformations for deep neural networks.'
<EOS>
b'In Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, pp. 1492\xe2\x80\x931500, 2017.'
<EOS>
b'Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals.'
<EOS>
b'Understanding\ndeep learning requires rethinking generalization.'
<EOS>
b'In International Conference on Learning Rep-\nresentations, 2017.'
<EOS>
b'URL https://openreview.net/forum?id=Sy8gdB9xx.'
<EOS>
b'Zhilu Zhang and Mert Sabuncu.'
<EOS>
b'Generalized cross entropy loss for training deep neural networks\nwith noisy labels.'
<EOS>
b'In Advances in Neural Information Processing Systems, pp. 8778\xe2\x80\x938788, 2018.'
<EOS>
b'11'
<EOS>
b'Published as a conference paper at ICLR 2020'
<EOS>
b'A APPENDIX'
<EOS>
b'A.1 MEAN TEACHER MODEL FOR ITERATIVE FILTERING'
<EOS>
b'We apply the Mean Teacher algorithm in each iteration i in the train(Df ilter, Dval) procedure as\nfollows.'
<EOS>
b'\xe2\x80\xa2 Input: examples with potentially clean labels'
<EOS>
b'Df ilter from the \xef\xac\x81ltering procedure.'
<EOS>
b'In the\n\nbeginning (i = 0), here Df ilter refers to entire labeled dataset.'
<EOS>
b'\xe2\x80\xa2 Initialize a supervised neural network as the student model'
<EOS>
b'M s'
<EOS>
b'i .'
<EOS>
b'\xe2\x80\xa2 Initialize the Mean Teacher model'
<EOS>
b'M t'
<EOS>
b'i as a copy of the student model with all weights\n\ndetached.'
<EOS>
b'\xe2\x80\xa2'
<EOS>
b'Let the loss function be the sum of normal classi\xef\xac\x81cation loss of M s'
<EOS>
b'i and the consistency\n\nloss between the outputs of M t'
<EOS>
b'i and M t'
<EOS>
b'i'
<EOS>
b'\xe2\x80\xa2'
<EOS>
b'Select an optimizer'
<EOS>
b'\xe2\x80\xa2'
<EOS>
b'In each training iteration:\n\n\xe2\x80\x93 Update the weights of M s\n\xe2\x80\x93 Update the weights of M t\n\xe2\x80\x93'
<EOS>
b'Evaluate performance of M s'
<EOS>
b'i using the selected optimizer'
<EOS>
b'i as an exponential moving-average of the student weights'
<EOS>
b'i over Dval to verify the early stopping criteria.'
<EOS>
b'i and M t'
<EOS>
b'\xe2\x80\xa2 Return the best M t'
<EOS>
b'i'
<EOS>
b'A.2'
<EOS>
b'ASSUMPTIONS DICUSSIONS'
<EOS>
b'Our method performs best when the following assumptions are hold.'
<EOS>
b'Natural robustness assumption of deep networks\n(Rolnick et al., 2017):'
<EOS>
b'The networks attempt\nto learn the easiest way to explain most of the data.'
<EOS>
b'SELF uses this assumption to kickstart the\nlearning process.'
<EOS>
b'Correct samples dominate over wrongly labeled samples At 80% noise on CIFAR-10, the cor-\nrectly labeled cats (20% out of all cat images) still dominates over samples wrongly labeled as cat\n(8.8% for each class).'
<EOS>
b'Independence results in less over\xef\xac\x81tting SELF performs best if the noises on the validation set\nand training set are i.i.d. .'
<EOS>
b'SELF uses the validation data for early stopping.'
<EOS>
b'Hence, a high correlation\nof label noise between train and valid increases the chance of model over\xef\xac\x81tting.'
<EOS>
b'Suf\xef\xac\x81cient label randomness assumption'
<EOS>
b'The subset of all correctly labeled samples capture all\nsamples clusters.'
<EOS>
b'In fact, many works from the active learning literature show that less than 100\n% of the labeled samples are required to achieve the highest model performance.'
<EOS>
b'SELF performs\nprogressive expansion of the correct labels sets.'
<EOS>
b'At larger noise ratios, not all clusters are covered\nby the identi\xef\xac\x81ed samples.'
<EOS>
b'Therefore on task containing many classes, e.g., CIFAR-100, the model'
<EOS>
b'performance decreases faster than on CIFAR-10.'
<EOS>
b'The model performance reduces when these assumptions are strongly violated.'
<EOS>
b'Each assumption\nshould have its own \xe2\x80\x9dcritical\xe2\x80\x9d threshold for violation.'
<EOS>
b'A future in-depth analysis to challenge the\nassumptions is an interesting future research direction.'
<EOS>
b'A.3'
<EOS>
b'TRAINING DETAILS'
<EOS>
b'A.3.1 CIFAR-10 AND CIFAR-100'
<EOS>
b'Dataset Tab.'
<EOS>
b'7 shows the details of CIFAR-10 and 100 datasets in our evaluation pipeline.'
<EOS>
b'The\nvalidation set is contaminated with the same noise ratio as the training data unless stated otherwise.'
<EOS>
b'12'
<EOS>
b'Published as a conference paper at ICLR 2020'
<EOS>
b'Network training For the training our model SELF, we use the standard con\xef\xac\x81guration provided\nby Tarvainen & Valpola (2017)'
<EOS>
b'2.'
<EOS>
b'Concretely, we use the SGD-optimizer with Nesterov Sutskever'
<EOS>
b'et al.'
<EOS>
b'(2013) momentum, a learning rate of 0.05 with cosine learning rate annealing Loshchilov &\nHutter (2016), a weight decay of 2e-4, max iteration per \xef\xac\x81ltering step of 300, patience of 50 epochs,\ntotal epochs count of 600.'
<EOS>
b'Table 7:'
<EOS>
b'Dataset description.'
<EOS>
b'Classi\xef\xac\x81cation tasks on CIFAR-10 and CIFAR-100 with uniform noise.'
<EOS>
b'Note that the noise on the training and validation set is not correlated.'
<EOS>
b'Hence, maximizing the\naccuracy on the noisy set provides a useful (but noisy) estimate for the generalization ability on\nunseen test data.'
<EOS>
b'TASK'
<EOS>
b'RESOLUTION\n\nDATA'
<EOS>
b'TYPE'
<EOS>
b'CIFAR-10'
<EOS>
b'CIFAR-100'
<EOS>
b'CLASSIFICATION'
<EOS>
b'100-WAY'
<EOS>
b'TRAIN (NOISY)'
<EOS>
b'VALID (NOISY)'
<EOS>
b'TEST (CLEAN)\n\n10-WAY'
<EOS>
b'32X32'
<EOS>
b'45000\n5000'
<EOS>
b'10000\n\n45000'
<EOS>
b'5000'
<EOS>
b'10000'
<EOS>
b'For basic training of baselines models without semi-supervised learning, we had to set the learning\nrate to 0.01.'
<EOS>
b'In the case of higher learning rates, the loss typically explodes.'
<EOS>
b'Every other option is\nkept the same.'
<EOS>
b'Semi-supervised learning For the mean teacher training'
<EOS>
b', additional hyperparameters are required.'
<EOS>
b'In both cases of CIFAR-10 and CIFAR-100, we again take the standard con\xef\xac\x81guration with the con-\nsistency loss to mean-squared-error and a consistency weight: 100.0, logit distance cost: 0.01,\nconsistency-ramp-up:5.'
<EOS>
b'The total batch-size is 512, with 124 samples being reserved for labeled\nsamples, 388 for unlabeled data.'
<EOS>
b'Each epoch is de\xef\xac\x81ned as a complete processing of all unlabeled\ndata.'
<EOS>
b'When training without semi-supervised-learning, the entire batch is used for labeled data.'
<EOS>
b'Data augmentation'
<EOS>
b'The data are normalized to zero-mean and standard-variance of one.'
<EOS>
b'Further,\nwe use real-time data augmentation with random translation and re\xef\xac\x82ection, subsequently random'
<EOS>
b'horizontal \xef\xac\x82ip.'
<EOS>
b'The standard PyTorch-library provides these transformations.'
<EOS>
b'A.3.2\n\nIMAGENET'
<EOS>
b'-ILSVRC-2015'
<EOS>
b'Network Training'
<EOS>
b'The network used for evaluation were ResNet'
<EOS>
b'He'
<EOS>
b'et al.'
<EOS>
b'(2016) and Resnext Xie'
<EOS>
b'et al.'
<EOS>
b'(2017) for training.'
<EOS>
b'All ResNext variants use a cardinality of 32 and base width of 4 (32x4d).'
<EOS>
b'ResNext models follow the same structure as their Resnet counterparts, except for the cardinality\nand base width.'
<EOS>
b'All other con\xef\xac\x81gurations are kept as close as possible to Tarvainen & Valpola (2017).'
<EOS>
b'The initial\nlearning rate to handle large batches Goyal et al.'
<EOS>
b'(2017) is set to 0.1; the base learning rate is 0.025\nwith a single cycle of cosine annealing.'
<EOS>
b'Semi-supervised learning Due to the large images, the batch size is set to 40 in total with 20/20\nfor labeled and unlabeled samples, respectively.'
<EOS>
b'We found the Kullback-divergence leads to no\nmeaningful network training.'
<EOS>
b'Hence, we set the consistency loss to mean-squared-error, with a\nweight of 1000.'
<EOS>
b'We use consistency ramp up of 5 epochs to give the mean teacher more time in\nthe beginning.'
<EOS>
b'Weight decay is set to 5e-5; patience is four epochs to stop training in the current\n\xef\xac\x81ltering iteration.'
<EOS>
b'Filtering'
<EOS>
b'We \xef\xac\x81lter noisy samples with the topk=5 strategy, instead of taking the maximum-\nlikelihood (ML) prediction as on CIFAR-10 and CIFAR-100.'
<EOS>
b'That means the samples are kept for\nsupervised training if their provided label lies within the top 5 predictions of the model.'
<EOS>
b'The main'
<EOS>
b'2https://github.com/CuriousAI/mean-teacher'
<EOS>
b'13'
<EOS>
b'Published as a conference paper at ICLR 2020'
<EOS>
b'(a)'
<EOS>
b'(b)\n\nFigure 5: Simple training losses to counter label noise.'
<EOS>
b'(a) shows the prediction of a sample given\na model.'
<EOS>
b'The red bar indicates the noisy label, blue the correct one.'
<EOS>
b'Arrows depict the magnitude\nof the gradients'
<EOS>
b'(b) Typical losses reweighting schemes are not wrong but suffer from the gradient\nvanishing problem.'
<EOS>
b'Non-linear losses such as Negative-log-likelihood are not designed for gradient\nascent.'
<EOS>
b'reason is that each image of ImageNet might contain multiple objects.'
<EOS>
b'Filtering with ML-predictions\nis too strict and would lead to a small recall of the detection of the correct sample.'
<EOS>
b'Data Augmentation'
<EOS>
b'For all data, we normalize the RGB-images by the mean: (0.485, 0.456,\n0.406) and the standard variance (0.229, 0.224, 0.225).'
<EOS>
b'For training data, we perform a random\nrotation of up to 10 degrees, randomly resize images to 224x224, apply random horizontal \xef\xac\x82ip\nand random color jittering.'
<EOS>
b'This noise is needed in regular mean-teacher training.'
<EOS>
b'The jittering'
<EOS>
b'setting are: brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1.'
<EOS>
b'The validation data are resized\nto 256x256 and randomly cropped to 224x224'
<EOS>
b'A.3.3 SEMI-SUPERVISED LOSSES'
<EOS>
b'For the learning of wrongly labeled samples, Fig. 6 shows the relationship between the typical\nreweighting scheme and our baseline push-away-loss.'
<EOS>
b'Typically, reweighting is applied directly to\nthe losses with samples weights w(k) for each sample k as shown in Eq.'
<EOS>
b'4'
<EOS>
b'min w(k)'
<EOS>
b'i N'
<EOS>
b'LL(y(k)'
<EOS>
b'label|x(k), D)'
<EOS>
b'D is the dataset, x(k) and y(k)'
<EOS>
b'the sample k at step'
<EOS>
b'i. Negative samples weights w(k)\nfrom the wrong labels.'
<EOS>
b'Let w(k)'
<EOS>
b'i with c(k)'
<EOS>
b'i ='
<EOS>
b'\xe2\x88\x92c(k)'
<EOS>
b'i'
<EOS>
b'label are the samples k and its noisy label.'
<EOS>
b'w(k)'
<EOS>
b'i > 0'
<EOS>
b', then we have:'
<EOS>
b'is'
<EOS>
b'the samples weight for\nare often assigned to push the network away'
<EOS>
b'i'
<EOS>
b'Which results in:\n\nmin'
<EOS>
b'\xe2\x88\x92c(k)'
<EOS>
b'i N'
<EOS>
b'LL(y(k)'
<EOS>
b'label|x(k), D)\n\nmax c(k)'
<EOS>
b'i N'
<EOS>
b'LL(y(k)'
<EOS>
b'label|x(k), D)'
<EOS>
b'In other words, we perform gradient ascent for wrongly labeled samples.'
<EOS>
b'However, the Negative-\nlog-likelihood is not designed for gradient ascent.'
<EOS>
b'Hence the gradients of wrongly labeled samples\nvanish if the prediction is too close to the noisy label.'
<EOS>
b'This effect is similar to the training of\nGenerative Adversarial Network (GAN)'
<EOS>
b'Goodfellow et al..'
<EOS>
b'In the GAN-framework, the generator\nloss is not simply set to the negated version of the discriminator\xe2\x80\x99s loss for the same reason.'
<EOS>
b'Therefore, to provide a fair comparison with our framework, we suggest the push-away-loss\nLP ush\xe2\x88\x92away(y(k)\n\nlabel, x(k), D) with improved gradients as follows:'
<EOS>
b'(1)'
<EOS>
b'(2)'
<EOS>
b'(3)'
<EOS>
b'(4)\n\n1'
<EOS>
b'(cid:88)'
<EOS>
b'min'
<EOS>
b'|Y'
<EOS>
b'|\xe2\x88\x921\n\ny,y(cid:54)=y(k)'
<EOS>
b'label\n\nc(k)'
<EOS>
b'i N LL(y|x(k)'
<EOS>
b', D)'
<EOS>
b'Whereby Y is the set of all classes in the training set.'
<EOS>
b'This loss has improved gradients to push the\nmodel away from the potentially wrong labels.'
<EOS>
b'14'
<EOS>
b'Published as a conference paper at ICLR 2020'
<EOS>
b'(a)'
<EOS>
b'(b)\n\n(a) Extreme predictions such as [0, 1]'
<EOS>
b'Figure 6:'
<EOS>
b'The entropy loss for semi-supervised learning.\nare encouraged by minimizing the entropy on each prediction.'
<EOS>
b'(b)'
<EOS>
b'Additionally, maximizing the\nentropy of the mean prediction on the entire dataset or a large batch forces the model to balance its\npredictions over multiple samples.'
<EOS>
b'Table 8: Accuracy of the complete removal of samples during iterative \xef\xac\x81ltering on CIFAR-10 and\nCIFAR-100.'
<EOS>
b'The underlying model is the MeanTeacher based on Resnet26.'
<EOS>
b'When samples are com-\npletely removed from the training set, they are no longer used for either supervised-or-unsupervised\nlearning.'
<EOS>
b'This common strategy from previous works leads to rapid performance breakdown.'
<EOS>
b'NOISE RATIO'
<EOS>
b'CIFAR-10\n\n40%\n\n80 %'
<EOS>
b'CIFAR-100'
<EOS>
b'40%\n80 %'
<EOS>
b'USING NOISY DATA'
<EOS>
b'ONLY'
<EOS>
b'DATA REMOVAL'
<EOS>
b'SELF (OURS)\n\n93.4'
<EOS>
b'93.7'
<EOS>
b'59.98'
<EOS>
b'69.91'
<EOS>
b'68.99'
<EOS>
b'71.98'
<EOS>
b'35.53'
<EOS>
b'42.09'
<EOS>
b'WITH CLEAN VALIDATION SET\n\nCOMPL.'
<EOS>
b'REMOVAL\nSELF (OURS)'
<EOS>
b'94.39'
<EOS>
b'95.1'
<EOS>
b'70.93'
<EOS>
b'79.93'
<EOS>
b'71.86'
<EOS>
b'74.76'
<EOS>
b'36.61'
<EOS>
b'46.43'
<EOS>
b'Entropy minimization'
<EOS>
b'The typical entropy loss for semi-supervised learning is shown in Fig.'
<EOS>
b'6.'
<EOS>
b'It encourages the model to provide extreme predictions (such as 0 or 1) for each sample.'
<EOS>
b'Over a\nlarge number of samples, the model should balance its predictions over all classes.'
<EOS>
b'The entropy loss can easily be applied to all samples to express the uncertainty about the provided\nlabels.'
<EOS>
b'Alternatively, the loss can be combined with a strict \xef\xac\x81ltering strategy, as in our work, which\nremoves the labels of potentially wrongly labeled samples.'
<EOS>
b'For a large noise ratio, predictions of wrongly labeled samples \xef\xac\x82uctuate strongly over previous\ntraining iterations.'
<EOS>
b'Amplifying these network decisions could lead to even noisier models model.'
<EOS>
b'Combined with iterative \xef\xac\x81ltering, the framework will have to rely on a single noisy model snapshot.'
<EOS>
b'In the case of an unsuitable snapshot, the \xef\xac\x81ltering step will make many wrong decisions.'
<EOS>
b'A.4 MORE EXPERIMENTS RESULTS'
<EOS>
b'A.4.1'
<EOS>
b'COMPLETE REMOVAL OF SAMPLES'
<EOS>
b'Tab. 8 shows the results of deleting samples from the training set.'
<EOS>
b'It leads to signi\xef\xac\x81cant performances'
<EOS>
b'gaps compared to our strategy (SELF), which considers the removed samples as unlabeled data.'
<EOS>
b'In\ncase of a considerable label noise of 80%, the gap is close to 9%.'
<EOS>
b'Continuously using the \xef\xac\x81ltered samples lead to signi\xef\xac\x81cantly better results.'
<EOS>
b'The unsupervised-loss\nprovides meaningful learning signals, which should be used for better model training.'
<EOS>
b'15'
<EOS>
b'Published as a conference paper at ICLR 2020'
<EOS>
b'(a)'
<EOS>
b'(b)\n\nFigure 7:'
<EOS>
b'Sample training curves of our approach SELF on CIFAR-100 with (a) 60% and (b) 80%\nnoise, using noisy validation data.'
<EOS>
b'Note that with our approach, the training loss remains close to 0.'
<EOS>
b'Further, note that the mean-teacher continously outperforms the noisy student models.'
<EOS>
b'This shows\nthe positive effect of temporal emsembling to counter label noise.'
<EOS>
b'A.4.2 SAMPLE'
<EOS>
b'TRAINING PROCESS'
<EOS>
b'Fig. 7 shows the sample training processes of SELF under 60% and 80% noise on CIFAR-100.'
<EOS>
b'The\nmean-teacher always outperform the student models.'
<EOS>
b'Further, note that regular training leads to\nrapid over-\xef\xac\x81tting to label noise.'
<EOS>
b'Contrary, with our effective \xef\xac\x81ltering strategy, both models slowly increase their performance while\nthe training accuracy approaches 100%.'
<EOS>
b'Hence, by using progressive \xef\xac\x81ltering, our model could\nerase the inconsistency in the provided labels set.'
<EOS>
b'16'
<EOS>
