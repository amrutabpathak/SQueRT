Published as a conference paper at ICLR 2020

SHARING KNOWLEDGE IN MULTI-TASK
DEEP REINFORCEMENT LEARNING

Carlo D’Eramo & Davide Tateo
Department of Computer Science
TU Darmstadt, IAS
Hochschulstraße 10, 64289, Darmstadt, Germany
{carlo.deramo,davide.tateo}@tu-darmstadt.de

Andrea Bonarini & Marcello Restelli
Politecnico di Milano, DEIB
Piazza Leonardo da Vinci 32, 20133, Milano
{andrea.bonarini,marcello.restelli}@polimi.it

Jan Peters
TU Darmstadt, IAS
Hochschulstraße 10, 64289, Darmstadt, Germany
Max Planck Institute for Intelligent Systems
Max-Planck-Ring 4, 72076, Tübingen, Germany
jan.peters@tu-darmstadt.de

ABSTRACT

We study the beneﬁt of sharing representations among tasks to enable the effective
use of deep neural networks in Multi-Task Reinforcement Learning. We leverage
the assumption that learning from different tasks, sharing common properties, is
helpful to generalize the knowledge of them resulting in a more effective feature ex-
traction compared to learning a single task. Intuitively, the resulting set of features
offers performance beneﬁts when used by Reinforcement Learning algorithms.
We prove this by providing theoretical guarantees that highlight the conditions
for which is convenient to share representations among tasks, extending the well-
known ﬁnite-time bounds of Approximate Value-Iteration to the multi-task setting.
In addition, we complement our analysis by proposing multi-task extensions of
three Reinforcement Learning algorithms that we empirically evaluate on widely
used Reinforcement Learning benchmarks showing signiﬁcant improvements over
the single-task counterparts in terms of sample efﬁciency and performance.

1

INTRODUCTION

Multi-Task Learning (MTL) ambitiously aims to learn multiple tasks jointly instead of learning them
separately, leveraging the assumption that the considered tasks have common properties which can be
exploited by Machine Learning (ML) models to generalize the learning of each of them. For instance,
the features extracted in the hidden layers of a neural network trained on multiple tasks have the
advantage of being a general representation of structures common to each other. This translates into
an effective way of learning multiple tasks at the same time, but it can also improve the learning
of each individual task compared to learning them separately (Caruana, 1997). Furthermore, the
learned representation can be used to perform Transfer Learning (TL), i.e. using it as a preliminary
knowledge to learn a new similar task resulting in a more effective and faster learning than learning
the new task from scratch (Baxter, 2000; Thrun & Pratt, 2012).

The same beneﬁts of extraction and exploitation of common features among the tasks achieved
in MTL, can be obtained in Multi-Task Reinforcement Learning (MTRL) when training a single
agent on multiple Reinforcement Learning (RL) problems with common structures (Taylor & Stone,
2009; Lazaric, 2012). In particular, in MTRL an agent can be trained on multiple tasks in the same

1

Published as a conference paper at ICLR 2020

domain, e.g. riding a bicycle or cycling while going towards a goal, or on different but similar
domains, e.g. balancing a pendulum or balancing a double pendulum1. Considering recent advances
in Deep Reinforcement Learning (DRL) and the resulting increase in the complexity of experimental
benchmarks, the use of Deep Learning (DL) models, e.g. deep neural networks, has become a popular
and effective way to extract common features among tasks in MTRL algorithms (Rusu et al., 2015;
Liu et al., 2016; Higgins et al., 2017). However, despite the high representational capacity of DL
models, the extraction of good features remains challenging. For instance, the performance of the
learning process can degrade when unrelated tasks are used together (Caruana, 1997; Baxter, 2000);
another detrimental issue may occur when the training of a single model is not balanced properly
among multiple tasks (Hessel et al., 2018).

Recent developments in MTRL achieve signiﬁcant results in feature extraction by means of algorithms
speciﬁcally developed to address these issues. While some of these works rely on a single deep
neural network to model the multi-task agent (Liu et al., 2016; Yang et al., 2017; Hessel et al., 2018;
Wulfmeier et al., 2019), others use multiple deep neural networks, e.g. one for each task and another
for the multi-task agent (Rusu et al., 2015; Parisotto et al., 2015; Higgins et al., 2017; Teh et al., 2017).
Intuitively, achieving good results in MTRL with a single deep neural network is more desirable
than using many of them, since the training time is likely much less and the whole architecture is
easier to implement. In this paper we study the beneﬁts of shared representations among tasks. We
theoretically motivate the intuitive effectiveness of our method, deriving theoretical guarantees that
exploit the theoretical framework provided by Maurer et al. (2016), in which the authors present
upper bounds on the quality of learning in MTL when extracting features for multiple tasks in a
single shared representation. The signiﬁcancy of this result is that the cost of learning the shared
representation decreases with a factor O(1/
T ), where T is the number of tasks for many function
approximator hypothesis classes. The main contribution of this work is twofold.

√

1. We derive upper conﬁdence bounds for Approximate Value-Iteration (AVI) and Approximate
Policy-Iteration (API)2 (Farahmand, 2011) in the MTRL setting, and we extend the approx-
imation error bounds in Maurer et al. (2016) to the case of multiple tasks with different
dimensionalities. Then, we show how to combine these results resulting in, to the best
of our knowledge, the ﬁrst proposed extension of the ﬁnite-time bounds of AVI/API to
MTRL. Despite being an extension of previous works, we derive these results to justify
our approach showing how the error propagation in AVI/API can theoretically beneﬁt from
learning multiple tasks jointly.

2. We leverage these results proposing a neural network architecture, for which these bounds
hold with minor assumptions, that allow us to learn multiple tasks with a single regressor
extracting a common representation. We show an empirical evidence of the consequence of
our bounds by means of a variant of Fitted Q-Iteration (FQI) (Ernst et al., 2005), based on our
shared network and for which our bounds apply, that we call Multi Fitted Q-Iteration (MFQI).
Then, we perform an empirical evaluation in challenging RL problems proposing multi-
task variants of the Deep Q-Network (DQN) (Mnih et al., 2015) and Deep Deterministic
Policy Gradient (DDPG) (Lillicrap et al., 2015) algorithms. These algorithms are practical
implementations of the more general AVI/API framework, designed to solve complex
problems. In this case, the bounds apply to these algorithms only with some assumptions,
e.g. stationary sampling distribution. The outcome of the empirical analysis joins the
theoretical results, showing signiﬁcant performance improvements compared to the single-
task version of the algorithms in various RL problems, including several MuJoCo (Todorov
et al., 2012) domains.

2 PRELIMINARIES

Let B(X ) be the space of bounded measurable functions w.r.t. the σ-algebra σX , and similarly
B(X , L) be the same bounded by L < ∞.

A Markov Decision Process (MDP) is deﬁned as a 5-tuple M =< S, A, P, R, γ >, where S is the
state space, A is the action space, P : S × A → S is the transition distribution where P(s(cid:48)|s, a)

1For simplicity, in this paper we refer to the concepts of task and domain interchangeably.
2All proofs and the theorem for API are in Appendix A.2.

2

Published as a conference paper at ICLR 2020

is the probability of reaching state s(cid:48) when performing action a in state s, R : S × A × S →
R is the reward function, and γ ∈ (0, 1] is the discount factor. A deterministic policy π maps,
for each state, the action to perform: π : S → A. Given a policy π, the value of an action
a in a state s represents the expected discounted cumulative reward obtained by performing a
in s and following π thereafter: Qπ(s, a) (cid:44) E[(cid:80)∞
k=0 γkri+k+1|si = s, ai = a, π], where ri+1
is the reward obtained after the i-th transition. The expected discounted cumulative reward is
maximized by following the optimal policy π∗ which is the one that determines the optimal action
values, i.e., the ones that satisfy the Bellman optimality equation (Bellman, 1954): Q∗(s, a) (cid:44)
(cid:82)
S P(s(cid:48)|s, a) [R(s, a, s(cid:48)) + γ maxa(cid:48) Q∗(s(cid:48), a(cid:48))] ds(cid:48). The solution of the Bellman optimality equation
is the ﬁxed point of the optimal Bellman operator T ∗ : B(S × A) → B(S × A) deﬁned as
(T ∗Q)(s, a) (cid:44) (cid:82)
S P(s(cid:48)|s, a)[R(s, a, s(cid:48)) + γ maxa(cid:48) Q(s(cid:48), a(cid:48))]ds(cid:48). In the MTRL setting, there are
multiple MDPs M(t) =< S (t), A(t), P (t), R(t), γ(t) > where t ∈ {1, . . . , T } and T is the number
of MDPs. For each MDP M(t), a deterministic policy πt : S (t) → A(t) induces an action-value
function Qπt
i+k+1|si = s(t), ai = a(t), πt]. In this setting, the goal is to
maximize the sum of the expected cumulative discounted reward of each task.

t (s(t), a(t)) = E[(cid:80)∞

k=0 γkr(t)

In our theoretical analysis of the MTRL problem, the complexity of representation plays a central role.
As done in Maurer et al. (2016), we consider the Gaussian complexity, a variant of the well-known
Rademacher complexity, to measure the complexity of the representation. Given a set ¯X ∈ X T n of n
input samples for each task t ∈ {1, . . . , T }, and a class H composed of k ∈ {1, . . . , K} functions,
the Gaussian complexity of a random set H( ¯X) = {(hk(Xti)) : h ∈ H} ⊆ RKT n is deﬁned as
follows:

G(H( ¯X)) = E

γtkihk(Xti)

(cid:34)

sup
h∈H

(cid:88)

tki

(cid:35)

,

(cid:12)
(cid:12)
(cid:12)
Xti
(cid:12)
(cid:12)

where γtki are independent standard normal variables. We also need to deﬁne the following quantity,
taken from Maurer (2016): let γ be a vector of m random standard normal variables, and f ∈ F :
Y → Rm, with Y ⊆ Rn, we deﬁne

O(F) =

sup

y,y(cid:48)∈Y,y(cid:54)=y(cid:48)

(cid:34)

E

sup
f ∈F

(cid:104)γ, f (y) − f (y(cid:48))(cid:105)

(cid:107)y − y(cid:48)(cid:107)

(cid:35)

.

(1)

(2)

Equation 2 can be viewed as a Gaussian average of Lipschitz quotients, and appears in the bounds
provided in this work. Finally, we deﬁne L(F) as the upper bound of the Lipschitz constant of all the
functions f in the function class F.

3 THEORETICAL ANALYSIS

The following theoretical study starts from the derivation of theoretical guarantees for MTRL in the
AVI framework, extending the results of Farahmand (2011) in the MTRL scenario. Then, to bound
the approximation error term in the AVI bound, we extend the result described in Maurer (2006)
to MTRL. As we discuss, the resulting bounds described in this section clearly show the beneﬁt of
sharing representation in MTRL. To the best of our knowledge, this is the ﬁrst general result for
MTRL; previous works have focused on ﬁnite MDPs (Brunskill & Li, 2013) or linear models (Lazaric
& Restelli, 2011).

3.1 MULTI-TASK REPRESENTATION LEARNING

The multi-task representation learning problem consists in learning simultaneously a set of T tasks
µt, modeled as probability measures over the space of the possible input-output pairs (x, y), with
x ∈ X and y ∈ R, being X the input space. Let w ∈ W : X → RJ , h ∈ H : RJ → RK and
f ∈ F : RK → R be functions chosen from their respective hypothesis classes. The functions
in the hypothesis classes must be Lipschitz continuous functions. Let ¯Z = (Z1, . . . , ZT ) be the
multi-sample over the set of tasks µ = (µ1, . . . , µT ), where Zt = (Zt1, . . . , Ztn) ∼ µn
t and
Zti = (Xti, Yti) ∼ µt. We can formalize our regression problem as the following minimization

3

Published as a conference paper at ICLR 2020

problem:

min

(cid:40)

1
nT

T
(cid:88)

N
(cid:88)

t=1

i=1

(cid:96)(ft(h(wt(Xti))), Yti) : f ∈ F T , h ∈ H, w ∈ W T

,

(3)

(cid:41)

where we use f = (f1, . . . , fT ), w = (w1, . . . , wT ), and deﬁne the minimizers of Equation (3) as ˆw,
ˆh, and ˆf . We assume that the loss function (cid:96) : R × R → [0, 1] is 1-Lipschitz in the ﬁrst argument for
every value of the second argument. While this assumption may seem restrictive, the result obtained
can be easily scaled to the general case. To use the principal result of this section, for a generic loss
function (cid:96)(cid:48), it is possible to use (cid:96)(·) = (cid:96)(cid:48)(·)/(cid:15)max, where (cid:15)max is the maximum value of (cid:96)(cid:48). The expected
loss over the tasks, given w, h and f is the task-averaged risk:

εavg(w, h, f ) =

E [(cid:96)(ft(h(wt(X))), Y )]

(4)

1
T

T
(cid:88)

t=1

The minimum task-averaged risk, given the set of tasks µ and the hypothesis classes W, H and F is
ε∗
avg, and the corresponding minimizers are w∗, h∗ and f ∗.

3.2 MULTI-TASK APPROXIMATE VALUE ITERATION BOUND

We start by considering the bound for the AVI framework which applies for the single-task scenario.
Theorem 1. (Theorem 3.4 of Farahmand (2011)) Let K be a positive integer, and Qmax ≤ Rmax
1−γ . Then
k=0 ⊂ B(S × A, Qmax) and the corresponding sequence (εk)K−1
for any sequence (Qk)K
k=0 , where
εk = (cid:107)Qk+1 − T ∗Qk(cid:107)2
ν, we have:

(cid:107)Q∗ − QπK (cid:107)1,ρ ≤

2γ

(cid:20)

(1 − γ)2

1
2

inf

r∈[0,1]

C

VI,ρ,ν(K; r)E

2 (ε0, . . . , εK−1; r) +

(cid:21)

γKRmax

,

(5)

2

1 − γ

where

CVI,ρ,ν(K; r) =

(cid:18) 1 − γ

(cid:19)2

2

sup
1,...,π(cid:48)
π(cid:48)
K

K−1
(cid:88)

k=0

a2(1−r)
k

(cid:88)

γm(cid:16)

m≥0

cVI1,ρ,ν(m, K − k; π(cid:48)

K)

1





+cVI2,ρ,ν(m + 1; π(cid:48)

k+1, . . . , π(cid:48)

K)



,

(6)


2

(cid:17)

with E(ε0, . . . , εK−1; r) = (cid:80)K−1
and ν, and the series αk are deﬁned as in Farahmand (2011).

k=0 α2r

k εk, the two coefﬁcients cVI1,ρ,ν, cVI2,ρ,ν, the distributions ρ

In the multi-task scenario, let the average approximation error across tasks be:

εavg,k( ˆwk, ˆhk, ˆfk) =

(cid:107)Qt,k+1 − T ∗

t Qt,k(cid:107)2
ν,

(7)

1
T

T
(cid:88)

t=1

is the optimal Bellman operator of task t.

where Qt,k+1 = ˆft,k ◦ ˆhk ◦ ˆwt,k, and T ∗
t
In the following, we extend the AVI bound of Theorem 1 to the multi-task scenario, by computing
the average loss across tasks and pushing inside the average using Jensen’s inequality.
Theorem 2. Let K be a positive integer, and Qmax ≤ Rmax
1−γ . Then for any sequence (Qk)K
A, Qmax) and the corresponding sequence (εavg,k)K−1
we have:

k=0 ⊂ B(S ×
t Qt,k(cid:107)2
ν,

k=0 , where εavg,k =

t=1(cid:107)Qt,k+1−T ∗

(cid:80)T

1
T

1
T

T
(cid:88)

t=1

(cid:107)Q∗

t − QπK

t (cid:107)1,ρ ≤

2γ

(cid:20)

(1 − γ)2

inf

r∈[0,1]

1
2

1
2

C

VI(K; r)E

avg(εavg,0, . . . , εavg,K−1; r) +

2γKRmax,avg

(cid:21)

1 − γ

(8)

with Eavg = (cid:80)K−1

k=0 α2r

k εavg,k, γ = max
(cid:40) (1−γ)γK−k−1

t∈{1,...,T }

1
T

(cid:80)T

t=1 Rmax,t and αk =

1−γK+1

(1−γ)γK
1−γK+1

0 ≤ k < K,

.

k = K

γt, C

VI(K; r) = max

C

VI,ρ,ν(K; t, r), Rmax,avg =

t∈{1,...,T }

1
2

1
2

4

Published as a conference paper at ICLR 2020

Remarks Theorem 2 retains most of the properties of Theorem 3.4 of Farahmand (2011), except
that the regression error in the bound is now task-averaged. Interestingly, the second term of the
sum in Equation (8) depends on the average maximum reward for each task. In order to obtain this
result we use an overly pessimistic bound on γ and the concentrability coefﬁcients, however this
approximation is not too loose if the MDPs are sufﬁciently similar.

3.3 MULTI-TASK APPROXIMATION ERROR BOUND

We bound the task-averaged approximation error εavg at each AVI iteration k involved in (8) following
a derivation similar to the one proposed by Maurer et al. (2016), obtaining:
Theorem 3. Let µ, W, H and F be deﬁned as above and assume 0 ∈ H and f (0) = 0, ∀f ∈ F.
Then for δ > 0 with probability at least 1 − δ in the draw of ¯Z ∼ (cid:81)T

t=1 µn

t we have that

εavg( ˆw, ˆh, ˆf ) ≤ L(F)

c1

(cid:18)

L(H) supl∈{1,...,T } G(W(Xl))

supw(cid:107)w( ¯X)(cid:107)O(H)

minp∈P G(H(p))

(cid:19)

suph,w(cid:107)h(w( ¯X))(cid:107)O(F)

+c3

nT

n

+ c4

+ c2

√

n

T

nT

(cid:115)

+

8 ln( 3
δ )
nT

+ ε∗

avg.

(9)

√

√

Remarks The assumptions 0 ∈ H and f (0) = 0 for all f ∈ F are not essential for the proof and
are only needed to simplify the result. For reasonable function classes, the Gaussian complexity
n). If supw(cid:107)w( ¯X)(cid:107) and suph,w(cid:107)h(w( ¯X))(cid:107) can be uniformly bounded, then
G(W(Xl)) is O(
they are O(
nT ). For some function classes, the Gaussian average of Lipschitz quotients O(·) can
be bounded independently from the number of samples. Given these assumptions, the ﬁrst and the
fourth term of the right hand side of Equation (9), which represent respectively the cost of learning the
meta-state space w and the task-speciﬁc f mappings, are both O(1/√
n). The second term represents
the cost of learning the multi-task representation h and is O(1/
nT ), thus vanishing in the multi-task
limit T → ∞. The third term can be removed if ∀h ∈ H, ∃p0 ∈ P : h(p) = 0; even when this
assumption does not hold, this term can be ignored for many classes of interest, e.g. neural networks,
as it can be arbitrarily small.
The last term to be bounded in (9) is the minimum average approximation error ε∗
avg at each AVI
iteration k. Recalling that the task-averaged approximation error is deﬁned as in (7), applying
Theorem 5.3 by Farahmand (2011) we obtain:
Lemma 4. Let Q∗
T ∗
t Qt,k(cid:107)2

t,k, ∀t ∈ {1, . . . , T } be the minimizers of ε∗

avg,k, ˇtk = arg maxt∈{1,...,T }(cid:107)Q∗

ν, and bk,i = (cid:107)Qˇtk,i+1 − T ∗

ˇt Qˇtk,i(cid:107)ν, then:

t,k+1 −

√

(cid:32)

(cid:33)2

ε∗
avg,k ≤

(cid:107)Q∗

ˇtk,k+1 − (T ∗

ˇt )k+1Qˇtk,0(cid:107)ν +

(γˇtk CAE(ν; ˇtk, P ))i+1bk,k−1−i

,

(10)

k−1
(cid:88)

i=0

with CAE deﬁned as in Farahmand (2011).

Final remarks The bound for MTRL is derived by composing the results in Theorems 2 and 3, and
Lemma 4. The results above highlight the advantage of learning a shared representation. The bound
in Theorem 2 shows that a small approximation error is critical to improve the convergence towards
the optimal action-value function, and the bound in Theorem 3 shows that the cost of learning the
shared representation at each AVI iteration is mitigated by using multiple tasks. This is particularly
beneﬁcial when the feature representation is complex, e.g. deep neural networks.

3.4 DISCUSSION

As stated in the remarks of Equation (9), the beneﬁt of MTRL is evinced by the second component
of the bound, i.e. the cost of learning h, which vanishes with the increase of the number of tasks.
Obviously, adding more tasks require the shared representation to be large enough to include all
of them, undesirably causing the term suph,w(cid:107)h(w( ¯X))(cid:107) in the fourth component of the bound to
increase. This introduces a tradeoff between the number of features and number of tasks; however, for

5

Published as a conference paper at ICLR 2020

(a) Shared network

(b) FQI vs MFQI

(c) #Task analysis

Figure 1: (a) The architecture of the neural network we propose to learn T tasks simultaneously.
The wt block maps each input xt from task µt to a shared set of layers h which extracts a common
representation of the tasks. Eventually, the shared representation is specialized in block ft and the
output yt of the network is computed. Note that each block can be composed of arbitrarily many
layers. (b) Results of FQI and MFQI averaged over 4 tasks in Car-On-Hill, showing (cid:107)Q∗ − QπK (cid:107) on
the left, and the discounted cumulative reward on the right. (c) Results of MFQI showing (cid:107)Q∗ −QπK (cid:107)
for increasing number of tasks. Both results in (b) and (c) are averaged over 100 experiments, and
show the 95% conﬁdence intervals.

a reasonable number of tasks the number of features used in the single-task case is enough to handle
them, as we show in some experiments in Section 5. Notably, since the AVI/API framework provided
by Farahmand (2011) provides an easy way to include the approximation error of a generic function
approximator, it is easy to show the beneﬁt in MTRL of the bound in Equation (9). Despite being just
multi-task extensions of previous works, our results are the ﬁrst one to theoretically show the beneﬁt
of sharing representation in MTRL. Moreover, they serve as a signiﬁcant theoretical motivation,
besides to the intuitive ones, of the practical algorithms that we describe in the following sections.

4 SHARING REPRESENTATIONS

We want to empirically evaluate the beneﬁt of our theoretical study in the problem of jointly learning
T different tasks µt, introducing a neural network architecture for which our bounds hold. Following
our theoretical framework, the network we propose extracts representations wt from inputs xt for each
task µt, mapping them to common features in a set of shared layers h, specializing the learning of
each task in respective separated layers ft, and ﬁnally computing the output yt = (ft ◦ h ◦ wt)(xt) =
ft(h(wt(xt))) (Figure 1(a)). The idea behind this architecture is not new in the literature. For
instance, similar ideas have already been used in DQN variants to improve exploration on the same
task via bootstrapping (Osband et al., 2016) and to perform MTRL (Liu et al., 2016).

The intuitive and desirable property of this architecture is the exploitation of the regularization effect
introduced by the shared representation of the jointly learned tasks. Indeed, unlike learning a single
task that may end up in overﬁtting, forcing the model to compute a shared representation of the tasks
helps the regression process to extract more general features, with a consequent reduction in the
variance of the learned function. This intuitive justiﬁcation for our approach, joins the theoretical
beneﬁt proven in Section 3. Note that our architecture can be used in any MTRL problem involving a
regression process; indeed, it can be easily used in value-based methods as a Q-function regressor,
or in policy search as a policy regressor. In both cases, the targets are learned for each task µt
in its respective output block ft. Remarkably, as we show in the experimental Section 5, it is
straightforward to extend RL algorithms to their multi-task variants only through the use of the
proposed network architecture, without major changes to the algorithms themselves.

5 EXPERIMENTAL RESULTS

To empirically evince the effect described by our bounds, we propose an extension of FQI (Ernst
et al., 2005; Riedmiller, 2005), that we call MFQI, for which our AVI bounds apply. Then, to
empirically evaluate our approach in challenging RL problems, we introduce multi-task variants
of two well-known DRL algorithms: DQN (Mnih et al., 2015) and DDPG (Lillicrap et al., 2015),
which we call Multi Deep Q-Network (MDQN) and Multi Deep Deterministic Policy Gradient
(MDDPG) respectively. Note that for these methodologies, our AVI and API bounds hold only with

6

hhw1w1w2w2wTwTf1f1f2f2fTfTInputOutputx1x2xTy1y2yT........02550# Iterations0.150.200.250.300.350.400.450.50Q*QKFQIMULTI02550# Iterations0.050.000.050.100.15Performance02550# Iterations0.150.200.250.300.350.400.450.50Q*QK1248Published as a conference paper at ICLR 2020

(a) Multi-task

(b) Transfer

Figure 2: Discounted cumulative reward averaged over 100 experiments of DQN and MDQN for
each task and for transfer learning in the Acrobot problem. An epoch consists of 1, 000 steps, after
which the greedy policy is evaluated for 2, 000 steps. The 95% conﬁdence intervals are shown.

the simplifying assumption that the samples are i.i.d.; nevertheless they are useful to show the beneﬁt
of our method also in complex scenarios, e.g. MuJoCo (Todorov et al., 2012). We remark that in
these experiments we are only interested in showing the beneﬁt of learning multiple tasks with a
shared representation w.r.t. learning a single task; therefore, we only compare our methods with
the single task counterparts, ignoring other works on MTRL in literature. Experiments have been
developed using the MushroomRL library (D’Eramo et al., 2020), and run on an NVIDIA R(cid:13) DGX
StationTM and Intel R(cid:13) AI DevCloud. Refer to Appendix B for all the details and our motivations
about the experimental settings.

5.1 MULTI FITTED Q-ITERATION

As a ﬁrst empirical evaluation, we consider FQI, as an example of an AVI algorithm, to show the
effect described by our theoretical AVI bounds in experiments. We consider the Car-On-Hill problem
as described in Ernst et al. (2005), and select four different tasks from it changing the mass of the
car and the value of the actions (details in Appendix B). Then, we run separate instances of FQI
with a single task network for each task respectively, and one of MFQI considering all the tasks
simultaneously. Figure 1(b) shows the L1-norm of the difference between Q∗ and QπK averaged
over all the tasks. It is clear how MFQI is able to get much closer to the optimal Q-function, thus
giving an empirical evidence of the AVI bounds in Theorem 2. For completeness, we also show the
advantage of MFQI w.r.t. FQI in performance. Then, in Figure 1(c) we provide an empirical evidence
of the beneﬁt of increasing the number of tasks in MFQI in terms of both quality and stability.

5.2 MULTI DEEP Q-NETWORK

As in Liu et al. (2016), our MDQN uses separate replay memories for each task and the batch
used in each training step is built picking the same number of samples from each replay memory.
Furthermore, a step of the algorithm consists of exactly one step in each task. These are the only
minor changes to the vanilla DQN algorithm we introduce, while all other aspects, such as the use of
the target network, are not modiﬁed. Thus, the time complexity of MDQN is considerably lower than
vanilla DQN thanks to the learning of T tasks with a single model, but at the cost of a higher memory
complexity for the collection of samples for each task. We consider ﬁve problems with similar
state spaces, sparse rewards and discrete actions: Cart-Pole, Acrobot, Mountain-Car, Car-On-Hill,
and Inverted-Pendulum. The implementation of the ﬁrst three problems is the one provided by the
OpenAI Gym library Brockman et al. (2016), while Car-On-Hill is described in Ernst et al. (2005)
and Inverted-Pendulum in Lagoudakis & Parr (2003).

Figure 2(a) shows the performance of MDQN w.r.t. to vanilla DQN that uses a single-task network
structured as the multi-task one in the case with T = 1. The ﬁrst three plots from the left show good
performance of MDQN, which is both higher and more stable than DQN. In Car-On-Hill, MDQN is
slightly slower than DQN to reach the best performance, but eventually manages to be more stable.
Finally, the Inverted-Pendulum experiment is clearly too easy to solve for both approaches, but it is
still useful for the shared feature extraction in MDQN. The described results provide important hints
about the better quality of the features extracted by MDQN w.r.t. DQN. To further demonstrate this,
we evaluate the performance of DQN on Acrobot, arguably the hardest of the ﬁve problems, using
a single-task network with the shared parameters in h initialized with the weights of a multi-task

7

02550#Epochs20406080PerformanceCart-Pole02550#Epochs10090807060Acrobot02550#Epochs10095908580757065Mountain-Car02550#Epochs0.00.10.20.30.4Car-On-Hill02550#Epochs0.60.40.20.0Inverted-PendulumDQNMULTI02550#Epochs10090807060PerformanceAcrobotNo initializationUnfreeze-0Unfreeze-10No unfreezePublished as a conference paper at ICLR 2020

(a) Multi-task for pendulums

(b) Transfer for pendulums

(c) Multi-task for walkers

(d) Transfer for walkers

Figure 3: Discounted cumulative reward averaged over 40 experiments of DDPG and MDDPG for
each task and for transfer learning in the Inverted-Double-Pendulum and Hopper problems. An
epoch consists of 10, 000 steps, after which the greedy policy is evaluated for 5, 000 steps. The 95%
conﬁdence intervals are shown.

network trained with MDQN on the other four problems. Arbitrarily, the pre-trained weights can be
adjusted during the learning of the new task or can be kept ﬁxed and only the remaining randomly
initialized parameters in w and f are trained. From Figure 2(b), the advantages of initializing the
weights are clear. In particular, we compare the performance of DQN without initialization w.r.t.
DQN with initialization in three settings: in Unfreeze-0 the initialized weights are adjusted, in No-
Unfreeze they are kept ﬁxed, and in Unfreeze-10 they are kept ﬁxed until epoch 10 after which they
start to be optimized. Interestingly, keeping the shared weights ﬁxed shows a signiﬁcant performance
improvement in the earliest epochs, but ceases to improve soon. On the other hand, the adjustment of
weights from the earliest epochs shows improvements only compared to the uninitialized network
in the intermediate stages of learning. The best results are achieved by starting to adjust the shared
weights after epoch 10, which is approximately the point at which the improvement given by the
ﬁxed initialization starts to lessen.

5.3 MULTI DEEP DETERMINISTIC POLICY GRADIENT

In order to show how the ﬂexibility of our approach easily allows to perform MTRL in policy search
algorithms, we propose MDDPG as a multi-task variant of DDPG. As an actor-critic method, DDPG
requires an actor network and a critic network. Intuitively, to obtain MDDPG both the actor and critic
networks should be built following our proposed structure. We perform separate experiments on two
sets of MuJoCo Todorov et al. (2012) problems with similar continuous state and action spaces: the
ﬁrst set includes Inverted-Pendulum, Inverted-Double-Pendulum, and Inverted-Pendulum-Swingup as
implemented in the pybullet library, whereas the second set includes Hopper-Stand, Walker-Walk,
and Half-Cheetah-Run as implemented in the DeepMind Control SuiteTassa et al. (2018). Figure 3(a)
shows a relevant improvement of MDDPG w.r.t. DDPG in the pendulum tasks. Indeed, while in
Inverted-Pendulum, which is the easiest problem among the three, the performance of MDDPG is
only slightly better than DDPG, the difference in the other two problems is signiﬁcant. The advantage
of MDDPG is conﬁrmed in Figure 3(c) where it performs better than DDPG in Hopper and equally
good in the other two tasks. Again, we perform a TL evaluation of DDPG in the problems where
it suffers the most, by initializing the shared weights of a single-task network with the ones of a
multi-task network trained with MDDPG on the other problems. Figures 3(b) and 3(d) show evident
advantages of pre-training the shared weights and a signiﬁcant difference between keeping them ﬁxed
or not.

8

050100#Epochs2030405060708090100PerformanceInverted-PendulumDDPGMULTI050100#Epochs100200300400500600700800Inverted-Double-Pendulum050100#Epochs100806040200Inverted-Pendulum-Swingup050100#Epochs200400600800PerformanceInverted-Double-PendulumNo initializationUnfreeze-0No unfreeze050100#Epochs05101520253035PerformanceHopper050100#Epochs010203040506070Walker050100#Epochs0510152025303540Half-CheetahDDPGMULTI050100#Epochs010203040PerformanceHopperNo initializationUnfreeze-0No unfreezePublished as a conference paper at ICLR 2020

6 RELATED WORKS

Our work is inspired from both theoretical and empirical studies in MTL and MTRL literature. In
particular, the theoretical analysis we provide follows previous results about the theoretical properties
of multi-task algorithms. For instance, Cavallanti et al. (2010) and Maurer (2006) prove the theoretical
advantages of MTL based on linear approximation. More in detail, Maurer (2006) derives bounds on
MTL when a linear approximator is used to extract a shared representation among tasks. Then, Maurer
et al. (2016), which we considered in this work, describes similar results that extend to the use of
non-linear approximators. Similar studies have been conducted in the context of MTRL. Among the
others, Lazaric & Restelli (2011) and Brunskill & Li (2013) give theoretical proofs of the advantage
of learning from multiple MDPs and introduces new algorithms to empirically support their claims,
as done in this work.

Generally, contributions in MTRL assume that properties of different tasks, e.g. dynamics and reward
function, are generated from a common generative model. About this, interesting analyses consider
Bayesian approaches; for instance Wilson et al. (2007) assumes that the tasks are generated from a
hierarchical Bayesian model, and likewise Lazaric & Ghavamzadeh (2010) considers the case when
the value functions are generated from a common prior distribution. Similar considerations, which
however does not use a Bayesian approach, are implicitly made in Taylor et al. (2007), Lazaric et al.
(2008), and also in this work.

In recent years, the advantages of MTRL have been empirically evinced also in DRL, especially
exploiting the powerful representational capacity of deep neural networks. For instance, Parisotto
et al. (2015) and Rusu et al. (2015) propose to derive a multi-task policy from the policies learned by
DQN experts trained separately on different tasks. Rusu et al. (2015) compares to a therein introduced
variant of DQN, which is very similar to our MDQN and the one in Liu et al. (2016), showing how
their method overcomes it in the Atari benchmark Bellemare et al. (2013). Further developments,
extend the analysis to policy search (Yang et al., 2017; Teh et al., 2017), and to multi-goal RL (Schaul
et al., 2015; Andrychowicz et al., 2017). Finally, Hessel et al. (2018) addresses the problem of
balancing the learning of multiple tasks with a single deep neural network proposing a method that
uniformly adapts the impact of each task on the training updates of the agent.

7 CONCLUSION

We have theoretically proved the advantage in RL of using a shared representation to learn multiple
tasks w.r.t. learning a single task. We have derived our results extending the AVI/API bounds (Farah-
mand, 2011) to MTRL, leveraging the upper bounds on the approximation error in MTL provided
in Maurer et al. (2016). The results of this analysis show that the error propagation during the
AVI/API iterations is reduced according to the number of tasks. Then, we proposed a practical way of
exploiting this theoretical beneﬁt which consists in an effective way of extracting shared representa-
tions of multiple tasks by means of deep neural networks. To empirically show the advantages of our
method, we carried out experiments on challenging RL problems with the introduction of multi-task
extensions of FQI, DQN, and DDPG based on the neural network structure we proposed. As desired,
the favorable empirical results conﬁrm the theoretical beneﬁt we described.

9

Published as a conference paper at ICLR 2020

ACKNOWLEDGMENTS

This project has received funding from the European Union’s Horizon 2020 research and innovation
programme under grant agreement No. #640554 (SKILLS4ROBOTS) and No. #713010 (GOAL-
Robots). This project has also been supported by grants from NVIDIA, the NVIDIA DGX Station,
and the Intel R(cid:13) AI DevCloud. The authors thank Alberto Maria Metelli, Andrea Tirinzoni and Matteo
Papini for their helpful insights during the development of the project.

REFERENCES

Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob
McGrew, Josh Tobin, OpenAI Pieter Abbeel, and Wojciech Zaremba. Hindsight experience replay.
In Advances in Neural Information Processing Systems, pp. 5048–5058, 2017.

Jonathan Baxter. A model of inductive bias learning. Journal of Artiﬁcial Intelligence Research, 12:

149–198, 2000.

Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environ-
ment: An evaluation platform for general agents. Journal of Artiﬁcial Intelligence Research, 47:
253–279, 2013.

Richard Bellman. The theory of dynamic programming. Technical report, RAND Corp Santa Monica

CA, 1954.

Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and

Wojciech Zaremba. Openai gym, 2016.

Emma Brunskill and Lihong Li. Sample complexity of multi-task reinforcement learning.
Proceedings of the Twenty-Ninth Conference on Uncertainty in Artiﬁcial Intelligence, 2013.

In

Rich Caruana. Multitask learning. Machine learning, 28(1):41–75, 1997.

Giovanni Cavallanti, Nicolo Cesa-Bianchi, and Claudio Gentile. Linear algorithms for online

multitask classiﬁcation. Journal of Machine Learning Research, 11(Oct):2901–2934, 2010.

Carlo D’Eramo, Davide Tateo, Andrea Bonarini, Marcello Restelli, and Jan Peters. Mushroomrl:

Simplifying reinforcement learning research. arXiv:2001.01102, 2020.

Damien Ernst, Pierre Geurts, and Louis Wehenkel. Tree-based batch mode reinforcement learning.

Journal of Machine Learning Research, 6(Apr):503–556, 2005.

Amir-massoud Farahmand. Regularization in reinforcement learning. 2011.

Matteo Hessel, Hubert Soyer, Lasse Espeholt, Wojciech Czarnecki, Simon Schmitt, and Hado van

Hasselt. Multi-task deep reinforcement learning with popart. arXiv:1809.04474, 2018.

Irina Higgins, Arka Pal, Andrei Rusu, Loic Matthey, Christopher Burgess, Alexander Pritzel, Matthew
Botvinick, Charles Blundell, and Alexander Lerchner. Darla: Improving zero-shot transfer in
reinforcement learning. In International Conference on Machine Learning, pp. 1480–1490, 2017.

Michail G Lagoudakis and Ronald Parr. Least-squares policy iteration. Journal of machine learning

research, 4(Dec):1107–1149, 2003.

Alessandro Lazaric. Transfer in reinforcement learning: a framework and a survey. In Reinforcement

Learning, pp. 143–173. Springer, 2012.

Alessandro Lazaric and Mohammad Ghavamzadeh. Bayesian multi-task reinforcement learning. In

ICML-27th International Conference on Machine Learning, pp. 599–606. Omnipress, 2010.

Alessandro Lazaric and Marcello Restelli. Transfer from multiple mdps. In Advances in Neural

Information Processing Systems, pp. 1746–1754, 2011.

Alessandro Lazaric, Marcello Restelli, and Andrea Bonarini. Transfer of samples in batch rein-
forcement learning. In Proceedings of the 25th international conference on Machine learning, pp.
544–551. ACM, 2008.

10

Published as a conference paper at ICLR 2020

Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv
preprint arXiv:1509.02971, 2015.

Lydia Liu, Urun Dogan, and Katja Hofmann. Decoding multitask dqn in the world of minecraft. In

European Workshop on Reinforcement Learning, 2016.

Andreas Maurer. Bounds for linear multi-task learning. Journal of Machine Learning Research, 7

(Jan):117–139, 2006.

Science, 650:109–122, 2016.

Andreas Maurer. A chain rule for the expected suprema of gaussian processes. Theoretical Computer

Andreas Maurer, Massimiliano Pontil, and Bernardino Romera-Paredes. The beneﬁt of multitask

representation learning. The Journal of Machine Learning Research, 17(1):2853–2884, 2016.

Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare,
Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control
through deep reinforcement learning. Nature, 518(7540):529, 2015.

Ian Osband, Charles Blundell, Alexander Pritzel, and Benjamin Van Roy. Deep exploration via
bootstrapped dqn. In Advances in neural information processing systems, pp. 4026–4034, 2016.

Emilio Parisotto, Jimmy Lei Ba, and Ruslan Salakhutdinov. Actor-mimic: Deep multitask and

transfer reinforcement learning. arXiv preprint arXiv:1511.06342, 2015.

Martin Riedmiller. Neural ﬁtted q iteration–ﬁrst experiences with a data efﬁcient neural reinforcement

learning method. In European Conference on Machine Learning, pp. 317–328. Springer, 2005.

Andrei A Rusu, Sergio Gomez Colmenarejo, Caglar Gulcehre, Guillaume Desjardins, James Kirk-
patrick, Razvan Pascanu, Volodymyr Mnih, Koray Kavukcuoglu, and Raia Hadsell. Policy
distillation. arXiv preprint arXiv:1511.06295, 2015.

Tom Schaul, Daniel Horgan, Karol Gregor, and David Silver. Universal value function approximators.

In International Conference on Machine Learning, pp. 1312–1320, 2015.

Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de Las Casas, David Budden,
Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq, Timothy P. Lillicrap, and Martin A. Riedmiller.
Deepmind control suite. CoRR, abs/1801.00690, 2018.

Matthew E Taylor and Peter Stone. Transfer learning for reinforcement learning domains: A survey.

Journal of Machine Learning Research, 10(Jul):1633–1685, 2009.

Matthew E Taylor, Peter Stone, and Yaxin Liu. Transfer learning via inter-task mappings for temporal

difference learning. Journal of Machine Learning Research, 8(Sep):2125–2167, 2007.

Yee Teh, Victor Bapst, Wojciech M Czarnecki, John Quan, James Kirkpatrick, Raia Hadsell, Nicolas
Heess, and Razvan Pascanu. Distral: Robust multitask reinforcement learning. In Advances in
Neural Information Processing Systems, pp. 4496–4506, 2017.

Sebastian Thrun and Lorien Pratt. Learning to learn. Springer Science & Business Media, 2012.

Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.

In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems. IEEE, 2012.

Aaron Wilson, Alan Fern, Soumya Ray, and Prasad Tadepalli. Multi-task reinforcement learning: a
hierarchical bayesian approach. In Proceedings of the 24th international conference on Machine
learning, pp. 1015–1022. ACM, 2007.

Markus Wulfmeier, Abbas Abdolmaleki, Roland Hafner, Jost Tobias Springenberg, Michael Neunert,
Tim Hertweck, Thomas Lampe, Noah Siegel, Nicolas Heess, and Martin Riedmiller. Regularized
hierarchical policies for compositional transfer in robotics. arXiv:1906.11228, 2019.

Zhaoyang Yang, Kathryn E Merrick, Hussein A Abbass, and Lianwen Jin. Multi-task deep reinforce-

ment learning for continuous action control. In IJCAI, pp. 3301–3307, 2017.

11

Published as a conference paper at ICLR 2020

A PROOFS

A.1 APPROXIMATED VALUE-ITERATION BOUNDS

Proof of Theorem 2. We compute the average expected loss across tasks:

1
T

T
(cid:88)

t=1

(cid:107)Q∗

t − QπK

t (cid:107)1,ρ

1
T

T
(cid:88)

t=1

2γ

(1 − γ)2

(1 − γ)2

(1 − γ)2

2γ

2γ

2γ

≤

≤

≤

≤

≤

2γt

(cid:20)

(1 − γt)2

inf

r∈[0,1]

T
(cid:88)

(cid:20)

t=1

inf

r∈[0,1]

1
T

T
(cid:88)

(cid:18)

t=1

inf

r∈[0,1]

1
T

inf

r∈[0,1]

T
(cid:88)

(cid:16)

t=1

1
T

(cid:34)

(cid:34)

(cid:34)

1
2

1
2

1
2

1
2

1
T

C

VI,ρ,ν(K; t, r)E

2 (εt,0, . . . , εt,K−1; t, r) +

C

VI,ρ,ν(K; t, r)E

2 (εt,0, . . . , εt,K−1; t, r) +

C

VI,ρ,ν(K; t, r)E

2 (εt,0, . . . , εt,K−1; t, r)

+

C

VI,ρ,ν(K; t, r)E

2 (εt,0, . . . , εt,K−1; t, r)

+

1

1

1

1

2

1 − γt

γK
t Rmax,t

2

1 − γt

γK
t Rmax,t

(cid:21)

(cid:21)

(cid:19)

2

1 − γ

γKRmax,avg

(cid:17)

2

1 − γ

γKRmax,avg

(cid:35)

(cid:35)

(cid:35)

(1 − γ)2

inf

r∈[0,1]

1
2

C

VI(K; r)

T
(cid:88)

(cid:16)

1

t=1

E

2 (εt,0, . . . , εt,K−1; t, r)

+

γKRmax,avg

(11)

(cid:17)

2

1 − γ

with γ = max

γt, C

VI(K; r) = max

t∈{1,...,T }

t∈{1,...,T }

1
2

1
2

VI,ρ,ν(K; t, r), and Rmax,avg = 1/T (cid:80)T
C

t=1 Rmax,t.

Considering the term 1/T (cid:80)T

(cid:104)

E 1

t=1

2 (εt,0, . . . , εt,K−1; t, r)

(cid:105)

= 1/T (cid:80)T

t=1

(cid:16)(cid:80)K−1

k=0 α2r

t,kεt,k

(cid:17) 1

2

let

αk =

(cid:40) (1−γ)γK−k−1

1−γK+1

(1−γ)γK
1−γK+1

0 ≤ k < K,

,

k = K

T
(cid:88)

(cid:32)K−1
(cid:88)

t=1

k=0

α2r

t,kεt,k

(cid:33) 1

2

≤

1
T

T
(cid:88)

(cid:32)K−1
(cid:88)

t=1

k=0

(cid:33) 1

2

α2r

k εt,k

.

T
(cid:88)

(cid:32)K−1
(cid:88)

t=1

k=0

(cid:33) 1

2

α2r

k εt,k

≤

(cid:32)K−1
(cid:88)

k=0

α2r
k

1
T

T
(cid:88)

t=1

(cid:33) 1

2

εt,k

.

then we bound

Using Jensen’s inequality:

1
T

1
T

So, now we can write (11) as

1
T

T
(cid:88)

t=1

(cid:107)Q∗

t − QπK

t (cid:107)1,ρ ≤

2γ

(cid:20)

(1 − γ)2

1
2

inf

r∈[0,1]

C

VI(K; r)E

avg(εavg,0, . . . , εavg,K−1; r)

1
2

(cid:21)

2

+

1 − γ

γKRmax,avg

,

with εavg,k = 1/T (cid:80)T

t=1 εt,k and Eavg(εavg,0, . . . , εavg,K−1; r) = (cid:80)K−1

k=0 α2r

k εavg,k.

Proof of Lemma 4. Let us start from the deﬁnition of optimal task-averaged risk:

ε∗
avg,k =

(cid:107)Q∗

t,k+1 − T ∗

t Qt,k(cid:107)2
ν,

1
T

T
(cid:88)

t=1

12

Published as a conference paper at ICLR 2020

where Q∗

t,k, with t ∈ [1, T ], are the minimizers of εavg,k.

Consider the task ˇt such that

we can write the following inequality:

ˇtk = arg max

(cid:107)Q∗

t,k+1 − T ∗

t Qt,k(cid:107)2
ν,

t∈{1,...,T }

(cid:113)

avg,k ≤ (cid:107)Q∗
ε∗

ˇtk,k+1 − T ∗

ˇt Qˇtk,k(cid:107)ν.

By the application of Theorem 5.3 by Farahmand (2011) to the right hand side, and deﬁning
bk,i = (cid:107)Qˇtk,i+1 − T ∗

ˇt Qˇtk,i(cid:107)ν, we obtain:

(cid:113)

avg,k ≤ (cid:107)Q∗
ε∗

ˇtk,k+1 − (T ∗

ˇt )k+1Qˇtk,0(cid:107)ν +

(γˇtk CAE(ν; ˇtk, P ))i+1bk,k−1−i.

Squaring both sides yields the result:

(cid:32)

ε∗
avg,k ≤

(cid:107)Q∗

ˇtk,k+1 − (T ∗

ˇt )k+1Qˇtk,0(cid:107)ν +

(γˇtk CAE(ν; ˇtk, P ))i+1bk,k−1−i

.

(cid:33)2

k−1
(cid:88)

i=0

k−1
(cid:88)

i=0

A.2 APPROXIMATED POLICY-ITERATION BOUNDS

We start by considering the bound for the API framework:
Theorem 5. (Theorem 3.2 of Farahmand (2011)) Let K be a positive integer, and Qmax ≤ Rmax
for any sequence (Qk)K−1
k=0 ⊂ B(S × A, Qmax) and the corresponding sequence (εk)K−1
εk = (cid:107)Qk − Qπk (cid:107)2

1−γ . Then
k=0 , where

1
2

C

PI,ρ,ν(K; r)E

1

2 (ε0, . . . , εK−1; r) + γK−1Rmax

,

(12)

(cid:21)

(cid:107)Q∗ − QπK (cid:107)1,ρ ≤

ν, we have:
2γ

(1 − γ)2

(cid:20)

inf

r∈[0,1]

where

CPI,ρ,ν(K; r) =

(cid:18) 1 − γ

(cid:19)2

2

sup
0,...,π(cid:48)
π(cid:48)
K

K−1
(cid:88)

k=0

a2(1−r)
k





(cid:88)

m≥0

(cid:88)

m≥1

γmcPI1,ρ,ν(K − k − 1, m + 1; π(cid:48)

k+1)+

γmcPI2,ρ,ν(K − k − 1, m; π(cid:48)

k+1, π(cid:48)

k) + cPI3,ρ,ν



;


2

(13)

with E(ε0, . . . , εK−1; r) = (cid:80)K−1
butions ρ and ν, and the series αk are deﬁned as in Farahmand (2011).

k=0 α2r

k εk, the three coefﬁcients cPI1,ρ,ν, cPI2,ρ,ν, cPI3,ρ,ν, the distri-

From Theorem 5, by computing the average loss across tasks and pushing inside the average using
Jensen’s inequality, we derive the API bounds averaged on multiple tasks.
Theorem 6. Let K be a positive integer, and Qmax ≤ Rmax
A, Qmax) and the corresponding sequence (εavg,k)K−1
have:

1−γ . Then for any sequence (Qk)K−1

k=0 , where εavg,k =

t=1(cid:107)Qt,k − Qπk

k=0 ⊂ B(S ×

ν, we

t (cid:107)2

(cid:80)T

1
T

1
T

T
(cid:88)

t=1

(cid:107)Q∗

t − QπK

t (cid:107)1,ρ ≤

2γ

(cid:20)

(1 − γ)2

inf

r∈[0,1]

1
2

1
2

C

PI(K; r)E

avg(εavg,0, . . . , εavg,K−1; r)

+γK−1Rmax,avg

(cid:3) ,

(14)

13

Published as a conference paper at ICLR 2020

with Eavg = (cid:80)K−1

k=0 α2r

k εavg,k, γ = max
(cid:40) (1−γ)γK−k−1

t∈{1,...,T }

1
T

(cid:80)T

t=1 Rmax,t and αk =

1−γK+1

(1−γ)γK
1−γK+1

0 ≤ k < K,

.

k = K

1
2

1
2

γt, C

PI(K; r) = max

C

PI,ρ,ν(K; t, r), Rmax,avg =

t∈{1,...,T }

Proof of Theorem 6. The proof is very similar to the one for AVI. We compute the average expected
loss across tasks:

1
T

T
(cid:88)

t=1

(cid:107)Q∗

t − QπK

t (cid:107)1,ρ

1
T

T
(cid:88)

t=1

2γ

(1 − γ)2

(1 − γ)2

(1 − γ)2

2γ

2γ

2γ

≤

≤

≤

≤

≤

2γt

(cid:20)

(1 − γt)2

inf

r∈[0,1]

T
(cid:88)

(cid:20)

t=1

inf

r∈[0,1]

1
T

T
(cid:88)

(cid:18)

t=1

inf

r∈[0,1]

1
T

inf

r∈[0,1]

T
(cid:88)

(cid:16)

t=1

1
T

(cid:34)

(cid:34)

(cid:34)

1
2

1
2

1
T

1
2

C

PI,ρ,ν(K; t, r)E

1

2 (εt,0, . . . , εt,K−1; t, r) + γK−1

Rmax,t

t

1
2

C

PI,ρ,ν(K; t, r)E

1

2 (εt,0, . . . , εt,K−1; t, r) + γK−1

Rmax,t

t

C

PI,ρ,ν(K; t, r)E

2 (εt,0, . . . , εt,K−1; t, r)

+ γK−1Rmax,avg

C

PI,ρ,ν(K; t, r)E

2 (εt,0, . . . , εt,K−1; t, r)

+ γK−1Rmax,avg

1

1

(cid:21)

(cid:21)

(cid:35)

(cid:35)

(cid:35)

(1 − γ)2

inf

r∈[0,1]

1
2

C

PI(K; r)

T
(cid:88)

(cid:16)

1

t=1

E

2 (εt,0, . . . , εt,K−1; t, r)

+ γK−1Rmax,avg

.

(15)

(cid:17)

Using Jensen’s inequality as in the AVI scenario, we can write (15) as:

1
T

T
(cid:88)

t=1

(cid:107)Q∗

t − QπK

t (cid:107)1,ρ ≤

2γ

(cid:20)

(1 − γ)2

inf

r∈[0,1]

1
2

1
2

C

PI(K; r)E

avg(εavg,0, . . . , εavg,K−1; r)

+γK−1Rmax,avg

(cid:3) ,

(16)

with εavg,k = 1/T (cid:80)T

t=1 εt,k and Eavg(εavg,0, . . . , εavg,K−1; r) = (cid:80)K−1

k=0 α2r

k εavg,k.

A.3 APPROXIMATION BOUNDS

1, . . . , w∗
Proof of Theorem 3. Let w∗
(cid:32)

T , h∗ and f ∗

1 , . . . , f ∗

T be the minimizers of ε∗

avg, then:

εavg( ˆw, ˆh, ˆf ) − ε∗

avg =

εavg( ˆw, ˆh, ˆf ) −

(cid:96)( ˆft(ˆh( ˆwt(Xti))), Yti)

(cid:19)

(cid:17)

(cid:33)

(cid:125)

(cid:96)( ˆft(ˆh( ˆwt(Xti))), Yti) −

(cid:96)(f ∗

t (h∗(w∗

t (Xti))), Yti)

(cid:124)
(cid:32)

(cid:124)
(cid:32)

(cid:124)

+

+

1
nT

(cid:88)

ti

1
nT

(cid:88)

ti

(cid:88)

1
nT

(cid:123)(cid:122)
B

ti

(cid:33)

(cid:125)

(cid:96)(f ∗

t (h∗(w∗

t (Xti))), Yti) − ε∗
avg

.

(cid:33)

(cid:125)

(17)

We proceed to bound the three components individually:

• C can be bounded using Hoeffding’s inequality, with probability 1 − δ/2 by (cid:112)ln(2/δ)/(2nT ),

as it contains only nT random variables bounded in the interval [0, 1];

(cid:88)

1
nT

ti
(cid:123)(cid:122)
A

(cid:123)(cid:122)
C

14

Published as a conference paper at ICLR 2020

• B can be bounded by 0, by deﬁnition of ˆw, ˆh and ˆf , as they are the minimizers of Equa-

tion (3);

• the bounding of A is less straightforward and is described in the following.

We deﬁne the following auxiliary function spaces:

• W (cid:48) = {x ∈ X → (wt(xti)) : (w1, . . . , wT ) ∈ W T },
• F (cid:48) = (cid:8)y ∈ RKT n → (ft(yti)) : (f1, . . . , fT ) ∈ F T (cid:9),

and the following auxiliary sets:

• S = (cid:8)((cid:96)(ft(h(wt(Xti))), Yti)) : f ∈ F T , h ∈ H, w ∈ W T (cid:9) ⊆ RT n,
• S(cid:48) = F (cid:48)(H(W (cid:48)( ¯X))) = (cid:8)(ft(h(wt(Xti)))) : f ∈ F T , h ∈ H, w ∈ W T (cid:9) ⊆ RT n,
• S(cid:48)(cid:48) = H(W (cid:48)( ¯X)) = (cid:8)(h(wt(Xti))) : h ∈ H, w ∈ W T (cid:9) ⊆ RKT n,

which will be useful in our proof.

Using Theorem 9 by Maurer et al. (2016), we can write:

εavg( ˆw, ˆh, ˆf ) −

(cid:96)( ˆft(ˆh( ˆwt(Xti))), Yti)

1
nT

(cid:88)

ti

sup

εavg(w, h, f ) −

(cid:96)(ft(h(wt(Xti))), Yti)

1
nT

(cid:88)

ti

(cid:32)

≤

≤

w∈W T ,h∈H,f ∈F T
(cid:115)
√

2πG(S)

+

nT

9 ln( 2
δ )
2nT

,

then by Lipschitz property of the loss function (cid:96) and the contraction lemma Corollary 11 Maurer et al.
1 and c(cid:48)
(2016): G(S) ≤ G(S(cid:48)). By Theorem 12 by Maurer et al. (2016), for universal constants c(cid:48)
2:

G(S(cid:48)) ≤ c(cid:48)

1L(F (cid:48))G(S(cid:48)(cid:48)) + c(cid:48)

2D(S(cid:48)(cid:48))O(F (cid:48)) + min
y∈Y

G(F(y)),

where L(F (cid:48)) is the largest value for the Lipschitz constants in the function space F (cid:48), and D(S(cid:48)(cid:48)) is
the Euclidean diameter of the set S(cid:48)(cid:48).
Using Theorem 12 by Maurer et al. (2016) again, for universal constants c(cid:48)(cid:48)

1 and c(cid:48)(cid:48)
2 :

G(S(cid:48)(cid:48)) ≤ c(cid:48)(cid:48)

1 L(H)G(W (cid:48)( ¯X)) + c(cid:48)(cid:48)

2 D(W (cid:48)( ¯X))O(H) + min

G(H(p)).

(20)

Putting (19) and (20) together:

(cid:18)

G(S(cid:48)) ≤ c(cid:48)

1L(F (cid:48))

1 L(H)G(W (cid:48)( ¯X)) + c(cid:48)(cid:48)
c(cid:48)(cid:48)

2 D(W (cid:48)( ¯X))O(H) + min

G(H(p))

p∈P

p∈P

(cid:19)

+ c(cid:48)

= c(cid:48)

1c(cid:48)(cid:48)

+ c(cid:48)

G(F(y))

2D(S(cid:48)(cid:48))O(F (cid:48)) + min
y∈Y
1 L(F (cid:48))L(H)G(W (cid:48)( ¯X)) + c(cid:48)
2D(S(cid:48)(cid:48))O(F (cid:48)) + min
y∈Y

G(F(y)).

1c(cid:48)(cid:48)

2 L(F (cid:48))D(W (cid:48)( ¯X))O(H) + c(cid:48)

1L(F (cid:48)) min
p∈P

G(H(p))

At this point, we have to bound the individual terms in the right hand side of (21), following the same
procedure proposed by Maurer et al. (2016).

15

(cid:33)

(18)

(19)

(21)

Published as a conference paper at ICLR 2020

Firstly, to bound L(F (cid:48)), let y, y(cid:48) ∈ RKT n, where y = (yti) with yti ∈ RK and y(cid:48) = (y(cid:48)
ti ∈ RK. We can write the following:
y(cid:48)

ti) with

(cid:107)f (y) − f (y(cid:48))(cid:107)2 =

(ft(yti) − ft(y(cid:48)

ti))2

(cid:88)

ti

≤ L(F)2 (cid:88)

(cid:107)yti − y(cid:48)

ti(cid:107)2

ti

= L(F)2(cid:107)y − y(cid:48)(cid:107)2,

(22)

(23)

(24)

whence L(F (cid:48)) ≤ L(F).

Then, we bound:

G(W (cid:48)( ¯X)) = E

(cid:34)

sup
w∈W T

(cid:12)
(cid:12)
(cid:12)
γktiwtk(Xti)
(cid:12)
(cid:12)

(cid:88)

kti

(cid:35)

(cid:88)

Xti

≤

(cid:34)

E

(cid:88)

(cid:12)
(cid:12)
(cid:12)
γkliwk(Xli)
(cid:12)
(cid:12)

(cid:35)

Xli

sup

l∈{1,...,T }

sup
w∈W

t
= T

sup

l∈{1,...,T }

ki
G(W(Xl)).

Then, since it is possible to bound the Euclidean diameter using the norm of the supremum value in
the set, we bound D(S(cid:48)(cid:48)) ≤ 2 suph,w(cid:107)h(w( ¯X))(cid:107) and D(W (cid:48)( ¯X)) ≤ 2 supw∈W T (cid:107)w( ¯X)(cid:107).
Also, we bound O(F (cid:48)):

(cid:21)
(cid:104)γ, g(y) − g(y(cid:48))(cid:105)

= E

(cid:34)

(cid:20)

E

sup
g∈F (cid:48)

γti (ft(yti) − ft(y(cid:48)

ti))

γi (f (yti) − f (y(cid:48)

(cid:35)

(cid:35)
ti))

(cid:88)

ti

sup
f ∈F T

(cid:34)

sup
f ∈F

(cid:88)

i

(cid:34)

(cid:88)

E

t

sup
f ∈F

(cid:88)

i

γi (f (yti) − f (y(cid:48)

ti))

(cid:88)

O(F)2 (cid:88)

(cid:107)yti − y(cid:48)

ti(cid:107)2

(cid:33) 1

2

(cid:88)

E

=

t

√



≤

T



(cid:32)

√

≤

T

√

t

i
T O(F)(cid:107)y − y(cid:48)(cid:107),

=

1
2

(cid:35)2


whence O(F (cid:48)) ≤

T O(F).

√

To minimize the last term, it is possible to choose y0 = 0, as f (0) = 0, ∀f ∈ F, resulting in
miny∈Y G(F(y)) = G(F(0)) = 0.
Then, substituting in (21), and recalling that G(S) ≤ G(S(cid:48)):

G(S) ≤ c(cid:48)

1c(cid:48)(cid:48)

1 L(F)L(H)T

sup

G(W(Xl)) + 2c(cid:48)

l∈{1,...,T }

1c(cid:48)(cid:48)
2 L(F) sup
w∈W T
√

(cid:107)w( ¯X)(cid:107)O(H)

+ c(cid:48)

1L(F) min
p∈P

G(H(p)) + 2c(cid:48)

(cid:107)h(w( ¯X))(cid:107)

T O(F).

(25)

2 sup
h,w

16

Published as a conference paper at ICLR 2020

Now, the ﬁrst term A of (17) can be bounded substituting (25) in (18):

εavg( ˆw, ˆh, ˆf ) −
√

1
nT

(cid:88)

ti

(cid:96)( ˆft(ˆh( ˆwt(Xti))), Yti)

≤

(cid:16)

2π
nT

c(cid:48)
1c(cid:48)(cid:48)

1 L(F)L(H)T

sup

G(W(Xl)) + 2c(cid:48)

1c(cid:48)(cid:48)

l∈{1,...,T }

(cid:107)w( ¯X)(cid:107)O(H)

+ c(cid:48)

1L(F) min
p∈P

G(H(p)) + 2c(cid:48)

(cid:107)h(w( ¯X))(cid:107)

2 sup
h,w

L(F)L(H) supl∈{1,...,T } G(W(Xl))

= c1

n

L(F) minp∈P G(H(p))

+ c3

+ c4

nT

2 L(F) sup
w∈W T
(cid:115)

(cid:17)

√

T O(F)

9 ln( 2
δ )
2nT
supw(cid:107)w( ¯X)(cid:107)L(F)O(H)

+

+ c2

nT
suph,w(cid:107)h(w( ¯X))(cid:107)O(F)

(cid:115)

+

9 ln( 2
δ )
2nT

.

A union bound between A, B and C of (17) completes the proof:

εavg( ˆw, ˆh, ˆf ) − ε∗

avg ≤ c1

L(F)L(H) supl∈{1,...,T } G(W(Xl))

√

n

T

n

+ c2

+ c3

+ c4

(cid:115)

+

supw(cid:107)w( ¯X)(cid:107)L(F)O(H)

L(F) minp∈P G(H(p))

suph,w(cid:107)h(w( ¯X))(cid:107)O(F)

nT

nT

√

n

T

8 ln( 3
δ )
nT

.

B ADDITIONAL DETAILS OF EMPIRICAL EVALUATION

B.1 MULTI FITTED Q-ITERATION

We consider Car-On-Hill problem with discount factor 0.95 and horizon 100. Running Adam
optimizer with learning rate 0.001 and using a mean squared loss, we train a neural network composed
of 2 shared layers of 30 neurons each, with sigmoidal activation function, as described in Riedmiller
(2005). We select 8 tasks for the problem changing the mass of the car m and the value of the
discrete actions a (Table 1). Figure 1(b) is computed considering the ﬁrst four tasks, while Figure 1(c)
considers task 1 in the result with 1 task, tasks 1 and 2 for the result with 2 tasks, tasks 1, 2, 3, and 4
for the result with 4 tasks, and all the tasks for the result with 8 tasks. To run FQI and MFQI, for each
task we collect transitions running an extra-tree trained following the procedure and setting in Ernst
et al. (2005), using an (cid:15)-greedy policy with (cid:15) = 0.1, to obtain a small, but representative dataset. The
optimal Q-function for each task is computed by tree-search3 for 100 states uniformly picked from
the state space, and the 2 discrete actions, for a total of 200 state-action tuples.

B.2 MULTI DEEP Q-NETWORK

The ﬁve problems we consider for this experiment are: Cart-Pole, Acrobot, Mountain-Car, Car-On-
Hill, and Inverted-Pendulum4. The discount factors are respectively 0.99, 0.99, 0.99, 0.95, and 0.95.
The horizons are respectively 500, 1, 000, 1, 000, 100, and 3, 000. The network we use consists of 80
ReLu units for each wt, t ∈ {1, . . . , T } block, with T = 5. Then, the shared block h consists of one

3We follow the method described in Ernst et al. (2005).
4The IDs of the problems in the OpenAI Gym library are: CartPole-v0, Acrobot-v1, and MountainCar-v0.

17

Published as a conference paper at ICLR 2020

Task Mass
1.0
0.8
1.0
1.2
1.0
1.0
0.8
0.85

1
2
3
4
5
6
7
8

Action set
{−4.0; 4.0}
{−4.0; 4.0}
{−4.5; 4.5}
{−4.5; 4.5}

{−4.125; 4.125}

{−4.25; 4.25}

{−4.375; 4.375}

{−4.0; 4.0}

Table 1: Different values of the mass of the car and available actions chosen for the Car-On-Hill tasks
in the MFQI empirical evaluation.

i ) = yt(s, a(t)

i ) = ft(h(wt(s)), a(t)

layer with 80 ReLu units and another one with 80 sigmoid units. Eventually, each ft has a number of
linear units equal to the number of discrete actions a(t)
, i ∈ {1, . . . , #A(t)} of task µt which outputs
i
the action-value Qt(s, a(t)
i ), ∀s ∈ S (t). The use of sigmoid units
in the second layer of h is due to our choice to extract meaningful shared features bounded between 0
and 1 to be used as input of the last linear layer, as in most RL approaches. In practice, we have also
found that sigmoid units help to reduce task interference in multi-task networks, where instead the
linear response of ReLu units cause a problematic increase in the feature values. Furthermore, the use
of a bounded feature space reduces the suph,w(cid:107)h(w( ¯X))(cid:107) term in the upper bound of Theorem 3,
corresponding to the upper bound of the diameter of the feature space, as shown in Appendix A.
The initial replay memory size for each task is 100 and the maximum size is 5, 000. We use Huber
loss with Adam optimizer using learning rate 10−3 and batch size of 100 samples for each task. The
target network is updated every 100 steps. The exploration is ε-greedy with ε linearly decaying from
1 to 0.01 in the ﬁrst 5, 000 steps.

B.3 MULTI DEEP DETERMINISTIC POLICY GRADIENT

The two set of problems we consider for this experiment are: one including Inverted-Pendulum,
Inverted-Double-Pendulum, and Inverted-Pendulum-Swingup, and another one including Hopper-
Stand, Walker-Walk, and Half-Cheetah-Run5. The discount factors are 0.99 and the horizons are
1, 000 for all problems. The actor network is composed of 600 ReLu units for each wt, t ∈ {1, . . . , T }
block, with T = 3. The shared block h has 500 units with ReLu activation function as for MDQN.
Finally, each ft has a number of tanh units equal to the number of dimensions of the continuous
actions a(t) ∈ A(t) of task µt which outputs the policy πt(s) = yt(s) = ft(h(wt(s))), ∀s ∈ S (t).
On the other hand, the critic network consists of the same wt units of the actor, except for the use of
sigmoidal units in the h layer, as in MDQN. In addition to this, the actions a(t) are given as input to
h. Finally, each ft has a single linear unit Qt(s, a(t)) = yt(s, a(t)) = ft(h(wt(s), a(t))), ∀s ∈ S (t).
The initial replay memory size for each task is 64 and the maximum size is 50, 000. We use Huber
loss to update the critic network and the policy gradient to update the actor network. In both cases
the optimization is performed with Adam optimizer and batch size of 64 samples for each task. The
learning rate of the actor is 10−4 and the learning rate of the critic is 10−3. Moreover, we apply
(cid:96)2-penalization to the critic network using a regularization coefﬁcient of 0.01. The target networks are
updated with soft-updates using τ = 10−3. The exploration is performed using the action computed
by the actor network adding a noise generated with an Ornstein-Uhlenbeck process with θ = 0.15
and σ = 0.2. Note that most of these values are taken from the original DDPG paper Lillicrap et al.
(2015), which optimizes them for the single-task scenario.

5The

IDs of

the problems

InvertedPendulumBulletEnv-v0,
InvertedDoublePendulumBulletEnv-v0, and InvertedPendulumSwingupBulletEnv-v0. The names of the
domain and the task of the problems in the DeepMind Control Suite are: hopper-stand, walker-walk, and
cheetah-run.

in the pybullet

library are:

18

