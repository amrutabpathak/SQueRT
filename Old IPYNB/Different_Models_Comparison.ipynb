{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8Gv8TK6pTQ6x"
   },
   "source": [
    "Models Installation:\n",
    "Please note to restart your runtime for these to take effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "WsxorWdiEyFZ",
    "outputId": "2a379a5a-1064-420c-e7d8-ca4a74c57e37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en_core_web_sm==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz#egg=en_core_web_sm==2.1.0 in /usr/local/lib/python3.6/dist-packages (2.1.0)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
      "/usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
      "/usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
      "You can now load the model via spacy.load('en')\n",
      "Collecting spacy-transformers[cuda100]==0.5.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/16/fb/5dbcf7391d6ba0003fb922737340bff5033729f9c967f08f0468259c4f6a/spacy-transformers-0.5.1.tar.gz (59kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 2.0MB/s \n",
      "\u001b[?25hCollecting spacy<2.3.0,>=2.2.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/55/2e/ac00f5c9d01e66cc6ab75eb2a460c9b0dc21ad99a12f810c86a58309e63c/spacy-2.2.4-cp36-cp36m-manylinux1_x86_64.whl (10.6MB)\n",
      "\u001b[K     |████████████████████████████████| 10.6MB 8.2MB/s \n",
      "\u001b[?25hCollecting transformers<2.1.0,>=2.0.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/66/99/ca0e4c35ccde7d290de3c9c236d5629d1879b04927e5ace9bd6d9183e236/transformers-2.0.0-py3-none-any.whl (290kB)\n",
      "\u001b[K     |████████████████████████████████| 296kB 54.9MB/s \n",
      "\u001b[?25hRequirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from spacy-transformers[cuda100]==0.5.1) (1.4.0)\n",
      "Collecting torchcontrib<0.1.0,>=0.0.2\n",
      "  Downloading https://files.pythonhosted.org/packages/72/36/45d475035ab35353911e72a03c1c1210eba63b71e5a6917a9e78a046aa10/torchcontrib-0.0.2.tar.gz\n",
      "Requirement already satisfied: srsly<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy-transformers[cuda100]==0.5.1) (1.0.1)\n",
      "Collecting ftfy<6.0.0,>=5.0.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ec/d8/5e877ac5e827eaa41a7ea8c0dc1d3042e05d7e337604dc2aedb854e7b500/ftfy-5.7.tar.gz (58kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 7.7MB/s \n",
      "\u001b[?25hCollecting dataclasses<0.7,>=0.6\n",
      "  Downloading https://files.pythonhosted.org/packages/26/2f/1095cdc2868052dd1e64520f7c0d5c8c550ad297e944e641dbf1ffbb9a5d/dataclasses-0.6-py3-none-any.whl\n",
      "Requirement already satisfied: importlib_metadata>=0.20 in /usr/local/lib/python3.6/dist-packages (from spacy-transformers[cuda100]==0.5.1) (1.5.0)\n",
      "Collecting thinc_gpu_ops<0.1.0,>=0.0.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a4/ad/11ab80a24bcedd7dd0cfabaedba2ceaeca11f1aaeeff432a3d2e63ca7d02/thinc_gpu_ops-0.0.4.tar.gz (483kB)\n",
      "\u001b[K     |████████████████████████████████| 491kB 45.4MB/s \n",
      "\u001b[?25hCollecting cupy-cuda100>=5.0.0b4\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/70/1022cc25659bbef5932c590dcd44443a68dad723229fbc49e540c864ea6d/cupy_cuda100-8.0.0a1-cp36-cp36m-manylinux1_x86_64.whl (337.6MB)\n",
      "\u001b[K     |████████████████████████████████| 337.6MB 49kB/s \n",
      "\u001b[?25hCollecting blis<0.5.0,>=0.4.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/41/19/f95c75562d18eb27219df3a3590b911e78d131b68466ad79fdf5847eaac4/blis-0.4.1-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n",
      "\u001b[K     |████████████████████████████████| 3.7MB 35.2MB/s \n",
      "\u001b[?25hRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.3.0,>=2.2.1->spacy-transformers[cuda100]==0.5.1) (2.0.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.3.0,>=2.2.1->spacy-transformers[cuda100]==0.5.1) (1.0.2)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.3.0,>=2.2.1->spacy-transformers[cuda100]==0.5.1) (0.6.0)\n",
      "Collecting tqdm<5.0.0,>=4.38.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/47/55/fd9170ba08a1a64a18a7f8a18f088037316f2a41be04d2fe6ece5a653e8f/tqdm-4.43.0-py2.py3-none-any.whl (59kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 8.1MB/s \n",
      "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy<2.3.0,>=2.2.1->spacy-transformers[cuda100]==0.5.1) (45.2.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.3.0,>=2.2.1->spacy-transformers[cuda100]==0.5.1) (2.21.0)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/db/6b/e07fad36913879757c90ba03d6fb7f406f7279e11dcefc105ee562de63ea/preshed-3.0.2-cp36-cp36m-manylinux1_x86_64.whl (119kB)\n",
      "\u001b[K     |████████████████████████████████| 122kB 60.8MB/s \n",
      "\u001b[?25hRequirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy<2.3.0,>=2.2.1->spacy-transformers[cuda100]==0.5.1) (0.9.6)\n",
      "Collecting catalogue<1.1.0,>=0.0.7\n",
      "  Downloading https://files.pythonhosted.org/packages/6c/f9/9a5658e2f56932e41eb264941f9a2cb7f3ce41a80cb36b2af6ab78e2f8af/catalogue-1.0.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.3.0,>=2.2.1->spacy-transformers[cuda100]==0.5.1) (1.17.5)\n",
      "Collecting thinc==7.4.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/73/ed/8e4559f1090fb05c0fa982a8a2caaa315967e7b460652be479d13fd1c813/thinc-7.4.0-cp36-cp36m-manylinux1_x86_64.whl (2.2MB)\n",
      "\u001b[K     |████████████████████████████████| 2.2MB 45.3MB/s \n",
      "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from transformers<2.1.0,>=2.0.0->spacy-transformers[cuda100]==0.5.1) (2019.12.20)\n",
      "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers<2.1.0,>=2.0.0->spacy-transformers[cuda100]==0.5.1) (1.11.15)\n",
      "Collecting sacremoses\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\n",
      "\u001b[K     |████████████████████████████████| 870kB 54.3MB/s \n",
      "\u001b[?25hCollecting sentencepiece\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
      "\u001b[K     |████████████████████████████████| 1.0MB 44.1MB/s \n",
      "\u001b[?25hRequirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from ftfy<6.0.0,>=5.0.0->spacy-transformers[cuda100]==0.5.1) (0.1.8)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib_metadata>=0.20->spacy-transformers[cuda100]==0.5.1) (3.1.0)\n",
      "Requirement already satisfied: fastrlock>=0.3 in /usr/local/lib/python3.6/dist-packages (from cupy-cuda100>=5.0.0b4->spacy-transformers[cuda100]==0.5.1) (0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.3.0,>=2.2.1->spacy-transformers[cuda100]==0.5.1) (2019.11.28)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.3.0,>=2.2.1->spacy-transformers[cuda100]==0.5.1) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.3.0,>=2.2.1->spacy-transformers[cuda100]==0.5.1) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.3.0,>=2.2.1->spacy-transformers[cuda100]==0.5.1) (1.24.3)\n",
      "Requirement already satisfied: botocore<1.15.0,>=1.14.15 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers<2.1.0,>=2.0.0->spacy-transformers[cuda100]==0.5.1) (1.14.15)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers<2.1.0,>=2.0.0->spacy-transformers[cuda100]==0.5.1) (0.3.3)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers<2.1.0,>=2.0.0->spacy-transformers[cuda100]==0.5.1) (0.9.4)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers<2.1.0,>=2.0.0->spacy-transformers[cuda100]==0.5.1) (1.12.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers<2.1.0,>=2.0.0->spacy-transformers[cuda100]==0.5.1) (7.0)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers<2.1.0,>=2.0.0->spacy-transformers[cuda100]==0.5.1) (0.14.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers<2.1.0,>=2.0.0->spacy-transformers[cuda100]==0.5.1) (2.6.1)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers<2.1.0,>=2.0.0->spacy-transformers[cuda100]==0.5.1) (0.15.2)\n",
      "Building wheels for collected packages: spacy-transformers, torchcontrib, ftfy, thinc-gpu-ops, sacremoses\n",
      "  Building wheel for spacy-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for spacy-transformers: filename=spacy_transformers-0.5.1-py2.py3-none-any.whl size=52835 sha256=6cbc4195bb8809e7d686bb179c9ddb5f1ec0276c8082e8e45d6cc4f175f4b6f6\n",
      "  Stored in directory: /root/.cache/pip/wheels/29/c2/17/625a3d14da8cabe9781ab1648d489d1b41a8a81dc289e5af1f\n",
      "  Building wheel for torchcontrib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for torchcontrib: filename=torchcontrib-0.0.2-cp36-none-any.whl size=7533 sha256=87998df17c9f4900eacb50348b21ca8591af006d9361251d8b5f8a1ae1fbe857\n",
      "  Stored in directory: /root/.cache/pip/wheels/06/06/7b/a5f5920bbf4f12a2c927e438fac17d4cd9560f8336b00e9a99\n",
      "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for ftfy: filename=ftfy-5.7-cp36-none-any.whl size=44593 sha256=3d47b5850208bc8b25926f9bd446aa9b27c0f5dbeaf88060682069e3b5ddd3a8\n",
      "  Stored in directory: /root/.cache/pip/wheels/8e/da/59/6c8925d571aacade638a0f515960c21c0887af1bfe31908fbf\n",
      "  Building wheel for thinc-gpu-ops (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for thinc-gpu-ops: filename=thinc_gpu_ops-0.0.4-cp36-cp36m-linux_x86_64.whl size=221824 sha256=57d6576dfc41633b3d53d11b86cd62d678271027b02229a335952e0ea3bc753e\n",
      "  Stored in directory: /root/.cache/pip/wheels/eb/ba/a3/9af9f326ed0d75a4540378af64a05a0e42be39d9b8513f3aea\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.38-cp36-none-any.whl size=884628 sha256=ebd92a8d6c3d823eb5f7e2a49d2113e8445ab096deaa7b641c41b1edf78fbc34\n",
      "  Stored in directory: /root/.cache/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n",
      "Successfully built spacy-transformers torchcontrib ftfy thinc-gpu-ops sacremoses\n",
      "\u001b[31mERROR: spacy 2.2.4 has requirement srsly<1.1.0,>=1.0.2, but you'll have srsly 1.0.1 which is incompatible.\u001b[0m\n",
      "Installing collected packages: blis, tqdm, preshed, catalogue, thinc, spacy, sacremoses, sentencepiece, transformers, torchcontrib, ftfy, dataclasses, thinc-gpu-ops, cupy-cuda100, spacy-transformers\n",
      "  Found existing installation: blis 0.2.4\n",
      "    Uninstalling blis-0.2.4:\n",
      "      Successfully uninstalled blis-0.2.4\n",
      "  Found existing installation: tqdm 4.28.1\n",
      "    Uninstalling tqdm-4.28.1:\n",
      "      Successfully uninstalled tqdm-4.28.1\n",
      "  Found existing installation: preshed 2.0.1\n",
      "    Uninstalling preshed-2.0.1:\n",
      "      Successfully uninstalled preshed-2.0.1\n",
      "  Found existing installation: thinc 7.0.8\n",
      "    Uninstalling thinc-7.0.8:\n",
      "      Successfully uninstalled thinc-7.0.8\n",
      "  Found existing installation: spacy 2.1.9\n",
      "    Uninstalling spacy-2.1.9:\n",
      "      Successfully uninstalled spacy-2.1.9\n",
      "  Found existing installation: dataclasses 0.7\n",
      "    Uninstalling dataclasses-0.7:\n",
      "      Successfully uninstalled dataclasses-0.7\n",
      "Successfully installed blis-0.4.1 catalogue-1.0.0 cupy-cuda100-8.0.0a1 dataclasses-0.6 ftfy-5.7 preshed-3.0.2 sacremoses-0.0.38 sentencepiece-0.1.85 spacy-2.2.4 spacy-transformers-0.5.1 thinc-7.4.0 thinc-gpu-ops-0.0.4 torchcontrib-0.0.2 tqdm-4.43.0 transformers-2.0.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "tqdm"
        ]
       }
      }
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;3m⚠ Skipping model package dependencies and setting `--no-deps`. You\n",
      "don't seem to have the spaCy package itself installed (maybe because you've\n",
      "built from source?), so installing the model dependencies would cause spaCy to\n",
      "be downloaded, which probably isn't what you want. If the model package has\n",
      "other dependencies, you'll have to install them manually.\u001b[0m\n",
      "Collecting en_trf_bertbaseuncased_lg==2.2.0\n",
      "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_trf_bertbaseuncased_lg-2.2.0/en_trf_bertbaseuncased_lg-2.2.0.tar.gz (405.8MB)\n",
      "\u001b[K     |████████████████████████████████| 405.8MB 1.2MB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: en-trf-bertbaseuncased-lg\n",
      "  Building wheel for en-trf-bertbaseuncased-lg (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for en-trf-bertbaseuncased-lg: filename=en_trf_bertbaseuncased_lg-2.2.0-cp36-none-any.whl size=405819945 sha256=c372f55f8909171a75b6f40d06e404ad4e296176d4bbb343a5c1fbf67e05e455\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-9xecsi5r/wheels/f6/60/8c/c6f517ef9729972f1be15c3aab4b93e7ec9fbeb71d072a84de\n",
      "Successfully built en-trf-bertbaseuncased-lg\n",
      "Installing collected packages: en-trf-bertbaseuncased-lg\n",
      "Successfully installed en-trf-bertbaseuncased-lg-2.2.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_trf_bertbaseuncased_lg')\n",
      "\u001b[38;5;3m⚠ Skipping model package dependencies and setting `--no-deps`. You\n",
      "don't seem to have the spaCy package itself installed (maybe because you've\n",
      "built from source?), so installing the model dependencies would cause spaCy to\n",
      "be downloaded, which probably isn't what you want. If the model package has\n",
      "other dependencies, you'll have to install them manually.\u001b[0m\n",
      "Collecting en_trf_robertabase_lg==2.2.0\n",
      "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_trf_robertabase_lg-2.2.0/en_trf_robertabase_lg-2.2.0.tar.gz (291.9MB)\n",
      "\u001b[K     |████████████████████████████████| 291.9MB 1.1MB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: en-trf-robertabase-lg\n",
      "  Building wheel for en-trf-robertabase-lg (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for en-trf-robertabase-lg: filename=en_trf_robertabase_lg-2.2.0-cp36-none-any.whl size=295935111 sha256=9b0b6fd310d6434199b7606ee7ee866fcc9c68ad0ff908a307de23a57f5c6a0a\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-g9kj2sda/wheels/82/b8/a4/e342de6946be5d55f0b249613d0448fc658f4941403ac48ad3\n",
      "Successfully built en-trf-robertabase-lg\n",
      "Installing collected packages: en-trf-robertabase-lg\n",
      "Successfully installed en-trf-robertabase-lg-2.2.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_trf_robertabase_lg')\n",
      "\u001b[38;5;3m⚠ Skipping model package dependencies and setting `--no-deps`. You\n",
      "don't seem to have the spaCy package itself installed (maybe because you've\n",
      "built from source?), so installing the model dependencies would cause spaCy to\n",
      "be downloaded, which probably isn't what you want. If the model package has\n",
      "other dependencies, you'll have to install them manually.\u001b[0m\n",
      "Collecting en_trf_distilbertbaseuncased_lg==2.2.0\n",
      "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_trf_distilbertbaseuncased_lg-2.2.0/en_trf_distilbertbaseuncased_lg-2.2.0.tar.gz (245.0MB)\n",
      "\u001b[K     |████████████████████████████████| 245.0MB 68.3MB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: en-trf-distilbertbaseuncased-lg\n",
      "  Building wheel for en-trf-distilbertbaseuncased-lg (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for en-trf-distilbertbaseuncased-lg: filename=en_trf_distilbertbaseuncased_lg-2.2.0-cp36-none-any.whl size=245033117 sha256=8cdd2123e5347c4be80923cd687436e9ee701e5f22c068e68045b3f717503b65\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-a__zkn6d/wheels/ed/90/91/d6d33eb6c8ac5696288f9c034cead21c5bcc1786d04625f69a\n",
      "Successfully built en-trf-distilbertbaseuncased-lg\n",
      "Installing collected packages: en-trf-distilbertbaseuncased-lg\n",
      "Successfully installed en-trf-distilbertbaseuncased-lg-2.2.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_trf_distilbertbaseuncased_lg')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en\n",
    "!pip install spacy-transformers[cuda100]==0.5.1\n",
    "!python -m spacy download en_trf_bertbaseuncased_lg\n",
    "!python -m spacy download en_trf_robertabase_lg\n",
    "!python -m spacy download en_trf_distilbertbaseuncased_lg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QVCNBUKUTbCe"
   },
   "source": [
    "One time step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 208
    },
    "colab_type": "code",
    "id": "1eCjcuff9qy1",
    "outputId": "bd92d6f6-43f2-489a-eceb-8f972ad00ce1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\r",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "./\n",
      "./tfhub_module.pb\n",
      "./variables/\n",
      "./variables/variables.data-00000-of-00001\n",
      " 91  745M   91  681M    0     0  54.6M      0  0:00:13  0:00:12  0:00:01 49.6M./variables/variables.index\n",
      "./assets/\n",
      "./saved_model.pb\n",
      "100  745M  100  745M    0     0  55.2M      0  0:00:13  0:00:13 --:--:-- 54.4M\n"
     ]
    }
   ],
   "source": [
    "#One time step\n",
    "!mkdir /content/googleUSE\n",
    "!curl -L \"https://tfhub.dev/google/universal-sentence-encoder-large/3?tf-hub-format=compressed\" | tar -zxvC /content/googleUSE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dfH1VnzY9YIy"
   },
   "outputs": [],
   "source": [
    "class QuerySnippet:\n",
    "\n",
    "    def __init__(self, query, snippet, similarity):\n",
    "        self.query = query\n",
    "        self.snippet = snippet\n",
    "        self.similarity = similarity\n",
    "\n",
    "    \n",
    "    def __gt__(self, other):\n",
    "        return self.similarity > other.similarity\n",
    "\n",
    "    def __lt__(self, other):\n",
    "        return self.similarity < other.similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "FX1KL8dV9jt4",
    "outputId": "f4b8619c-d0ef-445c-8d13-885a8c7fade5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_hub as hub\n",
    "import csv \n",
    "import numpy as np\n",
    "import heapq\n",
    "import tensorflow\n",
    "\n",
    "modelHeapDict2 = {}\n",
    "model = 'Google_USE'\n",
    "\n",
    "def googleUSEEmbedding(modelName):\n",
    "    with tensorflow.Graph().as_default():\n",
    "        researchStr = tensorflow.placeholder(tensorflow.string)\n",
    "        hubModuleFn = hub.Module(modelName)\n",
    "        reserachEmbedRes = hubModuleFn(researchStr)\n",
    "        s = tensorflow.train.MonitoredSession()\n",
    "    return lambda x: s.run(reserachEmbedRes, {researchStr: x})\n",
    "researchEmbedding = googleUSEEmbedding('/content/googleUSE/')\n",
    "\n",
    "def similarSentencesWIthGoogleUSE(researchPaper, queriesList, highestScoredNSnippets):\n",
    "  print('Inside')\n",
    "  with open(researchPaper) as researchPaperCSV:\n",
    "    researchPaperReader = csv.reader(researchPaperCSV)\n",
    "    print(researchPaperReader)\n",
    "    score_max_heap = [] \n",
    "    for query in queriesList:\n",
    "      #print(query)\n",
    "      embeddedQuery = researchEmbedding([query])\n",
    "      #print(embeddedQuery)\n",
    "      for snippet in researchPaperReader:\n",
    "          if('<EOS>' not in snippet):\n",
    "            snippetStr = \" \"\n",
    "            snippetStr = ' '.join([str(elem) for elem in snippet])\n",
    "            #print('snippetStr:', snippetStr)\n",
    "            embeddedSnippet = researchEmbedding([snippetStr])\n",
    "            #print(embeddedSnippet)\n",
    "            qs = QuerySnippet(query, snippet,  np.inner(embeddedQuery, embeddedSnippet))\n",
    "            print(qs.query)\n",
    "            print(qs.snippet)\n",
    "            print(qs.similarity)\n",
    "            if len(score_max_heap) < highestScoredNSnippets or qs.similarity > score_max_heap[0].similarity:\n",
    "              if len(score_max_heap) == highestScoredNSnippets: heapq.heappop(score_max_heap)\n",
    "              heapq.heappush( score_max_heap, qs )\n",
    "      modelHeapDict2[model] =  score_max_heap       \n",
    "  return  modelHeapDict2  \n",
    "\n",
    "#similarSentencesWIthGoogleUSE('/content/large_batch_optimization_for_deep_learning_training_bert_in_76_minutes_1.csv',[\"Which is the best model\"],10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "id": "43DaEWvF-Z8y",
    "outputId": "dc11b63c-465d-437e-c1b8-eeb06807258d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for h in modelHeapDict2:\\n  print(h)\\n  print(modelHeapDict2[h])\\n  while modelHeapDict2[h]:\\n    qs = heapq.heappop(modelHeapDict2[h])\\n    print(qs.snippet)\\n    print(qs.similarity)  \\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''for h in modelHeapDict2:\n",
    "  print(h)\n",
    "  print(modelHeapDict2[h])\n",
    "  while modelHeapDict2[h]:\n",
    "    qs = heapq.heappop(modelHeapDict2[h])\n",
    "    print(qs.snippet)\n",
    "    print(qs.similarity)  \n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K5SroDABUxPi"
   },
   "source": [
    "Pass the snippet csv file, query list and number of results desired.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "uA9uEOMb-iqT",
    "outputId": "70d9eb7f-7f80-4814-a810-3cabeba4ec82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside\n",
      "<_csv.reader object at 0x7f8f78b3e198>\n",
      "Which is the best model\n",
      "['Published as a conference paper at ICLR 2020  GRAPH INFERENCE LEARNING FOR SEMI-SUPERVISED CLASSIFICATION  Chunyan Xu', ' Zhen Cui∗', ' Xiaobin Hong', ' Tong Zhang', ' and Jian Yang School of Computer Science and Engineering', ' Nanjing University of Science and Technology', ' Nanjing', ' China {cyx', 'zhen.cui', 'xbhong', 'tong.zhang', 'csjyang}@njust.edu.cn  Wei Liu Tencent AI Lab', ' China wl2223@columbia.edu  ABSTRACT  In this work', ' we address semi-supervised classification of graph data', ' where the categories of those unlabeled nodes are inferred from labeled nodes as well as graph structures.Recent works often solve this problem via advanced graph convolution in a conventionally supervised manner', ' but the performance could degrade significantly when labeled data is scarce.To this end', ' we propose a Graph Inference Learning (GIL) framework to boost the performance of semi- supervised node classification by learning the inference of node labels on graph topology.']\n",
      "[[0.21148565]]\n",
      "Which is the best model\n",
      "['To bridge the connection between two nodes', ' we formally define a structure relation by encapsulating node attributes', ' between-node paths', ' and local topological structures together', ' which can make the inference conveniently deduced from one node to another node.For learning the inference process', ' we further introduce meta-optimization on structure relations from training nodes to validation nodes', ' such that the learnt graph inference capability can be better self-adapted to testing nodes.Comprehensive evaluations on four benchmark datasets (including Cora', ' Citeseer', ' Pubmed', ' and NELL) demonstrate the superiority of our proposed GIL when compared against state-of-the-art methods on the semi-supervised node classification task.']\n",
      "[[0.18775186]]\n",
      "Which is the best model\n",
      "[' 1  INTRODUCTION  Graph', ' which comprises a set of vertices/nodes together with connected edges', ' is a formal structural representation of non-regular data.Due to the strong representation ability', ' it accommodates many potential applications', ' e.g.', ' social network (Orsini et al', ' 2017)', ' world wide data (Page et al', ' 1999)', ' knowledge graph (Xu et al', ' 2017)', ' and protein-interaction network (Borgwardt et al', ' 2007).Among these', ' semi-supervised node classification on graphs is one of the most interesting also popular topics.']\n",
      "[[0.22200927]]\n",
      "Which is the best model\n",
      "['Given a graph in which some nodes are labeled', ' the aim of semi-supervised classification is to infer the categories of those remaining unlabeled nodes by using various priors of the graph. While there have been numerous previous works (Brandes et al', ' 2008; Zhou et al', ' 2004; Zhu et al', ' 2003; Yang et al', ' 2016; Zhao et al', ' 2019) devoted to semi-supervised node classification based on explicit graph Laplacian regularizations', ' it is hard to efficiently boost the performance of label prediction due to the strict assumption that connected nodes are likely to share the same label information.With the progress of deep learning on grid-shaped images/videos (He et al', ' 2016)', ' a few of graph convolutional neural networks (CNN) based methods', ' including spectral (Kipf & Welling', ' 2017) and spatial methods (Niepert et al', ' 2016; Pan et al', ' 2018; Yu et al', ' 2018)', ' have been proposed to learn local convolution filters on graphs in order to extract more discriminative node representations.']\n",
      "[[0.1369508]]\n",
      "Which is the best model\n",
      "['Although graph CNN based methods have achieved considerable capabilities of graph embedding by optimizing filters', ' they are limited into a conventionally semi-supervised framework and lack of an efficient inference mechanism on graphs.Especially', ' in the case of few-shot learning', ' where a small number of training nodes are labeled', ' this kind of methods would drastically compromise the performance.For example', ' the Pubmed graph dataset (Sen et al', ' 2008) consists  ∗Corresponding author: Zhen Cui.']\n",
      "[[0.21954583]]\n",
      "Which is the best model\n",
      "[' 1  \\x0cPublished as a conference paper at ICLR 2020  Figure 1: The illustration of our proposed GIL framework.For the problem of graph node labeling', ' the category information of these unlabeled nodes depends on the similarity computation between a query node (e.g.', ' vj) and these labeled reference nodes (e.g.', ' vi).We consider the similarity from three points: node attributes', ' the consistency of local topological structures (i.e.', ' the circle with dashed line)', ' and the between-node path reachability (i.e.', ' the red wave line from vi to vj).']\n",
      "[[0.18179213]]\n",
      "Which is the best model\n",
      "['Specifically', ' the local structures as well as node attributes are encoded as high-level features with graph convolution', ' while the between-node path reachability is abstracted as reachable probabilities of random walks.To better make the inference generalize to test nodes', ' we introduce a meta-learning strategy to optimize the structure relations learning from training nodes to validation nodes. of 19', '717 nodes and 44', '338 edges', ' but only 0.3% nodes are labeled for the semi-supervised node classification task.']\n",
      "[[0.09866817]]\n",
      "Which is the best model\n",
      "['These aforementioned works usually boil down to a general classification task', ' where the model is learnt on a training set and selected by checking a validation set.However', ' they do not put great efforts on how to learn to infer from one node to another node on a topological graph', ' especially in the few-shot regime. In this paper', ' we propose a graph inference learning (GIL) framework to teach the model itself to adaptively infer from reference labeled nodes to those query unlabeled nodes', ' and finally boost the performance of semi-supervised node classification in the case of a few number of labeled samples.']\n",
      "[[0.22600582]]\n",
      "Which is the best model\n",
      "['Given an input graph', ' GIL attempts to infer the unlabeled nodes from those observed nodes by building between-node relations.The between-node relations are structured as the integration of node attributes', ' connection paths', ' and graph topological structures.It means that the similarity between two nodes is decided from three aspects: the consistency of node attributes', ' the consistency of local topological structures', ' and the between-node path reachability', ' as shown in Fig 1.']\n",
      "[[0.10251306]]\n",
      "Which is the best model\n",
      "['The local structures anchored around each node as well as the attributes of nodes therein are jointly encoded with graph convolution (Defferrard et al', ' 2016) for the sake of high-level feature extraction.For the between-node path reachability', ' we adopt the random walk algorithm to obtain the characteristics from a labeled reference node vi to a query unlabeled node vj in a given graph.Based on the computed node representation and between-node reachability', ' the structure relations can be obtained by computing the similar scores/relationships from reference nodes to unlabeled nodes in a graph.']\n",
      "[[0.0741703]]\n",
      "Which is the best model\n",
      "['Inspired by the recent meta-learning strategy (Finn et al', ' 2017)', ' we learn to infer the structure relations from a training set to a validation set', ' which can benefit the generalization capability of the learned model.In other words', ' our proposed GIL attempts to learn some transferable knowledge underlying in the structure relations from training samples to validation samples', ' such that the learned structure relations can be better self-adapted to the new testing stage. We summarize the main contributions of this work as three folds:  • We propose a novel graph inference learning framework by building structure relations to infer unknown node labels from those labeled nodes in an end-to-end way.']\n",
      "[[0.12410951]]\n",
      "Which is the best model\n",
      "['The structure relations are well defined by jointly considering node attributes', ' between-node paths', ' and graph topological structures. • To make the inference model better generalize to test nodes', ' we introduce a meta-learning procedure to optimize structure relations', ' which could be the first time for graph node classification to the best of our knowledge. • Comprehensive evaluations on three citation network datasets (including Cora', ' Citeseer', ' and Pubmed) and one knowledge graph data (i.e.', ' NELL) demonstrate the superiority of our proposed GIL in contrast with other state-of-the-art methods on the semi-supervised classification task.']\n",
      "[[0.23086345]]\n",
      "Which is the best model\n",
      "[' 2     (b) The process of Graph inference learning. We extract the local representation from the local subgraph (the circle with dashed line     The red wave line denote the node reachability from     to     dt th hbilit f  d t th d   \\x0cPublished as a conference paper at ICLR 2020  2 RELATED WORK  Graph CNNs: With the rapid development of deep learning methods', ' various graph convolution neural networks (Kashima et al', ' 2003; Morris et al', ' 2017; Shervashidze et al', ' 2009; Yanardag & Vishwanathan', ' 2015; Jiang et al', ' 2019; Zhang et al', ' 2020) have been exploited to analyze the irregular graph-structured data.For better extending general convolutional neural networks to graph domains', ' two broad strategies have been proposed', ' including spectral and spatial convolution methods.']\n",
      "[[0.2230868]]\n",
      "Which is the best model\n",
      "['Specifically', ' spectral filtering methods (Henaff et al', ' 2015; Kipf & Welling', ' 2017) develop convolution-like operators in the spectral domain', ' and then perform a series of spectral filters by decomposing the graph Laplacian.Unfortunately', ' the spectral-based approaches often lead to a high computational complex due to the operation of eigenvalue decomposition', ' especially for a large number of graph nodes.To alleviate this computation burden', ' local spectral filtering methods (Defferrard et al', ' 2016) are then proposed by parameterizing the frequency responses as a Chebyshev polynomial approximation.']\n",
      "[[0.17733252]]\n",
      "Which is the best model\n",
      "['Another type of graph CNNs', ' namely spatial methods (Li et al', ' 2016; Niepert et al', ' 2016)', ' can perform the filtering operation by defining the spatial structures of adjacent vertices.Various approaches can be employed to aggregate or sort neighboring vertices', ' such as diffusion CNNs (Atwood & Towsley', ' 2016)', ' GraphSAGE (Hamilton et al', ' 2017)', ' PSCN (Niepert et al', ' 2016)', ' and NgramCNN (Luo et al', ' 2017).From the perspective of data distribution', ' recently', ' the Gaussian induced convolution model (Jiang et al', ' 2019) is proposed to disentangle the aggregation process through encoding adjacent regions with Gaussian mixture model.']\n",
      "[[0.21588269]]\n",
      "Which is the best model\n",
      "[' Semi-supervised node classification: Among various graph-related applications', ' semi-supervised node classification has gained increasing attention recently', ' and various approaches have been proposed to deal with this problem', ' including explicit graph Laplacian regularization and graph- embedding approaches.Several classic algorithms with graph Laplacian regularization contain the label propagation method using Gaussian random fields (Zhu et al', ' 2003)', ' the regularization framework by relying on the local/global consistency (Zhou et al', ' 2004)', ' and the random walk- based sampling algorithm for acquiring the context information (Yang et al', ' 2016).To further address scalable semi-supervised learning issues (Liu et al', ' 2012)', ' the Anchor Graph regularization approach (Liu et al', ' 2010) is proposed to scale linearly with the number of graph nodes and then applied to massive-scale graph datasets.']\n",
      "[[0.2200718]]\n",
      "Which is the best model\n",
      "['Several graph convolution network methods (Abu-El-Haija et al', ' 2018; Du et al', ' 2017; Thekumparampil et al', ' 2018; Velickovic et al', ' 2018; Zhuang & Ma', ' 2018) are then developed to obtain discriminative representations of input graphs.For example', ' Kipf et al (Kipf & Welling', ' 2017) proposed a scalable graph CNN model', ' which can scale linearly in the number of graph edges and learn graph representations by encoding both local graph structures and node attributes.Graph attention networks (GAT) (Velickovic et al', ' 2018) are proposed to compute hidden representations of each node for attending to its neighbors with a self-attention strategy.']\n",
      "[[0.18767118]]\n",
      "Which is the best model\n",
      "['By jointly considering the local- and global-consistency information', ' dual graph convolutional networks (Zhuang & Ma', ' 2018) are presented to deal with semi-supervised node classification.The critical difference between our proposed GIL and those previous semi-supervised node classification methods is to adopt a graph inference strategy by defining structure relations on graphs and then leverage a meta optimization mechanism to learn an inference model', ' which could be the first time to the best of our knowledge', ' while the existing graph CNNs take semi-supervised node classification as a general classification task. 3 THE PROPOSED MODEL 3.1 PROBLEM DEFINITION  Formally', ' we denote an undirected/directed graph as G = {V', ' E', ' X ', ' Y}', ' where V = {vi}n i=1 is the finite set of n (or |V|) vertices', ' E ∈ Rn×n defines the adjacency relationships (i.e.', ' edges) between vertices representing the topology of G', ' X ∈ Rn×d records the explicit/implicit attributes/signals of vertices', ' and Y ∈ Rn is the vertex labels of C classes.']\n",
      "[[0.21144313]]\n",
      "Which is the best model\n",
      "['The edge Eij = E(vi', ' vj) = 0 if and only if vertices vi', ' vj are not connected', ' otherwise Eij (cid:54)= 0.The attribute matrix X is attached to the vertex set V', ' whose i-th row Xvi (or Xi·) represents the attribute of the i-th vertex vi.It means that vi ∈ V carries a vector of d-dimensional signals.']\n",
      "[[0.15217632]]\n",
      "Which is the best model\n",
      "['Associated with each node vi ∈ V', ' there is a discrete label yi ∈ {1', ' 2', ' · · · ', ' C}. We consider the task of semi-supervised node classification over graph data', ' where only a small number of vertices are labeled for the model learning', ' i.e.', ' |VLabel| (cid:28) |V|.Generally', ' we have three node sets: a training set Vtr', ' a validation set Vval', ' and a testing set Vte.']\n",
      "[[0.2080113]]\n",
      "Which is the best model\n",
      "['In the standard protocol  3  \\x0cPublished as a conference paper at ICLR 2020  of prior literatures (Yang et al', ' 2016)', ' the three node sets share the same label space.We follow but do not restrict this protocol for our proposed method.Given the training and validation node sets', ' the aim is to predict the node labels of testing nodes by using node attributes as well as edge connections.']\n",
      "[[0.06709408]]\n",
      "Which is the best model\n",
      "['A sophisticated machine learning technique used in most existing methods (Kipf & Welling', ' 2017; Zhou et al', ' 2004) is to choose the optimal classifier (trained on a training set) after checking the performance on the validation set.However', ' these methods essentially ignore how to extract transferable knowledge from these known labeled nodes to unlabeled nodes', ' as the graph structure itself implies node connectivity/reachability.Moreover', ' due to the scarcity of labeled samples', ' the performance of such a classifier is usually not satisfying.']\n",
      "[[0.1899946]]\n",
      "Which is the best model\n",
      "['To address these issues', ' we introduce a meta-learning mechanism (Finn et al', ' 2017; Ravi & Larochelle', ' 2017; Sung et al', ' 2017) to learn to infer node labels on graphs.Specifically', ' the graph structure', ' between-node path reachability', ' and node attributes are jointly modeled into the learning process.Our aim is to learn to infer from labeled nodes to unlabeled nodes', ' so that the learner can perform better on a validation set and thus classify a testing set more accurately.']\n",
      "[[0.10268829]]\n",
      "Which is the best model\n",
      "[' 3.2 STRUCTURE RELATION For convenient inference', ' we specifically build a structure relation between two nodes on the topology graph.The labeled vertices (in a training set) are viewed as the reference nodes', ' and their information can be propagated into those unlabeled vertices for improving the label prediction accuracy.Formally', ' given a reference node vi ∈ VLabel', ' we define the score of a query node vj similar to vi as  (1) where Gvi and Gvj may be understood as the centralized subgraphs around vi and vj', ' respectively. fe', ' fr', ' fP are three abstract functions that we explain as follows:  si→j = fr(fe(Gvi )', ' fe(Gvj )', ' fP (vi', ' vj', ' E))', '  • Node representation fe(Gvi) −→ Rdv ', ' encodes the local representation of the centralized subgraph Gvi around node vi', ' and may thus be understood as a local filter function on graphs.']\n",
      "[[0.18886666]]\n",
      "Which is the best model\n",
      "['This function should not only take the signals of nodes therein as input', ' but also consider the local topological structure of the subgraph for more accurate similarity computation.To this end', ' we perform the spectral graph convolution on subgraphs to learn discriminative node features', ' analogous to the pixel-level feature extraction from convolution maps of gridded images.The details of feature extraction fe are described in Section 4.']\n",
      "[[0.10896072]]\n",
      "Which is the best model\n",
      "[' • Path reachability fP (vi', ' vj', ' E) −→ Rdp ', ' represents the characteristics of path reachability from vi to vj.As there usually exist multiple traversal paths between two nodes', ' we choose the function as reachable probabilities of different lengths of walks from vi to vj.More details will be introduced in Section 4.']\n",
      "[[0.03865173]]\n",
      "Which is the best model\n",
      "[' • Structure relation fr(Rdv ', ' Rdv ', ' Rdp ) −→ R', ' is a relational function computing the score of vj similar to vi.This function is not exchangeable for different orders of two nodes', ' due to the asymmetric reachable relationship fPIf necessary', ' we may easily revise it as a symmetry function', ' e.g.', ' summarizing two traversal directions.The score function depends on triple inputs: the local representations extracted from the subgraphs w.r.t. fe(Gvi) and fe(Gvj )', ' respectively', ' and the path reachability from vi to vj.']\n",
      "[[0.1623441]]\n",
      "Which is the best model\n",
      "[' In semi-supervised node classification', ' we take the training node set Vtr as the reference samples', ' and the validation set Vval as the query samples during the training stage.Given a query node vj ∈ Vval', ' we can derive the class similarity score of vj w.r.t. the c-th (c = 1', ' · · · ', ' C) category by weighting the reference samples Cc = {vk|yvk = c}.Formally', ' we can further revise Eqn (1) and define the class-to-node relationship function as  sCc→j = φr(FCc→vj  wi→j · fe(Gvi)', ' fe(Gvj ))', '  (cid:88)  vi∈Cc  s.t. wi→j = φw(fP (vi', ' vj', ' E))', '  (3) where the function φw maps a reachable vector fP (vi', ' vj', ' E) into a weight value', ' and the function φr computes the similar score between vj and the c-th class nodes.']\n",
      "[[0.1893254]]\n",
      "Which is the best model\n",
      "['The normalization factor FCc→vj of the c-th category w.r.t. vj is defined as  (2)  (4)  For the relation function φr and the weight function φw', ' we may choose some subnetworks to instantiate them in practice.The detailed implementation of our model can be found in Section 4. FCc→vj =  (cid:80)  1  vi∈Cc  wi→j  4  \\x0cPublished as a conference paper at ICLR 2020  3.3  INFERENCE LEARNING  According to the class-to-node relationship function in Eqn (2)', ' given a query node vj', ' we can obtain a score vector sC→j = [sC1→j', ' · · · ', ' sCC →j](cid:124) ∈ RC after computing the relations to all classesThe indexed category with the maximum score is assumed to be the estimated label.']\n",
      "[[0.22161376]]\n",
      "Which is the best model\n",
      "['Thus', ' we can define the loss function based on cross entropy as follows:  L = −  yj', 'c log ˆyCc→j', '  (cid:88)  C (cid:88)  vj  c=1  (5)  (6)  (7)  where yj', 'c is a binary indicator (i.e.', ' 0 or 1) of class label c for node vj', ' and the softmax operation is imposed on sCc→j', ' i.e.', ' ˆyCc→j = exp(sCc→j)/ (cid:80)C k=1 exp(sCk→j).Other error functions may be chosen as the loss function', ' e.g.', ' mean square error.In the regime of general classification', ' the cross entropy loss is a standard one that performs well.']\n",
      "[[0.05474748]]\n",
      "Which is the best model\n",
      "[' Given a training set Vtr', ' we expect that the best performance can be obtained on the validation set Vval after optimizing the model on Vtr.Given a trained/pretrained model Θ = {fe', ' φw', ' φr}', ' we perform iteratively gradient updates on the training set Vtr to obtain the new model', ' formally', '  Θ(cid:48) = Θ − α∇ΘLtr(Θ)', '  where α is the updating rate.Note that', ' in the computation of class scores', ' since the reference node and query node can be both from the training set Vtr', ' we set the computation weight wi→j = 0 if i = j in Eqn (3).']\n",
      "[[0.28814143]]\n",
      "Which is the best model\n",
      "['After several iterates of gradient descent on Vtr', ' we expect a better performance on the validation set Vval', ' i.e.', ' min Θ  Lval(Θ(cid:48)).Thus', ' we can perform the gradient update as follows  where β is the learning rate of meta optimization (Finn et al', ' 2017). Θ = Θ − β∇ΘLval(Θ(cid:48))', '  During the training process', ' we may perform batch sampling from training nodes and validation nodes', ' instead of taking all one time.']\n",
      "[[0.17423767]]\n",
      "Which is the best model\n",
      "['In the testing stage', ' we may take all training nodes and perform the model update according to Eqn (6) like the training process.The updated model is used as the final model and is then fed into Eqn (2) to infer the class labels for those query nodes. 4 MODULES In this section', ' we instantiate all modules (i.e.', ' functions) of the aforementioned structure relation.']\n",
      "[[0.18282932]]\n",
      "Which is the best model\n",
      "['The implementation details can be found in the following. Node Representation fe(Gvi): The local representation at vertex vi can be extracted by performing the graph convolution operation on subgraph GviSimilar to gridded images/videos', ' on which local convolution kernels are defined as multiple lattices with various receptive fields', ' the spectral graph convolution is used to encode the local representations of an input graph in our work. Given a graph sample G = {V', ' E', ' X }', ' the normalized graph Laplacian matrix is L = In − D−1/2ED−1/2 = UΛUT ', ' with a diagonal matrix of its eigenvalues Λ.']\n",
      "[[0.13109328]]\n",
      "Which is the best model\n",
      "['The spectral graph convo- lution can be defined as the multiplication of signal X with a filter gθ(Λ) = diag(θ) parameterized by θ in the Fourier domain: conv(X ) = gθ(L) ∗ X = Ugθ(Λ)UT X ', ' where parameter θ ∈ Rn is a vector of Fourier coefficients.To reduce the computational complexity and obtain the local information', ' we use an approximate local filter of the Chebyshev polynomial (Defferrard et al', ' 2016)', ' gθ(Λ) = (cid:80)K−1 k=0 θkTk(ˆΛ)', ' where parameter θ ∈ RK is a vector of Chebyshev coefficients and Tk(ˆΛ) ∈ Rn×n is the Chebyshev polynomial of order k evaluated at ˆΛ = 2Λ/λmax − In', ' a diagonal matrix of scaled eigenvalues.The graph filtering operation can then be expressed as gθ(Λ) ∗ X = (cid:80)K−1 k=0 θkTk(ˆL)X ', ' where Tk(ˆL) ∈ Rn×n is the Chebyshev polynomial of order k evaluated at the scaled Laplacian ˆL = 2L/λmax − In. Further', ' we can construct multi-scale receptive fields for each vertex based on the Laplacian matrix L', ' where each receptive field records hopping neighborhood relationships around the reference vertex vi', ' and forms a local centralized subgraph.']\n",
      "[[0.1687321]]\n",
      "Which is the best model\n",
      "[' Path Reachability fP (vi', ' vj', ' E): Here we compute the probabilities of paths from vertex i to vertex j by employing random walks on graphs', ' which refers to traversing the graph from vi to vj according to the probability matrix P. For the input graph G with n vertices', ' the random-walk transition matrix  5  \\x0cPublished as a conference paper at ICLR 2020  Datasets Nodes 2', '708 Cora 3', '327 Citeseer 19', '717 Pubmed NELL 65', '755  Edges 5', '429 4', '732 44', '338 266', '144  Classes 7 6 3 210  Features 1', '433 3', '703 500 5', '414  Label Rates 0.052 0.036 0.003 0.001  Table 1: The properties (especially for label rate) of various graph datasets used for the semi-supervised classification task. can be defined as P = D−1E', ' where D ∈ Rn×n is the diagonal degree matrix with Dii = (cid:80) That is to say', ' each element Pij is the probability of going from vertex i to vertex j in one step. i Eij.']\n",
      "[[0.19149576]]\n",
      "Which is the best model\n",
      "[' The sequence of nodes from vertex i to vertex j is a random walk on the graph', ' which can be modeled as a classical Markov chain by considering the set of graph vertices.To represent this formulation', ' we show that P t ij is the probability of getting from vertex vi to vertex vj in t steps.This fact is easily exhibited by considering a t-step path from vertex vi to vertex vj as first taking a single step to some vertex h', ' and then taking t − 1 steps to vj.']\n",
      "[[0.10557373]]\n",
      "Which is the best model\n",
      "['The transition probability P t in t steps can be formulated as  P t  ij =  PihP t−1 h', 'j  \\uf8f1 \\uf8f2  \\uf8f3  Pij (cid:88)  h  if t = 1 if t > 1 ', '  where each matrix entry P t steps.Finally', ' the node reachability from vi to vj can be written as a dp-dimensional vector:  ij denotes the probability of starting at vertex i and ending at vertex j in t  ij', '.', ' P dp ij ]', ' where dp refers to the step length of the longest path from vi to vj. fP (vi', ' vj', ' E) = [Pij', ' P 2  Class-to-Node Relationship sCc→j: To define the node relationship si→j from vi to vj', ' we simulta- neously consider the property of path reachability fP (vi', ' vj', ' E)', ' local representations fe(Gvi)', ' and fe(Gvj ) of nodes vi', ' vj.']\n",
      "[[0.13864568]]\n",
      "Which is the best model\n",
      "['The function φw(fP (vi', ' vj', ' E)) in Eqn (3)', ' which is to map the reachable vector fP (vi', ' vj', ' E) ∈ Rdp into a weight value', ' can be implemented with two 16-dimensional fully connected layers in our experiments.The computed value wi→j can be further used to weight the local features at node vi', ' fe(Gvi) ∈ RdvFor obtaining the similar score between vj and the c-th class nodes Cc in Eqn (2)', ' we perform a concatenation of two input features', ' where one refers to the weighted features of vertex vi', ' and another is the local features of vertex vj.One fully connected layer (w.r.t. φr) with C-dimensions is finally adopted to obtain the relation regression score.']\n",
      "[[0.17378241]]\n",
      "Which is the best model\n",
      "[' (8)  (9)  5 EXPERIMENTS  5.1 EXPERIMENTAL SETTINGS  We evaluate our proposed GIL method on three citation network datasets: Cora', ' Citeseer', ' Pubmed (Sen et al', ' 2008)', ' and one knowledge graph NELL dataset (Carlson et al', ' 2010).The statistical properties of graph data are summarized in Table 1.Following the previous protocol in (Kipf & Welling', ' 2017; Zhuang & Ma', ' 2018)', ' we split the graph data into a training set', ' a validation set', ' and a testing set.']\n",
      "[[0.2387707]]\n",
      "Which is the best model\n",
      "['Taking into account the graph convolution and pooling modules', ' we may alternately stack them into a multi-layer Graph convolutional network.The GIL model consists of two graph convolution layers', ' each of which is followed by a mean-pooling layer', ' a class-to-node relationship regression module', ' and a final softmax layer.We have given the detailed configuration of the relationship regression module in the class-to-node relationship of Section 4.']\n",
      "[[0.22005472]]\n",
      "Which is the best model\n",
      "['The parameter dp in Eqn (9) is set to the mean length of between-node reachability paths in the input graph.The channels of the 1-st and 2-nd convolutional layers are set to 128 and 256', ' respectively.The scale of the respective filed is 2 in both convolutional layers.']\n",
      "[[0.14432374]]\n",
      "Which is the best model\n",
      "['The dropout rate is set to 0.5 in the convolution and fully connected layers to avoid over-fitting', ' and the ReLU unit is leveraged as a nonlinear activation function.We pre-train our proposed GIL model for 200 iterations with the training set', ' where its initial learning rate', ' decay factor', ' and momentum are set to 0.05', ' 0.95', ' and 0.9', ' respectively.Here we train the GIL model using the stochastic gradient descent method with the batch size of 100.']\n",
      "[[0.18737909]]\n",
      "Which is the best model\n",
      "['We further improve the inference learning capability of the GIL model for 1200 iterations with the validation set', ' where the meta-learning rates α and β are both set to 0.001. 6  \\x0cPublished as a conference paper at ICLR 2020  5.2 COMPARISON WITH STATE-OF-THE-ARTS  We compare the GIL approach with several state-of-the-art methods (Monti et al', ' 2017; Kipf & Welling', ' 2017; Zhou et al', ' 2004; Zhuang & Ma', ' 2018) over four graph datasets', ' including Cora', ' Citeseer', ' Pubmed', ' and NELL.The classification accuracies for all methods are reported in Table 2.']\n",
      "[[0.24789748]]\n",
      "Which is the best model\n",
      "['Our proposed GIL can significantly outperform these graph Laplacian regularized methods on four graph datasets', ' including Deep walk (Zhou et al', ' 2004)', ' modularity clustering (Brandes et al', ' 2008)', ' Gaussian fields (Zhu et al', ' 2003)', ' and graph embedding (Yang et al', ' 2016) methods.For example', ' we can achieve much higher performance than the deepwalk method (Zhou et al', ' 2004)', ' e.g.', ' 43.2% vs 74.1% on the Citeseer dataset', ' 65.3% vs 83.1% on the Pubmed dataset', ' and 58.1% vs 78.9% on the NELL dataset.We find that the graph embedding method (Yang et al', ' 2016)', ' which has considered both label information and graph structure during sampling', ' can obtain lower accuracies than our proposed GIL by 9.4% on the Citeseer dataset and 10.5% on the Cora dataset', ' respectively.']\n",
      "[[0.2500379]]\n",
      "Which is the best model\n",
      "['This indicates that our proposed GIL can better optimize structure relations and thus improve the network generalization.We further compare our proposed GIL with several existing deep graph embedding methods', ' including graph attention network (Velickovic et al', ' 2018)', ' dual graph convolutional networks (Zhuang & Ma', ' 2018)', ' topology adaptive graph convolutional networks (Du et al', ' 2017)', ' Multi-scale graph convolution (Abu-El-Haija et al', ' 2018)', ' etc.For example', ' our proposed GIL achieves a very large gain', ' e.g.', ' 86.2% vs 83.3% (Du et al', ' 2017) on the Cora dataset', ' and 78.9% vs 66.0% (Kipf & Welling', ' 2017) on the NELL dataset.']\n",
      "[[0.23345757]]\n",
      "Which is the best model\n",
      "['We evaluate our proposed GIL method on a large graph dataset with a lower label rate', ' which can significantly outperform existing baselines on the Pubmed dataset: 3.1% over DGCN (Zhuang & Ma', ' 2018)', ' 4.1% over classic GCN (Kipf & Welling', ' 2017) and TAGCN (Du et al', ' 2017)', ' 3.2% over AGNN (Thekumparampil et al', ' 2018)', ' and 3.6% over N-GCN (Abu-El-Haija et al', ' 2018).It demonstrates that our proposed GIL performs very well on various graph datasets by building the graph inference learning process', ' where the limited label information and graph structures can be well employed in the predicted framework. Table 2: Performance comparisons of semi-supervised classification methods.']\n",
      "[[0.23945074]]\n",
      "Which is the best model\n",
      "[' Methods Clustering (Brandes et al', ' 2008) DeepWalk (Zhou et al', ' 2004) Gaussian (Zhu et al', ' 2003) G-embedding (Yang et al', ' 2016) DCNN (Atwood & Towsley', ' 2016) GCN (Kipf & Welling', ' 2017) MoNet (Monti et al', ' 2017) N-GCN (Abu-El-Haija et al', ' 2018) GAT (Velickovic et al', ' 2018) AGNN (Thekumparampil et al', ' 2018) TAGCN (Du et al', ' 2017) DGCN (Zhuang & Ma', ' 2018) Our GIL  Cora 59.5 67.2 68.0 75.7 76.8 81.5 81.7 83.0 83.0 83.1 83.3 83.5 86.2  Citeseer 60.1 43.2 45.3 64.7 - 70.3 - 72.2 72.5 71.7 72.5 72.6 74.1  Pubmed NELL 70.7 65.3 63.0 77.2 73.0 79.0 78.8 79.5 79.0 79.9 79.0 80.0 83.1  21.8 58.1 26.5 61.9 - 66.0 - - - - - 74.2 78.9  5.3 ANALYSIS  Meta-optimization: As can be seen in Table 3', ' we report the classification accuracies of semi-supervised classification with several variants of our proposed GIL and the classical GCN method (Kipf & Welling', ' 2017) when evaluating them on the Cora dataset.For analyzing the perfor- mance improvement of our proposed GIL with the graph inference learning process', ' we report the classification accuracies of GCN (Kipf & Welling', ' 2017) and our proposed GIL on the Cora dataset under two different situations', ' including “only learning with the training set Vtr\" and “with jointly learning on a training set Vtr and a validation set Vval\".“GCN /w jointly learning on Vtr & Vval\" achieves a better result than “GCN /w learning on Vtr\" by 3.6%', ' which demonstrates that the network performance can be improved by employing validation samples.']\n",
      "[[0.1869089]]\n",
      "Which is the best model\n",
      "['When using structure relations', ' “GIL /w learning on Vtr\" obtains an improvement of 1.9% (over “GCN /w learning on Vtr”)', ' which can be attributed to the building connection between nodes.The meta-optimization strategy (“GIL /w meta-training from Vtr → Vval\" vs “GIL /w learning on Vtr”) has a gain of 2.9%', ' which indicates that a good inference capability can be learnt through meta-optimization.It is worth noting that', ' GIL adopts a meta-optimization strategy to learn the inference model', ' which is a process of migrating  7  \\x0cPublished as a conference paper at ICLR 2020  from a training set to a validation set.']\n",
      "[[0.21950607]]\n",
      "Which is the best model\n",
      "['In other words', ' the validation set is only used to teach the model itself how to transfer to unseen data.In contrast', ' the conventional methods often employ a validation set to tune parameters of a certain model of interest. Table 3: Performance comparisons with several GIL variants and the classical GCN method on the Cora dataset.']\n",
      "[[0.25848994]]\n",
      "Which is the best model\n",
      "[' GCN (Kipf & Welling', ' 2017)  Methods  GIL  GIL+mean pooling  GIL+2  conv layers  /w learning on Vtr /w jointly learning on Vtr & Vval /w learning on Vtr /w meta-train Vtr → Vval /w 1  conv layer /w 2  conv layers /w 3  conv layers /w max-pooling /w mean pooling  Acc. (%) 81.4 84.0 83.3 86.2 84.5 86.2 85.4 85.2 86.2  Network settings: We explore the effectiveness of our proposed GIL with the same mean pooling mechanism', ' but with different numbers of convolutional layers', ' i.e.', ' “GIL + mean pooling\" with one', ' two', ' and three convolutional layers', ' respectively.As can be seen in Table 3', ' the proposed GIL with two convolutional layers can obtain a better performance on the Cora data than the other two network settings (i.e.', ' GIL with one or three convolutional layers).For example', ' the performance of ‘GIL /w 1  conv layer + mean pooling\" is slightly decreased by 1.7% over “GIL /w 2  conv layers + mean pooling\" on the Cora dataset.']\n",
      "[[0.23021542]]\n",
      "Which is the best model\n",
      "['Furthermore', ' we report the classification results of our proposed GIL by using mean and max-pooling mechanisms', ' respectively.GIL with mean pooling (i.e.', ' “GIL /w 2 conv layers + mean pooling\") can get a better result than the GIL method with max-pooling (i.e.', ' “GIL /w 2 conv layers + max-pooling\")', ' e.g.', ' 86.2% vs 85.2% on the Cora graph dataset.The reason may be that the graph network with two convolutional layers and the mean pooling mechanism can obtain the optimal graph embeddings', ' but when increasing the network layers', ' more parameters of a certain graph model need to be optimized', ' which may lead to the over-fitting issue.']\n",
      "[[0.22999161]]\n",
      "Which is the best model\n",
      "[' Inﬂuence of different between-node steps: We compare the classification performance within different between-node steps for our proposed GIL and GCN (Kipf & Welling', ' 2017)', ' as illustrated in Fig 2(a).The length of between-node steps can be computed with the shortest path between reference nodes and query nodes.When the step between nodes is smaller', ' both GIL and GCN methods can predict the category information for a small part of unlabeled nodes in the testing set.']\n",
      "[[0.18830034]]\n",
      "Which is the best model\n",
      "['The reason may be that the node category information may be disturbed by its nearest neighboring nodes with different labels and fewer nodes are within 1 or 2 steps in the testing set.The GIL and GCN methods can infer the category information for a part of unlabeled nodes by adopting node attributes', ' when two nodes are not connected in the graph (i.e.', ' step=∞).By increasing the length of reachability path', ' the inference process of the GIL method would become difficult and more graph structure information may be employed in the predicted process.']\n",
      "[[0.07787056]]\n",
      "Which is the best model\n",
      "['GIL can outperform the classic GCN by analyzing the accuracies within different between-node steps', ' which indicates that our proposed GIL has a better reference capability than GCN by using the meta-optimization mechanism from training nodes to validation nodes. (a)  (b)  Figure 2: (a) Performance comparisons within different between-node steps on the Cora dataset.The accuracy equals to the number of correctly classified nodes divided by all testing samples', ' and is accumulated from step 1 to step k. (b) Performance comparisons with different label rates on the Pubmed dataset.']\n",
      "[[0.25087366]]\n",
      "Which is the best model\n",
      "[' 8  1357911step0.00.20.40.60.8accuracyour GILGCNlabel rate0.30%0.60%0.90%1.20%1.50%1.80%GCN0.7920.7970.8050.8240.8290.834GIL(ours)0.8170.8240.8310.8360.8380.8421x2x3x4x5x6x77.0%79.0%81.0%83.0%85.0%1x2x3x4x5x6xGCNGIL(ours)Label rates Accuracy \\x0cPublished as a conference paper at ICLR 2020  Inﬂuence of different label rates: We also explore the performance comparisons of the GIL method with different label rates', ' and the detailed results on the Pubmed dataset can be shown in Fig 2(b).When label rates increase by multiplication', ' the performances of GIL and GCN are improved', ' but the relative gain becomes narrow.The reason is that', ' the reachable path lengths between unlabeled nodes and labeled nodes will be reduced with the increase of labeled nodes', ' which will weaken the effect of inference learning.']\n",
      "[[0.18744214]]\n",
      "Which is the best model\n",
      "['In the extreme case', ' labels of unlabeled nodes could be determined by those neighbors with the 1 ∼ 2 step reachability.In summary', ' our proposed GIL method prefers small ratio labeled nodes on the semi-supervised node classification task. Inference learning process: Classification errors of different epochs on the validation set of the Cora dataset can be illustrated in Fig 3.']\n",
      "[[0.1363555]]\n",
      "Which is the best model\n",
      "['Classification errors are rapidly decreasing as the number of iterations increases from the beginning to 400 iterations', ' while they are with a slow descent from 400 iterations to 1200 iterations.It demonstrates that the learned knowledge from the training samples can be transferred for inferring node category information from these reference labeled nodes.The performance of semi-supervised classification can be further increased by improving the generalized capability of the Graph CNN model.']\n",
      "[[0.20354426]]\n",
      "Which is the best model\n",
      "[' Table 4: Performance comparisons with different mod- ules on the Cora dataset', ' where fe', ' fP ', ' and fr denote node representation', ' path reachability', ' and structure re- lation', ' respectively. fP fr fe - - - (cid:88) - - (cid:88) (cid:88) - (cid:88) (cid:88) (cid:88)  Acc.(%) 56.0 81.5 85.0 86.2  Figure 3: Classification errors of different itera- tions on the validation set of the Cora dataset. Module analysis: We evaluate the effectiveness of different modules within our proposed GIL framework', ' including node representation fe', ' path reachability fP ', ' and structure relation fr.']\n",
      "[[0.2831314]]\n",
      "Which is the best model\n",
      "['Note that the last one fr defines on the former two ones', ' so we consider the cases in Table 4 by adding modules.When not using all modules', ' only original attributes of nodes are used to predict labels.The case of only using fe belongs to the GCN method', ' which can achieve 81.5% on the Cora dataset.']\n",
      "[[0.16234538]]\n",
      "Which is the best model\n",
      "['The large gain of using the relation module fr (i.e.', ' from 81.5% to 85.0%) may be contributed to the ability of inference learning on attributes as well as local topology structures which are implicitly encoded in fe.The path information fP can further boost the performance by 1.2%', ' e.g.', ' 86.2% vs 85.0%.It demonstrates that three different modules of our method can improve the graph inference learning capability.']\n",
      "[[0.22278605]]\n",
      "Which is the best model\n",
      "[' Computational complexity: For the computational complexity of our proposed GIL', ' the cost is mainly spent on the computations of node representation', ' between-node reachability', ' and class-to- node relationship', ' which are about O((ntr + nte) ∗ e ∗ din ∗ dout)', ' O((ntr + nte) ∗ e ∗ P )', ' and O(ntr ∗ nted2 out)', ' respectively. ntr and nte refer to the numbers of training and testing nodes', ' din and dout denote the input and output dimensions of node representation', ' e is about the average degree of graph node', ' and P is the step length of node reachability.Compared with those classic Graph CNNs (Kipf & Welling', ' 2017)', ' our proposed GIL has a slightly higher cost due to an extra inference learning process', ' but can complete the testing stage with several seconds on these benchmark datasets. 6 CONCLUSION  In this work', ' we tackled the semi-supervised node classification task with a graph inference learning method', ' which can better predict the categories of these unlabeled nodes in an end-to-end framework.']\n",
      "[[0.2298924]]\n",
      "Which is the best model\n",
      "['We can build a structure relation for obtaining the connection between any two graph nodes', ' where node attributes', ' between-node paths', ' and graph structure information can be encapsulated together.For better capturing the transferable knowledge', ' our method further learns to transfer the mined knowledge from the training samples to the validation set', ' finally boosting the prediction accuracy of the labels of unlabeled nodes in the testing set.The extensive experimental results demonstrate the effectiveness of our proposed GIL for solving the semi-supervised learning problem', ' even in the few-shot paradigm.']\n",
      "[[0.16588753]]\n",
      "Which is the best model\n",
      "['In the future', ' we would extend the graph inference method to handle more graph-related tasks', ' such as graph generation and social network analysis. 9  the number of iterations error \\x0cPublished as a conference paper at ICLR 2020  ACKNOWLEDGMENT  This work was supported by the National Natural Science Foundation of China (Nos 61972204', ' 61906094', ' U1713208)', ' the Natural Science Foundation of Jiangsu Province (Grant Nos.BK20191283 and BK20190019)', ' and Tencent AI Lab Rhino-Bird Focused Research Program (No.']\n",
      "[[0.235794]]\n",
      "Which is the best model\n",
      "['JR201922). REFERENCES  2001', ' 2016. Sami Abu-El-Haija', ' Amol Kapoor', ' Bryan Perozzi', ' and Joonseok Lee.']\n",
      "[[0.11207409]]\n",
      "Which is the best model\n",
      "['N-gcn: Multi-scale graph  convolution for semi-supervised node classification. arXiv preprint arXiv:1802.08888', ' 2018. James Atwood and Don Towsley.Diffusion-convolutional neural networks.']\n",
      "[[0.2726549]]\n",
      "Which is the best model\n",
      "['In NeurIPS', ' pp 1993–  Karsten M Borgwardt', ' Hans-Peter Kriegel', ' SVN Vishwanathan', ' and Nicol N Schraudolph.Graph ker- nels for disease outcome prediction from protein-protein interaction networks.Pacific Symposium on Biocomputing Pacific Symposium on Biocomputing', ' pp 4–15', ' 2007.']\n",
      "[[0.10185033]]\n",
      "Which is the best model\n",
      "[' Ulrik Brandes', ' Daniel Delling', ' Marco Gaertler', ' Robert Gorke', ' Martin Hoefer', ' Zoran Nikoloski', ' and Dorothea Wagner.On modularity clustering.IEEE transactions on knowledge and data engineering', ' 20(2):172–188', ' 2008.']\n",
      "[[0.285621]]\n",
      "Which is the best model\n",
      "[' Andrew Carlson', ' Justin Betteridge', ' Bryan Kisiel', ' Burr Settles', ' Estevam R. Hruschka Jr.', ' and Tom M.  Mitchell.Toward an architecture for never-ending language learning.In AAAI', ' 2010.']\n",
      "[[0.1298953]]\n",
      "Which is the best model\n",
      "[' Michaël Defferrard', ' Xavier Bresson', ' and Pierre Vandergheynst.Convolutional neural networks on  graphs with fast localized spectral filtering.In NeurIPS', ' pp 3844–3852', ' 2016.']\n",
      "[[0.23112667]]\n",
      "Which is the best model\n",
      "[' Jian Du', ' Shanghang Zhang', ' Guanhang Wu', ' José MF Moura', ' and Soummya Kar.Topology adaptive  graph convolutional networks. arXiv preprint arXiv:1710.10370', ' 2017. Chelsea Finn', ' Pieter Abbeel', ' and Sergey Levine.']\n",
      "[[0.3392583]]\n",
      "Which is the best model\n",
      "['Model-agnostic meta-learning for fast adaptation of  deep networks.In ICML', ' pp 1126–1135', ' 2017. Will Hamilton', ' Zhitao Ying', ' and Jure Leskovec.']\n",
      "[[0.21562305]]\n",
      "Which is the best model\n",
      "['Inductive representation learning on large graphs.In  NeurIPS', ' pp 1025–1035', ' 2017. Kaiming He', ' Xiangyu Zhang', ' Shaoqing Ren', ' and Jian Sun.']\n",
      "[[0.19616823]]\n",
      "Which is the best model\n",
      "['Deep residual learning for image  recognition.In CVPR', ' pp 770–778', ' 2016. Mikael Henaff', ' Joan Bruna', ' and Yann LeCun.']\n",
      "[[0.27928457]]\n",
      "Which is the best model\n",
      "['Deep convolutional networks on graph-structured data. arXiv preprint arXiv:1506.05163', ' 2015. Jiatao Jiang', ' Zhen Cui', ' Chunyan Xu', ' and Jian Yang.']\n",
      "[[0.28896636]]\n",
      "Which is the best model\n",
      "['Gaussian-induced convolution for graphs.In  AAAI', ' volume 33', ' pp 4007–4014', ' 2019. Hisashi Kashima', ' Koji Tsuda', ' and Akihiro Inokuchi.']\n",
      "[[0.19003351]]\n",
      "Which is the best model\n",
      "['Marginalized kernels between labeled graphs. In ICML', ' pp 321–328', ' 2003. Thomas N. Kipf and Max Welling.']\n",
      "[[0.1711461]]\n",
      "Which is the best model\n",
      "['Semi-supervised classification with graph convolutional networks. In ICLR', ' 2017. networks.']\n",
      "[[0.2551684]]\n",
      "Which is the best model\n",
      "['ICLR', ' 2016. learning.In ICML', ' 2010.']\n",
      "[[0.18605343]]\n",
      "Which is the best model\n",
      "[' Yujia Li', ' Daniel Tarlow', ' Marc Brockschmidt', ' and Richard Zemel.Gated graph sequence neural  Wei Liu', ' Junfeng He', ' and Shih-Fu Chang.Large graph construction for scalable semi-supervised  Wei Liu', ' Jun Wang', ' and Shih-Fu Chang.']\n",
      "[[0.28723192]]\n",
      "Which is the best model\n",
      "['Robust and scalable graph-based semisupervised learning. Proceedings of the IEEE', ' 100(9):2624–2638', ' 2012. Zhiling Luo', ' Ling Liu', ' Jianwei Yin', ' Ying Li', ' and Zhaohui Wu. Deep learning of graphs with ngram convolutional neural networks.']\n",
      "[[0.28264147]]\n",
      "Which is the best model\n",
      "['IEEE Transactions on Knowledge and Data Engineering', ' 29(10): 2125–2139', ' 2017. 10  \\x0cPublished as a conference paper at ICLR 2020  Federico Monti', ' Davide Boscaini', ' Jonathan Masci', ' Emanuele Rodola', ' Jan Svoboda', ' and Michael M Bronstein.Geometric deep learning on graphs and manifolds using mixture model cnns.']\n",
      "[[0.26835835]]\n",
      "Which is the best model\n",
      "['In CVPR', ' pp 5115–5124', ' 2017. Christopher Morris', ' Kristian Kersting', ' and Petra Mutzel.Glocalized weisfeiler-lehman graph kernels:  Global-local feature maps of graphs.']\n",
      "[[0.20643824]]\n",
      "Which is the best model\n",
      "['In ICDM', ' pp 327–336.IEEE', ' 2017. Mathias Niepert', ' Mohamed Ahmed', ' and Konstantin Kutzkov.']\n",
      "[[0.16122216]]\n",
      "Which is the best model\n",
      "['Learning convolutional neural networks  for graphs.In ICML', ' pp 2014–2023', ' 2016. Francesco Orsini', ' Daniele Baracchi', ' and Paolo Frasconi.']\n",
      "[[0.27505136]]\n",
      "Which is the best model\n",
      "['Shift aggregate extract networks. arXiv  preprint arXiv:1703.05537', ' 2017. Lawrence Page', ' Sergey Brin', ' Rajeev Motwani', ' and Terry Winograd.The pagerank citation ranking:  Bringing order to the web.']\n",
      "[[0.22044848]]\n",
      "Which is the best model\n",
      "['Technical Report 1999-66', ' 1999. Shirui Pan', ' Ruiqi Hu', ' Guodong Long', ' Jing Jiang', ' Lina Yao', ' and Chengqi Zhang.Adversarially  regularized graph autoencoder for graph embedding.']\n",
      "[[0.30004212]]\n",
      "Which is the best model\n",
      "['In IJCAI', ' pp 2609–2615', ' 2018. Sachin Ravi and Hugo Larochelle.Optimization as a model for few-shot learning.']\n",
      "[[0.16574393]]\n",
      "Which is the best model\n",
      "['In ICLR', ' 2017. Prithviraj Sen', ' Galileo Namata', ' Mustafa Bilgic', ' Lise Getoor', ' Brian Galligher', ' and Tina Eliassi-Rad. Collective classification in network data.']\n",
      "[[0.2322895]]\n",
      "Which is the best model\n",
      "['AI magazine', ' 29(3):93–93', ' 2008. Nino Shervashidze', ' SVN Vishwanathan', ' Tobias Petri', ' Kurt Mehlhorn', ' and Karsten Borgwardt.Efficient graphlet kernels for large graph comparison.']\n",
      "[[0.30746925]]\n",
      "Which is the best model\n",
      "['In Artificial Intelligence and Statistics', ' pp 488–495', ' 2009. Flood Sung', ' Li Zhang', ' Tao Xiang', ' Timothy Hospedales', ' and Yongxin Yang.Learning to learn:  Meta-critic networks for sample efficient learning. arXiv preprint arXiv:1706.09529', ' 2017.']\n",
      "[[0.1843113]]\n",
      "Which is the best model\n",
      "[' Kiran K Thekumparampil', ' Chong Wang', ' Sewoong Oh', ' and Li-Jia Li. Attention-based graph neural  network for semi-supervised learning. arXiv preprint arXiv:1803.03735', ' 2018. Petar Velickovic', ' Guillem Cucurull', ' Arantxa Casanova', ' Adriana Romero', ' Pietro Liò', ' and Yoshua  Bengio.Graph attention networks.']\n",
      "[[0.2676773]]\n",
      "Which is the best model\n",
      "['ICLR', ' 2018. Danfei Xu', ' Yuke Zhu', ' Christopher B Choy', ' and Li Fei-Fei.Scene graph generation by iterative  message passing.']\n",
      "[[0.18901111]]\n",
      "Which is the best model\n",
      "['In CVPR', ' pp 5410–5419', ' 2017. Pinar Yanardag and SVN Vishwanathan.Deep graph kernels.']\n",
      "[[0.21957695]]\n",
      "Which is the best model\n",
      "['In SIGKDD', ' pp 1365–1374', ' 2015. Zhilin Yang', ' William W Cohen', ' and Ruslan Salakhutdinov.Revisiting semi-supervised learning with  graph embeddings.']\n",
      "[[0.23861928]]\n",
      "Which is the best model\n",
      "['ICML', ' 2016. Bing Yu', ' Haoteng Yin', ' and Zhanxing Zhu.Spatio-temporal graph convolutional networks: A deep  learning framework for traffic forecasting.']\n",
      "[[0.28984088]]\n",
      "Which is the best model\n",
      "['In IJCAI', ' pp 3634–3640', ' 2018. Tong Zhang', ' Zhen Cui', ' Chunyan Xu', ' Wenming Zheng', ' and Jian Yang.Variational pathway reasoning  for eeg emotion recognition.']\n",
      "[[0.139051]]\n",
      "Which is the best model\n",
      "['In AAAI', ' 2020. Wenting Zhao', ' Zhen Cui', ' Chunyan Xu', ' Chengzheng Li', ' Tong Zhang', ' and Jian Yang.Hashing graph  convolution for node classification.']\n",
      "[[0.22876664]]\n",
      "Which is the best model\n",
      "['In CIKM', ' 2019. Dengyong Zhou', ' Olivier Bousquet', ' Thomas N Lal', ' Jason Weston', ' and Bernhard Schölkopf.Learning  with local and global consistency.']\n",
      "[[0.17952286]]\n",
      "Which is the best model\n",
      "['In NeurIPS', ' pp 321–328', ' 2004. Xiaojin Zhu', ' Zoubin Ghahramani', ' and John D Lafferty.Semi-supervised learning using gaussian  fields and harmonic functions.']\n",
      "[[0.20557871]]\n",
      "Which is the best model\n",
      "['In ICML', ' pp 912–919', ' 2003. Chenyi Zhuang and Qiang Ma. Dual graph convolutional networks for graph-based semi-supervised  classification.In WWW', ' pp 499–508', ' 2018.']\n",
      "[[0.27712306]]\n",
      "Which is the best model\n",
      "[' 11  \\x0c']\n",
      "[[0.07679898]]\n"
     ]
    }
   ],
   "source": [
    "useHeapDict=similarSentencesWIthGoogleUSE('/content/graph_inference_learning_for_semi_supervised_classification_3.csv',[\"Which is the best model\"],10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "id": "Sokrn4AU-qEC",
    "outputId": "b1be6493-cb29-4d18-9839-25eef1d7f010"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'modelResultFile = open(\"/content/Model.txt\", \"a\")\\n\\nfor model in useHeapDict:\\n  modelResultFile.write(model)\\n  modelResultFile.write(\"\\n\")\\n  modelHeap = useHeapDict[model]\\n  while modelHeap: \\n    qs = heapq.heappop(modelHeap)\\n    modelResultFile.write(qs.query)\\n    modelResultFile.write(\"\\n\")\\n    modelResultFile.write(\\' \\'.join([str(elem) for elem in qs.snippet]))\\n    modelResultFile.write(\"\\n\")\\n    modelResultFile.write(str(qs.similarity))\\n    modelResultFile.write(\"\\n\")\\n    \\n\\nmodelResultFile.close()\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''modelResultFile = open(\"/content/Model.txt\", \"a\")\n",
    "\n",
    "for model in useHeapDict:\n",
    "  modelResultFile.write(model)\n",
    "  modelResultFile.write(\"\\n\")\n",
    "  modelHeap = useHeapDict[model]\n",
    "  while modelHeap: \n",
    "    qs = heapq.heappop(modelHeap)\n",
    "    modelResultFile.write(qs.query)\n",
    "    modelResultFile.write(\"\\n\")\n",
    "    modelResultFile.write(' '.join([str(elem) for elem in qs.snippet]))\n",
    "    modelResultFile.write(\"\\n\")\n",
    "    modelResultFile.write(str(qs.similarity))\n",
    "    modelResultFile.write(\"\\n\")\n",
    "    \n",
    "\n",
    "modelResultFile.close()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pC7Y1U8RBOzK"
   },
   "source": [
    "Other Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RkRmOmY7132A"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WoXFlYwoEwbE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gvhvs01J1r2L"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import csv\n",
    "import heapq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "id": "jBAvL_uGBRC-",
    "outputId": "a0553fc9-ecdb-47c1-ae41-a18ceba601e2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nmodelHeapDict=compareDifferentLanguageModels('/content/large_batch_optimization_for_deep_learning_training_bert_in_76_minutes_1.csv')\\nprint(modelHeapDict)\\nfor h in modelHeapDict:\\n  print(h)\\n  print(modelHeapDict[h])\\n  while modelHeapDict[h]:\\n    qs = heapq.heappop(modelHeapDict[h])\\n    print(qs.snippet)\\n    print(qs.similarity)\\n\""
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def compareDifferentLanguageModels(researchPaper,queriesList,highestScoredNSnippets = 10):\n",
    "  spacy_model_list = ['en_trf_bertbaseuncased_lg','en_trf_robertabase_lg','en_trf_distilbertbaseuncased_lg']\n",
    "  #queriesList= [\"Which model is the best\"]\n",
    "  modelHeapDict = {}\n",
    "  #model_bert = 'en_trf_bertbaseuncased_lg'\n",
    "  \n",
    "  \n",
    "  #with open(researchPaper) as researchPaperCSV:\n",
    "  #researchPaperReader = csv.reader(researchPaperCSV)\n",
    "  #print(researchPaperCSV)\n",
    "  #For every model , iterate\n",
    "  for model in spacy_model_list:\n",
    "    researchPpr_NLP = spacy.load(model)\n",
    "\n",
    "    with open(researchPaper) as researchPaperCSV:\n",
    "      researchPaperReader = csv.reader(researchPaperCSV)\n",
    "      score_max_heap = [] \n",
    "      for query in queriesList:\n",
    "        queryObj = researchPpr_NLP(query)\n",
    "        print(queryObj)\n",
    "        for snippet in researchPaperReader:\n",
    "            #print(snippet)\n",
    "            #EOS tag ignore\n",
    "            if('<EOS>' not in snippet):\n",
    "              #print(snippet)\n",
    "              #snippet = 'In Advances in neural information processing systems pp 693–701 2011.'\n",
    "              snippetStr = \" \"\n",
    "              snippetStr = ' '.join([str(elem) for elem in snippet])\n",
    "              #print(snippetStr)\n",
    "              snippetObj = researchPpr_NLP(snippetStr)\n",
    "              #print(snippetObj)\n",
    "              qs = QuerySnippet(query, snippet,  queryObj.similarity(snippetObj))\n",
    "              print(model)\n",
    "              #print(qs.query)\n",
    "              print(qs.snippet)\n",
    "              print(qs.similarity)\n",
    "              #heap logic\n",
    "              if len(score_max_heap) < highestScoredNSnippets or qs.similarity > score_max_heap[0].similarity:\n",
    "                if len(score_max_heap) == highestScoredNSnippets: heapq.heappop(score_max_heap)\n",
    "                heapq.heappush( score_max_heap, qs )\n",
    "    modelHeapDict[model] =  score_max_heap\n",
    "    print(model)\n",
    "    print(len(score_max_heap))            \n",
    "  return  modelHeapDict         \n",
    "'''\n",
    "modelHeapDict=compareDifferentLanguageModels('/content/large_batch_optimization_for_deep_learning_training_bert_in_76_minutes_1.csv')\n",
    "print(modelHeapDict)\n",
    "for h in modelHeapDict:\n",
    "  print(h)\n",
    "  print(modelHeapDict[h])\n",
    "  while modelHeapDict[h]:\n",
    "    qs = heapq.heappop(modelHeapDict[h])\n",
    "    print(qs.snippet)\n",
    "    print(qs.similarity)\n",
    "'''\n",
    "  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l3rUdKuaUV7P"
   },
   "source": [
    "Pass The file and  Snippet arraylist below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "NpraG_78GttJ",
    "outputId": "2444cfb3-b13b-4e90-e2c8-0bc45d9f4963"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which is the best model\n",
      "en_trf_bertbaseuncased_lg\n",
      "['Published as a conference paper at ICLR 2020  GRAPH INFERENCE LEARNING FOR SEMI-SUPERVISED CLASSIFICATION  Chunyan Xu', ' Zhen Cui∗', ' Xiaobin Hong', ' Tong Zhang', ' and Jian Yang School of Computer Science and Engineering', ' Nanjing University of Science and Technology', ' Nanjing', ' China {cyx', 'zhen.cui', 'xbhong', 'tong.zhang', 'csjyang}@njust.edu.cn  Wei Liu Tencent AI Lab', ' China wl2223@columbia.edu  ABSTRACT  In this work', ' we address semi-supervised classification of graph data', ' where the categories of those unlabeled nodes are inferred from labeled nodes as well as graph structures.Recent works often solve this problem via advanced graph convolution in a conventionally supervised manner', ' but the performance could degrade significantly when labeled data is scarce.To this end', ' we propose a Graph Inference Learning (GIL) framework to boost the performance of semi- supervised node classification by learning the inference of node labels on graph topology.']\n",
      "0.5225788519433965\n",
      "en_trf_bertbaseuncased_lg\n",
      "['To bridge the connection between two nodes', ' we formally define a structure relation by encapsulating node attributes', ' between-node paths', ' and local topological structures together', ' which can make the inference conveniently deduced from one node to another node.For learning the inference process', ' we further introduce meta-optimization on structure relations from training nodes to validation nodes', ' such that the learnt graph inference capability can be better self-adapted to testing nodes.Comprehensive evaluations on four benchmark datasets (including Cora', ' Citeseer', ' Pubmed', ' and NELL) demonstrate the superiority of our proposed GIL when compared against state-of-the-art methods on the semi-supervised node classification task.']\n",
      "0.5754531797476954\n",
      "en_trf_bertbaseuncased_lg\n",
      "[' 1  INTRODUCTION  Graph', ' which comprises a set of vertices/nodes together with connected edges', ' is a formal structural representation of non-regular data.Due to the strong representation ability', ' it accommodates many potential applications', ' e.g.', ' social network (Orsini et al', ' 2017)', ' world wide data (Page et al', ' 1999)', ' knowledge graph (Xu et al', ' 2017)', ' and protein-interaction network (Borgwardt et al', ' 2007).Among these', ' semi-supervised node classification on graphs is one of the most interesting also popular topics.']\n",
      "0.5337811968027575\n",
      "en_trf_bertbaseuncased_lg\n",
      "['Given a graph in which some nodes are labeled', ' the aim of semi-supervised classification is to infer the categories of those remaining unlabeled nodes by using various priors of the graph. While there have been numerous previous works (Brandes et al', ' 2008; Zhou et al', ' 2004; Zhu et al', ' 2003; Yang et al', ' 2016; Zhao et al', ' 2019) devoted to semi-supervised node classification based on explicit graph Laplacian regularizations', ' it is hard to efficiently boost the performance of label prediction due to the strict assumption that connected nodes are likely to share the same label information.With the progress of deep learning on grid-shaped images/videos (He et al', ' 2016)', ' a few of graph convolutional neural networks (CNN) based methods', ' including spectral (Kipf & Welling', ' 2017) and spatial methods (Niepert et al', ' 2016; Pan et al', ' 2018; Yu et al', ' 2018)', ' have been proposed to learn local convolution filters on graphs in order to extract more discriminative node representations.']\n",
      "0.5120179771737032\n",
      "en_trf_bertbaseuncased_lg\n",
      "['Although graph CNN based methods have achieved considerable capabilities of graph embedding by optimizing filters', ' they are limited into a conventionally semi-supervised framework and lack of an efficient inference mechanism on graphs.Especially', ' in the case of few-shot learning', ' where a small number of training nodes are labeled', ' this kind of methods would drastically compromise the performance.For example', ' the Pubmed graph dataset (Sen et al', ' 2008) consists  ∗Corresponding author: Zhen Cui.']\n",
      "0.5707279842264241\n",
      "en_trf_bertbaseuncased_lg\n",
      "[' 1  \\x0cPublished as a conference paper at ICLR 2020  Figure 1: The illustration of our proposed GIL framework.For the problem of graph node labeling', ' the category information of these unlabeled nodes depends on the similarity computation between a query node (e.g.', ' vj) and these labeled reference nodes (e.g.', ' vi).We consider the similarity from three points: node attributes', ' the consistency of local topological structures (i.e.', ' the circle with dashed line)', ' and the between-node path reachability (i.e.', ' the red wave line from vi to vj).']\n",
      "0.5949153952983036\n",
      "en_trf_bertbaseuncased_lg\n",
      "['Specifically', ' the local structures as well as node attributes are encoded as high-level features with graph convolution', ' while the between-node path reachability is abstracted as reachable probabilities of random walks.To better make the inference generalize to test nodes', ' we introduce a meta-learning strategy to optimize the structure relations learning from training nodes to validation nodes. of 19', '717 nodes and 44', '338 edges', ' but only 0.3% nodes are labeled for the semi-supervised node classification task.']\n",
      "0.5971836695044469\n",
      "en_trf_bertbaseuncased_lg\n",
      "['These aforementioned works usually boil down to a general classification task', ' where the model is learnt on a training set and selected by checking a validation set.However', ' they do not put great efforts on how to learn to infer from one node to another node on a topological graph', ' especially in the few-shot regime. In this paper', ' we propose a graph inference learning (GIL) framework to teach the model itself to adaptively infer from reference labeled nodes to those query unlabeled nodes', ' and finally boost the performance of semi-supervised node classification in the case of a few number of labeled samples.']\n",
      "0.6150811538383633\n",
      "en_trf_bertbaseuncased_lg\n",
      "['Given an input graph', ' GIL attempts to infer the unlabeled nodes from those observed nodes by building between-node relations.The between-node relations are structured as the integration of node attributes', ' connection paths', ' and graph topological structures.It means that the similarity between two nodes is decided from three aspects: the consistency of node attributes', ' the consistency of local topological structures', ' and the between-node path reachability', ' as shown in Fig 1.']\n",
      "0.6117092669157036\n",
      "en_trf_bertbaseuncased_lg\n",
      "['The local structures anchored around each node as well as the attributes of nodes therein are jointly encoded with graph convolution (Defferrard et al', ' 2016) for the sake of high-level feature extraction.For the between-node path reachability', ' we adopt the random walk algorithm to obtain the characteristics from a labeled reference node vi to a query unlabeled node vj in a given graph.Based on the computed node representation and between-node reachability', ' the structure relations can be obtained by computing the similar scores/relationships from reference nodes to unlabeled nodes in a graph.']\n",
      "0.5977845220049605\n",
      "en_trf_bertbaseuncased_lg\n",
      "['Inspired by the recent meta-learning strategy (Finn et al', ' 2017)', ' we learn to infer the structure relations from a training set to a validation set', ' which can benefit the generalization capability of the learned model.In other words', ' our proposed GIL attempts to learn some transferable knowledge underlying in the structure relations from training samples to validation samples', ' such that the learned structure relations can be better self-adapted to the new testing stage. We summarize the main contributions of this work as three folds:  • We propose a novel graph inference learning framework by building structure relations to infer unknown node labels from those labeled nodes in an end-to-end way.']\n",
      "0.5848940612411568\n",
      "en_trf_bertbaseuncased_lg\n",
      "['The structure relations are well defined by jointly considering node attributes', ' between-node paths', ' and graph topological structures. • To make the inference model better generalize to test nodes', ' we introduce a meta-learning procedure to optimize structure relations', ' which could be the first time for graph node classification to the best of our knowledge. • Comprehensive evaluations on three citation network datasets (including Cora', ' Citeseer', ' and Pubmed) and one knowledge graph data (i.e.', ' NELL) demonstrate the superiority of our proposed GIL in contrast with other state-of-the-art methods on the semi-supervised classification task.']\n",
      "0.5661838624948892\n",
      "en_trf_bertbaseuncased_lg\n",
      "[' 2     (b) The process of Graph inference learning. We extract the local representation from the local subgraph (the circle with dashed line     The red wave line denote the node reachability from     to     dt th hbilit f  d t th d   \\x0cPublished as a conference paper at ICLR 2020  2 RELATED WORK  Graph CNNs: With the rapid development of deep learning methods', ' various graph convolution neural networks (Kashima et al', ' 2003; Morris et al', ' 2017; Shervashidze et al', ' 2009; Yanardag & Vishwanathan', ' 2015; Jiang et al', ' 2019; Zhang et al', ' 2020) have been exploited to analyze the irregular graph-structured data.For better extending general convolutional neural networks to graph domains', ' two broad strategies have been proposed', ' including spectral and spatial convolution methods.']\n",
      "0.5404000021536574\n",
      "en_trf_bertbaseuncased_lg\n",
      "['Specifically', ' spectral filtering methods (Henaff et al', ' 2015; Kipf & Welling', ' 2017) develop convolution-like operators in the spectral domain', ' and then perform a series of spectral filters by decomposing the graph Laplacian.Unfortunately', ' the spectral-based approaches often lead to a high computational complex due to the operation of eigenvalue decomposition', ' especially for a large number of graph nodes.To alleviate this computation burden', ' local spectral filtering methods (Defferrard et al', ' 2016) are then proposed by parameterizing the frequency responses as a Chebyshev polynomial approximation.']\n",
      "0.5292014952419993\n",
      "en_trf_bertbaseuncased_lg\n",
      "['Another type of graph CNNs', ' namely spatial methods (Li et al', ' 2016; Niepert et al', ' 2016)', ' can perform the filtering operation by defining the spatial structures of adjacent vertices.Various approaches can be employed to aggregate or sort neighboring vertices', ' such as diffusion CNNs (Atwood & Towsley', ' 2016)', ' GraphSAGE (Hamilton et al', ' 2017)', ' PSCN (Niepert et al', ' 2016)', ' and NgramCNN (Luo et al', ' 2017).From the perspective of data distribution', ' recently', ' the Gaussian induced convolution model (Jiang et al', ' 2019) is proposed to disentangle the aggregation process through encoding adjacent regions with Gaussian mixture model.']\n",
      "0.49678452951593705\n",
      "en_trf_bertbaseuncased_lg\n",
      "[' Semi-supervised node classification: Among various graph-related applications', ' semi-supervised node classification has gained increasing attention recently', ' and various approaches have been proposed to deal with this problem', ' including explicit graph Laplacian regularization and graph- embedding approaches.Several classic algorithms with graph Laplacian regularization contain the label propagation method using Gaussian random fields (Zhu et al', ' 2003)', ' the regularization framework by relying on the local/global consistency (Zhou et al', ' 2004)', ' and the random walk- based sampling algorithm for acquiring the context information (Yang et al', ' 2016).To further address scalable semi-supervised learning issues (Liu et al', ' 2012)', ' the Anchor Graph regularization approach (Liu et al', ' 2010) is proposed to scale linearly with the number of graph nodes and then applied to massive-scale graph datasets.']\n",
      "0.4755437606738042\n",
      "en_trf_bertbaseuncased_lg\n",
      "['Several graph convolution network methods (Abu-El-Haija et al', ' 2018; Du et al', ' 2017; Thekumparampil et al', ' 2018; Velickovic et al', ' 2018; Zhuang & Ma', ' 2018) are then developed to obtain discriminative representations of input graphs.For example', ' Kipf et al (Kipf & Welling', ' 2017) proposed a scalable graph CNN model', ' which can scale linearly in the number of graph edges and learn graph representations by encoding both local graph structures and node attributes.Graph attention networks (GAT) (Velickovic et al', ' 2018) are proposed to compute hidden representations of each node for attending to its neighbors with a self-attention strategy.']\n",
      "0.5227798657885866\n",
      "en_trf_bertbaseuncased_lg\n",
      "['By jointly considering the local- and global-consistency information', ' dual graph convolutional networks (Zhuang & Ma', ' 2018) are presented to deal with semi-supervised node classification.The critical difference between our proposed GIL and those previous semi-supervised node classification methods is to adopt a graph inference strategy by defining structure relations on graphs and then leverage a meta optimization mechanism to learn an inference model', ' which could be the first time to the best of our knowledge', ' while the existing graph CNNs take semi-supervised node classification as a general classification task. 3 THE PROPOSED MODEL 3.1 PROBLEM DEFINITION  Formally', ' we denote an undirected/directed graph as G = {V', ' E', ' X ', ' Y}', ' where V = {vi}n i=1 is the finite set of n (or |V|) vertices', ' E ∈ Rn×n defines the adjacency relationships (i.e.', ' edges) between vertices representing the topology of G', ' X ∈ Rn×d records the explicit/implicit attributes/signals of vertices', ' and Y ∈ Rn is the vertex labels of C classes.']\n",
      "0.5549599048449081\n",
      "en_trf_bertbaseuncased_lg\n",
      "['The edge Eij = E(vi', ' vj) = 0 if and only if vertices vi', ' vj are not connected', ' otherwise Eij (cid:54)= 0.The attribute matrix X is attached to the vertex set V', ' whose i-th row Xvi (or Xi·) represents the attribute of the i-th vertex vi.It means that vi ∈ V carries a vector of d-dimensional signals.']\n",
      "0.5390773005893315\n",
      "en_trf_bertbaseuncased_lg\n",
      "['Associated with each node vi ∈ V', ' there is a discrete label yi ∈ {1', ' 2', ' · · · ', ' C}. We consider the task of semi-supervised node classification over graph data', ' where only a small number of vertices are labeled for the model learning', ' i.e.', ' |VLabel| (cid:28) |V|.Generally', ' we have three node sets: a training set Vtr', ' a validation set Vval', ' and a testing set Vte.']\n",
      "0.6133591640925058\n",
      "en_trf_bertbaseuncased_lg\n",
      "['In the standard protocol  3  \\x0cPublished as a conference paper at ICLR 2020  of prior literatures (Yang et al', ' 2016)', ' the three node sets share the same label space.We follow but do not restrict this protocol for our proposed method.Given the training and validation node sets', ' the aim is to predict the node labels of testing nodes by using node attributes as well as edge connections.']\n",
      "0.6196305965963345\n",
      "en_trf_bertbaseuncased_lg\n",
      "['A sophisticated machine learning technique used in most existing methods (Kipf & Welling', ' 2017; Zhou et al', ' 2004) is to choose the optimal classifier (trained on a training set) after checking the performance on the validation set.However', ' these methods essentially ignore how to extract transferable knowledge from these known labeled nodes to unlabeled nodes', ' as the graph structure itself implies node connectivity/reachability.Moreover', ' due to the scarcity of labeled samples', ' the performance of such a classifier is usually not satisfying.']\n",
      "0.5930219389424481\n",
      "en_trf_bertbaseuncased_lg\n",
      "['To address these issues', ' we introduce a meta-learning mechanism (Finn et al', ' 2017; Ravi & Larochelle', ' 2017; Sung et al', ' 2017) to learn to infer node labels on graphs.Specifically', ' the graph structure', ' between-node path reachability', ' and node attributes are jointly modeled into the learning process.Our aim is to learn to infer from labeled nodes to unlabeled nodes', ' so that the learner can perform better on a validation set and thus classify a testing set more accurately.']\n",
      "0.5967069308189641\n",
      "en_trf_bertbaseuncased_lg\n",
      "[' 3.2 STRUCTURE RELATION For convenient inference', ' we specifically build a structure relation between two nodes on the topology graph.The labeled vertices (in a training set) are viewed as the reference nodes', ' and their information can be propagated into those unlabeled vertices for improving the label prediction accuracy.Formally', ' given a reference node vi ∈ VLabel', ' we define the score of a query node vj similar to vi as  (1) where Gvi and Gvj may be understood as the centralized subgraphs around vi and vj', ' respectively. fe', ' fr', ' fP are three abstract functions that we explain as follows:  si→j = fr(fe(Gvi )', ' fe(Gvj )', ' fP (vi', ' vj', ' E))', '  • Node representation fe(Gvi) −→ Rdv ', ' encodes the local representation of the centralized subgraph Gvi around node vi', ' and may thus be understood as a local filter function on graphs.']\n",
      "0.571099623564327\n",
      "en_trf_bertbaseuncased_lg\n",
      "['This function should not only take the signals of nodes therein as input', ' but also consider the local topological structure of the subgraph for more accurate similarity computation.To this end', ' we perform the spectral graph convolution on subgraphs to learn discriminative node features', ' analogous to the pixel-level feature extraction from convolution maps of gridded images.The details of feature extraction fe are described in Section 4.']\n",
      "0.5609832420974323\n",
      "en_trf_bertbaseuncased_lg\n",
      "[' • Path reachability fP (vi', ' vj', ' E) −→ Rdp ', ' represents the characteristics of path reachability from vi to vj.As there usually exist multiple traversal paths between two nodes', ' we choose the function as reachable probabilities of different lengths of walks from vi to vj.More details will be introduced in Section 4.']\n",
      "0.6497621031393012\n",
      "en_trf_bertbaseuncased_lg\n",
      "[' • Structure relation fr(Rdv ', ' Rdv ', ' Rdp ) −→ R', ' is a relational function computing the score of vj similar to vi.This function is not exchangeable for different orders of two nodes', ' due to the asymmetric reachable relationship fPIf necessary', ' we may easily revise it as a symmetry function', ' e.g.', ' summarizing two traversal directions.The score function depends on triple inputs: the local representations extracted from the subgraphs w.r.t. fe(Gvi) and fe(Gvj )', ' respectively', ' and the path reachability from vi to vj.']\n",
      "0.6191013410062812\n",
      "en_trf_bertbaseuncased_lg\n",
      "[' In semi-supervised node classification', ' we take the training node set Vtr as the reference samples', ' and the validation set Vval as the query samples during the training stage.Given a query node vj ∈ Vval', ' we can derive the class similarity score of vj w.r.t. the c-th (c = 1', ' · · · ', ' C) category by weighting the reference samples Cc = {vk|yvk = c}.Formally', ' we can further revise Eqn (1) and define the class-to-node relationship function as  sCc→j = φr(FCc→vj  wi→j · fe(Gvi)', ' fe(Gvj ))', '  (cid:88)  vi∈Cc  s.t. wi→j = φw(fP (vi', ' vj', ' E))', '  (3) where the function φw maps a reachable vector fP (vi', ' vj', ' E) into a weight value', ' and the function φr computes the similar score between vj and the c-th class nodes.']\n",
      "0.5651498287163952\n",
      "en_trf_bertbaseuncased_lg\n",
      "['The normalization factor FCc→vj of the c-th category w.r.t. vj is defined as  (2)  (4)  For the relation function φr and the weight function φw', ' we may choose some subnetworks to instantiate them in practice.The detailed implementation of our model can be found in Section 4. FCc→vj =  (cid:80)  1  vi∈Cc  wi→j  4  \\x0cPublished as a conference paper at ICLR 2020  3.3  INFERENCE LEARNING  According to the class-to-node relationship function in Eqn (2)', ' given a query node vj', ' we can obtain a score vector sC→j = [sC1→j', ' · · · ', ' sCC →j](cid:124) ∈ RC after computing the relations to all classesThe indexed category with the maximum score is assumed to be the estimated label.']\n",
      "0.5731975846266155\n",
      "en_trf_bertbaseuncased_lg\n",
      "['Thus', ' we can define the loss function based on cross entropy as follows:  L = −  yj', 'c log ˆyCc→j', '  (cid:88)  C (cid:88)  vj  c=1  (5)  (6)  (7)  where yj', 'c is a binary indicator (i.e.', ' 0 or 1) of class label c for node vj', ' and the softmax operation is imposed on sCc→j', ' i.e.', ' ˆyCc→j = exp(sCc→j)/ (cid:80)C k=1 exp(sCk→j).Other error functions may be chosen as the loss function', ' e.g.', ' mean square error.In the regime of general classification', ' the cross entropy loss is a standard one that performs well.']\n",
      "0.5357280755290261\n",
      "en_trf_bertbaseuncased_lg\n",
      "[' Given a training set Vtr', ' we expect that the best performance can be obtained on the validation set Vval after optimizing the model on Vtr.Given a trained/pretrained model Θ = {fe', ' φw', ' φr}', ' we perform iteratively gradient updates on the training set Vtr to obtain the new model', ' formally', '  Θ(cid:48) = Θ − α∇ΘLtr(Θ)', '  where α is the updating rate.Note that', ' in the computation of class scores', ' since the reference node and query node can be both from the training set Vtr', ' we set the computation weight wi→j = 0 if i = j in Eqn (3).']\n",
      "0.6004843133203193\n",
      "en_trf_bertbaseuncased_lg\n",
      "['After several iterates of gradient descent on Vtr', ' we expect a better performance on the validation set Vval', ' i.e.', ' min Θ  Lval(Θ(cid:48)).Thus', ' we can perform the gradient update as follows  where β is the learning rate of meta optimization (Finn et al', ' 2017). Θ = Θ − β∇ΘLval(Θ(cid:48))', '  During the training process', ' we may perform batch sampling from training nodes and validation nodes', ' instead of taking all one time.']\n",
      "0.5848604249470419\n",
      "en_trf_bertbaseuncased_lg\n",
      "['In the testing stage', ' we may take all training nodes and perform the model update according to Eqn (6) like the training process.The updated model is used as the final model and is then fed into Eqn (2) to infer the class labels for those query nodes. 4 MODULES In this section', ' we instantiate all modules (i.e.', ' functions) of the aforementioned structure relation.']\n",
      "0.6204926895437002\n",
      "en_trf_bertbaseuncased_lg\n",
      "['The implementation details can be found in the following. Node Representation fe(Gvi): The local representation at vertex vi can be extracted by performing the graph convolution operation on subgraph GviSimilar to gridded images/videos', ' on which local convolution kernels are defined as multiple lattices with various receptive fields', ' the spectral graph convolution is used to encode the local representations of an input graph in our work. Given a graph sample G = {V', ' E', ' X }', ' the normalized graph Laplacian matrix is L = In − D−1/2ED−1/2 = UΛUT ', ' with a diagonal matrix of its eigenvalues Λ.']\n",
      "0.5470497369462705\n",
      "en_trf_bertbaseuncased_lg\n",
      "['The spectral graph convo- lution can be defined as the multiplication of signal X with a filter gθ(Λ) = diag(θ) parameterized by θ in the Fourier domain: conv(X ) = gθ(L) ∗ X = Ugθ(Λ)UT X ', ' where parameter θ ∈ Rn is a vector of Fourier coefficients.To reduce the computational complexity and obtain the local information', ' we use an approximate local filter of the Chebyshev polynomial (Defferrard et al', ' 2016)', ' gθ(Λ) = (cid:80)K−1 k=0 θkTk(ˆΛ)', ' where parameter θ ∈ RK is a vector of Chebyshev coefficients and Tk(ˆΛ) ∈ Rn×n is the Chebyshev polynomial of order k evaluated at ˆΛ = 2Λ/λmax − In', ' a diagonal matrix of scaled eigenvalues.The graph filtering operation can then be expressed as gθ(Λ) ∗ X = (cid:80)K−1 k=0 θkTk(ˆL)X ', ' where Tk(ˆL) ∈ Rn×n is the Chebyshev polynomial of order k evaluated at the scaled Laplacian ˆL = 2L/λmax − In. Further', ' we can construct multi-scale receptive fields for each vertex based on the Laplacian matrix L', ' where each receptive field records hopping neighborhood relationships around the reference vertex vi', ' and forms a local centralized subgraph.']\n",
      "0.4960105195310297\n",
      "en_trf_bertbaseuncased_lg\n",
      "[' Path Reachability fP (vi', ' vj', ' E): Here we compute the probabilities of paths from vertex i to vertex j by employing random walks on graphs', ' which refers to traversing the graph from vi to vj according to the probability matrix P. For the input graph G with n vertices', ' the random-walk transition matrix  5  \\x0cPublished as a conference paper at ICLR 2020  Datasets Nodes 2', '708 Cora 3', '327 Citeseer 19', '717 Pubmed NELL 65', '755  Edges 5', '429 4', '732 44', '338 266', '144  Classes 7 6 3 210  Features 1', '433 3', '703 500 5', '414  Label Rates 0.052 0.036 0.003 0.001  Table 1: The properties (especially for label rate) of various graph datasets used for the semi-supervised classification task. can be defined as P = D−1E', ' where D ∈ Rn×n is the diagonal degree matrix with Dii = (cid:80) That is to say', ' each element Pij is the probability of going from vertex i to vertex j in one step. i Eij.']\n",
      "0.5571171621431286\n",
      "en_trf_bertbaseuncased_lg\n",
      "[' The sequence of nodes from vertex i to vertex j is a random walk on the graph', ' which can be modeled as a classical Markov chain by considering the set of graph vertices.To represent this formulation', ' we show that P t ij is the probability of getting from vertex vi to vertex vj in t steps.This fact is easily exhibited by considering a t-step path from vertex vi to vertex vj as first taking a single step to some vertex h', ' and then taking t − 1 steps to vj.']\n",
      "0.5755390471274423\n",
      "en_trf_bertbaseuncased_lg\n",
      "['The transition probability P t in t steps can be formulated as  P t  ij =  PihP t−1 h', 'j  \\uf8f1 \\uf8f2  \\uf8f3  Pij (cid:88)  h  if t = 1 if t > 1 ', '  where each matrix entry P t steps.Finally', ' the node reachability from vi to vj can be written as a dp-dimensional vector:  ij denotes the probability of starting at vertex i and ending at vertex j in t  ij', '.', ' P dp ij ]', ' where dp refers to the step length of the longest path from vi to vj. fP (vi', ' vj', ' E) = [Pij', ' P 2  Class-to-Node Relationship sCc→j: To define the node relationship si→j from vi to vj', ' we simulta- neously consider the property of path reachability fP (vi', ' vj', ' E)', ' local representations fe(Gvi)', ' and fe(Gvj ) of nodes vi', ' vj.']\n",
      "0.5584446486677682\n",
      "en_trf_bertbaseuncased_lg\n",
      "['The function φw(fP (vi', ' vj', ' E)) in Eqn (3)', ' which is to map the reachable vector fP (vi', ' vj', ' E) ∈ Rdp into a weight value', ' can be implemented with two 16-dimensional fully connected layers in our experiments.The computed value wi→j can be further used to weight the local features at node vi', ' fe(Gvi) ∈ RdvFor obtaining the similar score between vj and the c-th class nodes Cc in Eqn (2)', ' we perform a concatenation of two input features', ' where one refers to the weighted features of vertex vi', ' and another is the local features of vertex vj.One fully connected layer (w.r.t. φr) with C-dimensions is finally adopted to obtain the relation regression score.']\n",
      "0.5903809350854048\n",
      "en_trf_bertbaseuncased_lg\n",
      "[' (8)  (9)  5 EXPERIMENTS  5.1 EXPERIMENTAL SETTINGS  We evaluate our proposed GIL method on three citation network datasets: Cora', ' Citeseer', ' Pubmed (Sen et al', ' 2008)', ' and one knowledge graph NELL dataset (Carlson et al', ' 2010).The statistical properties of graph data are summarized in Table 1.Following the previous protocol in (Kipf & Welling', ' 2017; Zhuang & Ma', ' 2018)', ' we split the graph data into a training set', ' a validation set', ' and a testing set.']\n",
      "0.5480428951553592\n",
      "en_trf_bertbaseuncased_lg\n",
      "['Taking into account the graph convolution and pooling modules', ' we may alternately stack them into a multi-layer Graph convolutional network.The GIL model consists of two graph convolution layers', ' each of which is followed by a mean-pooling layer', ' a class-to-node relationship regression module', ' and a final softmax layer.We have given the detailed configuration of the relationship regression module in the class-to-node relationship of Section 4.']\n",
      "0.6017380935460193\n",
      "en_trf_bertbaseuncased_lg\n",
      "['The parameter dp in Eqn (9) is set to the mean length of between-node reachability paths in the input graph.The channels of the 1-st and 2-nd convolutional layers are set to 128 and 256', ' respectively.The scale of the respective filed is 2 in both convolutional layers.']\n",
      "0.601071321950123\n",
      "en_trf_bertbaseuncased_lg\n",
      "['The dropout rate is set to 0.5 in the convolution and fully connected layers to avoid over-fitting', ' and the ReLU unit is leveraged as a nonlinear activation function.We pre-train our proposed GIL model for 200 iterations with the training set', ' where its initial learning rate', ' decay factor', ' and momentum are set to 0.05', ' 0.95', ' and 0.9', ' respectively.Here we train the GIL model using the stochastic gradient descent method with the batch size of 100.']\n",
      "0.6256185225063298\n",
      "en_trf_bertbaseuncased_lg\n",
      "['We further improve the inference learning capability of the GIL model for 1200 iterations with the validation set', ' where the meta-learning rates α and β are both set to 0.001. 6  \\x0cPublished as a conference paper at ICLR 2020  5.2 COMPARISON WITH STATE-OF-THE-ARTS  We compare the GIL approach with several state-of-the-art methods (Monti et al', ' 2017; Kipf & Welling', ' 2017; Zhou et al', ' 2004; Zhuang & Ma', ' 2018) over four graph datasets', ' including Cora', ' Citeseer', ' Pubmed', ' and NELL.The classification accuracies for all methods are reported in Table 2.']\n",
      "0.5571524705721171\n",
      "en_trf_bertbaseuncased_lg\n",
      "['Our proposed GIL can significantly outperform these graph Laplacian regularized methods on four graph datasets', ' including Deep walk (Zhou et al', ' 2004)', ' modularity clustering (Brandes et al', ' 2008)', ' Gaussian fields (Zhu et al', ' 2003)', ' and graph embedding (Yang et al', ' 2016) methods.For example', ' we can achieve much higher performance than the deepwalk method (Zhou et al', ' 2004)', ' e.g.', ' 43.2% vs 74.1% on the Citeseer dataset', ' 65.3% vs 83.1% on the Pubmed dataset', ' and 58.1% vs 78.9% on the NELL dataset.We find that the graph embedding method (Yang et al', ' 2016)', ' which has considered both label information and graph structure during sampling', ' can obtain lower accuracies than our proposed GIL by 9.4% on the Citeseer dataset and 10.5% on the Cora dataset', ' respectively.']\n",
      "0.5457664066526592\n",
      "en_trf_bertbaseuncased_lg\n",
      "['This indicates that our proposed GIL can better optimize structure relations and thus improve the network generalization.We further compare our proposed GIL with several existing deep graph embedding methods', ' including graph attention network (Velickovic et al', ' 2018)', ' dual graph convolutional networks (Zhuang & Ma', ' 2018)', ' topology adaptive graph convolutional networks (Du et al', ' 2017)', ' Multi-scale graph convolution (Abu-El-Haija et al', ' 2018)', ' etc.For example', ' our proposed GIL achieves a very large gain', ' e.g.', ' 86.2% vs 83.3% (Du et al', ' 2017) on the Cora dataset', ' and 78.9% vs 66.0% (Kipf & Welling', ' 2017) on the NELL dataset.']\n",
      "0.5338389291941209\n",
      "en_trf_bertbaseuncased_lg\n",
      "['We evaluate our proposed GIL method on a large graph dataset with a lower label rate', ' which can significantly outperform existing baselines on the Pubmed dataset: 3.1% over DGCN (Zhuang & Ma', ' 2018)', ' 4.1% over classic GCN (Kipf & Welling', ' 2017) and TAGCN (Du et al', ' 2017)', ' 3.2% over AGNN (Thekumparampil et al', ' 2018)', ' and 3.6% over N-GCN (Abu-El-Haija et al', ' 2018).It demonstrates that our proposed GIL performs very well on various graph datasets by building the graph inference learning process', ' where the limited label information and graph structures can be well employed in the predicted framework. Table 2: Performance comparisons of semi-supervised classification methods.']\n",
      "0.5393566884070267\n",
      "en_trf_bertbaseuncased_lg\n",
      "[' Methods Clustering (Brandes et al', ' 2008) DeepWalk (Zhou et al', ' 2004) Gaussian (Zhu et al', ' 2003) G-embedding (Yang et al', ' 2016) DCNN (Atwood & Towsley', ' 2016) GCN (Kipf & Welling', ' 2017) MoNet (Monti et al', ' 2017) N-GCN (Abu-El-Haija et al', ' 2018) GAT (Velickovic et al', ' 2018) AGNN (Thekumparampil et al', ' 2018) TAGCN (Du et al', ' 2017) DGCN (Zhuang & Ma', ' 2018) Our GIL  Cora 59.5 67.2 68.0 75.7 76.8 81.5 81.7 83.0 83.0 83.1 83.3 83.5 86.2  Citeseer 60.1 43.2 45.3 64.7 - 70.3 - 72.2 72.5 71.7 72.5 72.6 74.1  Pubmed NELL 70.7 65.3 63.0 77.2 73.0 79.0 78.8 79.5 79.0 79.9 79.0 80.0 83.1  21.8 58.1 26.5 61.9 - 66.0 - - - - - 74.2 78.9  5.3 ANALYSIS  Meta-optimization: As can be seen in Table 3', ' we report the classification accuracies of semi-supervised classification with several variants of our proposed GIL and the classical GCN method (Kipf & Welling', ' 2017) when evaluating them on the Cora dataset.For analyzing the perfor- mance improvement of our proposed GIL with the graph inference learning process', ' we report the classification accuracies of GCN (Kipf & Welling', ' 2017) and our proposed GIL on the Cora dataset under two different situations', ' including “only learning with the training set Vtr\" and “with jointly learning on a training set Vtr and a validation set Vval\".“GCN /w jointly learning on Vtr & Vval\" achieves a better result than “GCN /w learning on Vtr\" by 3.6%', ' which demonstrates that the network performance can be improved by employing validation samples.']\n",
      "0.45675013963873046\n",
      "en_trf_bertbaseuncased_lg\n",
      "['When using structure relations', ' “GIL /w learning on Vtr\" obtains an improvement of 1.9% (over “GCN /w learning on Vtr”)', ' which can be attributed to the building connection between nodes.The meta-optimization strategy (“GIL /w meta-training from Vtr → Vval\" vs “GIL /w learning on Vtr”) has a gain of 2.9%', ' which indicates that a good inference capability can be learnt through meta-optimization.It is worth noting that', ' GIL adopts a meta-optimization strategy to learn the inference model', ' which is a process of migrating  7  \\x0cPublished as a conference paper at ICLR 2020  from a training set to a validation set.']\n",
      "0.6312060808024065\n",
      "en_trf_bertbaseuncased_lg\n",
      "['In other words', ' the validation set is only used to teach the model itself how to transfer to unseen data.In contrast', ' the conventional methods often employ a validation set to tune parameters of a certain model of interest. Table 3: Performance comparisons with several GIL variants and the classical GCN method on the Cora dataset.']\n",
      "0.6436403041153803\n",
      "en_trf_bertbaseuncased_lg\n",
      "[' GCN (Kipf & Welling', ' 2017)  Methods  GIL  GIL+mean pooling  GIL+2  conv layers  /w learning on Vtr /w jointly learning on Vtr & Vval /w learning on Vtr /w meta-train Vtr → Vval /w 1  conv layer /w 2  conv layers /w 3  conv layers /w max-pooling /w mean pooling  Acc. (%) 81.4 84.0 83.3 86.2 84.5 86.2 85.4 85.2 86.2  Network settings: We explore the effectiveness of our proposed GIL with the same mean pooling mechanism', ' but with different numbers of convolutional layers', ' i.e.', ' “GIL + mean pooling\" with one', ' two', ' and three convolutional layers', ' respectively.As can be seen in Table 3', ' the proposed GIL with two convolutional layers can obtain a better performance on the Cora data than the other two network settings (i.e.', ' GIL with one or three convolutional layers).For example', ' the performance of ‘GIL /w 1  conv layer + mean pooling\" is slightly decreased by 1.7% over “GIL /w 2  conv layers + mean pooling\" on the Cora dataset.']\n",
      "0.5818942730524941\n",
      "en_trf_bertbaseuncased_lg\n",
      "['Furthermore', ' we report the classification results of our proposed GIL by using mean and max-pooling mechanisms', ' respectively.GIL with mean pooling (i.e.', ' “GIL /w 2 conv layers + mean pooling\") can get a better result than the GIL method with max-pooling (i.e.', ' “GIL /w 2 conv layers + max-pooling\")', ' e.g.', ' 86.2% vs 85.2% on the Cora graph dataset.The reason may be that the graph network with two convolutional layers and the mean pooling mechanism can obtain the optimal graph embeddings', ' but when increasing the network layers', ' more parameters of a certain graph model need to be optimized', ' which may lead to the over-fitting issue.']\n",
      "0.6203900028208551\n",
      "en_trf_bertbaseuncased_lg\n",
      "[' Inﬂuence of different between-node steps: We compare the classification performance within different between-node steps for our proposed GIL and GCN (Kipf & Welling', ' 2017)', ' as illustrated in Fig 2(a).The length of between-node steps can be computed with the shortest path between reference nodes and query nodes.When the step between nodes is smaller', ' both GIL and GCN methods can predict the category information for a small part of unlabeled nodes in the testing set.']\n",
      "0.6094623532019257\n",
      "en_trf_bertbaseuncased_lg\n",
      "['The reason may be that the node category information may be disturbed by its nearest neighboring nodes with different labels and fewer nodes are within 1 or 2 steps in the testing set.The GIL and GCN methods can infer the category information for a part of unlabeled nodes by adopting node attributes', ' when two nodes are not connected in the graph (i.e.', ' step=∞).By increasing the length of reachability path', ' the inference process of the GIL method would become difficult and more graph structure information may be employed in the predicted process.']\n",
      "0.6142463550252816\n",
      "en_trf_bertbaseuncased_lg\n",
      "['GIL can outperform the classic GCN by analyzing the accuracies within different between-node steps', ' which indicates that our proposed GIL has a better reference capability than GCN by using the meta-optimization mechanism from training nodes to validation nodes. (a)  (b)  Figure 2: (a) Performance comparisons within different between-node steps on the Cora dataset.The accuracy equals to the number of correctly classified nodes divided by all testing samples', ' and is accumulated from step 1 to step k. (b) Performance comparisons with different label rates on the Pubmed dataset.']\n",
      "0.616913699113174\n",
      "en_trf_bertbaseuncased_lg\n",
      "[' 8  1357911step0.00.20.40.60.8accuracyour GILGCNlabel rate0.30%0.60%0.90%1.20%1.50%1.80%GCN0.7920.7970.8050.8240.8290.834GIL(ours)0.8170.8240.8310.8360.8380.8421x2x3x4x5x6x77.0%79.0%81.0%83.0%85.0%1x2x3x4x5x6xGCNGIL(ours)Label rates Accuracy \\x0cPublished as a conference paper at ICLR 2020  Inﬂuence of different label rates: We also explore the performance comparisons of the GIL method with different label rates', ' and the detailed results on the Pubmed dataset can be shown in Fig 2(b).When label rates increase by multiplication', ' the performances of GIL and GCN are improved', ' but the relative gain becomes narrow.The reason is that', ' the reachable path lengths between unlabeled nodes and labeled nodes will be reduced with the increase of labeled nodes', ' which will weaken the effect of inference learning.']\n",
      "0.5294832253139523\n",
      "en_trf_bertbaseuncased_lg\n",
      "['In the extreme case', ' labels of unlabeled nodes could be determined by those neighbors with the 1 ∼ 2 step reachability.In summary', ' our proposed GIL method prefers small ratio labeled nodes on the semi-supervised node classification task. Inference learning process: Classification errors of different epochs on the validation set of the Cora dataset can be illustrated in Fig 3.']\n",
      "0.6456181124170579\n",
      "en_trf_bertbaseuncased_lg\n",
      "['Classification errors are rapidly decreasing as the number of iterations increases from the beginning to 400 iterations', ' while they are with a slow descent from 400 iterations to 1200 iterations.It demonstrates that the learned knowledge from the training samples can be transferred for inferring node category information from these reference labeled nodes.The performance of semi-supervised classification can be further increased by improving the generalized capability of the Graph CNN model.']\n",
      "0.5850428903998104\n",
      "en_trf_bertbaseuncased_lg\n",
      "[' Table 4: Performance comparisons with different mod- ules on the Cora dataset', ' where fe', ' fP ', ' and fr denote node representation', ' path reachability', ' and structure re- lation', ' respectively. fP fr fe - - - (cid:88) - - (cid:88) (cid:88) - (cid:88) (cid:88) (cid:88)  Acc.(%) 56.0 81.5 85.0 86.2  Figure 3: Classification errors of different itera- tions on the validation set of the Cora dataset. Module analysis: We evaluate the effectiveness of different modules within our proposed GIL framework', ' including node representation fe', ' path reachability fP ', ' and structure relation fr.']\n",
      "0.6052077473327506\n",
      "en_trf_bertbaseuncased_lg\n",
      "['Note that the last one fr defines on the former two ones', ' so we consider the cases in Table 4 by adding modules.When not using all modules', ' only original attributes of nodes are used to predict labels.The case of only using fe belongs to the GCN method', ' which can achieve 81.5% on the Cora dataset.']\n",
      "0.6608844701429274\n",
      "en_trf_bertbaseuncased_lg\n",
      "['The large gain of using the relation module fr (i.e.', ' from 81.5% to 85.0%) may be contributed to the ability of inference learning on attributes as well as local topology structures which are implicitly encoded in fe.The path information fP can further boost the performance by 1.2%', ' e.g.', ' 86.2% vs 85.0%.It demonstrates that three different modules of our method can improve the graph inference learning capability.']\n",
      "0.5745699637778828\n",
      "en_trf_bertbaseuncased_lg\n",
      "[' Computational complexity: For the computational complexity of our proposed GIL', ' the cost is mainly spent on the computations of node representation', ' between-node reachability', ' and class-to- node relationship', ' which are about O((ntr + nte) ∗ e ∗ din ∗ dout)', ' O((ntr + nte) ∗ e ∗ P )', ' and O(ntr ∗ nted2 out)', ' respectively. ntr and nte refer to the numbers of training and testing nodes', ' din and dout denote the input and output dimensions of node representation', ' e is about the average degree of graph node', ' and P is the step length of node reachability.Compared with those classic Graph CNNs (Kipf & Welling', ' 2017)', ' our proposed GIL has a slightly higher cost due to an extra inference learning process', ' but can complete the testing stage with several seconds on these benchmark datasets. 6 CONCLUSION  In this work', ' we tackled the semi-supervised node classification task with a graph inference learning method', ' which can better predict the categories of these unlabeled nodes in an end-to-end framework.']\n",
      "0.5982642282373394\n",
      "en_trf_bertbaseuncased_lg\n",
      "['We can build a structure relation for obtaining the connection between any two graph nodes', ' where node attributes', ' between-node paths', ' and graph structure information can be encapsulated together.For better capturing the transferable knowledge', ' our method further learns to transfer the mined knowledge from the training samples to the validation set', ' finally boosting the prediction accuracy of the labels of unlabeled nodes in the testing set.The extensive experimental results demonstrate the effectiveness of our proposed GIL for solving the semi-supervised learning problem', ' even in the few-shot paradigm.']\n",
      "0.599660020107811\n",
      "en_trf_bertbaseuncased_lg\n",
      "['In the future', ' we would extend the graph inference method to handle more graph-related tasks', ' such as graph generation and social network analysis. 9  the number of iterations error \\x0cPublished as a conference paper at ICLR 2020  ACKNOWLEDGMENT  This work was supported by the National Natural Science Foundation of China (Nos 61972204', ' 61906094', ' U1713208)', ' the Natural Science Foundation of Jiangsu Province (Grant Nos.BK20191283 and BK20190019)', ' and Tencent AI Lab Rhino-Bird Focused Research Program (No.']\n",
      "0.5299078053640411\n",
      "en_trf_bertbaseuncased_lg\n",
      "['JR201922). REFERENCES  2001', ' 2016. Sami Abu-El-Haija', ' Amol Kapoor', ' Bryan Perozzi', ' and Joonseok Lee.']\n",
      "0.4729437711438984\n",
      "en_trf_bertbaseuncased_lg\n",
      "['N-gcn: Multi-scale graph  convolution for semi-supervised node classification. arXiv preprint arXiv:1802.08888', ' 2018. James Atwood and Don Towsley.Diffusion-convolutional neural networks.']\n",
      "0.530117944414035\n",
      "en_trf_bertbaseuncased_lg\n",
      "['In NeurIPS', ' pp 1993–  Karsten M Borgwardt', ' Hans-Peter Kriegel', ' SVN Vishwanathan', ' and Nicol N Schraudolph.Graph ker- nels for disease outcome prediction from protein-protein interaction networks.Pacific Symposium on Biocomputing Pacific Symposium on Biocomputing', ' pp 4–15', ' 2007.']\n",
      "0.5309400139226468\n",
      "en_trf_bertbaseuncased_lg\n",
      "[' Ulrik Brandes', ' Daniel Delling', ' Marco Gaertler', ' Robert Gorke', ' Martin Hoefer', ' Zoran Nikoloski', ' and Dorothea Wagner.On modularity clustering.IEEE transactions on knowledge and data engineering', ' 20(2):172–188', ' 2008.']\n",
      "0.5018394916234628\n",
      "en_trf_bertbaseuncased_lg\n",
      "[' Andrew Carlson', ' Justin Betteridge', ' Bryan Kisiel', ' Burr Settles', ' Estevam R. Hruschka Jr.', ' and Tom M.  Mitchell.Toward an architecture for never-ending language learning.In AAAI', ' 2010.']\n",
      "0.567999212339241\n",
      "en_trf_bertbaseuncased_lg\n",
      "[' Michaël Defferrard', ' Xavier Bresson', ' and Pierre Vandergheynst.Convolutional neural networks on  graphs with fast localized spectral filtering.In NeurIPS', ' pp 3844–3852', ' 2016.']\n",
      "0.5957524450312613\n",
      "en_trf_bertbaseuncased_lg\n",
      "[' Jian Du', ' Shanghang Zhang', ' Guanhang Wu', ' José MF Moura', ' and Soummya Kar.Topology adaptive  graph convolutional networks. arXiv preprint arXiv:1710.10370', ' 2017. Chelsea Finn', ' Pieter Abbeel', ' and Sergey Levine.']\n",
      "0.5148202872027168\n",
      "en_trf_bertbaseuncased_lg\n",
      "['Model-agnostic meta-learning for fast adaptation of  deep networks.In ICML', ' pp 1126–1135', ' 2017. Will Hamilton', ' Zhitao Ying', ' and Jure Leskovec.']\n",
      "0.5951559400412022\n",
      "en_trf_bertbaseuncased_lg\n",
      "['Inductive representation learning on large graphs.In  NeurIPS', ' pp 1025–1035', ' 2017. Kaiming He', ' Xiangyu Zhang', ' Shaoqing Ren', ' and Jian Sun.']\n",
      "0.5535924145671717\n",
      "en_trf_bertbaseuncased_lg\n",
      "['Deep residual learning for image  recognition.In CVPR', ' pp 770–778', ' 2016. Mikael Henaff', ' Joan Bruna', ' and Yann LeCun.']\n",
      "0.5763820172215518\n",
      "en_trf_bertbaseuncased_lg\n",
      "['Deep convolutional networks on graph-structured data. arXiv preprint arXiv:1506.05163', ' 2015. Jiatao Jiang', ' Zhen Cui', ' Chunyan Xu', ' and Jian Yang.']\n",
      "0.4282553339621461\n",
      "en_trf_bertbaseuncased_lg\n",
      "['Gaussian-induced convolution for graphs.In  AAAI', ' volume 33', ' pp 4007–4014', ' 2019. Hisashi Kashima', ' Koji Tsuda', ' and Akihiro Inokuchi.']\n",
      "0.520151834064179\n",
      "en_trf_bertbaseuncased_lg\n",
      "['Marginalized kernels between labeled graphs. In ICML', ' pp 321–328', ' 2003. Thomas N. Kipf and Max Welling.']\n",
      "0.6562072824702831\n",
      "en_trf_bertbaseuncased_lg\n",
      "['Semi-supervised classification with graph convolutional networks. In ICLR', ' 2017. networks.']\n",
      "0.5878728445702992\n",
      "en_trf_bertbaseuncased_lg\n",
      "['ICLR', ' 2016. learning.In ICML', ' 2010.']\n",
      "0.5836881949996747\n",
      "en_trf_bertbaseuncased_lg\n",
      "[' Yujia Li', ' Daniel Tarlow', ' Marc Brockschmidt', ' and Richard Zemel.Gated graph sequence neural  Wei Liu', ' Junfeng He', ' and Shih-Fu Chang.Large graph construction for scalable semi-supervised  Wei Liu', ' Jun Wang', ' and Shih-Fu Chang.']\n",
      "0.4668098264958929\n",
      "en_trf_bertbaseuncased_lg\n",
      "['Robust and scalable graph-based semisupervised learning. Proceedings of the IEEE', ' 100(9):2624–2638', ' 2012. Zhiling Luo', ' Ling Liu', ' Jianwei Yin', ' Ying Li', ' and Zhaohui Wu. Deep learning of graphs with ngram convolutional neural networks.']\n",
      "0.5023712965795504\n",
      "en_trf_bertbaseuncased_lg\n",
      "['IEEE Transactions on Knowledge and Data Engineering', ' 29(10): 2125–2139', ' 2017. 10  \\x0cPublished as a conference paper at ICLR 2020  Federico Monti', ' Davide Boscaini', ' Jonathan Masci', ' Emanuele Rodola', ' Jan Svoboda', ' and Michael M Bronstein.Geometric deep learning on graphs and manifolds using mixture model cnns.']\n",
      "0.5268063113980883\n",
      "en_trf_bertbaseuncased_lg\n",
      "['In CVPR', ' pp 5115–5124', ' 2017. Christopher Morris', ' Kristian Kersting', ' and Petra Mutzel.Glocalized weisfeiler-lehman graph kernels:  Global-local feature maps of graphs.']\n",
      "0.5499716012600308\n",
      "en_trf_bertbaseuncased_lg\n",
      "['In ICDM', ' pp 327–336.IEEE', ' 2017. Mathias Niepert', ' Mohamed Ahmed', ' and Konstantin Kutzkov.']\n",
      "0.5708417537178523\n",
      "en_trf_bertbaseuncased_lg\n",
      "['Learning convolutional neural networks  for graphs.In ICML', ' pp 2014–2023', ' 2016. Francesco Orsini', ' Daniele Baracchi', ' and Paolo Frasconi.']\n",
      "0.5278504563964487\n",
      "en_trf_bertbaseuncased_lg\n",
      "['Shift aggregate extract networks. arXiv  preprint arXiv:1703.05537', ' 2017. Lawrence Page', ' Sergey Brin', ' Rajeev Motwani', ' and Terry Winograd.The pagerank citation ranking:  Bringing order to the web.']\n",
      "0.5245257890202814\n",
      "en_trf_bertbaseuncased_lg\n",
      "['Technical Report 1999-66', ' 1999. Shirui Pan', ' Ruiqi Hu', ' Guodong Long', ' Jing Jiang', ' Lina Yao', ' and Chengqi Zhang.Adversarially  regularized graph autoencoder for graph embedding.']\n",
      "0.4763483954651658\n",
      "en_trf_bertbaseuncased_lg\n",
      "['In IJCAI', ' pp 2609–2615', ' 2018. Sachin Ravi and Hugo Larochelle.Optimization as a model for few-shot learning.']\n",
      "0.6407311347778408\n",
      "en_trf_bertbaseuncased_lg\n",
      "['In ICLR', ' 2017. Prithviraj Sen', ' Galileo Namata', ' Mustafa Bilgic', ' Lise Getoor', ' Brian Galligher', ' and Tina Eliassi-Rad. Collective classification in network data.']\n",
      "0.5336811727885047\n",
      "en_trf_bertbaseuncased_lg\n",
      "['AI magazine', ' 29(3):93–93', ' 2008. Nino Shervashidze', ' SVN Vishwanathan', ' Tobias Petri', ' Kurt Mehlhorn', ' and Karsten Borgwardt.Efficient graphlet kernels for large graph comparison.']\n",
      "0.5565965586158429\n",
      "en_trf_bertbaseuncased_lg\n",
      "['In Artificial Intelligence and Statistics', ' pp 488–495', ' 2009. Flood Sung', ' Li Zhang', ' Tao Xiang', ' Timothy Hospedales', ' and Yongxin Yang.Learning to learn:  Meta-critic networks for sample efficient learning. arXiv preprint arXiv:1706.09529', ' 2017.']\n",
      "0.5507454221206032\n",
      "en_trf_bertbaseuncased_lg\n",
      "[' Kiran K Thekumparampil', ' Chong Wang', ' Sewoong Oh', ' and Li-Jia Li. Attention-based graph neural  network for semi-supervised learning. arXiv preprint arXiv:1803.03735', ' 2018. Petar Velickovic', ' Guillem Cucurull', ' Arantxa Casanova', ' Adriana Romero', ' Pietro Liò', ' and Yoshua  Bengio.Graph attention networks.']\n",
      "0.49441752309943077\n",
      "en_trf_bertbaseuncased_lg\n",
      "['ICLR', ' 2018. Danfei Xu', ' Yuke Zhu', ' Christopher B Choy', ' and Li Fei-Fei.Scene graph generation by iterative  message passing.']\n",
      "0.4838417358700851\n",
      "en_trf_bertbaseuncased_lg\n",
      "['In CVPR', ' pp 5410–5419', ' 2017. Pinar Yanardag and SVN Vishwanathan.Deep graph kernels.']\n",
      "0.6060855620062933\n",
      "en_trf_bertbaseuncased_lg\n",
      "['In SIGKDD', ' pp 1365–1374', ' 2015. Zhilin Yang', ' William W Cohen', ' and Ruslan Salakhutdinov.Revisiting semi-supervised learning with  graph embeddings.']\n",
      "0.5955827190717856\n",
      "en_trf_bertbaseuncased_lg\n",
      "['ICML', ' 2016. Bing Yu', ' Haoteng Yin', ' and Zhanxing Zhu.Spatio-temporal graph convolutional networks: A deep  learning framework for traffic forecasting.']\n",
      "0.512785970945693\n",
      "en_trf_bertbaseuncased_lg\n",
      "['In IJCAI', ' pp 3634–3640', ' 2018. Tong Zhang', ' Zhen Cui', ' Chunyan Xu', ' Wenming Zheng', ' and Jian Yang.Variational pathway reasoning  for eeg emotion recognition.']\n",
      "0.48625312248671876\n",
      "en_trf_bertbaseuncased_lg\n",
      "['In AAAI', ' 2020. Wenting Zhao', ' Zhen Cui', ' Chunyan Xu', ' Chengzheng Li', ' Tong Zhang', ' and Jian Yang.Hashing graph  convolution for node classification.']\n",
      "0.4107759553052323\n",
      "en_trf_bertbaseuncased_lg\n",
      "['In CIKM', ' 2019. Dengyong Zhou', ' Olivier Bousquet', ' Thomas N Lal', ' Jason Weston', ' and Bernhard Schölkopf.Learning  with local and global consistency.']\n",
      "0.5539122574800021\n",
      "en_trf_bertbaseuncased_lg\n",
      "['In NeurIPS', ' pp 321–328', ' 2004. Xiaojin Zhu', ' Zoubin Ghahramani', ' and John D Lafferty.Semi-supervised learning using gaussian  fields and harmonic functions.']\n",
      "0.5763439266023839\n",
      "en_trf_bertbaseuncased_lg\n",
      "['In ICML', ' pp 912–919', ' 2003. Chenyi Zhuang and Qiang Ma. Dual graph convolutional networks for graph-based semi-supervised  classification.In WWW', ' pp 499–508', ' 2018.']\n",
      "0.5508975219872598\n",
      "en_trf_bertbaseuncased_lg\n",
      "[' 11  \\x0c']\n",
      "0.5278919656609371\n",
      "en_trf_bertbaseuncased_lg\n",
      "10\n",
      "Which is the best model\n",
      "en_trf_robertabase_lg\n",
      "['Published as a conference paper at ICLR 2020  GRAPH INFERENCE LEARNING FOR SEMI-SUPERVISED CLASSIFICATION  Chunyan Xu', ' Zhen Cui∗', ' Xiaobin Hong', ' Tong Zhang', ' and Jian Yang School of Computer Science and Engineering', ' Nanjing University of Science and Technology', ' Nanjing', ' China {cyx', 'zhen.cui', 'xbhong', 'tong.zhang', 'csjyang}@njust.edu.cn  Wei Liu Tencent AI Lab', ' China wl2223@columbia.edu  ABSTRACT  In this work', ' we address semi-supervised classification of graph data', ' where the categories of those unlabeled nodes are inferred from labeled nodes as well as graph structures.Recent works often solve this problem via advanced graph convolution in a conventionally supervised manner', ' but the performance could degrade significantly when labeled data is scarce.To this end', ' we propose a Graph Inference Learning (GIL) framework to boost the performance of semi- supervised node classification by learning the inference of node labels on graph topology.']\n",
      "0.9400941654712279\n",
      "en_trf_robertabase_lg\n",
      "['To bridge the connection between two nodes', ' we formally define a structure relation by encapsulating node attributes', ' between-node paths', ' and local topological structures together', ' which can make the inference conveniently deduced from one node to another node.For learning the inference process', ' we further introduce meta-optimization on structure relations from training nodes to validation nodes', ' such that the learnt graph inference capability can be better self-adapted to testing nodes.Comprehensive evaluations on four benchmark datasets (including Cora', ' Citeseer', ' Pubmed', ' and NELL) demonstrate the superiority of our proposed GIL when compared against state-of-the-art methods on the semi-supervised node classification task.']\n",
      "0.950274374296049\n",
      "en_trf_robertabase_lg\n",
      "[' 1  INTRODUCTION  Graph', ' which comprises a set of vertices/nodes together with connected edges', ' is a formal structural representation of non-regular data.Due to the strong representation ability', ' it accommodates many potential applications', ' e.g.', ' social network (Orsini et al', ' 2017)', ' world wide data (Page et al', ' 1999)', ' knowledge graph (Xu et al', ' 2017)', ' and protein-interaction network (Borgwardt et al', ' 2007).Among these', ' semi-supervised node classification on graphs is one of the most interesting also popular topics.']\n",
      "0.9461692109943785\n",
      "en_trf_robertabase_lg\n",
      "['Given a graph in which some nodes are labeled', ' the aim of semi-supervised classification is to infer the categories of those remaining unlabeled nodes by using various priors of the graph. While there have been numerous previous works (Brandes et al', ' 2008; Zhou et al', ' 2004; Zhu et al', ' 2003; Yang et al', ' 2016; Zhao et al', ' 2019) devoted to semi-supervised node classification based on explicit graph Laplacian regularizations', ' it is hard to efficiently boost the performance of label prediction due to the strict assumption that connected nodes are likely to share the same label information.With the progress of deep learning on grid-shaped images/videos (He et al', ' 2016)', ' a few of graph convolutional neural networks (CNN) based methods', ' including spectral (Kipf & Welling', ' 2017) and spatial methods (Niepert et al', ' 2016; Pan et al', ' 2018; Yu et al', ' 2018)', ' have been proposed to learn local convolution filters on graphs in order to extract more discriminative node representations.']\n",
      "0.9453688889389679\n",
      "en_trf_robertabase_lg\n",
      "['Although graph CNN based methods have achieved considerable capabilities of graph embedding by optimizing filters', ' they are limited into a conventionally semi-supervised framework and lack of an efficient inference mechanism on graphs.Especially', ' in the case of few-shot learning', ' where a small number of training nodes are labeled', ' this kind of methods would drastically compromise the performance.For example', ' the Pubmed graph dataset (Sen et al', ' 2008) consists  ∗Corresponding author: Zhen Cui.']\n",
      "0.9590205645853261\n",
      "en_trf_robertabase_lg\n",
      "[' 1  \\x0cPublished as a conference paper at ICLR 2020  Figure 1: The illustration of our proposed GIL framework.For the problem of graph node labeling', ' the category information of these unlabeled nodes depends on the similarity computation between a query node (e.g.', ' vj) and these labeled reference nodes (e.g.', ' vi).We consider the similarity from three points: node attributes', ' the consistency of local topological structures (i.e.', ' the circle with dashed line)', ' and the between-node path reachability (i.e.', ' the red wave line from vi to vj).']\n",
      "0.9357120893915921\n",
      "en_trf_robertabase_lg\n",
      "['Specifically', ' the local structures as well as node attributes are encoded as high-level features with graph convolution', ' while the between-node path reachability is abstracted as reachable probabilities of random walks.To better make the inference generalize to test nodes', ' we introduce a meta-learning strategy to optimize the structure relations learning from training nodes to validation nodes. of 19', '717 nodes and 44', '338 edges', ' but only 0.3% nodes are labeled for the semi-supervised node classification task.']\n",
      "0.9548972491903244\n",
      "en_trf_robertabase_lg\n",
      "['These aforementioned works usually boil down to a general classification task', ' where the model is learnt on a training set and selected by checking a validation set.However', ' they do not put great efforts on how to learn to infer from one node to another node on a topological graph', ' especially in the few-shot regime. In this paper', ' we propose a graph inference learning (GIL) framework to teach the model itself to adaptively infer from reference labeled nodes to those query unlabeled nodes', ' and finally boost the performance of semi-supervised node classification in the case of a few number of labeled samples.']\n",
      "0.9579426812671635\n",
      "en_trf_robertabase_lg\n",
      "['Given an input graph', ' GIL attempts to infer the unlabeled nodes from those observed nodes by building between-node relations.The between-node relations are structured as the integration of node attributes', ' connection paths', ' and graph topological structures.It means that the similarity between two nodes is decided from three aspects: the consistency of node attributes', ' the consistency of local topological structures', ' and the between-node path reachability', ' as shown in Fig 1.']\n",
      "0.9503520861578703\n",
      "en_trf_robertabase_lg\n",
      "['The local structures anchored around each node as well as the attributes of nodes therein are jointly encoded with graph convolution (Defferrard et al', ' 2016) for the sake of high-level feature extraction.For the between-node path reachability', ' we adopt the random walk algorithm to obtain the characteristics from a labeled reference node vi to a query unlabeled node vj in a given graph.Based on the computed node representation and between-node reachability', ' the structure relations can be obtained by computing the similar scores/relationships from reference nodes to unlabeled nodes in a graph.']\n",
      "0.9469594323190984\n",
      "en_trf_robertabase_lg\n",
      "['Inspired by the recent meta-learning strategy (Finn et al', ' 2017)', ' we learn to infer the structure relations from a training set to a validation set', ' which can benefit the generalization capability of the learned model.In other words', ' our proposed GIL attempts to learn some transferable knowledge underlying in the structure relations from training samples to validation samples', ' such that the learned structure relations can be better self-adapted to the new testing stage. We summarize the main contributions of this work as three folds:  • We propose a novel graph inference learning framework by building structure relations to infer unknown node labels from those labeled nodes in an end-to-end way.']\n",
      "0.946054381307001\n",
      "en_trf_robertabase_lg\n",
      "['The structure relations are well defined by jointly considering node attributes', ' between-node paths', ' and graph topological structures. • To make the inference model better generalize to test nodes', ' we introduce a meta-learning procedure to optimize structure relations', ' which could be the first time for graph node classification to the best of our knowledge. • Comprehensive evaluations on three citation network datasets (including Cora', ' Citeseer', ' and Pubmed) and one knowledge graph data (i.e.', ' NELL) demonstrate the superiority of our proposed GIL in contrast with other state-of-the-art methods on the semi-supervised classification task.']\n",
      "0.9583124855439761\n",
      "en_trf_robertabase_lg\n",
      "[' 2     (b) The process of Graph inference learning. We extract the local representation from the local subgraph (the circle with dashed line     The red wave line denote the node reachability from     to     dt th hbilit f  d t th d   \\x0cPublished as a conference paper at ICLR 2020  2 RELATED WORK  Graph CNNs: With the rapid development of deep learning methods', ' various graph convolution neural networks (Kashima et al', ' 2003; Morris et al', ' 2017; Shervashidze et al', ' 2009; Yanardag & Vishwanathan', ' 2015; Jiang et al', ' 2019; Zhang et al', ' 2020) have been exploited to analyze the irregular graph-structured data.For better extending general convolutional neural networks to graph domains', ' two broad strategies have been proposed', ' including spectral and spatial convolution methods.']\n",
      "0.9373722962319534\n",
      "en_trf_robertabase_lg\n",
      "['Specifically', ' spectral filtering methods (Henaff et al', ' 2015; Kipf & Welling', ' 2017) develop convolution-like operators in the spectral domain', ' and then perform a series of spectral filters by decomposing the graph Laplacian.Unfortunately', ' the spectral-based approaches often lead to a high computational complex due to the operation of eigenvalue decomposition', ' especially for a large number of graph nodes.To alleviate this computation burden', ' local spectral filtering methods (Defferrard et al', ' 2016) are then proposed by parameterizing the frequency responses as a Chebyshev polynomial approximation.']\n",
      "0.9550698295261597\n",
      "en_trf_robertabase_lg\n",
      "['Another type of graph CNNs', ' namely spatial methods (Li et al', ' 2016; Niepert et al', ' 2016)', ' can perform the filtering operation by defining the spatial structures of adjacent vertices.Various approaches can be employed to aggregate or sort neighboring vertices', ' such as diffusion CNNs (Atwood & Towsley', ' 2016)', ' GraphSAGE (Hamilton et al', ' 2017)', ' PSCN (Niepert et al', ' 2016)', ' and NgramCNN (Luo et al', ' 2017).From the perspective of data distribution', ' recently', ' the Gaussian induced convolution model (Jiang et al', ' 2019) is proposed to disentangle the aggregation process through encoding adjacent regions with Gaussian mixture model.']\n",
      "0.9437644768533817\n",
      "en_trf_robertabase_lg\n",
      "[' Semi-supervised node classification: Among various graph-related applications', ' semi-supervised node classification has gained increasing attention recently', ' and various approaches have been proposed to deal with this problem', ' including explicit graph Laplacian regularization and graph- embedding approaches.Several classic algorithms with graph Laplacian regularization contain the label propagation method using Gaussian random fields (Zhu et al', ' 2003)', ' the regularization framework by relying on the local/global consistency (Zhou et al', ' 2004)', ' and the random walk- based sampling algorithm for acquiring the context information (Yang et al', ' 2016).To further address scalable semi-supervised learning issues (Liu et al', ' 2012)', ' the Anchor Graph regularization approach (Liu et al', ' 2010) is proposed to scale linearly with the number of graph nodes and then applied to massive-scale graph datasets.']\n",
      "0.9442136559811726\n",
      "en_trf_robertabase_lg\n",
      "['Several graph convolution network methods (Abu-El-Haija et al', ' 2018; Du et al', ' 2017; Thekumparampil et al', ' 2018; Velickovic et al', ' 2018; Zhuang & Ma', ' 2018) are then developed to obtain discriminative representations of input graphs.For example', ' Kipf et al (Kipf & Welling', ' 2017) proposed a scalable graph CNN model', ' which can scale linearly in the number of graph edges and learn graph representations by encoding both local graph structures and node attributes.Graph attention networks (GAT) (Velickovic et al', ' 2018) are proposed to compute hidden representations of each node for attending to its neighbors with a self-attention strategy.']\n",
      "0.9526657083301846\n",
      "en_trf_robertabase_lg\n",
      "['By jointly considering the local- and global-consistency information', ' dual graph convolutional networks (Zhuang & Ma', ' 2018) are presented to deal with semi-supervised node classification.The critical difference between our proposed GIL and those previous semi-supervised node classification methods is to adopt a graph inference strategy by defining structure relations on graphs and then leverage a meta optimization mechanism to learn an inference model', ' which could be the first time to the best of our knowledge', ' while the existing graph CNNs take semi-supervised node classification as a general classification task. 3 THE PROPOSED MODEL 3.1 PROBLEM DEFINITION  Formally', ' we denote an undirected/directed graph as G = {V', ' E', ' X ', ' Y}', ' where V = {vi}n i=1 is the finite set of n (or |V|) vertices', ' E ∈ Rn×n defines the adjacency relationships (i.e.', ' edges) between vertices representing the topology of G', ' X ∈ Rn×d records the explicit/implicit attributes/signals of vertices', ' and Y ∈ Rn is the vertex labels of C classes.']\n",
      "0.9453768910019558\n",
      "en_trf_robertabase_lg\n",
      "['The edge Eij = E(vi', ' vj) = 0 if and only if vertices vi', ' vj are not connected', ' otherwise Eij (cid:54)= 0.The attribute matrix X is attached to the vertex set V', ' whose i-th row Xvi (or Xi·) represents the attribute of the i-th vertex vi.It means that vi ∈ V carries a vector of d-dimensional signals.']\n",
      "0.9359610103424171\n",
      "en_trf_robertabase_lg\n",
      "['Associated with each node vi ∈ V', ' there is a discrete label yi ∈ {1', ' 2', ' · · · ', ' C}. We consider the task of semi-supervised node classification over graph data', ' where only a small number of vertices are labeled for the model learning', ' i.e.', ' |VLabel| (cid:28) |V|.Generally', ' we have three node sets: a training set Vtr', ' a validation set Vval', ' and a testing set Vte.']\n",
      "0.942307946853776\n",
      "en_trf_robertabase_lg\n",
      "['In the standard protocol  3  \\x0cPublished as a conference paper at ICLR 2020  of prior literatures (Yang et al', ' 2016)', ' the three node sets share the same label space.We follow but do not restrict this protocol for our proposed method.Given the training and validation node sets', ' the aim is to predict the node labels of testing nodes by using node attributes as well as edge connections.']\n",
      "0.9627932951627081\n",
      "en_trf_robertabase_lg\n",
      "['A sophisticated machine learning technique used in most existing methods (Kipf & Welling', ' 2017; Zhou et al', ' 2004) is to choose the optimal classifier (trained on a training set) after checking the performance on the validation set.However', ' these methods essentially ignore how to extract transferable knowledge from these known labeled nodes to unlabeled nodes', ' as the graph structure itself implies node connectivity/reachability.Moreover', ' due to the scarcity of labeled samples', ' the performance of such a classifier is usually not satisfying.']\n",
      "0.9642085722761409\n",
      "en_trf_robertabase_lg\n",
      "['To address these issues', ' we introduce a meta-learning mechanism (Finn et al', ' 2017; Ravi & Larochelle', ' 2017; Sung et al', ' 2017) to learn to infer node labels on graphs.Specifically', ' the graph structure', ' between-node path reachability', ' and node attributes are jointly modeled into the learning process.Our aim is to learn to infer from labeled nodes to unlabeled nodes', ' so that the learner can perform better on a validation set and thus classify a testing set more accurately.']\n",
      "0.9596503441210885\n",
      "en_trf_robertabase_lg\n",
      "[' 3.2 STRUCTURE RELATION For convenient inference', ' we specifically build a structure relation between two nodes on the topology graph.The labeled vertices (in a training set) are viewed as the reference nodes', ' and their information can be propagated into those unlabeled vertices for improving the label prediction accuracy.Formally', ' given a reference node vi ∈ VLabel', ' we define the score of a query node vj similar to vi as  (1) where Gvi and Gvj may be understood as the centralized subgraphs around vi and vj', ' respectively. fe', ' fr', ' fP are three abstract functions that we explain as follows:  si→j = fr(fe(Gvi )', ' fe(Gvj )', ' fP (vi', ' vj', ' E))', '  • Node representation fe(Gvi) −→ Rdv ', ' encodes the local representation of the centralized subgraph Gvi around node vi', ' and may thus be understood as a local filter function on graphs.']\n",
      "0.9492812525154396\n",
      "en_trf_robertabase_lg\n",
      "['This function should not only take the signals of nodes therein as input', ' but also consider the local topological structure of the subgraph for more accurate similarity computation.To this end', ' we perform the spectral graph convolution on subgraphs to learn discriminative node features', ' analogous to the pixel-level feature extraction from convolution maps of gridded images.The details of feature extraction fe are described in Section 4.']\n",
      "0.955300203001878\n",
      "en_trf_robertabase_lg\n",
      "[' • Path reachability fP (vi', ' vj', ' E) −→ Rdp ', ' represents the characteristics of path reachability from vi to vj.As there usually exist multiple traversal paths between two nodes', ' we choose the function as reachable probabilities of different lengths of walks from vi to vj.More details will be introduced in Section 4.']\n",
      "0.9490714640539515\n",
      "en_trf_robertabase_lg\n",
      "[' • Structure relation fr(Rdv ', ' Rdv ', ' Rdp ) −→ R', ' is a relational function computing the score of vj similar to vi.This function is not exchangeable for different orders of two nodes', ' due to the asymmetric reachable relationship fPIf necessary', ' we may easily revise it as a symmetry function', ' e.g.', ' summarizing two traversal directions.The score function depends on triple inputs: the local representations extracted from the subgraphs w.r.t. fe(Gvi) and fe(Gvj )', ' respectively', ' and the path reachability from vi to vj.']\n",
      "0.9556630066091469\n",
      "en_trf_robertabase_lg\n",
      "[' In semi-supervised node classification', ' we take the training node set Vtr as the reference samples', ' and the validation set Vval as the query samples during the training stage.Given a query node vj ∈ Vval', ' we can derive the class similarity score of vj w.r.t. the c-th (c = 1', ' · · · ', ' C) category by weighting the reference samples Cc = {vk|yvk = c}.Formally', ' we can further revise Eqn (1) and define the class-to-node relationship function as  sCc→j = φr(FCc→vj  wi→j · fe(Gvi)', ' fe(Gvj ))', '  (cid:88)  vi∈Cc  s.t. wi→j = φw(fP (vi', ' vj', ' E))', '  (3) where the function φw maps a reachable vector fP (vi', ' vj', ' E) into a weight value', ' and the function φr computes the similar score between vj and the c-th class nodes.']\n",
      "0.9419496587765206\n",
      "en_trf_robertabase_lg\n",
      "['The normalization factor FCc→vj of the c-th category w.r.t. vj is defined as  (2)  (4)  For the relation function φr and the weight function φw', ' we may choose some subnetworks to instantiate them in practice.The detailed implementation of our model can be found in Section 4. FCc→vj =  (cid:80)  1  vi∈Cc  wi→j  4  \\x0cPublished as a conference paper at ICLR 2020  3.3  INFERENCE LEARNING  According to the class-to-node relationship function in Eqn (2)', ' given a query node vj', ' we can obtain a score vector sC→j = [sC1→j', ' · · · ', ' sCC →j](cid:124) ∈ RC after computing the relations to all classesThe indexed category with the maximum score is assumed to be the estimated label.']\n",
      "0.9422303271947365\n",
      "en_trf_robertabase_lg\n",
      "['Thus', ' we can define the loss function based on cross entropy as follows:  L = −  yj', 'c log ˆyCc→j', '  (cid:88)  C (cid:88)  vj  c=1  (5)  (6)  (7)  where yj', 'c is a binary indicator (i.e.', ' 0 or 1) of class label c for node vj', ' and the softmax operation is imposed on sCc→j', ' i.e.', ' ˆyCc→j = exp(sCc→j)/ (cid:80)C k=1 exp(sCk→j).Other error functions may be chosen as the loss function', ' e.g.', ' mean square error.In the regime of general classification', ' the cross entropy loss is a standard one that performs well.']\n",
      "0.9205103608492107\n",
      "en_trf_robertabase_lg\n",
      "[' Given a training set Vtr', ' we expect that the best performance can be obtained on the validation set Vval after optimizing the model on Vtr.Given a trained/pretrained model Θ = {fe', ' φw', ' φr}', ' we perform iteratively gradient updates on the training set Vtr to obtain the new model', ' formally', '  Θ(cid:48) = Θ − α∇ΘLtr(Θ)', '  where α is the updating rate.Note that', ' in the computation of class scores', ' since the reference node and query node can be both from the training set Vtr', ' we set the computation weight wi→j = 0 if i = j in Eqn (3).']\n",
      "0.9477171221324627\n",
      "en_trf_robertabase_lg\n",
      "['After several iterates of gradient descent on Vtr', ' we expect a better performance on the validation set Vval', ' i.e.', ' min Θ  Lval(Θ(cid:48)).Thus', ' we can perform the gradient update as follows  where β is the learning rate of meta optimization (Finn et al', ' 2017). Θ = Θ − β∇ΘLval(Θ(cid:48))', '  During the training process', ' we may perform batch sampling from training nodes and validation nodes', ' instead of taking all one time.']\n",
      "0.9446455319548246\n",
      "en_trf_robertabase_lg\n",
      "['In the testing stage', ' we may take all training nodes and perform the model update according to Eqn (6) like the training process.The updated model is used as the final model and is then fed into Eqn (2) to infer the class labels for those query nodes. 4 MODULES In this section', ' we instantiate all modules (i.e.', ' functions) of the aforementioned structure relation.']\n",
      "0.9614559044744097\n",
      "en_trf_robertabase_lg\n",
      "['The implementation details can be found in the following. Node Representation fe(Gvi): The local representation at vertex vi can be extracted by performing the graph convolution operation on subgraph GviSimilar to gridded images/videos', ' on which local convolution kernels are defined as multiple lattices with various receptive fields', ' the spectral graph convolution is used to encode the local representations of an input graph in our work. Given a graph sample G = {V', ' E', ' X }', ' the normalized graph Laplacian matrix is L = In − D−1/2ED−1/2 = UΛUT ', ' with a diagonal matrix of its eigenvalues Λ.']\n",
      "0.9427393807900761\n",
      "en_trf_robertabase_lg\n",
      "['The spectral graph convo- lution can be defined as the multiplication of signal X with a filter gθ(Λ) = diag(θ) parameterized by θ in the Fourier domain: conv(X ) = gθ(L) ∗ X = Ugθ(Λ)UT X ', ' where parameter θ ∈ Rn is a vector of Fourier coefficients.To reduce the computational complexity and obtain the local information', ' we use an approximate local filter of the Chebyshev polynomial (Defferrard et al', ' 2016)', ' gθ(Λ) = (cid:80)K−1 k=0 θkTk(ˆΛ)', ' where parameter θ ∈ RK is a vector of Chebyshev coefficients and Tk(ˆΛ) ∈ Rn×n is the Chebyshev polynomial of order k evaluated at ˆΛ = 2Λ/λmax − In', ' a diagonal matrix of scaled eigenvalues.The graph filtering operation can then be expressed as gθ(Λ) ∗ X = (cid:80)K−1 k=0 θkTk(ˆL)X ', ' where Tk(ˆL) ∈ Rn×n is the Chebyshev polynomial of order k evaluated at the scaled Laplacian ˆL = 2L/λmax − In. Further', ' we can construct multi-scale receptive fields for each vertex based on the Laplacian matrix L', ' where each receptive field records hopping neighborhood relationships around the reference vertex vi', ' and forms a local centralized subgraph.']\n",
      "0.9401887150915573\n",
      "en_trf_robertabase_lg\n",
      "[' Path Reachability fP (vi', ' vj', ' E): Here we compute the probabilities of paths from vertex i to vertex j by employing random walks on graphs', ' which refers to traversing the graph from vi to vj according to the probability matrix P. For the input graph G with n vertices', ' the random-walk transition matrix  5  \\x0cPublished as a conference paper at ICLR 2020  Datasets Nodes 2', '708 Cora 3', '327 Citeseer 19', '717 Pubmed NELL 65', '755  Edges 5', '429 4', '732 44', '338 266', '144  Classes 7 6 3 210  Features 1', '433 3', '703 500 5', '414  Label Rates 0.052 0.036 0.003 0.001  Table 1: The properties (especially for label rate) of various graph datasets used for the semi-supervised classification task. can be defined as P = D−1E', ' where D ∈ Rn×n is the diagonal degree matrix with Dii = (cid:80) That is to say', ' each element Pij is the probability of going from vertex i to vertex j in one step. i Eij.']\n",
      "0.9327230632228929\n",
      "en_trf_robertabase_lg\n",
      "[' The sequence of nodes from vertex i to vertex j is a random walk on the graph', ' which can be modeled as a classical Markov chain by considering the set of graph vertices.To represent this formulation', ' we show that P t ij is the probability of getting from vertex vi to vertex vj in t steps.This fact is easily exhibited by considering a t-step path from vertex vi to vertex vj as first taking a single step to some vertex h', ' and then taking t − 1 steps to vj.']\n",
      "0.9485607568573099\n",
      "en_trf_robertabase_lg\n",
      "['The transition probability P t in t steps can be formulated as  P t  ij =  PihP t−1 h', 'j  \\uf8f1 \\uf8f2  \\uf8f3  Pij (cid:88)  h  if t = 1 if t > 1 ', '  where each matrix entry P t steps.Finally', ' the node reachability from vi to vj can be written as a dp-dimensional vector:  ij denotes the probability of starting at vertex i and ending at vertex j in t  ij', '.', ' P dp ij ]', ' where dp refers to the step length of the longest path from vi to vj. fP (vi', ' vj', ' E) = [Pij', ' P 2  Class-to-Node Relationship sCc→j: To define the node relationship si→j from vi to vj', ' we simulta- neously consider the property of path reachability fP (vi', ' vj', ' E)', ' local representations fe(Gvi)', ' and fe(Gvj ) of nodes vi', ' vj.']\n",
      "0.9420216701355847\n",
      "en_trf_robertabase_lg\n",
      "['The function φw(fP (vi', ' vj', ' E)) in Eqn (3)', ' which is to map the reachable vector fP (vi', ' vj', ' E) ∈ Rdp into a weight value', ' can be implemented with two 16-dimensional fully connected layers in our experiments.The computed value wi→j can be further used to weight the local features at node vi', ' fe(Gvi) ∈ RdvFor obtaining the similar score between vj and the c-th class nodes Cc in Eqn (2)', ' we perform a concatenation of two input features', ' where one refers to the weighted features of vertex vi', ' and another is the local features of vertex vj.One fully connected layer (w.r.t. φr) with C-dimensions is finally adopted to obtain the relation regression score.']\n",
      "0.9458274367247762\n",
      "en_trf_robertabase_lg\n",
      "[' (8)  (9)  5 EXPERIMENTS  5.1 EXPERIMENTAL SETTINGS  We evaluate our proposed GIL method on three citation network datasets: Cora', ' Citeseer', ' Pubmed (Sen et al', ' 2008)', ' and one knowledge graph NELL dataset (Carlson et al', ' 2010).The statistical properties of graph data are summarized in Table 1.Following the previous protocol in (Kipf & Welling', ' 2017; Zhuang & Ma', ' 2018)', ' we split the graph data into a training set', ' a validation set', ' and a testing set.']\n",
      "0.9265660831555128\n",
      "en_trf_robertabase_lg\n",
      "['Taking into account the graph convolution and pooling modules', ' we may alternately stack them into a multi-layer Graph convolutional network.The GIL model consists of two graph convolution layers', ' each of which is followed by a mean-pooling layer', ' a class-to-node relationship regression module', ' and a final softmax layer.We have given the detailed configuration of the relationship regression module in the class-to-node relationship of Section 4.']\n",
      "0.9563264531232587\n",
      "en_trf_robertabase_lg\n",
      "['The parameter dp in Eqn (9) is set to the mean length of between-node reachability paths in the input graph.The channels of the 1-st and 2-nd convolutional layers are set to 128 and 256', ' respectively.The scale of the respective filed is 2 in both convolutional layers.']\n",
      "0.9580801048248773\n",
      "en_trf_robertabase_lg\n",
      "['The dropout rate is set to 0.5 in the convolution and fully connected layers to avoid over-fitting', ' and the ReLU unit is leveraged as a nonlinear activation function.We pre-train our proposed GIL model for 200 iterations with the training set', ' where its initial learning rate', ' decay factor', ' and momentum are set to 0.05', ' 0.95', ' and 0.9', ' respectively.Here we train the GIL model using the stochastic gradient descent method with the batch size of 100.']\n",
      "0.9578455683604954\n",
      "en_trf_robertabase_lg\n",
      "['We further improve the inference learning capability of the GIL model for 1200 iterations with the validation set', ' where the meta-learning rates α and β are both set to 0.001. 6  \\x0cPublished as a conference paper at ICLR 2020  5.2 COMPARISON WITH STATE-OF-THE-ARTS  We compare the GIL approach with several state-of-the-art methods (Monti et al', ' 2017; Kipf & Welling', ' 2017; Zhou et al', ' 2004; Zhuang & Ma', ' 2018) over four graph datasets', ' including Cora', ' Citeseer', ' Pubmed', ' and NELL.The classification accuracies for all methods are reported in Table 2.']\n",
      "0.9481036084584893\n",
      "en_trf_robertabase_lg\n",
      "['Our proposed GIL can significantly outperform these graph Laplacian regularized methods on four graph datasets', ' including Deep walk (Zhou et al', ' 2004)', ' modularity clustering (Brandes et al', ' 2008)', ' Gaussian fields (Zhu et al', ' 2003)', ' and graph embedding (Yang et al', ' 2016) methods.For example', ' we can achieve much higher performance than the deepwalk method (Zhou et al', ' 2004)', ' e.g.', ' 43.2% vs 74.1% on the Citeseer dataset', ' 65.3% vs 83.1% on the Pubmed dataset', ' and 58.1% vs 78.9% on the NELL dataset.We find that the graph embedding method (Yang et al', ' 2016)', ' which has considered both label information and graph structure during sampling', ' can obtain lower accuracies than our proposed GIL by 9.4% on the Citeseer dataset and 10.5% on the Cora dataset', ' respectively.']\n",
      "0.9476825642874349\n",
      "en_trf_robertabase_lg\n",
      "['This indicates that our proposed GIL can better optimize structure relations and thus improve the network generalization.We further compare our proposed GIL with several existing deep graph embedding methods', ' including graph attention network (Velickovic et al', ' 2018)', ' dual graph convolutional networks (Zhuang & Ma', ' 2018)', ' topology adaptive graph convolutional networks (Du et al', ' 2017)', ' Multi-scale graph convolution (Abu-El-Haija et al', ' 2018)', ' etc.For example', ' our proposed GIL achieves a very large gain', ' e.g.', ' 86.2% vs 83.3% (Du et al', ' 2017) on the Cora dataset', ' and 78.9% vs 66.0% (Kipf & Welling', ' 2017) on the NELL dataset.']\n",
      "0.9492452157087766\n",
      "en_trf_robertabase_lg\n",
      "['We evaluate our proposed GIL method on a large graph dataset with a lower label rate', ' which can significantly outperform existing baselines on the Pubmed dataset: 3.1% over DGCN (Zhuang & Ma', ' 2018)', ' 4.1% over classic GCN (Kipf & Welling', ' 2017) and TAGCN (Du et al', ' 2017)', ' 3.2% over AGNN (Thekumparampil et al', ' 2018)', ' and 3.6% over N-GCN (Abu-El-Haija et al', ' 2018).It demonstrates that our proposed GIL performs very well on various graph datasets by building the graph inference learning process', ' where the limited label information and graph structures can be well employed in the predicted framework. Table 2: Performance comparisons of semi-supervised classification methods.']\n",
      "0.9392411405395262\n",
      "en_trf_robertabase_lg\n",
      "[' Methods Clustering (Brandes et al', ' 2008) DeepWalk (Zhou et al', ' 2004) Gaussian (Zhu et al', ' 2003) G-embedding (Yang et al', ' 2016) DCNN (Atwood & Towsley', ' 2016) GCN (Kipf & Welling', ' 2017) MoNet (Monti et al', ' 2017) N-GCN (Abu-El-Haija et al', ' 2018) GAT (Velickovic et al', ' 2018) AGNN (Thekumparampil et al', ' 2018) TAGCN (Du et al', ' 2017) DGCN (Zhuang & Ma', ' 2018) Our GIL  Cora 59.5 67.2 68.0 75.7 76.8 81.5 81.7 83.0 83.0 83.1 83.3 83.5 86.2  Citeseer 60.1 43.2 45.3 64.7 - 70.3 - 72.2 72.5 71.7 72.5 72.6 74.1  Pubmed NELL 70.7 65.3 63.0 77.2 73.0 79.0 78.8 79.5 79.0 79.9 79.0 80.0 83.1  21.8 58.1 26.5 61.9 - 66.0 - - - - - 74.2 78.9  5.3 ANALYSIS  Meta-optimization: As can be seen in Table 3', ' we report the classification accuracies of semi-supervised classification with several variants of our proposed GIL and the classical GCN method (Kipf & Welling', ' 2017) when evaluating them on the Cora dataset.For analyzing the perfor- mance improvement of our proposed GIL with the graph inference learning process', ' we report the classification accuracies of GCN (Kipf & Welling', ' 2017) and our proposed GIL on the Cora dataset under two different situations', ' including “only learning with the training set Vtr\" and “with jointly learning on a training set Vtr and a validation set Vval\".“GCN /w jointly learning on Vtr & Vval\" achieves a better result than “GCN /w learning on Vtr\" by 3.6%', ' which demonstrates that the network performance can be improved by employing validation samples.']\n",
      "0.9012018863104129\n",
      "en_trf_robertabase_lg\n",
      "['When using structure relations', ' “GIL /w learning on Vtr\" obtains an improvement of 1.9% (over “GCN /w learning on Vtr”)', ' which can be attributed to the building connection between nodes.The meta-optimization strategy (“GIL /w meta-training from Vtr → Vval\" vs “GIL /w learning on Vtr”) has a gain of 2.9%', ' which indicates that a good inference capability can be learnt through meta-optimization.It is worth noting that', ' GIL adopts a meta-optimization strategy to learn the inference model', ' which is a process of migrating  7  \\x0cPublished as a conference paper at ICLR 2020  from a training set to a validation set.']\n",
      "0.9576788974903971\n",
      "en_trf_robertabase_lg\n",
      "['In other words', ' the validation set is only used to teach the model itself how to transfer to unseen data.In contrast', ' the conventional methods often employ a validation set to tune parameters of a certain model of interest. Table 3: Performance comparisons with several GIL variants and the classical GCN method on the Cora dataset.']\n",
      "0.9617517456508072\n",
      "en_trf_robertabase_lg\n",
      "[' GCN (Kipf & Welling', ' 2017)  Methods  GIL  GIL+mean pooling  GIL+2  conv layers  /w learning on Vtr /w jointly learning on Vtr & Vval /w learning on Vtr /w meta-train Vtr → Vval /w 1  conv layer /w 2  conv layers /w 3  conv layers /w max-pooling /w mean pooling  Acc. (%) 81.4 84.0 83.3 86.2 84.5 86.2 85.4 85.2 86.2  Network settings: We explore the effectiveness of our proposed GIL with the same mean pooling mechanism', ' but with different numbers of convolutional layers', ' i.e.', ' “GIL + mean pooling\" with one', ' two', ' and three convolutional layers', ' respectively.As can be seen in Table 3', ' the proposed GIL with two convolutional layers can obtain a better performance on the Cora data than the other two network settings (i.e.', ' GIL with one or three convolutional layers).For example', ' the performance of ‘GIL /w 1  conv layer + mean pooling\" is slightly decreased by 1.7% over “GIL /w 2  conv layers + mean pooling\" on the Cora dataset.']\n",
      "0.9373372836708651\n",
      "en_trf_robertabase_lg\n",
      "['Furthermore', ' we report the classification results of our proposed GIL by using mean and max-pooling mechanisms', ' respectively.GIL with mean pooling (i.e.', ' “GIL /w 2 conv layers + mean pooling\") can get a better result than the GIL method with max-pooling (i.e.', ' “GIL /w 2 conv layers + max-pooling\")', ' e.g.', ' 86.2% vs 85.2% on the Cora graph dataset.The reason may be that the graph network with two convolutional layers and the mean pooling mechanism can obtain the optimal graph embeddings', ' but when increasing the network layers', ' more parameters of a certain graph model need to be optimized', ' which may lead to the over-fitting issue.']\n",
      "0.9559509021747077\n",
      "en_trf_robertabase_lg\n",
      "[' Inﬂuence of different between-node steps: We compare the classification performance within different between-node steps for our proposed GIL and GCN (Kipf & Welling', ' 2017)', ' as illustrated in Fig 2(a).The length of between-node steps can be computed with the shortest path between reference nodes and query nodes.When the step between nodes is smaller', ' both GIL and GCN methods can predict the category information for a small part of unlabeled nodes in the testing set.']\n",
      "0.9473189103034007\n",
      "en_trf_robertabase_lg\n",
      "['The reason may be that the node category information may be disturbed by its nearest neighboring nodes with different labels and fewer nodes are within 1 or 2 steps in the testing set.The GIL and GCN methods can infer the category information for a part of unlabeled nodes by adopting node attributes', ' when two nodes are not connected in the graph (i.e.', ' step=∞).By increasing the length of reachability path', ' the inference process of the GIL method would become difficult and more graph structure information may be employed in the predicted process.']\n",
      "0.9427030852085443\n",
      "en_trf_robertabase_lg\n",
      "['GIL can outperform the classic GCN by analyzing the accuracies within different between-node steps', ' which indicates that our proposed GIL has a better reference capability than GCN by using the meta-optimization mechanism from training nodes to validation nodes. (a)  (b)  Figure 2: (a) Performance comparisons within different between-node steps on the Cora dataset.The accuracy equals to the number of correctly classified nodes divided by all testing samples', ' and is accumulated from step 1 to step k. (b) Performance comparisons with different label rates on the Pubmed dataset.']\n",
      "0.9574953099300373\n",
      "en_trf_robertabase_lg\n",
      "[' 8  1357911step0.00.20.40.60.8accuracyour GILGCNlabel rate0.30%0.60%0.90%1.20%1.50%1.80%GCN0.7920.7970.8050.8240.8290.834GIL(ours)0.8170.8240.8310.8360.8380.8421x2x3x4x5x6x77.0%79.0%81.0%83.0%85.0%1x2x3x4x5x6xGCNGIL(ours)Label rates Accuracy \\x0cPublished as a conference paper at ICLR 2020  Inﬂuence of different label rates: We also explore the performance comparisons of the GIL method with different label rates', ' and the detailed results on the Pubmed dataset can be shown in Fig 2(b).When label rates increase by multiplication', ' the performances of GIL and GCN are improved', ' but the relative gain becomes narrow.The reason is that', ' the reachable path lengths between unlabeled nodes and labeled nodes will be reduced with the increase of labeled nodes', ' which will weaken the effect of inference learning.']\n",
      "0.905258502308824\n",
      "en_trf_robertabase_lg\n",
      "['In the extreme case', ' labels of unlabeled nodes could be determined by those neighbors with the 1 ∼ 2 step reachability.In summary', ' our proposed GIL method prefers small ratio labeled nodes on the semi-supervised node classification task. Inference learning process: Classification errors of different epochs on the validation set of the Cora dataset can be illustrated in Fig 3.']\n",
      "0.9589946161714582\n",
      "en_trf_robertabase_lg\n",
      "['Classification errors are rapidly decreasing as the number of iterations increases from the beginning to 400 iterations', ' while they are with a slow descent from 400 iterations to 1200 iterations.It demonstrates that the learned knowledge from the training samples can be transferred for inferring node category information from these reference labeled nodes.The performance of semi-supervised classification can be further increased by improving the generalized capability of the Graph CNN model.']\n",
      "0.9545808820196113\n",
      "en_trf_robertabase_lg\n",
      "[' Table 4: Performance comparisons with different mod- ules on the Cora dataset', ' where fe', ' fP ', ' and fr denote node representation', ' path reachability', ' and structure re- lation', ' respectively. fP fr fe - - - (cid:88) - - (cid:88) (cid:88) - (cid:88) (cid:88) (cid:88)  Acc.(%) 56.0 81.5 85.0 86.2  Figure 3: Classification errors of different itera- tions on the validation set of the Cora dataset. Module analysis: We evaluate the effectiveness of different modules within our proposed GIL framework', ' including node representation fe', ' path reachability fP ', ' and structure relation fr.']\n",
      "0.9491054234843067\n",
      "en_trf_robertabase_lg\n",
      "['Note that the last one fr defines on the former two ones', ' so we consider the cases in Table 4 by adding modules.When not using all modules', ' only original attributes of nodes are used to predict labels.The case of only using fe belongs to the GCN method', ' which can achieve 81.5% on the Cora dataset.']\n",
      "0.9692078493296902\n",
      "en_trf_robertabase_lg\n",
      "['The large gain of using the relation module fr (i.e.', ' from 81.5% to 85.0%) may be contributed to the ability of inference learning on attributes as well as local topology structures which are implicitly encoded in fe.The path information fP can further boost the performance by 1.2%', ' e.g.', ' 86.2% vs 85.0%.It demonstrates that three different modules of our method can improve the graph inference learning capability.']\n",
      "0.9472356384890684\n",
      "en_trf_robertabase_lg\n",
      "[' Computational complexity: For the computational complexity of our proposed GIL', ' the cost is mainly spent on the computations of node representation', ' between-node reachability', ' and class-to- node relationship', ' which are about O((ntr + nte) ∗ e ∗ din ∗ dout)', ' O((ntr + nte) ∗ e ∗ P )', ' and O(ntr ∗ nted2 out)', ' respectively. ntr and nte refer to the numbers of training and testing nodes', ' din and dout denote the input and output dimensions of node representation', ' e is about the average degree of graph node', ' and P is the step length of node reachability.Compared with those classic Graph CNNs (Kipf & Welling', ' 2017)', ' our proposed GIL has a slightly higher cost due to an extra inference learning process', ' but can complete the testing stage with several seconds on these benchmark datasets. 6 CONCLUSION  In this work', ' we tackled the semi-supervised node classification task with a graph inference learning method', ' which can better predict the categories of these unlabeled nodes in an end-to-end framework.']\n",
      "0.9536654141907654\n",
      "en_trf_robertabase_lg\n",
      "['We can build a structure relation for obtaining the connection between any two graph nodes', ' where node attributes', ' between-node paths', ' and graph structure information can be encapsulated together.For better capturing the transferable knowledge', ' our method further learns to transfer the mined knowledge from the training samples to the validation set', ' finally boosting the prediction accuracy of the labels of unlabeled nodes in the testing set.The extensive experimental results demonstrate the effectiveness of our proposed GIL for solving the semi-supervised learning problem', ' even in the few-shot paradigm.']\n",
      "0.9540076681106551\n",
      "en_trf_robertabase_lg\n",
      "['In the future', ' we would extend the graph inference method to handle more graph-related tasks', ' such as graph generation and social network analysis. 9  the number of iterations error \\x0cPublished as a conference paper at ICLR 2020  ACKNOWLEDGMENT  This work was supported by the National Natural Science Foundation of China (Nos 61972204', ' 61906094', ' U1713208)', ' the Natural Science Foundation of Jiangsu Province (Grant Nos.BK20191283 and BK20190019)', ' and Tencent AI Lab Rhino-Bird Focused Research Program (No.']\n",
      "0.9534325787065553\n",
      "en_trf_robertabase_lg\n",
      "['JR201922). REFERENCES  2001', ' 2016. Sami Abu-El-Haija', ' Amol Kapoor', ' Bryan Perozzi', ' and Joonseok Lee.']\n",
      "0.9462048268571395\n",
      "en_trf_robertabase_lg\n",
      "['N-gcn: Multi-scale graph  convolution for semi-supervised node classification. arXiv preprint arXiv:1802.08888', ' 2018. James Atwood and Don Towsley.Diffusion-convolutional neural networks.']\n",
      "0.9617157319046222\n",
      "en_trf_robertabase_lg\n",
      "['In NeurIPS', ' pp 1993–  Karsten M Borgwardt', ' Hans-Peter Kriegel', ' SVN Vishwanathan', ' and Nicol N Schraudolph.Graph ker- nels for disease outcome prediction from protein-protein interaction networks.Pacific Symposium on Biocomputing Pacific Symposium on Biocomputing', ' pp 4–15', ' 2007.']\n",
      "0.9503968680150656\n",
      "en_trf_robertabase_lg\n",
      "[' Ulrik Brandes', ' Daniel Delling', ' Marco Gaertler', ' Robert Gorke', ' Martin Hoefer', ' Zoran Nikoloski', ' and Dorothea Wagner.On modularity clustering.IEEE transactions on knowledge and data engineering', ' 20(2):172–188', ' 2008.']\n",
      "0.9470617408091859\n",
      "en_trf_robertabase_lg\n",
      "[' Andrew Carlson', ' Justin Betteridge', ' Bryan Kisiel', ' Burr Settles', ' Estevam R. Hruschka Jr.', ' and Tom M.  Mitchell.Toward an architecture for never-ending language learning.In AAAI', ' 2010.']\n",
      "0.9480600255543005\n",
      "en_trf_robertabase_lg\n",
      "[' Michaël Defferrard', ' Xavier Bresson', ' and Pierre Vandergheynst.Convolutional neural networks on  graphs with fast localized spectral filtering.In NeurIPS', ' pp 3844–3852', ' 2016.']\n",
      "0.9589048579617585\n",
      "en_trf_robertabase_lg\n",
      "[' Jian Du', ' Shanghang Zhang', ' Guanhang Wu', ' José MF Moura', ' and Soummya Kar.Topology adaptive  graph convolutional networks. arXiv preprint arXiv:1710.10370', ' 2017. Chelsea Finn', ' Pieter Abbeel', ' and Sergey Levine.']\n",
      "0.954096494626875\n",
      "en_trf_robertabase_lg\n",
      "['Model-agnostic meta-learning for fast adaptation of  deep networks.In ICML', ' pp 1126–1135', ' 2017. Will Hamilton', ' Zhitao Ying', ' and Jure Leskovec.']\n",
      "0.9620299779231642\n",
      "en_trf_robertabase_lg\n",
      "['Inductive representation learning on large graphs.In  NeurIPS', ' pp 1025–1035', ' 2017. Kaiming He', ' Xiangyu Zhang', ' Shaoqing Ren', ' and Jian Sun.']\n",
      "0.9556959048739773\n",
      "en_trf_robertabase_lg\n",
      "['Deep residual learning for image  recognition.In CVPR', ' pp 770–778', ' 2016. Mikael Henaff', ' Joan Bruna', ' and Yann LeCun.']\n",
      "0.9583900001815807\n",
      "en_trf_robertabase_lg\n",
      "['Deep convolutional networks on graph-structured data. arXiv preprint arXiv:1506.05163', ' 2015. Jiatao Jiang', ' Zhen Cui', ' Chunyan Xu', ' and Jian Yang.']\n",
      "0.9543494213616291\n",
      "en_trf_robertabase_lg\n",
      "['Gaussian-induced convolution for graphs.In  AAAI', ' volume 33', ' pp 4007–4014', ' 2019. Hisashi Kashima', ' Koji Tsuda', ' and Akihiro Inokuchi.']\n",
      "0.9586642047595862\n",
      "en_trf_robertabase_lg\n",
      "['Marginalized kernels between labeled graphs. In ICML', ' pp 321–328', ' 2003. Thomas N. Kipf and Max Welling.']\n",
      "0.9597088105245442\n",
      "en_trf_robertabase_lg\n",
      "['Semi-supervised classification with graph convolutional networks. In ICLR', ' 2017. networks.']\n",
      "0.9600898415757922\n",
      "en_trf_robertabase_lg\n",
      "['ICLR', ' 2016. learning.In ICML', ' 2010.']\n",
      "0.9428530205786076\n",
      "en_trf_robertabase_lg\n",
      "[' Yujia Li', ' Daniel Tarlow', ' Marc Brockschmidt', ' and Richard Zemel.Gated graph sequence neural  Wei Liu', ' Junfeng He', ' and Shih-Fu Chang.Large graph construction for scalable semi-supervised  Wei Liu', ' Jun Wang', ' and Shih-Fu Chang.']\n",
      "0.9455983198579833\n",
      "en_trf_robertabase_lg\n",
      "['Robust and scalable graph-based semisupervised learning. Proceedings of the IEEE', ' 100(9):2624–2638', ' 2012. Zhiling Luo', ' Ling Liu', ' Jianwei Yin', ' Ying Li', ' and Zhaohui Wu. Deep learning of graphs with ngram convolutional neural networks.']\n",
      "0.9569053770915183\n",
      "en_trf_robertabase_lg\n",
      "['IEEE Transactions on Knowledge and Data Engineering', ' 29(10): 2125–2139', ' 2017. 10  \\x0cPublished as a conference paper at ICLR 2020  Federico Monti', ' Davide Boscaini', ' Jonathan Masci', ' Emanuele Rodola', ' Jan Svoboda', ' and Michael M Bronstein.Geometric deep learning on graphs and manifolds using mixture model cnns.']\n",
      "0.9479370451772083\n",
      "en_trf_robertabase_lg\n",
      "['In CVPR', ' pp 5115–5124', ' 2017. Christopher Morris', ' Kristian Kersting', ' and Petra Mutzel.Glocalized weisfeiler-lehman graph kernels:  Global-local feature maps of graphs.']\n",
      "0.9625336887797311\n",
      "en_trf_robertabase_lg\n",
      "['In ICDM', ' pp 327–336.IEEE', ' 2017. Mathias Niepert', ' Mohamed Ahmed', ' and Konstantin Kutzkov.']\n",
      "0.946912206019184\n",
      "en_trf_robertabase_lg\n",
      "['Learning convolutional neural networks  for graphs.In ICML', ' pp 2014–2023', ' 2016. Francesco Orsini', ' Daniele Baracchi', ' and Paolo Frasconi.']\n",
      "0.957070780159879\n",
      "en_trf_robertabase_lg\n",
      "['Shift aggregate extract networks. arXiv  preprint arXiv:1703.05537', ' 2017. Lawrence Page', ' Sergey Brin', ' Rajeev Motwani', ' and Terry Winograd.The pagerank citation ranking:  Bringing order to the web.']\n",
      "0.9575337381301414\n",
      "en_trf_robertabase_lg\n",
      "['Technical Report 1999-66', ' 1999. Shirui Pan', ' Ruiqi Hu', ' Guodong Long', ' Jing Jiang', ' Lina Yao', ' and Chengqi Zhang.Adversarially  regularized graph autoencoder for graph embedding.']\n",
      "0.9495967886382252\n",
      "en_trf_robertabase_lg\n",
      "['In IJCAI', ' pp 2609–2615', ' 2018. Sachin Ravi and Hugo Larochelle.Optimization as a model for few-shot learning.']\n",
      "0.9658407911474166\n",
      "en_trf_robertabase_lg\n",
      "['In ICLR', ' 2017. Prithviraj Sen', ' Galileo Namata', ' Mustafa Bilgic', ' Lise Getoor', ' Brian Galligher', ' and Tina Eliassi-Rad. Collective classification in network data.']\n",
      "0.9450420858090756\n",
      "en_trf_robertabase_lg\n",
      "['AI magazine', ' 29(3):93–93', ' 2008. Nino Shervashidze', ' SVN Vishwanathan', ' Tobias Petri', ' Kurt Mehlhorn', ' and Karsten Borgwardt.Efficient graphlet kernels for large graph comparison.']\n",
      "0.952043630428839\n",
      "en_trf_robertabase_lg\n",
      "['In Artificial Intelligence and Statistics', ' pp 488–495', ' 2009. Flood Sung', ' Li Zhang', ' Tao Xiang', ' Timothy Hospedales', ' and Yongxin Yang.Learning to learn:  Meta-critic networks for sample efficient learning. arXiv preprint arXiv:1706.09529', ' 2017.']\n",
      "0.9601408916867652\n",
      "en_trf_robertabase_lg\n",
      "[' Kiran K Thekumparampil', ' Chong Wang', ' Sewoong Oh', ' and Li-Jia Li. Attention-based graph neural  network for semi-supervised learning. arXiv preprint arXiv:1803.03735', ' 2018. Petar Velickovic', ' Guillem Cucurull', ' Arantxa Casanova', ' Adriana Romero', ' Pietro Liò', ' and Yoshua  Bengio.Graph attention networks.']\n",
      "0.9517721291678496\n",
      "en_trf_robertabase_lg\n",
      "['ICLR', ' 2018. Danfei Xu', ' Yuke Zhu', ' Christopher B Choy', ' and Li Fei-Fei.Scene graph generation by iterative  message passing.']\n",
      "0.9540344771925888\n",
      "en_trf_robertabase_lg\n",
      "['In CVPR', ' pp 5410–5419', ' 2017. Pinar Yanardag and SVN Vishwanathan.Deep graph kernels.']\n",
      "0.9539940494815182\n",
      "en_trf_robertabase_lg\n",
      "['In SIGKDD', ' pp 1365–1374', ' 2015. Zhilin Yang', ' William W Cohen', ' and Ruslan Salakhutdinov.Revisiting semi-supervised learning with  graph embeddings.']\n",
      "0.9608219303369394\n",
      "en_trf_robertabase_lg\n",
      "['ICML', ' 2016. Bing Yu', ' Haoteng Yin', ' and Zhanxing Zhu.Spatio-temporal graph convolutional networks: A deep  learning framework for traffic forecasting.']\n",
      "0.9578240968847788\n",
      "en_trf_robertabase_lg\n",
      "['In IJCAI', ' pp 3634–3640', ' 2018. Tong Zhang', ' Zhen Cui', ' Chunyan Xu', ' Wenming Zheng', ' and Jian Yang.Variational pathway reasoning  for eeg emotion recognition.']\n",
      "0.9516668300093313\n",
      "en_trf_robertabase_lg\n",
      "['In AAAI', ' 2020. Wenting Zhao', ' Zhen Cui', ' Chunyan Xu', ' Chengzheng Li', ' Tong Zhang', ' and Jian Yang.Hashing graph  convolution for node classification.']\n",
      "0.9472463162521287\n",
      "en_trf_robertabase_lg\n",
      "['In CIKM', ' 2019. Dengyong Zhou', ' Olivier Bousquet', ' Thomas N Lal', ' Jason Weston', ' and Bernhard Schölkopf.Learning  with local and global consistency.']\n",
      "0.9532384315587121\n",
      "en_trf_robertabase_lg\n",
      "['In NeurIPS', ' pp 321–328', ' 2004. Xiaojin Zhu', ' Zoubin Ghahramani', ' and John D Lafferty.Semi-supervised learning using gaussian  fields and harmonic functions.']\n",
      "0.9586840974665577\n",
      "en_trf_robertabase_lg\n",
      "['In ICML', ' pp 912–919', ' 2003. Chenyi Zhuang and Qiang Ma. Dual graph convolutional networks for graph-based semi-supervised  classification.In WWW', ' pp 499–508', ' 2018.']\n",
      "0.9614585965982105\n",
      "en_trf_robertabase_lg\n",
      "[' 11  \\x0c']\n",
      "0.9059242250337588\n",
      "en_trf_robertabase_lg\n",
      "10\n",
      "Which is the best model\n",
      "en_trf_distilbertbaseuncased_lg\n",
      "['Published as a conference paper at ICLR 2020  GRAPH INFERENCE LEARNING FOR SEMI-SUPERVISED CLASSIFICATION  Chunyan Xu', ' Zhen Cui∗', ' Xiaobin Hong', ' Tong Zhang', ' and Jian Yang School of Computer Science and Engineering', ' Nanjing University of Science and Technology', ' Nanjing', ' China {cyx', 'zhen.cui', 'xbhong', 'tong.zhang', 'csjyang}@njust.edu.cn  Wei Liu Tencent AI Lab', ' China wl2223@columbia.edu  ABSTRACT  In this work', ' we address semi-supervised classification of graph data', ' where the categories of those unlabeled nodes are inferred from labeled nodes as well as graph structures.Recent works often solve this problem via advanced graph convolution in a conventionally supervised manner', ' but the performance could degrade significantly when labeled data is scarce.To this end', ' we propose a Graph Inference Learning (GIL) framework to boost the performance of semi- supervised node classification by learning the inference of node labels on graph topology.']\n",
      "0.6598405712126595\n",
      "en_trf_distilbertbaseuncased_lg\n",
      "['To bridge the connection between two nodes', ' we formally define a structure relation by encapsulating node attributes', ' between-node paths', ' and local topological structures together', ' which can make the inference conveniently deduced from one node to another node.For learning the inference process', ' we further introduce meta-optimization on structure relations from training nodes to validation nodes', ' such that the learnt graph inference capability can be better self-adapted to testing nodes.Comprehensive evaluations on four benchmark datasets (including Cora', ' Citeseer', ' Pubmed', ' and NELL) demonstrate the superiority of our proposed GIL when compared against state-of-the-art methods on the semi-supervised node classification task.']\n",
      "0.7116532189400636\n",
      "en_trf_distilbertbaseuncased_lg\n",
      "[' 1  INTRODUCTION  Graph', ' which comprises a set of vertices/nodes together with connected edges', ' is a formal structural representation of non-regular data.Due to the strong representation ability', ' it accommodates many potential applications', ' e.g.', ' social network (Orsini et al', ' 2017)', ' world wide data (Page et al', ' 1999)', ' knowledge graph (Xu et al', ' 2017)', ' and protein-interaction network (Borgwardt et al', ' 2007).Among these', ' semi-supervised node classification on graphs is one of the most interesting also popular topics.']\n",
      "0.720259329177488\n",
      "en_trf_distilbertbaseuncased_lg\n",
      "['Given a graph in which some nodes are labeled', ' the aim of semi-supervised classification is to infer the categories of those remaining unlabeled nodes by using various priors of the graph. While there have been numerous previous works (Brandes et al', ' 2008; Zhou et al', ' 2004; Zhu et al', ' 2003; Yang et al', ' 2016; Zhao et al', ' 2019) devoted to semi-supervised node classification based on explicit graph Laplacian regularizations', ' it is hard to efficiently boost the performance of label prediction due to the strict assumption that connected nodes are likely to share the same label information.With the progress of deep learning on grid-shaped images/videos (He et al', ' 2016)', ' a few of graph convolutional neural networks (CNN) based methods', ' including spectral (Kipf & Welling', ' 2017) and spatial methods (Niepert et al', ' 2016; Pan et al', ' 2018; Yu et al', ' 2018)', ' have been proposed to learn local convolution filters on graphs in order to extract more discriminative node representations.']\n",
      "0.6793290677370123\n",
      "en_trf_distilbertbaseuncased_lg\n",
      "['Although graph CNN based methods have achieved considerable capabilities of graph embedding by optimizing filters', ' they are limited into a conventionally semi-supervised framework and lack of an efficient inference mechanism on graphs.Especially', ' in the case of few-shot learning', ' where a small number of training nodes are labeled', ' this kind of methods would drastically compromise the performance.For example', ' the Pubmed graph dataset (Sen et al', ' 2008) consists  ∗Corresponding author: Zhen Cui.']\n",
      "0.718548697377055\n",
      "en_trf_distilbertbaseuncased_lg\n",
      "[' 1  \\x0cPublished as a conference paper at ICLR 2020  Figure 1: The illustration of our proposed GIL framework.For the problem of graph node labeling', ' the category information of these unlabeled nodes depends on the similarity computation between a query node (e.g.', ' vj) and these labeled reference nodes (e.g.', ' vi).We consider the similarity from three points: node attributes', ' the consistency of local topological structures (i.e.', ' the circle with dashed line)', ' and the between-node path reachability (i.e.', ' the red wave line from vi to vj).']\n",
      "0.7213162476966196\n",
      "en_trf_distilbertbaseuncased_lg\n",
      "['Specifically', ' the local structures as well as node attributes are encoded as high-level features with graph convolution', ' while the between-node path reachability is abstracted as reachable probabilities of random walks.To better make the inference generalize to test nodes', ' we introduce a meta-learning strategy to optimize the structure relations learning from training nodes to validation nodes. of 19', '717 nodes and 44', '338 edges', ' but only 0.3% nodes are labeled for the semi-supervised node classification task.']\n",
      "0.7262336651752506\n",
      "en_trf_distilbertbaseuncased_lg\n",
      "['These aforementioned works usually boil down to a general classification task', ' where the model is learnt on a training set and selected by checking a validation set.However', ' they do not put great efforts on how to learn to infer from one node to another node on a topological graph', ' especially in the few-shot regime. In this paper', ' we propose a graph inference learning (GIL) framework to teach the model itself to adaptively infer from reference labeled nodes to those query unlabeled nodes', ' and finally boost the performance of semi-supervised node classification in the case of a few number of labeled samples.']\n",
      "0.7368934587521948\n",
      "en_trf_distilbertbaseuncased_lg\n",
      "['Given an input graph', ' GIL attempts to infer the unlabeled nodes from those observed nodes by building between-node relations.The between-node relations are structured as the integration of node attributes', ' connection paths', ' and graph topological structures.It means that the similarity between two nodes is decided from three aspects: the consistency of node attributes', ' the consistency of local topological structures', ' and the between-node path reachability', ' as shown in Fig 1.']\n",
      "0.7142434274696483\n",
      "en_trf_distilbertbaseuncased_lg\n",
      "['The local structures anchored around each node as well as the attributes of nodes therein are jointly encoded with graph convolution (Defferrard et al', ' 2016) for the sake of high-level feature extraction.For the between-node path reachability', ' we adopt the random walk algorithm to obtain the characteristics from a labeled reference node vi to a query unlabeled node vj in a given graph.Based on the computed node representation and between-node reachability', ' the structure relations can be obtained by computing the similar scores/relationships from reference nodes to unlabeled nodes in a graph.']\n",
      "0.7036719304775196\n",
      "en_trf_distilbertbaseuncased_lg\n",
      "['Inspired by the recent meta-learning strategy (Finn et al', ' 2017)', ' we learn to infer the structure relations from a training set to a validation set', ' which can benefit the generalization capability of the learned model.In other words', ' our proposed GIL attempts to learn some transferable knowledge underlying in the structure relations from training samples to validation samples', ' such that the learned structure relations can be better self-adapted to the new testing stage. We summarize the main contributions of this work as three folds:  • We propose a novel graph inference learning framework by building structure relations to infer unknown node labels from those labeled nodes in an end-to-end way.']\n",
      "0.723976799422266\n",
      "en_trf_distilbertbaseuncased_lg\n",
      "['The structure relations are well defined by jointly considering node attributes', ' between-node paths', ' and graph topological structures. • To make the inference model better generalize to test nodes', ' we introduce a meta-learning procedure to optimize structure relations', ' which could be the first time for graph node classification to the best of our knowledge. • Comprehensive evaluations on three citation network datasets (including Cora', ' Citeseer', ' and Pubmed) and one knowledge graph data (i.e.', ' NELL) demonstrate the superiority of our proposed GIL in contrast with other state-of-the-art methods on the semi-supervised classification task.']\n",
      "0.7318422951080253\n",
      "en_trf_distilbertbaseuncased_lg\n",
      "[' 2     (b) The process of Graph inference learning. We extract the local representation from the local subgraph (the circle with dashed line     The red wave line denote the node reachability from     to     dt th hbilit f  d t th d   \\x0cPublished as a conference paper at ICLR 2020  2 RELATED WORK  Graph CNNs: With the rapid development of deep learning methods', ' various graph convolution neural networks (Kashima et al', ' 2003; Morris et al', ' 2017; Shervashidze et al', ' 2009; Yanardag & Vishwanathan', ' 2015; Jiang et al', ' 2019; Zhang et al', ' 2020) have been exploited to analyze the irregular graph-structured data.For better extending general convolutional neural networks to graph domains', ' two broad strategies have been proposed', ' including spectral and spatial convolution methods.']\n",
      "0.680187583594975\n",
      "en_trf_distilbertbaseuncased_lg\n",
      "['Specifically', ' spectral filtering methods (Henaff et al', ' 2015; Kipf & Welling', ' 2017) develop convolution-like operators in the spectral domain', ' and then perform a series of spectral filters by decomposing the graph Laplacian.Unfortunately', ' the spectral-based approaches often lead to a high computational complex due to the operation of eigenvalue decomposition', ' especially for a large number of graph nodes.To alleviate this computation burden', ' local spectral filtering methods (Defferrard et al', ' 2016) are then proposed by parameterizing the frequency responses as a Chebyshev polynomial approximation.']\n",
      "0.6792361013354785\n",
      "en_trf_distilbertbaseuncased_lg\n",
      "['Another type of graph CNNs', ' namely spatial methods (Li et al', ' 2016; Niepert et al', ' 2016)', ' can perform the filtering operation by defining the spatial structures of adjacent vertices.Various approaches can be employed to aggregate or sort neighboring vertices', ' such as diffusion CNNs (Atwood & Towsley', ' 2016)', ' GraphSAGE (Hamilton et al', ' 2017)', ' PSCN (Niepert et al', ' 2016)', ' and NgramCNN (Luo et al', ' 2017).From the perspective of data distribution', ' recently', ' the Gaussian induced convolution model (Jiang et al', ' 2019) is proposed to disentangle the aggregation process through encoding adjacent regions with Gaussian mixture model.']\n",
      "0.6665461241156696\n",
      "en_trf_distilbertbaseuncased_lg\n",
      "[' Semi-supervised node classification: Among various graph-related applications', ' semi-supervised node classification has gained increasing attention recently', ' and various approaches have been proposed to deal with this problem', ' including explicit graph Laplacian regularization and graph- embedding approaches.Several classic algorithms with graph Laplacian regularization contain the label propagation method using Gaussian random fields (Zhu et al', ' 2003)', ' the regularization framework by relying on the local/global consistency (Zhou et al', ' 2004)', ' and the random walk- based sampling algorithm for acquiring the context information (Yang et al', ' 2016).To further address scalable semi-supervised learning issues (Liu et al', ' 2012)', ' the Anchor Graph regularization approach (Liu et al', ' 2010) is proposed to scale linearly with the number of graph nodes and then applied to massive-scale graph datasets.']\n",
      "0.6635530656683251\n",
      "en_trf_distilbertbaseuncased_lg\n",
      "['Several graph convolution network methods (Abu-El-Haija et al', ' 2018; Du et al', ' 2017; Thekumparampil et al', ' 2018; Velickovic et al', ' 2018; Zhuang & Ma', ' 2018) are then developed to obtain discriminative representations of input graphs.For example', ' Kipf et al (Kipf & Welling', ' 2017) proposed a scalable graph CNN model', ' which can scale linearly in the number of graph edges and learn graph representations by encoding both local graph structures and node attributes.Graph attention networks (GAT) (Velickovic et al', ' 2018) are proposed to compute hidden representations of each node for attending to its neighbors with a self-attention strategy.']\n",
      "0.6798229514522106\n",
      "en_trf_distilbertbaseuncased_lg\n",
      "['By jointly considering the local- and global-consistency information', ' dual graph convolutional networks (Zhuang & Ma', ' 2018) are presented to deal with semi-supervised node classification.The critical difference between our proposed GIL and those previous semi-supervised node classification methods is to adopt a graph inference strategy by defining structure relations on graphs and then leverage a meta optimization mechanism to learn an inference model', ' which could be the first time to the best of our knowledge', ' while the existing graph CNNs take semi-supervised node classification as a general classification task. 3 THE PROPOSED MODEL 3.1 PROBLEM DEFINITION  Formally', ' we denote an undirected/directed graph as G = {V', ' E', ' X ', ' Y}', ' where V = {vi}n i=1 is the finite set of n (or |V|) vertices', ' E ∈ Rn×n defines the adjacency relationships (i.e.', ' edges) between vertices representing the topology of G', ' X ∈ Rn×d records the explicit/implicit attributes/signals of vertices', ' and Y ∈ Rn is the vertex labels of C classes.']\n",
      "0.6967960103952426\n",
      "en_trf_distilbertbaseuncased_lg\n",
      "['The edge Eij = E(vi', ' vj) = 0 if and only if vertices vi', ' vj are not connected', ' otherwise Eij (cid:54)= 0.The attribute matrix X is attached to the vertex set V', ' whose i-th row Xvi (or Xi·) represents the attribute of the i-th vertex vi.It means that vi ∈ V carries a vector of d-dimensional signals.']\n",
      "0.6757696809727957\n",
      "en_trf_distilbertbaseuncased_lg\n",
      "['Associated with each node vi ∈ V', ' there is a discrete label yi ∈ {1', ' 2', ' · · · ', ' C}. We consider the task of semi-supervised node classification over graph data', ' where only a small number of vertices are labeled for the model learning', ' i.e.', ' |VLabel| (cid:28) |V|.Generally', ' we have three node sets: a training set Vtr', ' a validation set Vval', ' and a testing set Vte.']\n",
      "0.7251525807441309\n",
      "en_trf_distilbertbaseuncased_lg\n",
      "['In the standard protocol  3  \\x0cPublished as a conference paper at ICLR 2020  of prior literatures (Yang et al', ' 2016)', ' the three node sets share the same label space.We follow but do not restrict this protocol for our proposed method.Given the training and validation node sets', ' the aim is to predict the node labels of testing nodes by using node attributes as well as edge connections.']\n",
      "0.7518548587494518\n",
      "en_trf_distilbertbaseuncased_lg\n",
      "['A sophisticated machine learning technique used in most existing methods (Kipf & Welling', ' 2017; Zhou et al', ' 2004) is to choose the optimal classifier (trained on a training set) after checking the performance on the validation set.However', ' these methods essentially ignore how to extract transferable knowledge from these known labeled nodes to unlabeled nodes', ' as the graph structure itself implies node connectivity/reachability.Moreover', ' due to the scarcity of labeled samples', ' the performance of such a classifier is usually not satisfying.']\n",
      "0.7412468679626706\n",
      "en_trf_distilbertbaseuncased_lg\n",
      "['To address these issues', ' we introduce a meta-learning mechanism (Finn et al', ' 2017; Ravi & Larochelle', ' 2017; Sung et al', ' 2017) to learn to infer node labels on graphs.Specifically', ' the graph structure', ' between-node path reachability', ' and node attributes are jointly modeled into the learning process.Our aim is to learn to infer from labeled nodes to unlabeled nodes', ' so that the learner can perform better on a validation set and thus classify a testing set more accurately.']\n",
      "0.7298495677930164\n",
      "en_trf_distilbertbaseuncased_lg\n",
      "[' 3.2 STRUCTURE RELATION For convenient inference', ' we specifically build a structure relation between two nodes on the topology graph.The labeled vertices (in a training set) are viewed as the reference nodes', ' and their information can be propagated into those unlabeled vertices for improving the label prediction accuracy.Formally', ' given a reference node vi ∈ VLabel', ' we define the score of a query node vj similar to vi as  (1) where Gvi and Gvj may be understood as the centralized subgraphs around vi and vj', ' respectively. fe', ' fr', ' fP are three abstract functions that we explain as follows:  si→j = fr(fe(Gvi )', ' fe(Gvj )', ' fP (vi', ' vj', ' E))', '  • Node representation fe(Gvi) −→ Rdv ', ' encodes the local representation of the centralized subgraph Gvi around node vi', ' and may thus be understood as a local filter function on graphs.']\n",
      "0.6873115197760201\n",
      "en_trf_distilbertbaseuncased_lg\n",
      "['This function should not only take the signals of nodes therein as input', ' but also consider the local topological structure of the subgraph for more accurate similarity computation.To this end', ' we perform the spectral graph convolution on subgraphs to learn discriminative node features', ' analogous to the pixel-level feature extraction from convolution maps of gridded images.The details of feature extraction fe are described in Section 4.']\n",
      "0.6965390840636356\n",
      "en_trf_distilbertbaseuncased_lg\n",
      "[' • Path reachability fP (vi', ' vj', ' E) −→ Rdp ', ' represents the characteristics of path reachability from vi to vj.As there usually exist multiple traversal paths between two nodes', ' we choose the function as reachable probabilities of different lengths of walks from vi to vj.More details will be introduced in Section 4.']\n",
      "0.7456832886220308\n",
      "en_trf_distilbertbaseuncased_lg\n",
      "[' • Structure relation fr(Rdv ', ' Rdv ', ' Rdp ) −→ R', ' is a relational function computing the score of vj similar to vi.This function is not exchangeable for different orders of two nodes', ' due to the asymmetric reachable relationship fPIf necessary', ' we may easily revise it as a symmetry function', ' e.g.', ' summarizing two traversal directions.The score function depends on triple inputs: the local representations extracted from the subgraphs w.r.t. fe(Gvi) and fe(Gvj )', ' respectively', ' and the path reachability from vi to vj.']\n",
      "0.7220458414961337\n",
      "en_trf_distilbertbaseuncased_lg\n",
      "[' In semi-supervised node classification', ' we take the training node set Vtr as the reference samples', ' and the validation set Vval as the query samples during the training stage.Given a query node vj ∈ Vval', ' we can derive the class similarity score of vj w.r.t. the c-th (c = 1', ' · · · ', ' C) category by weighting the reference samples Cc = {vk|yvk = c}.Formally', ' we can further revise Eqn (1) and define the class-to-node relationship function as  sCc→j = φr(FCc→vj  wi→j · fe(Gvi)', ' fe(Gvj ))', '  (cid:88)  vi∈Cc  s.t. wi→j = φw(fP (vi', ' vj', ' E))', '  (3) where the function φw maps a reachable vector fP (vi', ' vj', ' E) into a weight value', ' and the function φr computes the similar score between vj and the c-th class nodes.']\n",
      "0.6682172849535871\n",
      "en_trf_distilbertbaseuncased_lg\n",
      "['The normalization factor FCc→vj of the c-th category w.r.t. vj is defined as  (2)  (4)  For the relation function φr and the weight function φw', ' we may choose some subnetworks to instantiate them in practice.The detailed implementation of our model can be found in Section 4. FCc→vj =  (cid:80)  1  vi∈Cc  wi→j  4  \\x0cPublished as a conference paper at ICLR 2020  3.3  INFERENCE LEARNING  According to the class-to-node relationship function in Eqn (2)', ' given a query node vj', ' we can obtain a score vector sC→j = [sC1→j', ' · · · ', ' sCC →j](cid:124) ∈ RC after computing the relations to all classesThe indexed category with the maximum score is assumed to be the estimated label.']\n",
      "0.6969351480861932\n",
      "en_trf_distilbertbaseuncased_lg\n",
      "['Thus', ' we can define the loss function based on cross entropy as follows:  L = −  yj', 'c log ˆyCc→j', '  (cid:88)  C (cid:88)  vj  c=1  (5)  (6)  (7)  where yj', 'c is a binary indicator (i.e.', ' 0 or 1) of class label c for node vj', ' and the softmax operation is imposed on sCc→j', ' i.e.', ' ˆyCc→j = exp(sCc→j)/ (cid:80)C k=1 exp(sCk→j).Other error functions may be chosen as the loss function', ' e.g.', ' mean square error.In the regime of general classification', ' the cross entropy loss is a standard one that performs well.']\n",
      "0.7016913230858028\n",
      "en_trf_distilbertbaseuncased_lg\n",
      "[' Given a training set Vtr', ' we expect that the best performance can be obtained on the validation set Vval after optimizing the model on Vtr.Given a trained/pretrained model Θ = {fe', ' φw', ' φr}', ' we perform iteratively gradient updates on the training set Vtr to obtain the new model', ' formally', '  Θ(cid:48) = Θ − α∇ΘLtr(Θ)', '  where α is the updating rate.Note that', ' in the computation of class scores', ' since the reference node and query node can be both from the training set Vtr', ' we set the computation weight wi→j = 0 if i = j in Eqn (3).']\n",
      "0.7345016484259347\n",
      "en_trf_distilbertbaseuncased_lg\n",
      "['After several iterates of gradient descent on Vtr', ' we expect a better performance on the validation set Vval', ' i.e.', ' min Θ  Lval(Θ(cid:48)).Thus', ' we can perform the gradient update as follows  where β is the learning rate of meta optimization (Finn et al', ' 2017). Θ = Θ − β∇ΘLval(Θ(cid:48))', '  During the training process', ' we may perform batch sampling from training nodes and validation nodes', ' instead of taking all one time.']\n",
      "0.723318930129734\n",
      "en_trf_distilbertbaseuncased_lg\n",
      "['In the testing stage', ' we may take all training nodes and perform the model update according to Eqn (6) like the training process.The updated model is used as the final model and is then fed into Eqn (2) to infer the class labels for those query nodes. 4 MODULES In this section', ' we instantiate all modules (i.e.', ' functions) of the aforementioned structure relation.']\n",
      "0.7573500433857987\n",
      "en_trf_distilbertbaseuncased_lg\n",
      "['The implementation details can be found in the following. Node Representation fe(Gvi): The local representation at vertex vi can be extracted by performing the graph convolution operation on subgraph GviSimilar to gridded images/videos', ' on which local convolution kernels are defined as multiple lattices with various receptive fields', ' the spectral graph convolution is used to encode the local representations of an input graph in our work. Given a graph sample G = {V', ' E', ' X }', ' the normalized graph Laplacian matrix is L = In − D−1/2ED−1/2 = UΛUT ', ' with a diagonal matrix of its eigenvalues Λ.']\n",
      "0.671019162644238\n",
      "en_trf_distilbertbaseuncased_lg\n",
      "['The spectral graph convo- lution can be defined as the multiplication of signal X with a filter gθ(Λ) = diag(θ) parameterized by θ in the Fourier domain: conv(X ) = gθ(L) ∗ X = Ugθ(Λ)UT X ', ' where parameter θ ∈ Rn is a vector of Fourier coefficients.To reduce the computational complexity and obtain the local information', ' we use an approximate local filter of the Chebyshev polynomial (Defferrard et al', ' 2016)', ' gθ(Λ) = (cid:80)K−1 k=0 θkTk(ˆΛ)', ' where parameter θ ∈ RK is a vector of Chebyshev coefficients and Tk(ˆΛ) ∈ Rn×n is the Chebyshev polynomial of order k evaluated at ˆΛ = 2Λ/λmax − In', ' a diagonal matrix of scaled eigenvalues.The graph filtering operation can then be expressed as gθ(Λ) ∗ X = (cid:80)K−1 k=0 θkTk(ˆL)X ', ' where Tk(ˆL) ∈ Rn×n is the Chebyshev polynomial of order k evaluated at the scaled Laplacian ˆL = 2L/λmax − In. Further', ' we can construct multi-scale receptive fields for each vertex based on the Laplacian matrix L', ' where each receptive field records hopping neighborhood relationships around the reference vertex vi', ' and forms a local centralized subgraph.']\n",
      "0.6497158875951979\n",
      "en_trf_distilbertbaseuncased_lg\n",
      "[' Path Reachability fP (vi', ' vj', ' E): Here we compute the probabilities of paths from vertex i to vertex j by employing random walks on graphs', ' which refers to traversing the graph from vi to vj according to the probability matrix P. For the input graph G with n vertices', ' the random-walk transition matrix  5  \\x0cPublished as a conference paper at ICLR 2020  Datasets Nodes 2', '708 Cora 3', '327 Citeseer 19', '717 Pubmed NELL 65', '755  Edges 5', '429 4', '732 44', '338 266', '144  Classes 7 6 3 210  Features 1', '433 3', '703 500 5', '414  Label Rates 0.052 0.036 0.003 0.001  Table 1: The properties (especially for label rate) of various graph datasets used for the semi-supervised classification task. can be defined as P = D−1E', ' where D ∈ Rn×n is the diagonal degree matrix with Dii = (cid:80) That is to say', ' each element Pij is the probability of going from vertex i to vertex j in one step. i Eij.']\n",
      "0.6899764553454961\n",
      "en_trf_distilbertbaseuncased_lg\n",
      "[' The sequence of nodes from vertex i to vertex j is a random walk on the graph', ' which can be modeled as a classical Markov chain by considering the set of graph vertices.To represent this formulation', ' we show that P t ij is the probability of getting from vertex vi to vertex vj in t steps.This fact is easily exhibited by considering a t-step path from vertex vi to vertex vj as first taking a single step to some vertex h', ' and then taking t − 1 steps to vj.']\n",
      "0.7125177725983322\n",
      "en_trf_distilbertbaseuncased_lg\n",
      "['The transition probability P t in t steps can be formulated as  P t  ij =  PihP t−1 h', 'j  \\uf8f1 \\uf8f2  \\uf8f3  Pij (cid:88)  h  if t = 1 if t > 1 ', '  where each matrix entry P t steps.Finally', ' the node reachability from vi to vj can be written as a dp-dimensional vector:  ij denotes the probability of starting at vertex i and ending at vertex j in t  ij', '.', ' P dp ij ]', ' where dp refers to the step length of the longest path from vi to vj. fP (vi', ' vj', ' E) = [Pij', ' P 2  Class-to-Node Relationship sCc→j: To define the node relationship si→j from vi to vj', ' we simulta- neously consider the property of path reachability fP (vi', ' vj', ' E)', ' local representations fe(Gvi)', ' and fe(Gvj ) of nodes vi', ' vj.']\n",
      "0.6715475092395076\n",
      "en_trf_distilbertbaseuncased_lg\n",
      "['The function φw(fP (vi', ' vj', ' E)) in Eqn (3)', ' which is to map the reachable vector fP (vi', ' vj', ' E) ∈ Rdp into a weight value', ' can be implemented with two 16-dimensional fully connected layers in our experiments.The computed value wi→j can be further used to weight the local features at node vi', ' fe(Gvi) ∈ RdvFor obtaining the similar score between vj and the c-th class nodes Cc in Eqn (2)', ' we perform a concatenation of two input features', ' where one refers to the weighted features of vertex vi', ' and another is the local features of vertex vj.One fully connected layer (w.r.t. φr) with C-dimensions is finally adopted to obtain the relation regression score.']\n",
      "0.7096337871024712\n",
      "en_trf_distilbertbaseuncased_lg\n",
      "[' (8)  (9)  5 EXPERIMENTS  5.1 EXPERIMENTAL SETTINGS  We evaluate our proposed GIL method on three citation network datasets: Cora', ' Citeseer', ' Pubmed (Sen et al', ' 2008)', ' and one knowledge graph NELL dataset (Carlson et al', ' 2010).The statistical properties of graph data are summarized in Table 1.Following the previous protocol in (Kipf & Welling', ' 2017; Zhuang & Ma', ' 2018)', ' we split the graph data into a training set', ' a validation set', ' and a testing set.']\n",
      "0.6971374956661782\n",
      "en_trf_distilbertbaseuncased_lg\n",
      "['Taking into account the graph convolution and pooling modules', ' we may alternately stack them into a multi-layer Graph convolutional network.The GIL model consists of two graph convolution layers', ' each of which is followed by a mean-pooling layer', ' a class-to-node relationship regression module', ' and a final softmax layer.We have given the detailed configuration of the relationship regression module in the class-to-node relationship of Section 4.']\n",
      "0.7045695429002168\n",
      "en_trf_distilbertbaseuncased_lg\n",
      "['The parameter dp in Eqn (9) is set to the mean length of between-node reachability paths in the input graph.The channels of the 1-st and 2-nd convolutional layers are set to 128 and 256', ' respectively.The scale of the respective filed is 2 in both convolutional layers.']\n",
      "0.7156971994066132\n",
      "en_trf_distilbertbaseuncased_lg\n",
      "['The dropout rate is set to 0.5 in the convolution and fully connected layers to avoid over-fitting', ' and the ReLU unit is leveraged as a nonlinear activation function.We pre-train our proposed GIL model for 200 iterations with the training set', ' where its initial learning rate', ' decay factor', ' and momentum are set to 0.05', ' 0.95', ' and 0.9', ' respectively.Here we train the GIL model using the stochastic gradient descent method with the batch size of 100.']\n",
      "0.749372092411089\n",
      "en_trf_distilbertbaseuncased_lg\n",
      "['We further improve the inference learning capability of the GIL model for 1200 iterations with the validation set', ' where the meta-learning rates α and β are both set to 0.001. 6  \\x0cPublished as a conference paper at ICLR 2020  5.2 COMPARISON WITH STATE-OF-THE-ARTS  We compare the GIL approach with several state-of-the-art methods (Monti et al', ' 2017; Kipf & Welling', ' 2017; Zhou et al', ' 2004; Zhuang & Ma', ' 2018) over four graph datasets', ' including Cora', ' Citeseer', ' Pubmed', ' and NELL.The classification accuracies for all methods are reported in Table 2.']\n",
      "0.7097618668678299\n",
      "en_trf_distilbertbaseuncased_lg\n",
      "['Our proposed GIL can significantly outperform these graph Laplacian regularized methods on four graph datasets', ' including Deep walk (Zhou et al', ' 2004)', ' modularity clustering (Brandes et al', ' 2008)', ' Gaussian fields (Zhu et al', ' 2003)', ' and graph embedding (Yang et al', ' 2016) methods.For example', ' we can achieve much higher performance than the deepwalk method (Zhou et al', ' 2004)', ' e.g.', ' 43.2% vs 74.1% on the Citeseer dataset', ' 65.3% vs 83.1% on the Pubmed dataset', ' and 58.1% vs 78.9% on the NELL dataset.We find that the graph embedding method (Yang et al', ' 2016)', ' which has considered both label information and graph structure during sampling', ' can obtain lower accuracies than our proposed GIL by 9.4% on the Citeseer dataset and 10.5% on the Cora dataset', ' respectively.']\n",
      "0.7075516749538895\n",
      "en_trf_distilbertbaseuncased_lg\n",
      "['This indicates that our proposed GIL can better optimize structure relations and thus improve the network generalization.We further compare our proposed GIL with several existing deep graph embedding methods', ' including graph attention network (Velickovic et al', ' 2018)', ' dual graph convolutional networks (Zhuang & Ma', ' 2018)', ' topology adaptive graph convolutional networks (Du et al', ' 2017)', ' Multi-scale graph convolution (Abu-El-Haija et al', ' 2018)', ' etc.For example', ' our proposed GIL achieves a very large gain', ' e.g.', ' 86.2% vs 83.3% (Du et al', ' 2017) on the Cora dataset', ' and 78.9% vs 66.0% (Kipf & Welling', ' 2017) on the NELL dataset.']\n",
      "0.706613481831039\n",
      "en_trf_distilbertbaseuncased_lg\n",
      "['We evaluate our proposed GIL method on a large graph dataset with a lower label rate', ' which can significantly outperform existing baselines on the Pubmed dataset: 3.1% over DGCN (Zhuang & Ma', ' 2018)', ' 4.1% over classic GCN (Kipf & Welling', ' 2017) and TAGCN (Du et al', ' 2017)', ' 3.2% over AGNN (Thekumparampil et al', ' 2018)', ' and 3.6% over N-GCN (Abu-El-Haija et al', ' 2018).It demonstrates that our proposed GIL performs very well on various graph datasets by building the graph inference learning process', ' where the limited label information and graph structures can be well employed in the predicted framework. Table 2: Performance comparisons of semi-supervised classification methods.']\n",
      "0.6840206723942514\n",
      "en_trf_distilbertbaseuncased_lg\n",
      "[' Methods Clustering (Brandes et al', ' 2008) DeepWalk (Zhou et al', ' 2004) Gaussian (Zhu et al', ' 2003) G-embedding (Yang et al', ' 2016) DCNN (Atwood & Towsley', ' 2016) GCN (Kipf & Welling', ' 2017) MoNet (Monti et al', ' 2017) N-GCN (Abu-El-Haija et al', ' 2018) GAT (Velickovic et al', ' 2018) AGNN (Thekumparampil et al', ' 2018) TAGCN (Du et al', ' 2017) DGCN (Zhuang & Ma', ' 2018) Our GIL  Cora 59.5 67.2 68.0 75.7 76.8 81.5 81.7 83.0 83.0 83.1 83.3 83.5 86.2  Citeseer 60.1 43.2 45.3 64.7 - 70.3 - 72.2 72.5 71.7 72.5 72.6 74.1  Pubmed NELL 70.7 65.3 63.0 77.2 73.0 79.0 78.8 79.5 79.0 79.9 79.0 80.0 83.1  21.8 58.1 26.5 61.9 - 66.0 - - - - - 74.2 78.9  5.3 ANALYSIS  Meta-optimization: As can be seen in Table 3', ' we report the classification accuracies of semi-supervised classification with several variants of our proposed GIL and the classical GCN method (Kipf & Welling', ' 2017) when evaluating them on the Cora dataset.For analyzing the perfor- mance improvement of our proposed GIL with the graph inference learning process', ' we report the classification accuracies of GCN (Kipf & Welling', ' 2017) and our proposed GIL on the Cora dataset under two different situations', ' including “only learning with the training set Vtr\" and “with jointly learning on a training set Vtr and a validation set Vval\".“GCN /w jointly learning on Vtr & Vval\" achieves a better result than “GCN /w learning on Vtr\" by 3.6%', ' which demonstrates that the network performance can be improved by employing validation samples.']\n",
      "0.6301740961993549\n",
      "en_trf_distilbertbaseuncased_lg\n",
      "['When using structure relations', ' “GIL /w learning on Vtr\" obtains an improvement of 1.9% (over “GCN /w learning on Vtr”)', ' which can be attributed to the building connection between nodes.The meta-optimization strategy (“GIL /w meta-training from Vtr → Vval\" vs “GIL /w learning on Vtr”) has a gain of 2.9%', ' which indicates that a good inference capability can be learnt through meta-optimization.It is worth noting that', ' GIL adopts a meta-optimization strategy to learn the inference model', ' which is a process of migrating  7  \\x0cPublished as a conference paper at ICLR 2020  from a training set to a validation set.']\n",
      "0.7490114852212207\n",
      "en_trf_distilbertbaseuncased_lg\n",
      "['In other words', ' the validation set is only used to teach the model itself how to transfer to unseen data.In contrast', ' the conventional methods often employ a validation set to tune parameters of a certain model of interest. Table 3: Performance comparisons with several GIL variants and the classical GCN method on the Cora dataset.']\n",
      "0.7663656172437061\n",
      "en_trf_distilbertbaseuncased_lg\n",
      "[' GCN (Kipf & Welling', ' 2017)  Methods  GIL  GIL+mean pooling  GIL+2  conv layers  /w learning on Vtr /w jointly learning on Vtr & Vval /w learning on Vtr /w meta-train Vtr → Vval /w 1  conv layer /w 2  conv layers /w 3  conv layers /w max-pooling /w mean pooling  Acc. (%) 81.4 84.0 83.3 86.2 84.5 86.2 85.4 85.2 86.2  Network settings: We explore the effectiveness of our proposed GIL with the same mean pooling mechanism', ' but with different numbers of convolutional layers', ' i.e.', ' “GIL + mean pooling\" with one', ' two', ' and three convolutional layers', ' respectively.As can be seen in Table 3', ' the proposed GIL with two convolutional layers can obtain a better performance on the Cora data than the other two network settings (i.e.', ' GIL with one or three convolutional layers).For example', ' the performance of ‘GIL /w 1  conv layer + mean pooling\" is slightly decreased by 1.7% over “GIL /w 2  conv layers + mean pooling\" on the Cora dataset.']\n",
      "0.6963314881759313\n",
      "en_trf_distilbertbaseuncased_lg\n",
      "['Furthermore', ' we report the classification results of our proposed GIL by using mean and max-pooling mechanisms', ' respectively.GIL with mean pooling (i.e.', ' “GIL /w 2 conv layers + mean pooling\") can get a better result than the GIL method with max-pooling (i.e.', ' “GIL /w 2 conv layers + max-pooling\")', ' e.g.', ' 86.2% vs 85.2% on the Cora graph dataset.The reason may be that the graph network with two convolutional layers and the mean pooling mechanism can obtain the optimal graph embeddings', ' but when increasing the network layers', ' more parameters of a certain graph model need to be optimized', ' which may lead to the over-fitting issue.']\n",
      "0.7480377612141431\n",
      "en_trf_distilbertbaseuncased_lg\n",
      "[' Inﬂuence of different between-node steps: We compare the classification performance within different between-node steps for our proposed GIL and GCN (Kipf & Welling', ' 2017)', ' as illustrated in Fig 2(a).The length of between-node steps can be computed with the shortest path between reference nodes and query nodes.When the step between nodes is smaller', ' both GIL and GCN methods can predict the category information for a small part of unlabeled nodes in the testing set.']\n",
      "0.7359417313107176\n",
      "en_trf_distilbertbaseuncased_lg\n",
      "['The reason may be that the node category information may be disturbed by its nearest neighboring nodes with different labels and fewer nodes are within 1 or 2 steps in the testing set.The GIL and GCN methods can infer the category information for a part of unlabeled nodes by adopting node attributes', ' when two nodes are not connected in the graph (i.e.', ' step=∞).By increasing the length of reachability path', ' the inference process of the GIL method would become difficult and more graph structure information may be employed in the predicted process.']\n",
      "0.7240922596681936\n",
      "en_trf_distilbertbaseuncased_lg\n",
      "['GIL can outperform the classic GCN by analyzing the accuracies within different between-node steps', ' which indicates that our proposed GIL has a better reference capability than GCN by using the meta-optimization mechanism from training nodes to validation nodes. (a)  (b)  Figure 2: (a) Performance comparisons within different between-node steps on the Cora dataset.The accuracy equals to the number of correctly classified nodes divided by all testing samples', ' and is accumulated from step 1 to step k. (b) Performance comparisons with different label rates on the Pubmed dataset.']\n",
      "0.7358959836171903\n",
      "en_trf_distilbertbaseuncased_lg\n",
      "[' 8  1357911step0.00.20.40.60.8accuracyour GILGCNlabel rate0.30%0.60%0.90%1.20%1.50%1.80%GCN0.7920.7970.8050.8240.8290.834GIL(ours)0.8170.8240.8310.8360.8380.8421x2x3x4x5x6x77.0%79.0%81.0%83.0%85.0%1x2x3x4x5x6xGCNGIL(ours)Label rates Accuracy \\x0cPublished as a conference paper at ICLR 2020  Inﬂuence of different label rates: We also explore the performance comparisons of the GIL method with different label rates', ' and the detailed results on the Pubmed dataset can be shown in Fig 2(b).When label rates increase by multiplication', ' the performances of GIL and GCN are improved', ' but the relative gain becomes narrow.The reason is that', ' the reachable path lengths between unlabeled nodes and labeled nodes will be reduced with the increase of labeled nodes', ' which will weaken the effect of inference learning.']\n",
      "0.6616369416390632\n",
      "en_trf_distilbertbaseuncased_lg\n",
      "['In the extreme case', ' labels of unlabeled nodes could be determined by those neighbors with the 1 ∼ 2 step reachability.In summary', ' our proposed GIL method prefers small ratio labeled nodes on the semi-supervised node classification task. Inference learning process: Classification errors of different epochs on the validation set of the Cora dataset can be illustrated in Fig 3.']\n",
      "0.7641087296994077\n",
      "en_trf_distilbertbaseuncased_lg\n",
      "['Classification errors are rapidly decreasing as the number of iterations increases from the beginning to 400 iterations', ' while they are with a slow descent from 400 iterations to 1200 iterations.It demonstrates that the learned knowledge from the training samples can be transferred for inferring node category information from these reference labeled nodes.The performance of semi-supervised classification can be further increased by improving the generalized capability of the Graph CNN model.']\n",
      "0.7422375002776623\n",
      "en_trf_distilbertbaseuncased_lg\n",
      "[' Table 4: Performance comparisons with different mod- ules on the Cora dataset', ' where fe', ' fP ', ' and fr denote node representation', ' path reachability', ' and structure re- lation', ' respectively. fP fr fe - - - (cid:88) - - (cid:88) (cid:88) - (cid:88) (cid:88) (cid:88)  Acc.(%) 56.0 81.5 85.0 86.2  Figure 3: Classification errors of different itera- tions on the validation set of the Cora dataset. Module analysis: We evaluate the effectiveness of different modules within our proposed GIL framework', ' including node representation fe', ' path reachability fP ', ' and structure relation fr.']\n",
      "0.7143911803993367\n",
      "en_trf_distilbertbaseuncased_lg\n",
      "['Note that the last one fr defines on the former two ones', ' so we consider the cases in Table 4 by adding modules.When not using all modules', ' only original attributes of nodes are used to predict labels.The case of only using fe belongs to the GCN method', ' which can achieve 81.5% on the Cora dataset.']\n",
      "0.7819935650942139\n",
      "en_trf_distilbertbaseuncased_lg\n",
      "['The large gain of using the relation module fr (i.e.', ' from 81.5% to 85.0%) may be contributed to the ability of inference learning on attributes as well as local topology structures which are implicitly encoded in fe.The path information fP can further boost the performance by 1.2%', ' e.g.', ' 86.2% vs 85.0%.It demonstrates that three different modules of our method can improve the graph inference learning capability.']\n",
      "0.7306510349944503\n",
      "en_trf_distilbertbaseuncased_lg\n",
      "[' Computational complexity: For the computational complexity of our proposed GIL', ' the cost is mainly spent on the computations of node representation', ' between-node reachability', ' and class-to- node relationship', ' which are about O((ntr + nte) ∗ e ∗ din ∗ dout)', ' O((ntr + nte) ∗ e ∗ P )', ' and O(ntr ∗ nted2 out)', ' respectively. ntr and nte refer to the numbers of training and testing nodes', ' din and dout denote the input and output dimensions of node representation', ' e is about the average degree of graph node', ' and P is the step length of node reachability.Compared with those classic Graph CNNs (Kipf & Welling', ' 2017)', ' our proposed GIL has a slightly higher cost due to an extra inference learning process', ' but can complete the testing stage with several seconds on these benchmark datasets. 6 CONCLUSION  In this work', ' we tackled the semi-supervised node classification task with a graph inference learning method', ' which can better predict the categories of these unlabeled nodes in an end-to-end framework.']\n",
      "0.7192145026289943\n",
      "en_trf_distilbertbaseuncased_lg\n",
      "['We can build a structure relation for obtaining the connection between any two graph nodes', ' where node attributes', ' between-node paths', ' and graph structure information can be encapsulated together.For better capturing the transferable knowledge', ' our method further learns to transfer the mined knowledge from the training samples to the validation set', ' finally boosting the prediction accuracy of the labels of unlabeled nodes in the testing set.The extensive experimental results demonstrate the effectiveness of our proposed GIL for solving the semi-supervised learning problem', ' even in the few-shot paradigm.']\n",
      "0.7409984395110806\n",
      "en_trf_distilbertbaseuncased_lg\n",
      "['In the future', ' we would extend the graph inference method to handle more graph-related tasks', ' such as graph generation and social network analysis. 9  the number of iterations error \\x0cPublished as a conference paper at ICLR 2020  ACKNOWLEDGMENT  This work was supported by the National Natural Science Foundation of China (Nos 61972204', ' 61906094', ' U1713208)', ' the Natural Science Foundation of Jiangsu Province (Grant Nos.BK20191283 and BK20190019)', ' and Tencent AI Lab Rhino-Bird Focused Research Program (No.']\n",
      "0.6613404447211799\n",
      "en_trf_distilbertbaseuncased_lg\n",
      "['JR201922). REFERENCES  2001', ' 2016. Sami Abu-El-Haija', ' Amol Kapoor', ' Bryan Perozzi', ' and Joonseok Lee.']\n",
      "0.6262457527166468\n",
      "en_trf_distilbertbaseuncased_lg\n",
      "['N-gcn: Multi-scale graph  convolution for semi-supervised node classification. arXiv preprint arXiv:1802.08888', ' 2018. James Atwood and Don Towsley.Diffusion-convolutional neural networks.']\n",
      "0.7199549624559242\n",
      "en_trf_distilbertbaseuncased_lg\n",
      "['In NeurIPS', ' pp 1993–  Karsten M Borgwardt', ' Hans-Peter Kriegel', ' SVN Vishwanathan', ' and Nicol N Schraudolph.Graph ker- nels for disease outcome prediction from protein-protein interaction networks.Pacific Symposium on Biocomputing Pacific Symposium on Biocomputing', ' pp 4–15', ' 2007.']\n",
      "0.6660491974313932\n",
      "en_trf_distilbertbaseuncased_lg\n",
      "[' Ulrik Brandes', ' Daniel Delling', ' Marco Gaertler', ' Robert Gorke', ' Martin Hoefer', ' Zoran Nikoloski', ' and Dorothea Wagner.On modularity clustering.IEEE transactions on knowledge and data engineering', ' 20(2):172–188', ' 2008.']\n",
      "0.6640361691785374\n",
      "en_trf_distilbertbaseuncased_lg\n",
      "[' Andrew Carlson', ' Justin Betteridge', ' Bryan Kisiel', ' Burr Settles', ' Estevam R. Hruschka Jr.', ' and Tom M.  Mitchell.Toward an architecture for never-ending language learning.In AAAI', ' 2010.']\n",
      "0.6838727871443141\n",
      "en_trf_distilbertbaseuncased_lg\n",
      "[' Michaël Defferrard', ' Xavier Bresson', ' and Pierre Vandergheynst.Convolutional neural networks on  graphs with fast localized spectral filtering.In NeurIPS', ' pp 3844–3852', ' 2016.']\n",
      "0.7068555972614889\n",
      "en_trf_distilbertbaseuncased_lg\n",
      "[' Jian Du', ' Shanghang Zhang', ' Guanhang Wu', ' José MF Moura', ' and Soummya Kar.Topology adaptive  graph convolutional networks. arXiv preprint arXiv:1710.10370', ' 2017. Chelsea Finn', ' Pieter Abbeel', ' and Sergey Levine.']\n",
      "0.6755020294019137\n",
      "en_trf_distilbertbaseuncased_lg\n",
      "['Model-agnostic meta-learning for fast adaptation of  deep networks.In ICML', ' pp 1126–1135', ' 2017. Will Hamilton', ' Zhitao Ying', ' and Jure Leskovec.']\n",
      "0.7382835656651398\n",
      "en_trf_distilbertbaseuncased_lg\n",
      "['Inductive representation learning on large graphs.In  NeurIPS', ' pp 1025–1035', ' 2017. Kaiming He', ' Xiangyu Zhang', ' Shaoqing Ren', ' and Jian Sun.']\n",
      "0.7070355350564833\n",
      "en_trf_distilbertbaseuncased_lg\n",
      "['Deep residual learning for image  recognition.In CVPR', ' pp 770–778', ' 2016. Mikael Henaff', ' Joan Bruna', ' and Yann LeCun.']\n",
      "0.6946267412429752\n",
      "en_trf_distilbertbaseuncased_lg\n",
      "['Deep convolutional networks on graph-structured data. arXiv preprint arXiv:1506.05163', ' 2015. Jiatao Jiang', ' Zhen Cui', ' Chunyan Xu', ' and Jian Yang.']\n",
      "0.6807123875993347\n",
      "en_trf_distilbertbaseuncased_lg\n",
      "['Gaussian-induced convolution for graphs.In  AAAI', ' volume 33', ' pp 4007–4014', ' 2019. Hisashi Kashima', ' Koji Tsuda', ' and Akihiro Inokuchi.']\n",
      "0.6826614863224303\n",
      "en_trf_distilbertbaseuncased_lg\n",
      "['Marginalized kernels between labeled graphs. In ICML', ' pp 321–328', ' 2003. Thomas N. Kipf and Max Welling.']\n",
      "0.7515062052637874\n",
      "en_trf_distilbertbaseuncased_lg\n",
      "['Semi-supervised classification with graph convolutional networks. In ICLR', ' 2017. networks.']\n",
      "0.7431150067662854\n",
      "en_trf_distilbertbaseuncased_lg\n",
      "['ICLR', ' 2016. learning.In ICML', ' 2010.']\n",
      "0.7435763637323585\n",
      "en_trf_distilbertbaseuncased_lg\n",
      "[' Yujia Li', ' Daniel Tarlow', ' Marc Brockschmidt', ' and Richard Zemel.Gated graph sequence neural  Wei Liu', ' Junfeng He', ' and Shih-Fu Chang.Large graph construction for scalable semi-supervised  Wei Liu', ' Jun Wang', ' and Shih-Fu Chang.']\n",
      "0.6510694745528843\n",
      "en_trf_distilbertbaseuncased_lg\n",
      "['Robust and scalable graph-based semisupervised learning. Proceedings of the IEEE', ' 100(9):2624–2638', ' 2012. Zhiling Luo', ' Ling Liu', ' Jianwei Yin', ' Ying Li', ' and Zhaohui Wu. Deep learning of graphs with ngram convolutional neural networks.']\n",
      "0.7082142915380919\n",
      "en_trf_distilbertbaseuncased_lg\n",
      "['IEEE Transactions on Knowledge and Data Engineering', ' 29(10): 2125–2139', ' 2017. 10  \\x0cPublished as a conference paper at ICLR 2020  Federico Monti', ' Davide Boscaini', ' Jonathan Masci', ' Emanuele Rodola', ' Jan Svoboda', ' and Michael M Bronstein.Geometric deep learning on graphs and manifolds using mixture model cnns.']\n",
      "0.6859799642141368\n",
      "en_trf_distilbertbaseuncased_lg\n",
      "['In CVPR', ' pp 5115–5124', ' 2017. Christopher Morris', ' Kristian Kersting', ' and Petra Mutzel.Glocalized weisfeiler-lehman graph kernels:  Global-local feature maps of graphs.']\n",
      "0.6915940554653696\n",
      "en_trf_distilbertbaseuncased_lg\n",
      "['In ICDM', ' pp 327–336.IEEE', ' 2017. Mathias Niepert', ' Mohamed Ahmed', ' and Konstantin Kutzkov.']\n",
      "0.6924954727080702\n",
      "en_trf_distilbertbaseuncased_lg\n",
      "['Learning convolutional neural networks  for graphs.In ICML', ' pp 2014–2023', ' 2016. Francesco Orsini', ' Daniele Baracchi', ' and Paolo Frasconi.']\n",
      "0.6998759801405435\n",
      "en_trf_distilbertbaseuncased_lg\n",
      "['Shift aggregate extract networks. arXiv  preprint arXiv:1703.05537', ' 2017. Lawrence Page', ' Sergey Brin', ' Rajeev Motwani', ' and Terry Winograd.The pagerank citation ranking:  Bringing order to the web.']\n",
      "0.7205778157717858\n",
      "en_trf_distilbertbaseuncased_lg\n",
      "['Technical Report 1999-66', ' 1999. Shirui Pan', ' Ruiqi Hu', ' Guodong Long', ' Jing Jiang', ' Lina Yao', ' and Chengqi Zhang.Adversarially  regularized graph autoencoder for graph embedding.']\n",
      "0.6753808509818262\n",
      "en_trf_distilbertbaseuncased_lg\n",
      "['In IJCAI', ' pp 2609–2615', ' 2018. Sachin Ravi and Hugo Larochelle.Optimization as a model for few-shot learning.']\n",
      "0.7689564858620309\n",
      "en_trf_distilbertbaseuncased_lg\n",
      "['In ICLR', ' 2017. Prithviraj Sen', ' Galileo Namata', ' Mustafa Bilgic', ' Lise Getoor', ' Brian Galligher', ' and Tina Eliassi-Rad. Collective classification in network data.']\n",
      "0.653399294409945\n",
      "en_trf_distilbertbaseuncased_lg\n",
      "['AI magazine', ' 29(3):93–93', ' 2008. Nino Shervashidze', ' SVN Vishwanathan', ' Tobias Petri', ' Kurt Mehlhorn', ' and Karsten Borgwardt.Efficient graphlet kernels for large graph comparison.']\n",
      "0.7028998760253056\n",
      "en_trf_distilbertbaseuncased_lg\n",
      "['In Artificial Intelligence and Statistics', ' pp 488–495', ' 2009. Flood Sung', ' Li Zhang', ' Tao Xiang', ' Timothy Hospedales', ' and Yongxin Yang.Learning to learn:  Meta-critic networks for sample efficient learning. arXiv preprint arXiv:1706.09529', ' 2017.']\n",
      "0.7179982979271866\n",
      "en_trf_distilbertbaseuncased_lg\n",
      "[' Kiran K Thekumparampil', ' Chong Wang', ' Sewoong Oh', ' and Li-Jia Li. Attention-based graph neural  network for semi-supervised learning. arXiv preprint arXiv:1803.03735', ' 2018. Petar Velickovic', ' Guillem Cucurull', ' Arantxa Casanova', ' Adriana Romero', ' Pietro Liò', ' and Yoshua  Bengio.Graph attention networks.']\n",
      "0.6699817552569162\n",
      "en_trf_distilbertbaseuncased_lg\n",
      "['ICLR', ' 2018. Danfei Xu', ' Yuke Zhu', ' Christopher B Choy', ' and Li Fei-Fei.Scene graph generation by iterative  message passing.']\n",
      "0.6509750806564468\n",
      "en_trf_distilbertbaseuncased_lg\n",
      "['In CVPR', ' pp 5410–5419', ' 2017. Pinar Yanardag and SVN Vishwanathan.Deep graph kernels.']\n",
      "0.7175530131898438\n",
      "en_trf_distilbertbaseuncased_lg\n",
      "['In SIGKDD', ' pp 1365–1374', ' 2015. Zhilin Yang', ' William W Cohen', ' and Ruslan Salakhutdinov.Revisiting semi-supervised learning with  graph embeddings.']\n",
      "0.7201967693478526\n",
      "en_trf_distilbertbaseuncased_lg\n",
      "['ICML', ' 2016. Bing Yu', ' Haoteng Yin', ' and Zhanxing Zhu.Spatio-temporal graph convolutional networks: A deep  learning framework for traffic forecasting.']\n",
      "0.6922174563451967\n",
      "en_trf_distilbertbaseuncased_lg\n",
      "['In IJCAI', ' pp 3634–3640', ' 2018. Tong Zhang', ' Zhen Cui', ' Chunyan Xu', ' Wenming Zheng', ' and Jian Yang.Variational pathway reasoning  for eeg emotion recognition.']\n",
      "0.6660774512954963\n",
      "en_trf_distilbertbaseuncased_lg\n",
      "['In AAAI', ' 2020. Wenting Zhao', ' Zhen Cui', ' Chunyan Xu', ' Chengzheng Li', ' Tong Zhang', ' and Jian Yang.Hashing graph  convolution for node classification.']\n",
      "0.6408981433049431\n",
      "en_trf_distilbertbaseuncased_lg\n",
      "['In CIKM', ' 2019. Dengyong Zhou', ' Olivier Bousquet', ' Thomas N Lal', ' Jason Weston', ' and Bernhard Schölkopf.Learning  with local and global consistency.']\n",
      "0.6911636333753343\n",
      "en_trf_distilbertbaseuncased_lg\n",
      "['In NeurIPS', ' pp 321–328', ' 2004. Xiaojin Zhu', ' Zoubin Ghahramani', ' and John D Lafferty.Semi-supervised learning using gaussian  fields and harmonic functions.']\n",
      "0.70538609354292\n",
      "en_trf_distilbertbaseuncased_lg\n",
      "['In ICML', ' pp 912–919', ' 2003. Chenyi Zhuang and Qiang Ma. Dual graph convolutional networks for graph-based semi-supervised  classification.In WWW', ' pp 499–508', ' 2018.']\n",
      "0.7236034312913712\n",
      "en_trf_distilbertbaseuncased_lg\n",
      "[' 11  \\x0c']\n",
      "0.5259510101016309\n",
      "en_trf_distilbertbaseuncased_lg\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "modelHeapDict=compareDifferentLanguageModels('/content/graph_inference_learning_for_semi_supervised_classification_3.csv',[\"Which is the best model\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wrPCt8TiUkPl"
   },
   "source": [
    "Pass the filename with path to save the results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 537
    },
    "colab_type": "code",
    "id": "FnblDGWbEaee",
    "outputId": "054ab3fc-c074-48b8-8cc1-da48961143f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6203900028208551\n",
      "0.6204926895437002\n",
      "0.6256185225063298\n",
      "0.6312060808024065\n",
      "0.6407311347778408\n",
      "0.6436403041153803\n",
      "0.6456181124170579\n",
      "0.6497621031393012\n",
      "0.6562072824702831\n",
      "0.6608844701429274\n",
      "0.9614559044744097\n",
      "0.9614585965982105\n",
      "0.9617157319046222\n",
      "0.9617517456508072\n",
      "0.9620299779231642\n",
      "0.9625336887797311\n",
      "0.9627932951627081\n",
      "0.9642085722761409\n",
      "0.9658407911474166\n",
      "0.9692078493296902\n",
      "0.7480377612141431\n",
      "0.7490114852212207\n",
      "0.749372092411089\n",
      "0.7515062052637874\n",
      "0.7518548587494518\n",
      "0.7573500433857987\n",
      "0.7641087296994077\n",
      "0.7663656172437061\n",
      "0.7689564858620309\n",
      "0.7819935650942139\n"
     ]
    }
   ],
   "source": [
    "\n",
    "modelResultFile = open(\"/content/ModelComparison.tsv\", \"a\")\n",
    "\n",
    "for model in modelHeapDict:\n",
    "  modelResultFile.write(model)\n",
    "  modelResultFile.write(\"\\n\")\n",
    "  modelHeap = modelHeapDict[model]\n",
    "  while modelHeap:\n",
    "    qs = heapq.heappop(modelHeap)\n",
    "    modelResultFile.write(model)\n",
    "    modelResultFile.write(\"\\t\")\n",
    "    modelResultFile.write(qs.query)\n",
    "    modelResultFile.write(\"\\t\")\n",
    "    modelResultFile.write(' '.join([str(elem) for elem in qs.snippet]))\n",
    "    modelResultFile.write(\"\\t\")\n",
    "    modelResultFile.write(str(qs.similarity))\n",
    "    print(qs.similarity)\n",
    "    modelResultFile.write(\"\\n\")\n",
    "\n",
    "for model in useHeapDict:\n",
    "  modelResultFile.write(model)\n",
    "  modelResultFile.write(\"\\n\")\n",
    "  modelHeap = useHeapDict[model]\n",
    "  while modelHeap: \n",
    "    qs = heapq.heappop(modelHeap)\n",
    "    modelResultFile.write(model)\n",
    "    modelResultFile.write(\"\\t\")\n",
    "    modelResultFile.write(qs.query)\n",
    "    modelResultFile.write(\"\\t\")\n",
    "    modelResultFile.write(' '.join([str(elem) for elem in qs.snippet]))\n",
    "    modelResultFile.write(\"\\t\")\n",
    "    modelResultFile.write(str(qs.similarity))\n",
    "    modelResultFile.write(\"\\n\")\n",
    "    \n",
    "\n",
    "modelResultFile.close()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Different_Models_Comparison.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
