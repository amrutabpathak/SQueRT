{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import time\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "from transformers import (\n",
    "    AlbertConfig,\n",
    "    AlbertForQuestionAnswering,\n",
    "    AlbertTokenizer,\n",
    "    squad_convert_examples_to_features\n",
    ")\n",
    "\n",
    "from transformers.data.processors.squad import SquadResult, SquadV2Processor, SquadExample\n",
    "\n",
    "from transformers.data.metrics.squad_metrics import compute_predictions_logits\n",
    "\n",
    "# READER NOTE: Set this flag to use own model, or use pretrained model in the Hugging Face repository\n",
    "use_own_model = False\n",
    "\n",
    "if use_own_model:\n",
    "  model_name_or_path = \"/content/model_output\"\n",
    "else:\n",
    "  model_name_or_path = \"ktrapeznikov/albert-xlarge-v2-squad-v2\"\n",
    "\n",
    "output_dir = \"\"\n",
    "\n",
    "# Config\n",
    "n_best_size = 1\n",
    "max_answer_length = 30\n",
    "do_lower_case = True\n",
    "null_score_diff_threshold = 0.0\n",
    "\n",
    "def to_list(tensor):\n",
    "    return tensor.detach().cpu().tolist()\n",
    "\n",
    "# Setup model\n",
    "config_class, model_class, tokenizer_class = (\n",
    "    AlbertConfig, AlbertForQuestionAnswering, AlbertTokenizer)\n",
    "config = config_class.from_pretrained(model_name_or_path)\n",
    "tokenizer = tokenizer_class.from_pretrained(\n",
    "    model_name_or_path, do_lower_case=True)\n",
    "model = model_class.from_pretrained(model_name_or_path, config=config)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "processor = SquadV2Processor()\n",
    "\n",
    "def run_prediction(question_texts, context_text):\n",
    "    \"\"\"Setup function to compute predictions\"\"\"\n",
    "    examples = []\n",
    "\n",
    "    for i, question_text in enumerate(question_texts):\n",
    "        example = SquadExample(\n",
    "            qas_id=str(i),\n",
    "            question_text=question_text,\n",
    "            context_text=context_text,\n",
    "            answer_text=None,\n",
    "            start_position_character=None,\n",
    "            title=\"Predict\",\n",
    "            is_impossible=False,\n",
    "            answers=None,\n",
    "        )\n",
    "\n",
    "        examples.append(example)\n",
    "\n",
    "    features, dataset = squad_convert_examples_to_features(\n",
    "        examples=examples,\n",
    "        tokenizer=tokenizer,\n",
    "        max_seq_length=384,\n",
    "        doc_stride=128,\n",
    "        max_query_length=64,\n",
    "        is_training=False,\n",
    "        return_dataset=\"pt\",\n",
    "        threads=1,\n",
    "    )\n",
    "\n",
    "    eval_sampler = SequentialSampler(dataset)\n",
    "    eval_dataloader = DataLoader(dataset, sampler=eval_sampler, batch_size=10)\n",
    "\n",
    "    all_results = []\n",
    "\n",
    "    for batch in eval_dataloader:\n",
    "        model.eval()\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            inputs = {\n",
    "                \"input_ids\": batch[0],\n",
    "                \"attention_mask\": batch[1],\n",
    "                \"token_type_ids\": batch[2],\n",
    "            }\n",
    "\n",
    "            example_indices = batch[3]\n",
    "\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "            for i, example_index in enumerate(example_indices):\n",
    "                eval_feature = features[example_index.item()]\n",
    "                unique_id = int(eval_feature.unique_id)\n",
    "\n",
    "                output = [to_list(output[i]) for output in outputs]\n",
    "\n",
    "                start_logits, end_logits = output\n",
    "                result = SquadResult(unique_id, start_logits, end_logits)\n",
    "                all_results.append(result)\n",
    "\n",
    "    output_prediction_file = \"predictions.json\"\n",
    "    output_nbest_file = \"nbest_predictions.json\"\n",
    "    output_null_log_odds_file = \"null_predictions.json\"\n",
    "\n",
    "    predictions = compute_predictions_logits(\n",
    "        examples,\n",
    "        features,\n",
    "        all_results,\n",
    "        n_best_size,\n",
    "        max_answer_length,\n",
    "        do_lower_case,\n",
    "        output_prediction_file,\n",
    "        output_nbest_file,\n",
    "        output_null_log_odds_file,\n",
    "        False,  # verbose_logging\n",
    "        True,  # version_2_with_negative\n",
    "        null_score_diff_threshold,\n",
    "        tokenizer,\n",
    "    )\n",
    "\n",
    "    return predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
