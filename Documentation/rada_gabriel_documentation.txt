Model Auto Downloading
Our models are quite large for this project and are not conducive to be put on git hub because their size. But for simplicity sake and the ability to users to easily deploy the project the models must be auto downloaded. This is simply done through requests package in python and downloading from there origin URLs and checking to see if the path exists before downloading to avoid redownloading these large files.
Implemented Albert second stage question answering 
Albert is the second stage for the question answering. Its responsibility is to take in the relevant snippets provided by the stage one and using a pretrained model on SQUAD V2.0 dataset return what it thinks is the answer to the query over the relevant snippets. So, for each relevant snippet there will be an answer. Though that answer can be blank if Albert cannot identify a relevant answer within the snippet.
The answers are then aggregated using fuzzy string comparison. This is done using the Levenshtein’s distance, which just the number of changes it will take to match another string. The distance was implemented using a package called fuzzy wuzzy. Then the frequency of the string matching within a certain threshold (currently 70%) is aggregated within a map and the most frequent string is returned as the answer to the query. In the case of ties the longest string is returned in hopes that will be the most descriptive of the answers.
	
Modular size snippet producer and data cleaning
In ProcessText.py the function snippetProducerSplit is responsible for snippetizing the paper. Snippetizing is the separating of each section of the paper into small chunks to make it easier for the first stage processing (identifying relevant snippets) to get accurate results. The models that identify relevant snippets are more accurate the shorter the results are so snippetProducerSplit splits the paper into sentences. This is done through a regex looking at what appears before and after the periods, question marks and exclamations points to not separate decimals within number or other places periods are used. Then through a passed parameter the snippet size will be that number of sentences. So, if the parameter is three then each snippet will have three sentences. Three was the parameter that we decided would work the best for the project because its not so long that it makes the relevant snippets inaccurate but provides enough context that its more likely that stage two will return the relevant answer.
The snippetizer function also is responsible for data cleaning. It replaces words that end in periods that are not at the end of sentences. Such as et al. which would be replaced to et al without a period. It also removes the reference section from the end of the paper because our first stage model kept on identifying references as relevant snippets even though they weren’t. Also, it attempts to remove some amount of the intext citations from the paper. It does this by using a regex that looks at the pattern of words within the parenthesis and if it matches a citation pattern it removes the words within the parenthesis and the parenthesis themselves.
Deployment to Heroku
The project is hosted on Heroku. This is so others can access our application and do not have to run it on their local machine.

Attempted but failed work
For snippetizing I attempted to use spaCy sentence separation, but for our dataset especially with the amount of numbers and other special characters spaCy did not separate the sentences out very well. Therefore we decided to go with the regex approach.
I also implemented the removal of Conditional Random Fields (CRF) for the removal of references. This however was not working as expected (ie it was not removing the references and annotation for CRF would take a while) and we as the group decided to move to the use of regex which would work better within the time constraints. 
Implemented the first stage USE model to identify relevant snippets. Distillbert ended up being a more accurate model so we ended up using that, but as a proof of concept made a prototype first stage model that was responsible for the for filtering out relevant snippets.

