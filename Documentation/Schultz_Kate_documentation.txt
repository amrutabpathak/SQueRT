Implemented:
Script that scrapes arXiv for relevant pdfs given a specified query/keyword and number of results desired. It downloads the pdfs for use by other scripts. The script incorporates politeness policies and has exception handling for when no search results are returned. I also edited Main() several times to cooperate with the scraper and the Flask app.

Flask app and RESTful Swagger API for SQueRT. The API accepts user string input for a search query and keyword under the /predict endpoint. The Flask script then invokes Main() with the user input. The API has another endpoint, /feedback, that accepts user feedback about how helpful the results are. This input is given as a selection of one of three options: helpful, somewhat helpful, or unhelpful. Finally, there is a third endpoint to check the health of the system without running it end-to-end. The /predict and /feedback endpoints have POST operations, while the /health endpoint makes a GET request. There is a corresponding yaml file where the endpoints, parameters, and functions are defined and described. The functions all return JSON response objects that include informative messages and data where relevant. The API can be viewed in any browser on any OS when the script is invoked and the user goes to their local host and adds a /ui to the URL.

I also wrote a user overview and Q&A document about our tool (the SQueRT one pager) and helped with scoring result snippets with the rest of the team. 

Tried but not yet included:
I'm still troubleshooting the "fancy front end", which includes html, js, and css scripts. Troubleshooting has been very difficult given all of the obstacles I have had in attempting to run Main() as well as my utter lack of front-end experience. I very much hope to have it working soon so that I can push these scripts (hundreds of lines of code all together!!) and we can have a real webpage. 

Apps/libraries to load:
scraper: sys, BeautifulSoup, urllib, os, time, re
Flask and API: connexion, flask