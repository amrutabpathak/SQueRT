Implemented:
Script that scrapes arXiv for relevant pdfs given a specified query/keyword and number of results desired. It downloads the pdfs for use by other scripts. The script incorporates politeness policies and has exception handling for when no search results are returned. I also edited Main() several times to cooperate with the scraper and the Flask app.

Flask app and RESTful Swagger API for SQueRT. The API accepts user string input for a search query and keyword under the /predict endpoint. The Flask script then invokes Main() with the user input. The API has another endpoint, /feedback, that accepts user feedback about how helpful the results are. This input is given as a selection of one of three options: helpful, somewhat helpful, or unhelpful. Finally, there is a third endpoint to check the health of the system without running it end-to-end. The /predict and /feedback endpoints have POST operations, while the /health endpoint makes a GET request. There is a corresponding yaml file where the endpoints, parameters, and functions are defined and described. The functions all return JSON response objects that include informative messages and data where relevant. The API can be viewed in any browser on any OS when the script is invoked and the user goes to their local host and adds a /ui to the URL.

I also wrote a user overview and Q&A document about our tool (the SQueRT one pager) and helped with scoring result snippets with the rest of the team. 

I wrote html, js, and css scripts to render the API into a webpage. The webpage has a title banner with a description and an Inputs section with boxes for the user to enter an arXiv search query and keyword, with a submit button. Then there are three Output return boxes: one for the PDF link, one for snippets, and one for predictions.

Tried but not yet included:
I still have some front end functionality to work out - namely incorporating the feedback. 

Apps/libraries to load:
scraper: sys, BeautifulSoup, urllib, os, time, re
Flask and API: connexion, flask